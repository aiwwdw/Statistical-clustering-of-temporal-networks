nohup: ignoring input
  0%|          | 0/100 [00:00<?, ?it/s]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/adj_generator.py:108: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1720538439675/work/torch/csrc/utils/tensor_new.cpp:278.)
  Y = torch.tensor(Y)
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  1%|          | 1/100 [50:00<82:30:18, 3000.19s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  2%|▏         | 2/100 [1:35:33<77:24:09, 2843.36s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  3%|▎         | 3/100 [2:25:43<78:39:51, 2919.50s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  4%|▍         | 4/100 [3:15:43<78:41:52, 2951.17s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  5%|▌         | 5/100 [4:04:08<77:26:19, 2934.53s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  6%|▌         | 6/100 [4:44:52<72:16:14, 2767.81s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  7%|▋         | 7/100 [5:30:27<71:13:14, 2756.93s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  8%|▊         | 8/100 [6:16:05<70:18:03, 2750.91s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  9%|▉         | 9/100 [7:08:26<72:37:10, 2872.86s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
-----------------------------------------------------------------------------------------
This iteration is 0
True Objective function: Loss = -11593.207760278518
Iteration 0: Loss = -24262.236451746838
Iteration 10: Loss = -11601.68840550797
Iteration 20: Loss = -11573.406225859797
Iteration 30: Loss = -11573.408603988908
1
Iteration 40: Loss = -11573.40901144612
2
Iteration 50: Loss = -11573.409069586613
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7115, 0.2885],
        [0.2572, 0.7428]], dtype=torch.float64)
alpha: tensor([0.4628, 0.5372])
beta: tensor([[[0.2015, 0.1082],
         [0.1742, 0.2944]],

        [[0.6219, 0.1029],
         [0.8551, 0.2335]],

        [[0.6239, 0.1106],
         [0.2179, 0.9893]],

        [[0.1801, 0.1080],
         [0.2400, 0.3959]],

        [[0.1853, 0.1168],
         [0.7878, 0.5017]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 3
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 93
Adjusted Rand Index: 0.7369583604949977
time is 4
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.9214417611804818
Average Adjusted Rand Index: 0.9235505762252958
tensor([[-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24202.670073625737
Iteration 100: Loss = -11680.470591929969
Iteration 200: Loss = -11657.679807930772
Iteration 300: Loss = -11654.88105285424
Iteration 400: Loss = -11653.667473707137
Iteration 500: Loss = -11652.978171506798
Iteration 600: Loss = -11652.540748947022
Iteration 700: Loss = -11650.267333797485
Iteration 800: Loss = -11649.945987912106
Iteration 900: Loss = -11649.754805473143
Iteration 1000: Loss = -11649.035086219897
Iteration 1100: Loss = -11648.857223814122
Iteration 1200: Loss = -11648.777781916428
Iteration 1300: Loss = -11648.713345556707
Iteration 1400: Loss = -11648.663755915986
Iteration 1500: Loss = -11648.622336247316
Iteration 1600: Loss = -11648.588348915182
Iteration 1700: Loss = -11648.559544720329
Iteration 1800: Loss = -11648.534797406668
Iteration 1900: Loss = -11648.513036039769
Iteration 2000: Loss = -11648.493026929526
Iteration 2100: Loss = -11648.476523854792
Iteration 2200: Loss = -11648.46159022524
Iteration 2300: Loss = -11648.439538752718
Iteration 2400: Loss = -11637.248076626023
Iteration 2500: Loss = -11637.23166662472
Iteration 2600: Loss = -11637.222133848947
Iteration 2700: Loss = -11637.27332900211
1
Iteration 2800: Loss = -11637.204107562511
Iteration 2900: Loss = -11637.1993638982
Iteration 3000: Loss = -11637.19073373753
Iteration 3100: Loss = -11637.188064996448
Iteration 3200: Loss = -11637.17973917893
Iteration 3300: Loss = -11637.175292361548
Iteration 3400: Loss = -11637.170658508267
Iteration 3500: Loss = -11637.166652376474
Iteration 3600: Loss = -11637.176671030864
1
Iteration 3700: Loss = -11637.159639846688
Iteration 3800: Loss = -11637.15646988836
Iteration 3900: Loss = -11637.154699005354
Iteration 4000: Loss = -11637.150852209868
Iteration 4100: Loss = -11637.14836734104
Iteration 4200: Loss = -11637.2341036337
1
Iteration 4300: Loss = -11637.143902304106
Iteration 4400: Loss = -11637.14190309458
Iteration 4500: Loss = -11637.140078239203
Iteration 4600: Loss = -11637.138488912662
Iteration 4700: Loss = -11637.136724188822
Iteration 4800: Loss = -11637.135217245279
Iteration 4900: Loss = -11637.133837677588
Iteration 5000: Loss = -11637.132507945928
Iteration 5100: Loss = -11637.131274410078
Iteration 5200: Loss = -11637.130539815913
Iteration 5300: Loss = -11637.128981534548
Iteration 5400: Loss = -11637.127972523243
Iteration 5500: Loss = -11637.127289320058
Iteration 5600: Loss = -11637.1261217311
Iteration 5700: Loss = -11637.125360437914
Iteration 5800: Loss = -11637.124511813292
Iteration 5900: Loss = -11637.123742591848
Iteration 6000: Loss = -11637.316146733683
1
Iteration 6100: Loss = -11637.122358161632
Iteration 6200: Loss = -11637.121723539653
Iteration 6300: Loss = -11637.136319147814
1
Iteration 6400: Loss = -11637.120578605753
Iteration 6500: Loss = -11637.120060105955
Iteration 6600: Loss = -11637.120191574815
1
Iteration 6700: Loss = -11637.11907644222
Iteration 6800: Loss = -11637.118645717845
Iteration 6900: Loss = -11637.118163525573
Iteration 7000: Loss = -11637.117710582732
Iteration 7100: Loss = -11637.371031248484
1
Iteration 7200: Loss = -11637.117026993887
Iteration 7300: Loss = -11637.116728169385
Iteration 7400: Loss = -11637.122494409936
1
Iteration 7500: Loss = -11637.115728414423
Iteration 7600: Loss = -11637.115508184952
Iteration 7700: Loss = -11637.115205556442
Iteration 7800: Loss = -11637.114952511582
Iteration 7900: Loss = -11637.114730567559
Iteration 8000: Loss = -11637.11446729658
Iteration 8100: Loss = -11637.144559575214
1
Iteration 8200: Loss = -11637.114031599367
Iteration 8300: Loss = -11637.113845615562
Iteration 8400: Loss = -11637.114225987887
1
Iteration 8500: Loss = -11637.113501145588
Iteration 8600: Loss = -11637.113352443981
Iteration 8700: Loss = -11637.113698065417
1
Iteration 8800: Loss = -11637.113042231387
Iteration 8900: Loss = -11637.109459527748
Iteration 9000: Loss = -11637.110059661229
1
Iteration 9100: Loss = -11637.109046590678
Iteration 9200: Loss = -11637.130477859579
1
Iteration 9300: Loss = -11637.10874979859
Iteration 9400: Loss = -11637.112307833715
1
Iteration 9500: Loss = -11637.108545633366
Iteration 9600: Loss = -11637.108444075644
Iteration 9700: Loss = -11637.118552130612
1
Iteration 9800: Loss = -11637.108277970161
Iteration 9900: Loss = -11637.108201643265
Iteration 10000: Loss = -11637.1114329034
1
Iteration 10100: Loss = -11637.107921135244
Iteration 10200: Loss = -11637.289652854846
1
Iteration 10300: Loss = -11637.107557972136
Iteration 10400: Loss = -11637.107509517367
Iteration 10500: Loss = -11637.110177782968
1
Iteration 10600: Loss = -11637.107390075855
Iteration 10700: Loss = -11637.26845295903
1
Iteration 10800: Loss = -11637.107293379662
Iteration 10900: Loss = -11637.107240735068
Iteration 11000: Loss = -11637.107402448879
1
Iteration 11100: Loss = -11637.107180602698
Iteration 11200: Loss = -11637.200440304106
1
Iteration 11300: Loss = -11637.107072611889
Iteration 11400: Loss = -11637.107058469861
Iteration 11500: Loss = -11637.109450122083
1
Iteration 11600: Loss = -11637.106973204987
Iteration 11700: Loss = -11637.106920645956
Iteration 11800: Loss = -11637.106971524861
1
Iteration 11900: Loss = -11637.106868111394
Iteration 12000: Loss = -11637.11484122281
1
Iteration 12100: Loss = -11637.106806396401
Iteration 12200: Loss = -11637.114923121926
1
Iteration 12300: Loss = -11637.106827278192
2
Iteration 12400: Loss = -11637.106731128877
Iteration 12500: Loss = -11637.198799346885
1
Iteration 12600: Loss = -11637.106726306207
Iteration 12700: Loss = -11637.106673135186
Iteration 12800: Loss = -11637.106763255522
1
Iteration 12900: Loss = -11637.106610006029
Iteration 13000: Loss = -11637.11424935601
1
Iteration 13100: Loss = -11637.106472253345
Iteration 13200: Loss = -11637.289713471944
1
Iteration 13300: Loss = -11637.103955518294
Iteration 13400: Loss = -11637.43944860043
1
Iteration 13500: Loss = -11637.103905230373
Iteration 13600: Loss = -11637.104577644332
1
Iteration 13700: Loss = -11637.103908540665
2
Iteration 13800: Loss = -11637.103840279256
Iteration 13900: Loss = -11637.104928823668
1
Iteration 14000: Loss = -11637.103838123128
Iteration 14100: Loss = -11637.157758003003
1
Iteration 14200: Loss = -11637.103829065532
Iteration 14300: Loss = -11637.10379655275
Iteration 14400: Loss = -11637.104736354431
1
Iteration 14500: Loss = -11637.103740682847
Iteration 14600: Loss = -11637.224872596014
1
Iteration 14700: Loss = -11637.103723889551
Iteration 14800: Loss = -11637.103693731453
Iteration 14900: Loss = -11637.10366408711
Iteration 15000: Loss = -11637.103319301059
Iteration 15100: Loss = -11636.963293093679
Iteration 15200: Loss = -11636.300005832403
Iteration 15300: Loss = -11636.160072186649
Iteration 15400: Loss = -11629.0395367085
Iteration 15500: Loss = -11625.547192656757
Iteration 15600: Loss = -11618.481805769427
Iteration 15700: Loss = -11606.514627664104
Iteration 15800: Loss = -11605.48433507789
Iteration 15900: Loss = -11594.629714127032
Iteration 16000: Loss = -11594.392968306354
Iteration 16100: Loss = -11593.710982093357
Iteration 16200: Loss = -11579.963651733004
Iteration 16300: Loss = -11579.939845735023
Iteration 16400: Loss = -11575.12587350475
Iteration 16500: Loss = -11575.048553336956
Iteration 16600: Loss = -11575.043709898737
Iteration 16700: Loss = -11575.048958427804
1
Iteration 16800: Loss = -11575.042018756048
Iteration 16900: Loss = -11575.036482599433
Iteration 17000: Loss = -11575.036180374755
Iteration 17100: Loss = -11575.066305181597
1
Iteration 17200: Loss = -11575.043437135311
2
Iteration 17300: Loss = -11575.090073682875
3
Iteration 17400: Loss = -11575.036582643077
4
Iteration 17500: Loss = -11575.035335566816
Iteration 17600: Loss = -11575.034351215714
Iteration 17700: Loss = -11575.017047839605
Iteration 17800: Loss = -11574.911465937163
Iteration 17900: Loss = -11574.912063659056
1
Iteration 18000: Loss = -11574.94118027691
2
Iteration 18100: Loss = -11574.911106319312
Iteration 18200: Loss = -11569.379835464691
Iteration 18300: Loss = -11569.38879791069
1
Iteration 18400: Loss = -11569.24591842481
Iteration 18500: Loss = -11569.201166474157
Iteration 18600: Loss = -11569.201500183945
1
Iteration 18700: Loss = -11569.2103421596
2
Iteration 18800: Loss = -11569.200343838385
Iteration 18900: Loss = -11569.20031832163
Iteration 19000: Loss = -11569.200671442673
1
Iteration 19100: Loss = -11569.200296284334
Iteration 19200: Loss = -11569.200111401502
Iteration 19300: Loss = -11569.199838760926
Iteration 19400: Loss = -11569.199369306816
Iteration 19500: Loss = -11569.1761226147
Iteration 19600: Loss = -11569.176806596639
1
Iteration 19700: Loss = -11569.176007825572
Iteration 19800: Loss = -11569.202219420322
1
Iteration 19900: Loss = -11569.175997251985
tensor([[ -5.6612,   1.0459],
        [ -6.4581,   1.8429],
        [ -6.0633,   1.4481],
        [  2.6997,  -7.3149],
        [ -4.8016,   0.1864],
        [  6.1918, -10.8070],
        [  2.7766,  -7.3918],
        [ -4.3844,  -0.2308],
        [  0.9089,  -5.5241],
        [ -9.1849,   4.5696],
        [  0.6165,  -5.2317],
        [ -8.1008,   3.4856],
        [ -4.8307,   0.2155],
        [ -0.7102,  -3.9050],
        [  2.8045,  -7.4197],
        [  0.8294,  -5.4446],
        [ -8.6418,   4.0266],
        [ -6.1307,   1.5155],
        [  0.7487,  -5.3639],
        [ -7.1902,   2.5750],
        [ -5.1939,   0.5786],
        [ -9.1960,   4.5808],
        [  2.1515,  -6.7667],
        [ -7.1216,   2.5063],
        [ -6.7238,   2.1085],
        [ -4.7215,   0.1063],
        [ -8.8367,   4.2214],
        [ -4.2468,  -0.3685],
        [ -0.2661,  -4.3492],
        [  2.8451,  -7.4604],
        [ -4.0973,  -0.5179],
        [ -8.9942,   4.3790],
        [ -6.4200,   1.8047],
        [  2.4966,  -7.1118],
        [  3.7577,  -8.3729],
        [  1.5627,  -6.1779],
        [  2.1543,  -6.7696],
        [ -5.4906,   0.8754],
        [ -6.8593,   2.2441],
        [ -8.4596,   3.8444],
        [ -4.8351,   0.2198],
        [ -9.2198,   4.6046],
        [ -4.4320,  -0.1832],
        [ -8.7721,   4.1569],
        [ -7.2214,   2.6062],
        [  0.6315,  -5.2467],
        [  2.9966,  -7.6118],
        [  1.2353,  -5.8505],
        [  5.2401,  -9.8553],
        [ -7.9370,   3.3218],
        [ -9.1817,   4.5665],
        [  5.6511, -10.2663],
        [  0.6815,  -5.2967],
        [ -4.1645,  -0.4507],
        [ -8.6798,   4.0645],
        [ -9.1297,   4.5145],
        [ -5.0883,   0.4731],
        [ -8.1761,   3.5608],
        [  4.1017,  -8.7169],
        [  5.0650,  -9.6802],
        [ -7.3198,   2.7045],
        [ -7.1891,   2.5738],
        [ -8.6073,   3.9921],
        [  2.3626,  -6.9778],
        [  0.8142,  -5.4295],
        [  1.9980,  -6.6132],
        [ -9.1095,   4.4942],
        [  0.3834,  -4.9986],
        [ -6.8357,   2.2205],
        [ -9.8061,   5.1909],
        [  1.0208,  -5.6360],
        [  4.1029,  -8.7182],
        [ -9.0892,   4.4740],
        [  6.7352, -11.3504],
        [  4.0965,  -8.7117],
        [  2.2213,  -6.8365],
        [  5.5676, -10.1829],
        [ -9.7404,   5.1252],
        [ -8.0792,   3.4640],
        [  4.3352,  -8.9505],
        [ -7.5444,   2.9292],
        [ -4.1333,  -0.4819],
        [ -8.9454,   4.3302],
        [ -1.2648,  -3.3504],
        [ -8.4432,   3.8280],
        [  2.2301,  -6.8453],
        [ -1.1259,  -3.4893],
        [ -9.2337,   4.6185],
        [ -8.9251,   4.3099],
        [  5.5137, -10.1289],
        [ -7.9695,   3.3543],
        [ -9.7187,   5.1035],
        [ -5.9613,   1.3461],
        [  1.5957,  -6.2109],
        [ -8.1871,   3.5718],
        [  5.2868,  -9.9021],
        [ -9.7311,   5.1159],
        [ -8.0182,   3.4030],
        [ -0.8586,  -3.7567],
        [ -0.8430,  -3.7722]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7291, 0.2709],
        [0.2359, 0.7641]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4253, 0.5747], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2074, 0.1080],
         [0.1742, 0.2982]],

        [[0.6219, 0.1020],
         [0.8551, 0.2335]],

        [[0.6239, 0.1088],
         [0.2179, 0.9893]],

        [[0.1801, 0.1090],
         [0.2400, 0.3959]],

        [[0.1853, 0.1163],
         [0.7878, 0.5017]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208031161549398
time is 3
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 94
Adjusted Rand Index: 0.7721212121212121
time is 4
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.9291531268164688
Average Adjusted Rand Index: 0.9305836430625403
Iteration 0: Loss = -18581.2103487282
Iteration 10: Loss = -11578.168176266314
Iteration 20: Loss = -11573.51731991107
Iteration 30: Loss = -11573.412217787416
Iteration 40: Loss = -11573.409121612514
Iteration 50: Loss = -11573.40905076296
Iteration 60: Loss = -11573.409056306182
1
Iteration 70: Loss = -11573.409065645292
2
Iteration 80: Loss = -11573.40905870862
3
Stopping early at iteration 79 due to no improvement.
pi: tensor([[0.7115, 0.2885],
        [0.2572, 0.7428]], dtype=torch.float64)
alpha: tensor([0.4628, 0.5372])
beta: tensor([[[0.2015, 0.1082],
         [0.1929, 0.2944]],

        [[0.1366, 0.1029],
         [0.5713, 0.2351]],

        [[0.4333, 0.1106],
         [0.8778, 0.7886]],

        [[0.1615, 0.1080],
         [0.3489, 0.4004]],

        [[0.2827, 0.1168],
         [0.6201, 0.9195]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 3
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 93
Adjusted Rand Index: 0.7369583604949977
time is 4
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.9214417611804818
Average Adjusted Rand Index: 0.9235505762252958
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18580.9046387066
Iteration 100: Loss = -11861.121792594271
Iteration 200: Loss = -11841.539965019103
Iteration 300: Loss = -11838.716381209566
Iteration 400: Loss = -11837.870995469024
Iteration 500: Loss = -11837.320504286005
Iteration 600: Loss = -11836.690219878217
Iteration 700: Loss = -11834.098164418787
Iteration 800: Loss = -11777.878467074113
Iteration 900: Loss = -11674.696238500848
Iteration 1000: Loss = -11638.310174693499
Iteration 1100: Loss = -11637.088710705024
Iteration 1200: Loss = -11626.62646526368
Iteration 1300: Loss = -11622.547640703107
Iteration 1400: Loss = -11620.330139671236
Iteration 1500: Loss = -11619.308980259339
Iteration 1600: Loss = -11612.738346003243
Iteration 1700: Loss = -11612.70141598411
Iteration 1800: Loss = -11612.154352848716
Iteration 1900: Loss = -11612.14281816492
Iteration 2000: Loss = -11612.133360035637
Iteration 2100: Loss = -11612.120001511572
Iteration 2200: Loss = -11611.546921060064
Iteration 2300: Loss = -11611.176617713929
Iteration 2400: Loss = -11611.172041589753
Iteration 2500: Loss = -11611.168482054629
Iteration 2600: Loss = -11611.164521022207
Iteration 2700: Loss = -11611.15724680161
Iteration 2800: Loss = -11611.133460542014
Iteration 2900: Loss = -11607.8187118498
Iteration 3000: Loss = -11606.637023150402
Iteration 3100: Loss = -11600.203302905014
Iteration 3200: Loss = -11595.374984255055
Iteration 3300: Loss = -11595.367907045626
Iteration 3400: Loss = -11595.36164819528
Iteration 3500: Loss = -11591.916728406588
Iteration 3600: Loss = -11587.266040498833
Iteration 3700: Loss = -11587.262508797152
Iteration 3800: Loss = -11587.257184473043
Iteration 3900: Loss = -11587.250944928708
Iteration 4000: Loss = -11587.24525735645
Iteration 4100: Loss = -11587.241291121196
Iteration 4200: Loss = -11584.61587951824
Iteration 4300: Loss = -11584.614504389636
Iteration 4400: Loss = -11584.611032668985
Iteration 4500: Loss = -11581.408044981645
Iteration 4600: Loss = -11581.407091665962
Iteration 4700: Loss = -11581.41036895754
1
Iteration 4800: Loss = -11581.405886132954
Iteration 4900: Loss = -11581.405080475326
Iteration 5000: Loss = -11581.408236226018
1
Iteration 5100: Loss = -11581.403678829935
Iteration 5200: Loss = -11581.402971031677
Iteration 5300: Loss = -11581.401844333537
Iteration 5400: Loss = -11581.400240943041
Iteration 5500: Loss = -11581.398588433138
Iteration 5600: Loss = -11581.375899340812
Iteration 5700: Loss = -11580.451567821772
Iteration 5800: Loss = -11573.90981615852
Iteration 5900: Loss = -11573.909434134725
Iteration 6000: Loss = -11573.90918705059
Iteration 6100: Loss = -11573.908934673806
Iteration 6200: Loss = -11573.911941844439
1
Iteration 6300: Loss = -11573.862125674104
Iteration 6400: Loss = -11573.86189224323
Iteration 6500: Loss = -11573.86187960334
Iteration 6600: Loss = -11573.861723214632
Iteration 6700: Loss = -11573.863843817486
1
Iteration 6800: Loss = -11573.864540201363
2
Iteration 6900: Loss = -11573.886986117577
3
Iteration 7000: Loss = -11573.86080100406
Iteration 7100: Loss = -11573.886142844913
1
Iteration 7200: Loss = -11573.860225347213
Iteration 7300: Loss = -11573.83906587512
Iteration 7400: Loss = -11573.83846297172
Iteration 7500: Loss = -11573.837530914345
Iteration 7600: Loss = -11573.8317990075
Iteration 7700: Loss = -11573.837885064402
1
Iteration 7800: Loss = -11573.83171302968
Iteration 7900: Loss = -11573.831644261592
Iteration 8000: Loss = -11573.832935146966
1
Iteration 8100: Loss = -11573.835156774052
2
Iteration 8200: Loss = -11573.831714669997
3
Iteration 8300: Loss = -11573.834411613356
4
Iteration 8400: Loss = -11573.831814598265
5
Iteration 8500: Loss = -11573.827626325205
Iteration 8600: Loss = -11573.8275808851
Iteration 8700: Loss = -11573.829556084487
1
Iteration 8800: Loss = -11573.827430396532
Iteration 8900: Loss = -11573.827376987241
Iteration 9000: Loss = -11573.827429225059
1
Iteration 9100: Loss = -11573.827244876242
Iteration 9200: Loss = -11572.951836449021
Iteration 9300: Loss = -11569.170256466401
Iteration 9400: Loss = -11569.170645930055
1
Iteration 9500: Loss = -11569.170112427675
Iteration 9600: Loss = -11569.168888425222
Iteration 9700: Loss = -11569.169244170513
1
Iteration 9800: Loss = -11569.168613362035
Iteration 9900: Loss = -11569.179605210484
1
Iteration 10000: Loss = -11569.168424022451
Iteration 10100: Loss = -11569.217346335758
1
Iteration 10200: Loss = -11569.168382201198
Iteration 10300: Loss = -11569.195698227608
1
Iteration 10400: Loss = -11569.17043719477
2
Iteration 10500: Loss = -11569.168385091287
3
Iteration 10600: Loss = -11569.168645021005
4
Iteration 10700: Loss = -11569.179784800306
5
Iteration 10800: Loss = -11569.168353088238
Iteration 10900: Loss = -11569.168564822381
1
Iteration 11000: Loss = -11569.16836117478
2
Iteration 11100: Loss = -11569.168865872267
3
Iteration 11200: Loss = -11569.168304549126
Iteration 11300: Loss = -11569.175659895838
1
Iteration 11400: Loss = -11569.168230814468
Iteration 11500: Loss = -11569.16816073463
Iteration 11600: Loss = -11569.1682337778
1
Iteration 11700: Loss = -11569.16809572775
Iteration 11800: Loss = -11569.4481179922
1
Iteration 11900: Loss = -11569.16811732439
2
Iteration 12000: Loss = -11569.168087467411
Iteration 12100: Loss = -11569.171251793492
1
Iteration 12200: Loss = -11569.168085242934
Iteration 12300: Loss = -11569.168250576848
1
Iteration 12400: Loss = -11569.172194375164
2
Iteration 12500: Loss = -11569.165637927323
Iteration 12600: Loss = -11569.167244320659
1
Iteration 12700: Loss = -11569.165647280244
2
Iteration 12800: Loss = -11569.16918995195
3
Iteration 12900: Loss = -11569.17973998218
4
Iteration 13000: Loss = -11569.286217888213
5
Iteration 13100: Loss = -11569.167399418804
6
Iteration 13200: Loss = -11569.16553788176
Iteration 13300: Loss = -11569.197094102981
1
Iteration 13400: Loss = -11569.165471129862
Iteration 13500: Loss = -11569.192887612686
1
Iteration 13600: Loss = -11569.164954981805
Iteration 13700: Loss = -11569.164893392823
Iteration 13800: Loss = -11569.165460169632
1
Iteration 13900: Loss = -11569.164885158261
Iteration 14000: Loss = -11569.164855583944
Iteration 14100: Loss = -11569.165191001977
1
Iteration 14200: Loss = -11569.164846482481
Iteration 14300: Loss = -11569.164811329501
Iteration 14400: Loss = -11569.165105706208
1
Iteration 14500: Loss = -11569.278061325202
2
Iteration 14600: Loss = -11569.164658323874
Iteration 14700: Loss = -11569.356406471461
1
Iteration 14800: Loss = -11569.15807493188
Iteration 14900: Loss = -11569.158052215198
Iteration 15000: Loss = -11569.158238842332
1
Iteration 15100: Loss = -11569.158037071013
Iteration 15200: Loss = -11569.40430188837
1
Iteration 15300: Loss = -11569.158088638182
2
Iteration 15400: Loss = -11569.158082415657
3
Iteration 15500: Loss = -11569.237677310324
4
Iteration 15600: Loss = -11569.158054216154
5
Iteration 15700: Loss = -11569.262446540662
6
Iteration 15800: Loss = -11569.158055465583
7
Iteration 15900: Loss = -11569.15814661099
8
Iteration 16000: Loss = -11569.158148952567
9
Iteration 16100: Loss = -11569.168618977988
10
Stopping early at iteration 16100 due to no improvement.
tensor([[ -4.0443,   2.6555],
        [ -4.8388,   3.4525],
        [ -5.0695,   2.4335],
        [  4.3098,  -5.6984],
        [ -3.3703,   1.6083],
        [  7.3148,  -8.7258],
        [  3.5912,  -6.5692],
        [ -2.7671,   1.3734],
        [  2.5118,  -3.9092],
        [ -8.8077,   7.1765],
        [  2.0300,  -3.7989],
        [ -6.5378,   5.0813],
        [ -3.2179,   1.8274],
        [  0.2283,  -2.9543],
        [  5.4230, -10.0382],
        [  7.1681,  -8.6593],
        [ -7.6418,   5.4047],
        [ -4.5844,   3.0542],
        [  0.7416,  -5.3568],
        [ -5.6681,   4.0918],
        [ -3.5761,   2.1841],
        [ -7.2988,   5.3519],
        [  3.7656,  -5.1607],
        [ -5.7633,   3.8560],
        [ -5.1678,   3.6610],
        [ -3.1118,   1.7118],
        [ -8.7024,   4.8216],
        [ -2.6952,   1.1887],
        [  0.3561,  -3.7269],
        [  4.4075,  -5.8838],
        [ -2.8806,   0.7050],
        [ -6.2431,   4.6133],
        [ -4.8041,   3.4175],
        [  4.0621,  -5.5421],
        [  7.2646,  -8.8294],
        [  3.1382,  -4.5986],
        [  6.5289,  -9.6801],
        [ -3.9098,   2.4598],
        [ -5.3024,   3.7925],
        [ -7.2308,   5.8080],
        [ -3.2559,   1.7994],
        [ -6.0179,   3.5061],
        [ -2.9071,   1.3397],
        [ -4.1824,   2.6647],
        [ -6.4837,   3.3332],
        [  2.2349,  -3.6366],
        [  6.6860,  -8.0800],
        [  1.2256,  -5.8408],
        [  6.6373,  -8.6425],
        [ -6.6428,   4.6156],
        [ -3.8391,   2.0907],
        [  7.8270,  -9.4824],
        [  2.1779,  -3.8069],
        [ -2.5721,   1.1425],
        [ -7.2623,   5.2504],
        [ -8.2796,   6.4802],
        [ -3.5021,   2.0536],
        [ -6.6519,   5.1268],
        [  7.8775, -11.0331],
        [  5.6980,  -8.6469],
        [ -6.0196,   3.9813],
        [ -5.6580,   4.0930],
        [ -7.8563,   4.9059],
        [  3.9741,  -5.3604],
        [  1.7649,  -4.4672],
        [  3.6017,  -5.0010],
        [ -4.9266,   3.4393],
        [  1.9853,  -3.3718],
        [ -5.7104,   3.3357],
        [ -9.8340,   8.1048],
        [  2.6340,  -4.0208],
        [  5.6089,  -7.2145],
        [ -8.2176,   6.5106],
        [  8.1891,  -9.9167],
        [  5.9788,  -7.3653],
        [  7.1424,  -9.1106],
        [  8.3707,  -9.9666],
        [ -9.0806,   7.5537],
        [ -6.4877,   5.0521],
        [  8.1599,  -9.5463],
        [ -6.2380,   4.2441],
        [ -2.8059,   0.8416],
        [ -3.5449,   2.1080],
        [  0.3426,  -1.7295],
        [ -7.0198,   5.4120],
        [  3.7787,  -5.2949],
        [  0.4568,  -1.8991],
        [ -6.3040,   4.2657],
        [ -8.4211,   6.3470],
        [  7.0744,  -8.4791],
        [ -6.4036,   4.9560],
        [-10.8491,   7.8961],
        [ -4.3968,   2.9067],
        [  3.1141,  -4.6858],
        [ -6.6553,   5.1273],
        [  1.4265,  -4.3123],
        [-10.0150,   8.1167],
        [ -4.2155,   1.1958],
        [  0.7293,  -2.1514],
        [  0.6162,  -2.2985]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7313, 0.2687],
        [0.2386, 0.7614]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4266, 0.5734], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2063, 0.1079],
         [0.1929, 0.2996]],

        [[0.1366, 0.1027],
         [0.5713, 0.2351]],

        [[0.4333, 0.1095],
         [0.8778, 0.7886]],

        [[0.1615, 0.1096],
         [0.3489, 0.4004]],

        [[0.2827, 0.1171],
         [0.6201, 0.9195]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208031161549398
time is 3
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 94
Adjusted Rand Index: 0.7721212121212121
time is 4
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.9291531268164688
Average Adjusted Rand Index: 0.9305836430625403
Iteration 0: Loss = -15131.899305951674
Iteration 10: Loss = -11635.465285651737
Iteration 20: Loss = -11573.647599765203
Iteration 30: Loss = -11573.416062197446
Iteration 40: Loss = -11573.409262291303
Iteration 50: Loss = -11573.409059386186
Iteration 60: Loss = -11573.409051075472
Iteration 70: Loss = -11573.40906529884
1
Iteration 80: Loss = -11573.409065298945
2
Iteration 90: Loss = -11573.409065298945
3
Stopping early at iteration 89 due to no improvement.
pi: tensor([[0.7428, 0.2572],
        [0.2885, 0.7115]], dtype=torch.float64)
alpha: tensor([0.5372, 0.4628])
beta: tensor([[[0.2944, 0.1082],
         [0.7229, 0.2015]],

        [[0.7939, 0.1029],
         [0.7062, 0.6600]],

        [[0.0605, 0.1106],
         [0.1396, 0.0837]],

        [[0.1030, 0.1080],
         [0.5891, 0.4663]],

        [[0.5539, 0.1168],
         [0.8746, 0.6719]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 3
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 7
Adjusted Rand Index: 0.7369583604949977
time is 4
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.9214417611804818
Average Adjusted Rand Index: 0.9235505762252958
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -15131.652993095202
Iteration 100: Loss = -11646.305351049125
Iteration 200: Loss = -11570.676547722249
Iteration 300: Loss = -11569.695111388868
Iteration 400: Loss = -11569.424343268334
Iteration 500: Loss = -11569.331066353416
Iteration 600: Loss = -11569.2807611515
Iteration 700: Loss = -11569.249891149051
Iteration 800: Loss = -11569.229323923284
Iteration 900: Loss = -11569.214883853689
Iteration 1000: Loss = -11569.204306489119
Iteration 1100: Loss = -11569.196297413244
Iteration 1200: Loss = -11569.190160130742
Iteration 1300: Loss = -11569.185367272295
Iteration 1400: Loss = -11569.181510079397
Iteration 1500: Loss = -11569.17833410278
Iteration 1600: Loss = -11569.175738758164
Iteration 1700: Loss = -11569.173566456973
Iteration 1800: Loss = -11569.171692348089
Iteration 1900: Loss = -11569.170083694335
Iteration 2000: Loss = -11569.16869038596
Iteration 2100: Loss = -11569.167492301152
Iteration 2200: Loss = -11569.166510083358
Iteration 2300: Loss = -11569.16557041433
Iteration 2400: Loss = -11569.164753131521
Iteration 2500: Loss = -11569.164095346156
Iteration 2600: Loss = -11569.163432343827
Iteration 2700: Loss = -11569.16286166107
Iteration 2800: Loss = -11569.162374201345
Iteration 2900: Loss = -11569.161875101905
Iteration 3000: Loss = -11569.161442532353
Iteration 3100: Loss = -11569.160793427458
Iteration 3200: Loss = -11569.160424298152
Iteration 3300: Loss = -11569.160152273986
Iteration 3400: Loss = -11569.159861966666
Iteration 3500: Loss = -11569.15961752037
Iteration 3600: Loss = -11569.159427449766
Iteration 3700: Loss = -11569.159241499558
Iteration 3800: Loss = -11569.159048241432
Iteration 3900: Loss = -11569.15884413381
Iteration 4000: Loss = -11569.158689150867
Iteration 4100: Loss = -11569.158569163696
Iteration 4200: Loss = -11569.158412802673
Iteration 4300: Loss = -11569.158294074843
Iteration 4400: Loss = -11569.158189199181
Iteration 4500: Loss = -11569.158046350385
Iteration 4600: Loss = -11569.157989068215
Iteration 4700: Loss = -11569.157980216256
Iteration 4800: Loss = -11569.157754604425
Iteration 4900: Loss = -11569.15772300585
Iteration 5000: Loss = -11569.161380361957
1
Iteration 5100: Loss = -11569.157687703313
Iteration 5200: Loss = -11569.159085844996
1
Iteration 5300: Loss = -11569.15726256882
Iteration 5400: Loss = -11569.15770527871
1
Iteration 5500: Loss = -11569.158888346994
2
Iteration 5600: Loss = -11569.157093194919
Iteration 5700: Loss = -11569.157340079335
1
Iteration 5800: Loss = -11569.157324615624
2
Iteration 5900: Loss = -11569.159059708585
3
Iteration 6000: Loss = -11569.157574078847
4
Iteration 6100: Loss = -11569.156810097089
Iteration 6200: Loss = -11569.156895211347
1
Iteration 6300: Loss = -11569.198068217933
2
Iteration 6400: Loss = -11569.157739855298
3
Iteration 6500: Loss = -11569.182300794575
4
Iteration 6600: Loss = -11569.156843223467
5
Iteration 6700: Loss = -11569.169601480397
6
Iteration 6800: Loss = -11569.15661837845
Iteration 6900: Loss = -11569.1565607933
Iteration 7000: Loss = -11569.15654928908
Iteration 7100: Loss = -11569.156519612347
Iteration 7200: Loss = -11569.15649714106
Iteration 7300: Loss = -11569.190512699233
1
Iteration 7400: Loss = -11569.15645056106
Iteration 7500: Loss = -11569.156435293613
Iteration 7600: Loss = -11569.16334197296
1
Iteration 7700: Loss = -11569.15526419345
Iteration 7800: Loss = -11569.155225316954
Iteration 7900: Loss = -11569.156881110524
1
Iteration 8000: Loss = -11569.155181709019
Iteration 8100: Loss = -11569.155135065379
Iteration 8200: Loss = -11569.15523576472
1
Iteration 8300: Loss = -11569.15514183773
2
Iteration 8400: Loss = -11569.15732136876
3
Iteration 8500: Loss = -11569.155107036593
Iteration 8600: Loss = -11569.159591308695
1
Iteration 8700: Loss = -11569.171177476312
2
Iteration 8800: Loss = -11569.155089291093
Iteration 8900: Loss = -11569.18345018772
1
Iteration 9000: Loss = -11569.15505045009
Iteration 9100: Loss = -11569.155053707796
1
Iteration 9200: Loss = -11569.155676768854
2
Iteration 9300: Loss = -11569.155039953781
Iteration 9400: Loss = -11569.15502362623
Iteration 9500: Loss = -11569.156126518374
1
Iteration 9600: Loss = -11569.15497872095
Iteration 9700: Loss = -11569.15501122362
1
Iteration 9800: Loss = -11569.178526960704
2
Iteration 9900: Loss = -11569.154938959953
Iteration 10000: Loss = -11569.15494410751
1
Iteration 10100: Loss = -11569.160318893179
2
Iteration 10200: Loss = -11569.154966715303
3
Iteration 10300: Loss = -11569.154999413457
4
Iteration 10400: Loss = -11569.158248224276
5
Iteration 10500: Loss = -11569.179460163949
6
Iteration 10600: Loss = -11569.154929650447
Iteration 10700: Loss = -11569.155665875858
1
Iteration 10800: Loss = -11569.15495314255
2
Iteration 10900: Loss = -11569.155084807391
3
Iteration 11000: Loss = -11569.15492097254
Iteration 11100: Loss = -11569.154932860183
1
Iteration 11200: Loss = -11569.155265353374
2
Iteration 11300: Loss = -11569.15494928025
3
Iteration 11400: Loss = -11569.154921557478
4
Iteration 11500: Loss = -11569.155086873534
5
Iteration 11600: Loss = -11569.154897097158
Iteration 11700: Loss = -11569.154910083982
1
Iteration 11800: Loss = -11569.15505969173
2
Iteration 11900: Loss = -11569.154948216026
3
Iteration 12000: Loss = -11569.306440230592
4
Iteration 12100: Loss = -11569.15488193634
Iteration 12200: Loss = -11569.157565605436
1
Iteration 12300: Loss = -11569.154915682408
2
Iteration 12400: Loss = -11569.15546054583
3
Iteration 12500: Loss = -11569.154882083343
4
Iteration 12600: Loss = -11569.164533248611
5
Iteration 12700: Loss = -11569.154871453024
Iteration 12800: Loss = -11569.154882240857
1
Iteration 12900: Loss = -11569.15495647791
2
Iteration 13000: Loss = -11569.154878228552
3
Iteration 13100: Loss = -11569.15495105361
4
Iteration 13200: Loss = -11569.154901798942
5
Iteration 13300: Loss = -11569.154852157117
Iteration 13400: Loss = -11569.155924262392
1
Iteration 13500: Loss = -11569.154902395365
2
Iteration 13600: Loss = -11569.155351881347
3
Iteration 13700: Loss = -11569.175001684913
4
Iteration 13800: Loss = -11569.15552061357
5
Iteration 13900: Loss = -11569.154994944778
6
Iteration 14000: Loss = -11569.155530118765
7
Iteration 14100: Loss = -11569.159705920216
8
Iteration 14200: Loss = -11569.154995506553
9
Iteration 14300: Loss = -11569.155781563704
10
Stopping early at iteration 14300 due to no improvement.
tensor([[ 1.7160e+00, -4.9840e+00],
        [ 3.3358e+00, -4.9557e+00],
        [ 3.0537e+00, -4.4502e+00],
        [-5.6998e+00,  4.3084e+00],
        [ 1.6634e+00, -3.3175e+00],
        [-6.4401e+00,  4.8310e+00],
        [-6.0851e+00,  4.0720e+00],
        [ 1.0047e+00, -3.1363e+00],
        [-3.9815e+00,  2.4437e+00],
        [ 6.7351e+00, -9.3401e+00],
        [-3.6923e+00,  2.1459e+00],
        [ 4.7220e+00, -6.9005e+00],
        [ 1.7872e+00, -3.2595e+00],
        [-2.3435e+00,  8.4332e-01],
        [-5.8070e+00,  4.4075e+00],
        [-3.8355e+00,  2.4349e+00],
        [ 5.3942e+00, -7.6726e+00],
        [ 2.0051e+00, -5.6332e+00],
        [-3.7551e+00,  2.3501e+00],
        [ 4.0035e+00, -5.7556e+00],
        [ 1.7767e+00, -3.9830e+00],
        [ 5.5580e+00, -7.0914e+00],
        [-5.8210e+00,  3.1022e+00],
        [ 3.8855e+00, -5.7330e+00],
        [ 2.6815e+00, -6.1459e+00],
        [ 7.3073e-01, -4.0968e+00],
        [ 6.0633e+00, -7.4619e+00],
        [ 1.1731e+00, -2.7072e+00],
        [-3.9496e+00,  1.3018e-01],
        [-5.8847e+00,  4.4066e+00],
        [ 8.5910e-01, -2.7316e+00],
        [ 4.6483e+00, -6.2071e+00],
        [ 3.3909e+00, -4.8310e+00],
        [-5.5341e+00,  4.0669e+00],
        [-6.8297e+00,  5.2873e+00],
        [-4.5689e+00,  3.1663e+00],
        [-5.4047e+00,  3.5101e+00],
        [ 2.2044e+00, -4.1616e+00],
        [ 3.5820e+00, -5.5120e+00],
        [ 5.1926e+00, -7.8593e+00],
        [ 1.5118e+00, -3.5452e+00],
        [ 3.9826e+00, -5.5412e+00],
        [ 5.4230e-01, -3.7049e+00],
        [ 2.7224e+00, -4.1266e+00],
        [ 4.0536e+00, -5.7630e+00],
        [-4.1776e+00,  1.6938e+00],
        [-6.3117e+00,  4.2986e+00],
        [-4.6124e+00,  2.4616e+00],
        [-5.7479e+00,  4.3068e+00],
        [ 4.8349e+00, -6.4207e+00],
        [ 2.2202e+00, -3.7089e+00],
        [-7.7028e+00,  5.8826e+00],
        [-3.6884e+00,  2.3003e+00],
        [ 1.1485e+00, -2.5693e+00],
        [ 5.4638e+00, -7.0510e+00],
        [ 6.6444e+00, -8.0400e+00],
        [ 1.7497e+00, -3.8128e+00],
        [ 5.1581e+00, -6.6198e+00],
        [-7.1090e+00,  5.7096e+00],
        [-8.5075e+00,  5.8359e+00],
        [ 4.1309e+00, -5.8696e+00],
        [ 3.9228e+00, -5.8271e+00],
        [ 5.1202e+00, -7.6453e+00],
        [-5.3852e+00,  3.9499e+00],
        [-4.9060e+00,  1.3324e+00],
        [-4.9995e+00,  3.6044e+00],
        [ 2.9049e+00, -5.4636e+00],
        [-3.6676e+00,  1.6980e+00],
        [ 3.6251e+00, -5.4193e+00],
        [ 7.8252e+00, -1.0611e+01],
        [-4.0406e+00,  2.6187e+00],
        [-7.1948e+00,  5.6285e+00],
        [ 6.6544e+00, -8.0416e+00],
        [-9.7045e+00,  6.1957e+00],
        [-7.5338e+00,  5.8070e+00],
        [-5.8647e+00,  3.1814e+00],
        [-8.8602e+00,  6.8063e+00],
        [ 7.2249e+00, -1.0177e+01],
        [ 4.5601e+00, -6.9800e+00],
        [-7.4099e+00,  5.8712e+00],
        [ 4.5365e+00, -5.9464e+00],
        [ 7.0613e-01, -2.9404e+00],
        [ 1.9856e+00, -3.6723e+00],
        [-2.0702e+00,  6.0335e-03],
        [ 4.9066e+00, -7.5251e+00],
        [-5.4830e+00,  3.5897e+00],
        [-1.8779e+00,  4.8315e-01],
        [ 4.4577e+00, -6.1167e+00],
        [ 5.9182e+00, -8.8093e+00],
        [-5.4101e+00,  2.0388e+00],
        [ 4.9442e+00, -6.4174e+00],
        [ 7.7638e+00, -9.2560e+00],
        [ 2.5252e+00, -4.7825e+00],
        [-4.6405e+00,  3.1605e+00],
        [ 5.1803e+00, -6.6032e+00],
        [-5.1797e+00,  5.6444e-01],
        [ 7.5163e+00, -8.9376e+00],
        [ 3.9747e-01, -5.0127e+00],
        [-2.4321e+00,  4.5579e-01],
        [-2.2842e+00,  6.3425e-01]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7628, 0.2372],
        [0.2696, 0.7304]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5716, 0.4284], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2990, 0.1082],
         [0.7229, 0.2067]],

        [[0.7939, 0.1023],
         [0.7062, 0.6600]],

        [[0.0605, 0.1092],
         [0.1396, 0.0837]],

        [[0.1030, 0.1093],
         [0.5891, 0.4663]],

        [[0.5539, 0.1167],
         [0.8746, 0.6719]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 3
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 6
Adjusted Rand Index: 0.7721212121212121
time is 4
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.9291531268164688
Average Adjusted Rand Index: 0.9305836430625403
Iteration 0: Loss = -20728.52988200469
Iteration 10: Loss = -11824.84641937995
Iteration 20: Loss = -11688.30996978101
Iteration 30: Loss = -11573.803406169629
Iteration 40: Loss = -11573.420626436018
Iteration 50: Loss = -11573.40937321232
Iteration 60: Loss = -11573.409082221558
Iteration 70: Loss = -11573.40905454166
Iteration 80: Loss = -11573.409064869142
1
Iteration 90: Loss = -11573.40906225905
2
Iteration 100: Loss = -11573.409062259016
3
Stopping early at iteration 99 due to no improvement.
pi: tensor([[0.7428, 0.2572],
        [0.2885, 0.7115]], dtype=torch.float64)
alpha: tensor([0.5372, 0.4628])
beta: tensor([[[0.2944, 0.1082],
         [0.4667, 0.2015]],

        [[0.3730, 0.1029],
         [0.3047, 0.9038]],

        [[0.7962, 0.1106],
         [0.4775, 0.8187]],

        [[0.9831, 0.1080],
         [0.9678, 0.2179]],

        [[0.5267, 0.1168],
         [0.8372, 0.5988]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 3
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 7
Adjusted Rand Index: 0.7369583604949977
time is 4
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.9214417611804818
Average Adjusted Rand Index: 0.9235505762252958
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20728.200520307768
Iteration 100: Loss = -11853.332777243511
Iteration 200: Loss = -11844.651443722572
Iteration 300: Loss = -11839.347090899399
Iteration 400: Loss = -11835.234040329298
Iteration 500: Loss = -11821.718073175723
Iteration 600: Loss = -11700.454152643044
Iteration 700: Loss = -11620.485050908994
Iteration 800: Loss = -11600.461982485695
Iteration 900: Loss = -11580.169870625145
Iteration 1000: Loss = -11579.493147250669
Iteration 1100: Loss = -11579.327316902765
Iteration 1200: Loss = -11579.11560607732
Iteration 1300: Loss = -11578.20585224665
Iteration 1400: Loss = -11578.16468447005
Iteration 1500: Loss = -11578.128394368674
Iteration 1600: Loss = -11578.071921213193
Iteration 1700: Loss = -11577.914501616093
Iteration 1800: Loss = -11577.89298121635
Iteration 1900: Loss = -11574.410755187513
Iteration 2000: Loss = -11574.350002262625
Iteration 2100: Loss = -11574.340676906293
Iteration 2200: Loss = -11574.335344968122
Iteration 2300: Loss = -11574.325716955876
Iteration 2400: Loss = -11574.318713346576
Iteration 2500: Loss = -11574.31098936596
Iteration 2600: Loss = -11574.293962215204
Iteration 2700: Loss = -11574.281880813394
Iteration 2800: Loss = -11574.270955297212
Iteration 2900: Loss = -11574.262046946878
Iteration 3000: Loss = -11574.258772259169
Iteration 3100: Loss = -11574.256264239984
Iteration 3200: Loss = -11574.254224025164
Iteration 3300: Loss = -11574.25238394979
Iteration 3400: Loss = -11574.250869735926
Iteration 3500: Loss = -11574.249437850232
Iteration 3600: Loss = -11574.24807679223
Iteration 3700: Loss = -11574.246925546298
Iteration 3800: Loss = -11574.245406535523
Iteration 3900: Loss = -11574.237366516703
Iteration 4000: Loss = -11574.236495977135
Iteration 4100: Loss = -11574.237572785132
1
Iteration 4200: Loss = -11574.234684695897
Iteration 4300: Loss = -11574.23391723306
Iteration 4400: Loss = -11574.235975736947
1
Iteration 4500: Loss = -11574.2327157909
Iteration 4600: Loss = -11574.232161586106
Iteration 4700: Loss = -11574.231713603294
Iteration 4800: Loss = -11574.231179657481
Iteration 4900: Loss = -11574.227756707152
Iteration 5000: Loss = -11574.22711269331
Iteration 5100: Loss = -11574.257343591175
1
Iteration 5200: Loss = -11574.226460839891
Iteration 5300: Loss = -11574.226131249263
Iteration 5400: Loss = -11574.245781297228
1
Iteration 5500: Loss = -11574.225642872108
Iteration 5600: Loss = -11574.225387271665
Iteration 5700: Loss = -11574.225185379357
Iteration 5800: Loss = -11574.22500564019
Iteration 5900: Loss = -11574.224754228238
Iteration 6000: Loss = -11574.226868916881
1
Iteration 6100: Loss = -11574.22441096763
Iteration 6200: Loss = -11574.224272260359
Iteration 6300: Loss = -11574.22409182973
Iteration 6400: Loss = -11574.22394882111
Iteration 6500: Loss = -11574.223784156831
Iteration 6600: Loss = -11574.223637693136
Iteration 6700: Loss = -11574.22707572853
1
Iteration 6800: Loss = -11574.223230522079
Iteration 6900: Loss = -11574.222778900988
Iteration 7000: Loss = -11574.222109003234
Iteration 7100: Loss = -11574.221714781732
Iteration 7200: Loss = -11574.655081305335
1
Iteration 7300: Loss = -11574.221468256246
Iteration 7400: Loss = -11574.221384580822
Iteration 7500: Loss = -11574.227814236645
1
Iteration 7600: Loss = -11574.22124999999
Iteration 7700: Loss = -11574.221217488664
Iteration 7800: Loss = -11574.225912485026
1
Iteration 7900: Loss = -11574.221093777358
Iteration 8000: Loss = -11574.221006479063
Iteration 8100: Loss = -11574.243409918286
1
Iteration 8200: Loss = -11574.220929847477
Iteration 8300: Loss = -11574.220901492577
Iteration 8400: Loss = -11574.27938068189
1
Iteration 8500: Loss = -11574.22080821964
Iteration 8600: Loss = -11574.2207757004
Iteration 8700: Loss = -11574.287123724931
1
Iteration 8800: Loss = -11574.22068270163
Iteration 8900: Loss = -11574.220657413389
Iteration 9000: Loss = -11574.22060771795
Iteration 9100: Loss = -11574.220792099524
1
Iteration 9200: Loss = -11574.220510834755
Iteration 9300: Loss = -11574.220445007624
Iteration 9400: Loss = -11574.220573399087
1
Iteration 9500: Loss = -11574.220435154797
Iteration 9600: Loss = -11574.226190766845
1
Iteration 9700: Loss = -11574.220338757368
Iteration 9800: Loss = -11574.220288955114
Iteration 9900: Loss = -11574.420020900372
1
Iteration 10000: Loss = -11574.217838971315
Iteration 10100: Loss = -11574.217813729618
Iteration 10200: Loss = -11574.114376216781
Iteration 10300: Loss = -11574.10726757019
Iteration 10400: Loss = -11574.10568365167
Iteration 10500: Loss = -11574.104694520145
Iteration 10600: Loss = -11574.11912674822
1
Iteration 10700: Loss = -11574.104602996345
Iteration 10800: Loss = -11574.146202834481
1
Iteration 10900: Loss = -11569.404946731018
Iteration 11000: Loss = -11569.397132855214
Iteration 11100: Loss = -11569.345898984162
Iteration 11200: Loss = -11569.348035751356
1
Iteration 11300: Loss = -11569.345923534263
2
Iteration 11400: Loss = -11569.30459540498
Iteration 11500: Loss = -11569.296528838144
Iteration 11600: Loss = -11569.292883495595
Iteration 11700: Loss = -11569.292601548248
Iteration 11800: Loss = -11569.292575726895
Iteration 11900: Loss = -11569.292532523452
Iteration 12000: Loss = -11569.292788035104
1
Iteration 12100: Loss = -11569.295146896582
2
Iteration 12200: Loss = -11569.303743535247
3
Iteration 12300: Loss = -11569.292356572867
Iteration 12400: Loss = -11569.623847541818
1
Iteration 12500: Loss = -11569.29238990833
2
Iteration 12600: Loss = -11569.29235449789
Iteration 12700: Loss = -11569.310582389697
1
Iteration 12800: Loss = -11569.292143125545
Iteration 12900: Loss = -11569.233160557525
Iteration 13000: Loss = -11569.229757454683
Iteration 13100: Loss = -11569.227504137409
Iteration 13200: Loss = -11569.231953689665
1
Iteration 13300: Loss = -11569.227342126385
Iteration 13400: Loss = -11569.227278980228
Iteration 13500: Loss = -11569.408951679427
1
Iteration 13600: Loss = -11569.227160367715
Iteration 13700: Loss = -11569.229649395724
1
Iteration 13800: Loss = -11569.227131684951
Iteration 13900: Loss = -11569.223277420217
Iteration 14000: Loss = -11569.227305563643
1
Iteration 14100: Loss = -11569.221035099385
Iteration 14200: Loss = -11569.221036873674
1
Iteration 14300: Loss = -11569.221239135375
2
Iteration 14400: Loss = -11569.221021609372
Iteration 14500: Loss = -11569.400817132191
1
Iteration 14600: Loss = -11569.221042345729
2
Iteration 14700: Loss = -11569.220898257696
Iteration 14800: Loss = -11569.223895592577
1
Iteration 14900: Loss = -11569.22085967853
Iteration 15000: Loss = -11569.221130017599
1
Iteration 15100: Loss = -11569.220861771375
2
Iteration 15200: Loss = -11569.23071778945
3
Iteration 15300: Loss = -11569.220839451007
Iteration 15400: Loss = -11569.220667316635
Iteration 15500: Loss = -11569.222656930064
1
Iteration 15600: Loss = -11569.220701651897
2
Iteration 15700: Loss = -11569.251941864386
3
Iteration 15800: Loss = -11569.220702026762
4
Iteration 15900: Loss = -11569.221039015341
5
Iteration 16000: Loss = -11569.221348950807
6
Iteration 16100: Loss = -11569.2454378951
7
Iteration 16200: Loss = -11569.22066680067
Iteration 16300: Loss = -11569.223773291016
1
Iteration 16400: Loss = -11569.220520455696
Iteration 16500: Loss = -11569.226256817159
1
Iteration 16600: Loss = -11569.214358192981
Iteration 16700: Loss = -11569.615925748541
1
Iteration 16800: Loss = -11569.214392263464
2
Iteration 16900: Loss = -11569.214356286133
Iteration 17000: Loss = -11569.21792579859
1
Iteration 17100: Loss = -11569.214219638738
Iteration 17200: Loss = -11569.223392126962
1
Iteration 17300: Loss = -11569.21422217918
2
Iteration 17400: Loss = -11569.215733286435
3
Iteration 17500: Loss = -11569.215813402383
4
Iteration 17600: Loss = -11569.216885150467
5
Iteration 17700: Loss = -11569.214103991359
Iteration 17800: Loss = -11569.21435621877
1
Iteration 17900: Loss = -11569.214071234055
Iteration 18000: Loss = -11569.21427905432
1
Iteration 18100: Loss = -11569.21411389243
2
Iteration 18200: Loss = -11569.215396753792
3
Iteration 18300: Loss = -11569.167525300985
Iteration 18400: Loss = -11569.179465804345
1
Iteration 18500: Loss = -11569.166996966966
Iteration 18600: Loss = -11569.169906363759
1
Iteration 18700: Loss = -11569.16701344992
2
Iteration 18800: Loss = -11569.16787279134
3
Iteration 18900: Loss = -11569.16699468048
Iteration 19000: Loss = -11569.167511681779
1
Iteration 19100: Loss = -11569.167007586166
2
Iteration 19200: Loss = -11569.179345631768
3
Iteration 19300: Loss = -11569.165717840553
Iteration 19400: Loss = -11569.165702161774
Iteration 19500: Loss = -11569.165833399813
1
Iteration 19600: Loss = -11569.17608564897
2
Iteration 19700: Loss = -11569.193638514604
3
Iteration 19800: Loss = -11569.202801857067
4
Iteration 19900: Loss = -11569.348726976203
5
tensor([[  2.5390,  -4.1617],
        [  2.9153,  -5.3775],
        [  2.6308,  -4.8748],
        [ -6.7838,   3.2245],
        [  1.7338,  -3.2451],
        [ -6.3287,   4.9424],
        [ -5.8208,   4.3404],
        [  0.7910,  -3.3454],
        [ -3.9850,   2.4442],
        [  7.3113,  -9.7620],
        [ -4.9236,   0.9187],
        [  9.3888, -10.7751],
        [  1.6472,  -3.3989],
        [ -2.5468,   0.6404],
        [ -5.8356,   4.3825],
        [ -3.8977,   2.3726],
        [  7.8932, -12.3434],
        [  2.4321,  -5.2073],
        [ -3.7647,   2.3412],
        [  4.1761,  -5.5852],
        [  1.8928,  -3.8660],
        [  6.0152,  -9.0895],
        [ -5.8431,   3.0805],
        [  4.0679,  -5.5525],
        [  2.8563,  -5.9727],
        [  1.4494,  -3.3759],
        [  6.0715,  -7.4616],
        [  1.0941,  -2.7871],
        [ -2.9455,   1.1327],
        [ -7.4142,   2.8772],
        [  0.3629,  -3.2261],
        [  4.7315,  -6.1213],
        [  9.2090, -10.6580],
        [ -5.6473,   3.9575],
        [ -6.8102,   5.3069],
        [ -6.1744,   1.5592],
        [ -5.2271,   3.6880],
        [  2.4902,  -3.8765],
        [  3.2171,  -5.8784],
        [  9.1735, -10.6017],
        [  1.6872,  -3.3681],
        [  3.9895,  -5.5356],
        [  1.4209,  -2.8248],
        [  2.2309,  -4.6194],
        [  4.1234,  -5.6946],
        [ -3.6716,   2.2003],
        [ -6.4752,   4.1354],
        [ -4.9416,   2.1338],
        [ -5.7752,   4.2835],
        [  9.6479, -11.1700],
        [  2.0573,  -3.8709],
        [ -7.4848,   6.0961],
        [ -3.7307,   2.2614],
        [  0.2786,  -3.4372],
        [  9.0678, -10.5206],
        [  6.2226,  -9.0990],
        [  2.0639,  -3.4987],
        [  5.1569,  -6.6248],
        [ -7.8110,   5.0051],
        [ -8.2566,   6.0909],
        [  4.3052,  -5.6934],
        [  6.6699,  -9.3643],
        [  5.5873,  -7.1807],
        [ -5.4625,   3.8765],
        [-11.0514,   9.2749],
        [ -5.0063,   3.5978],
        [  3.4206,  -4.9487],
        [ -3.6923,   1.6726],
        [  3.8294,  -5.2161],
        [  9.4338, -10.8502],
        [ -4.3691,   2.2942],
        [ -7.1068,   5.7156],
        [  5.7555,  -9.0214],
        [ -9.4873,   6.5110],
        [ -7.3639,   5.9776],
        [ -5.4384,   3.6075],
        [ -8.8312,   6.9091],
        [ 10.3321, -11.7345],
        [  4.8064,  -6.7298],
        [ -8.9464,   4.3312],
        [  4.1183,  -6.3659],
        [  0.8354,  -2.8105],
        [  2.0418,  -3.6159],
        [ -1.9143,   0.1645],
        [  5.5233,  -6.9150],
        [ -5.2546,   3.8183],
        [ -2.1280,   0.2367],
        [  4.2926,  -6.2819],
        [  9.2440, -10.6472],
        [ -4.4357,   3.0131],
        [  8.7956, -11.4536],
        [  9.5978, -11.0384],
        [  2.9590,  -4.3457],
        [ -4.6068,   3.1943],
        [  9.1085, -10.5088],
        [ -4.9284,   0.8159],
        [  8.8552, -10.2428],
        [  1.7566,  -3.6533],
        [ -2.1779,   0.7102],
        [ -2.1693,   0.7516]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7629, 0.2371],
        [0.2698, 0.7302]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5720, 0.4280], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2990, 0.1081],
         [0.4667, 0.2068]],

        [[0.3730, 0.1023],
         [0.3047, 0.9038]],

        [[0.7962, 0.1092],
         [0.4775, 0.8187]],

        [[0.9831, 0.1093],
         [0.9678, 0.2179]],

        [[0.5267, 0.1167],
         [0.8372, 0.5988]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 3
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 6
Adjusted Rand Index: 0.7721212121212121
time is 4
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.9291531268164688
Average Adjusted Rand Index: 0.9305836430625403
Iteration 0: Loss = -20104.66854361109
Iteration 10: Loss = -11832.119737740757
Iteration 20: Loss = -11798.051400222914
Iteration 30: Loss = -11575.151665019052
Iteration 40: Loss = -11573.462529305476
Iteration 50: Loss = -11573.41056869547
Iteration 60: Loss = -11573.409116392128
Iteration 70: Loss = -11573.409039491497
Iteration 80: Loss = -11573.40906572309
1
Iteration 90: Loss = -11573.40906970232
2
Iteration 100: Loss = -11573.40906970228
3
Stopping early at iteration 99 due to no improvement.
pi: tensor([[0.7428, 0.2572],
        [0.2885, 0.7115]], dtype=torch.float64)
alpha: tensor([0.5372, 0.4628])
beta: tensor([[[0.2944, 0.1082],
         [0.5976, 0.2015]],

        [[0.3503, 0.1029],
         [0.9660, 0.3673]],

        [[0.0082, 0.1106],
         [0.4410, 0.3377]],

        [[0.1598, 0.1080],
         [0.0551, 0.7946]],

        [[0.8954, 0.1168],
         [0.8300, 0.0671]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 3
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 7
Adjusted Rand Index: 0.7369583604949977
time is 4
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.9214417611804818
Average Adjusted Rand Index: 0.9235505762252958
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20104.45522408541
Iteration 100: Loss = -11854.778197664686
Iteration 200: Loss = -11837.795176225645
Iteration 300: Loss = -11830.223402491985
Iteration 400: Loss = -11765.055461822454
Iteration 500: Loss = -11657.570733788183
Iteration 600: Loss = -11616.276387225693
Iteration 700: Loss = -11589.115025317291
Iteration 800: Loss = -11588.592204778659
Iteration 900: Loss = -11587.770085167176
Iteration 1000: Loss = -11587.601531172304
Iteration 1100: Loss = -11587.532328278983
Iteration 1200: Loss = -11587.473405700088
Iteration 1300: Loss = -11587.270603313116
Iteration 1400: Loss = -11587.235526643188
Iteration 1500: Loss = -11587.20698936479
Iteration 1600: Loss = -11587.133797932533
Iteration 1700: Loss = -11586.908762650839
Iteration 1800: Loss = -11586.894651923114
Iteration 1900: Loss = -11586.881604655224
Iteration 2000: Loss = -11586.848751433832
Iteration 2100: Loss = -11581.027840897379
Iteration 2200: Loss = -11580.991453202534
Iteration 2300: Loss = -11575.376791508412
Iteration 2400: Loss = -11571.646210830862
Iteration 2500: Loss = -11571.510472788863
Iteration 2600: Loss = -11571.504746978511
Iteration 2700: Loss = -11571.499885196265
Iteration 2800: Loss = -11571.495598635252
Iteration 2900: Loss = -11571.49051453288
Iteration 3000: Loss = -11571.483618976868
Iteration 3100: Loss = -11571.475962740436
Iteration 3200: Loss = -11571.473177011592
Iteration 3300: Loss = -11571.470965956676
Iteration 3400: Loss = -11571.468912634877
Iteration 3500: Loss = -11571.462546693652
Iteration 3600: Loss = -11569.513650826619
Iteration 3700: Loss = -11569.509582409733
Iteration 3800: Loss = -11569.495763695839
Iteration 3900: Loss = -11569.405743040448
Iteration 4000: Loss = -11569.402745516449
Iteration 4100: Loss = -11569.39707626574
Iteration 4200: Loss = -11569.394649414313
Iteration 4300: Loss = -11569.392977084482
Iteration 4400: Loss = -11569.390986907494
Iteration 4500: Loss = -11569.3586298025
Iteration 4600: Loss = -11569.33526691122
Iteration 4700: Loss = -11569.334441889267
Iteration 4800: Loss = -11569.333853636608
Iteration 4900: Loss = -11569.334586219336
1
Iteration 5000: Loss = -11569.332937527555
Iteration 5100: Loss = -11569.332606410468
Iteration 5200: Loss = -11569.33215578743
Iteration 5300: Loss = -11569.33942890751
1
Iteration 5400: Loss = -11569.331270110399
Iteration 5500: Loss = -11569.336735425133
1
Iteration 5600: Loss = -11569.330155517111
Iteration 5700: Loss = -11569.339753381006
1
Iteration 5800: Loss = -11569.329702368506
Iteration 5900: Loss = -11569.329099436907
Iteration 6000: Loss = -11569.326187462457
Iteration 6100: Loss = -11569.32668335209
1
Iteration 6200: Loss = -11569.32572636032
Iteration 6300: Loss = -11569.325874477372
1
Iteration 6400: Loss = -11569.325422752023
Iteration 6500: Loss = -11569.324889429776
Iteration 6600: Loss = -11569.324202852067
Iteration 6700: Loss = -11569.322995719443
Iteration 6800: Loss = -11569.327216956164
1
Iteration 6900: Loss = -11569.32194662372
Iteration 7000: Loss = -11569.32126713873
Iteration 7100: Loss = -11569.282538707042
Iteration 7200: Loss = -11569.282384080983
Iteration 7300: Loss = -11569.282546413118
1
Iteration 7400: Loss = -11569.28194656444
Iteration 7500: Loss = -11569.281746408647
Iteration 7600: Loss = -11569.318990762053
1
Iteration 7700: Loss = -11569.281354851237
Iteration 7800: Loss = -11569.280735365599
Iteration 7900: Loss = -11569.278835036594
Iteration 8000: Loss = -11569.278760963347
Iteration 8100: Loss = -11569.280750905782
1
Iteration 8200: Loss = -11569.30179695337
2
Iteration 8300: Loss = -11569.278438810206
Iteration 8400: Loss = -11569.28275174645
1
Iteration 8500: Loss = -11569.278381249565
Iteration 8600: Loss = -11569.352283616334
1
Iteration 8700: Loss = -11569.278317485774
Iteration 8800: Loss = -11569.280801102863
1
Iteration 8900: Loss = -11569.278240740437
Iteration 9000: Loss = -11569.278447808687
1
Iteration 9100: Loss = -11569.328420415772
2
Iteration 9200: Loss = -11569.27818627557
Iteration 9300: Loss = -11569.417196435495
1
Iteration 9400: Loss = -11569.278122381842
Iteration 9500: Loss = -11569.278097330684
Iteration 9600: Loss = -11569.278244661362
1
Iteration 9700: Loss = -11569.278127196072
2
Iteration 9800: Loss = -11569.301258155803
3
Iteration 9900: Loss = -11569.278272045836
4
Iteration 10000: Loss = -11569.397298234619
5
Iteration 10100: Loss = -11569.278073691654
Iteration 10200: Loss = -11569.27809748049
1
Iteration 10300: Loss = -11569.288063896383
2
Iteration 10400: Loss = -11569.27802173384
Iteration 10500: Loss = -11569.27811866563
1
Iteration 10600: Loss = -11569.277891996186
Iteration 10700: Loss = -11569.274637588627
Iteration 10800: Loss = -11569.274559737705
Iteration 10900: Loss = -11569.274587957
1
Iteration 11000: Loss = -11569.274506460866
Iteration 11100: Loss = -11569.27790430878
1
Iteration 11200: Loss = -11569.274498554785
Iteration 11300: Loss = -11569.27444744972
Iteration 11400: Loss = -11569.274726007978
1
Iteration 11500: Loss = -11569.274458253329
2
Iteration 11600: Loss = -11569.274530209628
3
Iteration 11700: Loss = -11569.274469646727
4
Iteration 11800: Loss = -11569.274428484743
Iteration 11900: Loss = -11569.298338162916
1
Iteration 12000: Loss = -11569.280410268806
2
Iteration 12100: Loss = -11569.274577063412
3
Iteration 12200: Loss = -11569.275428107305
4
Iteration 12300: Loss = -11569.288512608135
5
Iteration 12400: Loss = -11569.2744189665
Iteration 12500: Loss = -11569.274606413463
1
Iteration 12600: Loss = -11569.278710882836
2
Iteration 12700: Loss = -11569.274545297314
3
Iteration 12800: Loss = -11569.274497003391
4
Iteration 12900: Loss = -11569.399892153751
5
Iteration 13000: Loss = -11569.273830114194
Iteration 13100: Loss = -11569.330226266537
1
Iteration 13200: Loss = -11569.273830182916
2
Iteration 13300: Loss = -11569.2869230967
3
Iteration 13400: Loss = -11569.273715881536
Iteration 13500: Loss = -11569.27364157747
Iteration 13600: Loss = -11569.280740510612
1
Iteration 13700: Loss = -11569.273604339787
Iteration 13800: Loss = -11569.341741076129
1
Iteration 13900: Loss = -11569.27421404152
2
Iteration 14000: Loss = -11569.473374863299
3
Iteration 14100: Loss = -11569.273518778396
Iteration 14200: Loss = -11569.275603043203
1
Iteration 14300: Loss = -11569.272908274012
Iteration 14400: Loss = -11569.273813429012
1
Iteration 14500: Loss = -11569.272881976558
Iteration 14600: Loss = -11569.2731326329
1
Iteration 14700: Loss = -11569.272886231078
2
Iteration 14800: Loss = -11569.27911853034
3
Iteration 14900: Loss = -11569.272860895293
Iteration 15000: Loss = -11569.272879597984
1
Iteration 15100: Loss = -11569.272914245254
2
Iteration 15200: Loss = -11569.271977726949
Iteration 15300: Loss = -11569.523343138375
1
Iteration 15400: Loss = -11569.272000560562
2
Iteration 15500: Loss = -11569.27195776683
Iteration 15600: Loss = -11569.27785393708
1
Iteration 15700: Loss = -11569.271961237404
2
Iteration 15800: Loss = -11569.27195529942
Iteration 15900: Loss = -11569.273915738433
1
Iteration 16000: Loss = -11569.271937510897
Iteration 16100: Loss = -11569.271946639194
1
Iteration 16200: Loss = -11569.27199457979
2
Iteration 16300: Loss = -11569.271932840385
Iteration 16400: Loss = -11569.27289323448
1
Iteration 16500: Loss = -11569.477853537624
2
Iteration 16600: Loss = -11569.271939260658
3
Iteration 16700: Loss = -11569.272449495918
4
Iteration 16800: Loss = -11569.271498518696
Iteration 16900: Loss = -11569.271291874853
Iteration 17000: Loss = -11569.27131606667
1
Iteration 17100: Loss = -11569.271325277821
2
Iteration 17200: Loss = -11569.271251257753
Iteration 17300: Loss = -11569.271752413137
1
Iteration 17400: Loss = -11569.271276963347
2
Iteration 17500: Loss = -11569.312934683709
3
Iteration 17600: Loss = -11569.271288909831
4
Iteration 17700: Loss = -11569.271282916776
5
Iteration 17800: Loss = -11569.271327659977
6
Iteration 17900: Loss = -11569.27128783998
7
Iteration 18000: Loss = -11569.27212856667
8
Iteration 18100: Loss = -11569.271953942336
9
Iteration 18200: Loss = -11569.283319602831
10
Stopping early at iteration 18200 due to no improvement.
tensor([[  2.6432,  -4.0722],
        [  1.8422,  -6.4575],
        [  2.9601,  -4.5459],
        [ -6.0525,   3.9574],
        [  1.7984,  -3.1920],
        [ -6.3982,   4.8767],
        [ -5.8490,   4.3154],
        [  1.3352,  -2.8039],
        [ -3.9053,   2.5180],
        [  7.0811,  -9.0776],
        [ -3.8438,   2.0007],
        [  4.2842,  -7.3745],
        [  1.7665,  -3.2998],
        [ -2.3768,   0.8046],
        [ -6.9260,   3.2917],
        [ -4.0625,   2.2030],
        [  5.7200,  -7.3232],
        [  3.1351,  -4.5223],
        [ -3.8295,   2.2732],
        [  3.0011,  -6.7647],
        [  2.0565,  -3.7076],
        [  6.0893,  -7.8867],
        [ -5.2009,   3.7263],
        [  4.0920,  -5.5267],
        [  3.6942,  -5.1287],
        [  1.6421,  -3.1973],
        [  5.7427,  -7.7860],
        [  1.0212,  -2.8552],
        [ -2.7393,   1.3446],
        [ -5.8930,   4.4035],
        [  0.8897,  -2.7131],
        [  4.6045,  -6.2539],
        [  3.3547,  -4.8906],
        [ -5.5142,   4.0892],
        [ -6.7593,   5.3596],
        [ -4.7706,   2.9744],
        [ -6.7650,   2.1498],
        [  1.9601,  -4.4020],
        [  3.8507,  -5.2400],
        [  8.0453,  -9.7938],
        [  1.8232,  -3.2276],
        [  4.0634,  -5.4586],
        [  1.2062,  -3.0449],
        [  2.5565,  -4.3043],
        [  3.9540,  -5.8600],
        [ -3.9021,   1.9544],
        [ -6.0392,   4.5747],
        [ -4.2799,   2.7962],
        [ -5.7363,   4.3199],
        [  4.7112,  -6.5433],
        [  2.1475,  -3.7857],
        [ -7.5037,   6.0808],
        [ -3.7329,   2.2543],
        [  1.0996,  -2.5665],
        [  5.5609,  -6.9508],
        [  6.8284,  -8.3764],
        [  1.7759,  -3.7940],
        [  5.1393,  -6.6353],
        [ -8.7163,   4.1011],
        [ -8.6369,   5.7320],
        [  4.2247,  -5.7787],
        [  4.1662,  -5.5832],
        [  4.7025,  -8.0584],
        [ -5.4505,   3.8887],
        [ -3.8192,   2.4266],
        [ -5.6590,   2.9451],
        [  3.4134,  -4.9588],
        [ -4.1207,   1.2478],
        [  3.7571,  -5.2845],
        [  8.7049, -10.6568],
        [ -4.0382,   2.6229],
        [ -7.1192,   5.7058],
        [  8.3612,  -9.7985],
        [ -8.6281,   7.1705],
        [-10.3617,   6.4200],
        [ -5.2406,   3.8075],
        [ -8.7829,   6.9587],
        [  9.1298, -11.1307],
        [  8.6790, -10.3103],
        [ -7.3578,   5.9303],
        [  9.0557, -12.4521],
        [  1.1279,  -2.5148],
        [  2.0600,  -3.6069],
        [ -1.7288,   0.3424],
        [  5.4095,  -7.0370],
        [ -5.3848,   3.6881],
        [ -1.8859,   0.4791],
        [  4.5610,  -6.0155],
        [  6.0283,  -8.7403],
        [ -4.4204,   3.0340],
        [  4.3312,  -7.0287],
        [  8.9148, -10.3069],
        [  2.5965,  -4.7219],
        [ -4.7254,   3.0799],
        [  5.1878,  -6.5948],
        [ -3.7388,   2.0005],
        [  9.5428, -13.0950],
        [  1.3914,  -3.9815],
        [ -2.2593,   0.6303],
        [ -2.5982,   0.3164]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7620, 0.2380],
        [0.2696, 0.7304]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5697, 0.4303], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2995, 0.1085],
         [0.5976, 0.2065]],

        [[0.3503, 0.1022],
         [0.9660, 0.3673]],

        [[0.0082, 0.1093],
         [0.4410, 0.3377]],

        [[0.1598, 0.1095],
         [0.0551, 0.7946]],

        [[0.8954, 0.1168],
         [0.8300, 0.0671]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 3
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 6
Adjusted Rand Index: 0.7721212121212121
time is 4
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.9214413799558656
Average Adjusted Rand Index: 0.9225832042261034
11593.207760278518
new:  [0.9291531268164688, 0.9291531268164688, 0.9291531268164688, 0.9214413799558656] [0.9305836430625403, 0.9305836430625403, 0.9305836430625403, 0.9225832042261034] [11569.168618977988, 11569.155781563704, 11569.165726748995, 11569.283319602831]
prior:  [0.9214417611804818, 0.9214417611804818, 0.9214417611804818, 0.9214417611804818] [0.9235505762252958, 0.9235505762252958, 0.9235505762252958, 0.9235505762252958] [11573.40905870862, 11573.409065298945, 11573.409062259016, 11573.40906970228]
-----------------------------------------------------------------------------------------
This iteration is 1
True Objective function: Loss = -11403.158788486695
Iteration 0: Loss = -18574.956852031144
Iteration 10: Loss = -11369.207009072945
Iteration 20: Loss = -11369.226297170882
1
Iteration 30: Loss = -11369.230454192519
2
Iteration 40: Loss = -11369.230598703463
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[0.7058, 0.2942],
        [0.2495, 0.7505]], dtype=torch.float64)
alpha: tensor([0.4453, 0.5547])
beta: tensor([[[0.1901, 0.0981],
         [0.2330, 0.2948]],

        [[0.7150, 0.1014],
         [0.1502, 0.1241]],

        [[0.0854, 0.1087],
         [0.1727, 0.4848]],

        [[0.2096, 0.0983],
         [0.7855, 0.9253]],

        [[0.1575, 0.1018],
         [0.7429, 0.6578]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 1
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9598877889442053
time is 2
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8823823075947004
time is 3
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448373142749671
time is 4
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208023791611811
Global Adjusted Rand Index: 0.9061019342890484
Average Adjusted Rand Index: 0.905742084713997
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18529.2678973649
Iteration 100: Loss = -11698.083723084352
Iteration 200: Loss = -11676.156445973733
Iteration 300: Loss = -11404.202915460997
Iteration 400: Loss = -11369.078990036038
Iteration 500: Loss = -11368.780426204714
Iteration 600: Loss = -11368.650404437296
Iteration 700: Loss = -11368.58735391046
Iteration 800: Loss = -11368.548362347365
Iteration 900: Loss = -11368.523498502904
Iteration 1000: Loss = -11368.50662095669
Iteration 1100: Loss = -11368.490419016314
Iteration 1200: Loss = -11366.069134052328
Iteration 1300: Loss = -11366.044446374342
Iteration 1400: Loss = -11366.035137645404
Iteration 1500: Loss = -11366.026649888281
Iteration 1600: Loss = -11366.018365913704
Iteration 1700: Loss = -11365.988989561887
Iteration 1800: Loss = -11365.970880356806
Iteration 1900: Loss = -11365.96754159307
Iteration 2000: Loss = -11365.965449424653
Iteration 2100: Loss = -11365.963733881172
Iteration 2200: Loss = -11365.962356508913
Iteration 2300: Loss = -11365.961189049407
Iteration 2400: Loss = -11365.960154616268
Iteration 2500: Loss = -11365.959153225429
Iteration 2600: Loss = -11365.958355847206
Iteration 2700: Loss = -11365.957636239604
Iteration 2800: Loss = -11365.956932347011
Iteration 2900: Loss = -11365.95629194328
Iteration 3000: Loss = -11365.95574257275
Iteration 3100: Loss = -11365.955175026525
Iteration 3200: Loss = -11365.954631052615
Iteration 3300: Loss = -11365.954082894139
Iteration 3400: Loss = -11365.953604044436
Iteration 3500: Loss = -11365.953129667903
Iteration 3600: Loss = -11365.952643888499
Iteration 3700: Loss = -11365.952213292145
Iteration 3800: Loss = -11365.951873232225
Iteration 3900: Loss = -11365.951495955711
Iteration 4000: Loss = -11365.951242521167
Iteration 4100: Loss = -11365.951219495146
Iteration 4200: Loss = -11365.950763326318
Iteration 4300: Loss = -11365.950673482255
Iteration 4400: Loss = -11365.951370847328
1
Iteration 4500: Loss = -11365.950253117158
Iteration 4600: Loss = -11365.950100926271
Iteration 4700: Loss = -11365.950862051483
1
Iteration 4800: Loss = -11365.949578178182
Iteration 4900: Loss = -11365.94977573103
1
Iteration 5000: Loss = -11365.949375783197
Iteration 5100: Loss = -11365.949271137732
Iteration 5200: Loss = -11366.057076844165
1
Iteration 5300: Loss = -11365.949098760211
Iteration 5400: Loss = -11365.949033967721
Iteration 5500: Loss = -11366.052303400647
1
Iteration 5600: Loss = -11365.94880088539
Iteration 5700: Loss = -11365.948491206713
Iteration 5800: Loss = -11365.947270876157
Iteration 5900: Loss = -11365.946403447408
Iteration 6000: Loss = -11365.961490924137
1
Iteration 6100: Loss = -11365.94516920999
Iteration 6200: Loss = -11365.9464968262
1
Iteration 6300: Loss = -11365.94500237999
Iteration 6400: Loss = -11365.946001618655
1
Iteration 6500: Loss = -11365.944944059587
Iteration 6600: Loss = -11365.99607074683
1
Iteration 6700: Loss = -11365.94485851095
Iteration 6800: Loss = -11365.95093842613
1
Iteration 6900: Loss = -11365.944747491494
Iteration 7000: Loss = -11365.944969148317
1
Iteration 7100: Loss = -11365.946041689833
2
Iteration 7200: Loss = -11365.947155796237
3
Iteration 7300: Loss = -11365.95966439807
4
Iteration 7400: Loss = -11365.944187344576
Iteration 7500: Loss = -11365.945280252017
1
Iteration 7600: Loss = -11365.962490075444
2
Iteration 7700: Loss = -11365.944117927724
Iteration 7800: Loss = -11365.947634647016
1
Iteration 7900: Loss = -11365.944005267078
Iteration 8000: Loss = -11365.958970084721
1
Iteration 8100: Loss = -11365.944009268816
2
Iteration 8200: Loss = -11365.94411068394
3
Iteration 8300: Loss = -11365.947014194486
4
Iteration 8400: Loss = -11365.943964960103
Iteration 8500: Loss = -11365.943997198374
1
Iteration 8600: Loss = -11365.94395379721
Iteration 8700: Loss = -11365.943912325021
Iteration 8800: Loss = -11365.949016242284
1
Iteration 8900: Loss = -11365.943877884247
Iteration 9000: Loss = -11365.943875989864
Iteration 9100: Loss = -11365.944101407435
1
Iteration 9200: Loss = -11365.943871727668
Iteration 9300: Loss = -11365.945772448245
1
Iteration 9400: Loss = -11365.9445535603
2
Iteration 9500: Loss = -11365.943945592051
3
Iteration 9600: Loss = -11365.943908490484
4
Iteration 9700: Loss = -11365.944459905815
5
Iteration 9800: Loss = -11366.0460308419
6
Iteration 9900: Loss = -11365.94826726103
7
Iteration 10000: Loss = -11365.943874451455
8
Iteration 10100: Loss = -11365.978930238822
9
Iteration 10200: Loss = -11365.940713718735
Iteration 10300: Loss = -11365.940910672463
1
Iteration 10400: Loss = -11365.945430697664
2
Iteration 10500: Loss = -11365.948615511948
3
Iteration 10600: Loss = -11365.940630784804
Iteration 10700: Loss = -11365.951351517708
1
Iteration 10800: Loss = -11365.941987198355
2
Iteration 10900: Loss = -11365.94197091474
3
Iteration 11000: Loss = -11365.942484682448
4
Iteration 11100: Loss = -11365.943726879237
5
Iteration 11200: Loss = -11365.94060139337
Iteration 11300: Loss = -11365.946746242607
1
Iteration 11400: Loss = -11365.940554567936
Iteration 11500: Loss = -11365.940729613412
1
Iteration 11600: Loss = -11365.945385146086
2
Iteration 11700: Loss = -11365.940742146837
3
Iteration 11800: Loss = -11365.942033183532
4
Iteration 11900: Loss = -11365.944548921445
5
Iteration 12000: Loss = -11365.944436968342
6
Iteration 12100: Loss = -11365.942041196611
7
Iteration 12200: Loss = -11365.944674089693
8
Iteration 12300: Loss = -11365.941658716303
9
Iteration 12400: Loss = -11365.940921768526
10
Stopping early at iteration 12400 due to no improvement.
tensor([[-7.0602e+00,  2.4449e+00],
        [-6.9408e+00,  2.3256e+00],
        [-8.8685e+00,  4.2533e+00],
        [ 4.1626e+00, -8.7778e+00],
        [ 6.9058e+00, -1.1521e+01],
        [-1.0986e+00, -3.5166e+00],
        [ 5.6129e+00, -1.0228e+01],
        [-6.1528e+00,  1.5376e+00],
        [ 5.9276e+00, -1.0543e+01],
        [ 6.4482e+00, -1.1063e+01],
        [ 1.7060e+00, -6.3213e+00],
        [-5.6489e+00,  1.0336e+00],
        [ 2.9399e+00, -7.5551e+00],
        [-8.4065e+00,  3.7913e+00],
        [ 2.4115e+00, -7.0267e+00],
        [ 1.4199e+00, -6.0351e+00],
        [ 2.0530e+00, -6.6682e+00],
        [ 6.4344e+00, -1.1050e+01],
        [-7.1075e+00,  2.4923e+00],
        [ 3.4106e+00, -8.0258e+00],
        [ 8.8719e-01, -5.5024e+00],
        [-3.5067e+00, -1.1086e+00],
        [-8.4970e+00,  3.8818e+00],
        [ 1.5429e+00, -6.1581e+00],
        [-5.9195e+00,  1.3042e+00],
        [-4.5762e+00, -3.9054e-02],
        [ 3.1194e+00, -7.7346e+00],
        [ 3.9465e+00, -8.5617e+00],
        [-7.5818e+00,  2.9666e+00],
        [ 2.8417e+00, -7.4569e+00],
        [-4.1464e+00, -4.6884e-01],
        [-5.7721e-01, -4.0380e+00],
        [-8.4845e+00,  3.8693e+00],
        [ 1.3808e+00, -5.9960e+00],
        [-9.4416e+00,  4.8264e+00],
        [ 5.5907e+00, -1.0206e+01],
        [ 5.3660e+00, -9.9813e+00],
        [ 7.7774e-01, -5.3930e+00],
        [-1.0859e+01,  6.2436e+00],
        [-5.3800e+00,  7.6475e-01],
        [ 4.9576e+00, -9.5729e+00],
        [-5.6581e-02, -4.5586e+00],
        [-5.1211e+00,  5.0583e-01],
        [-7.7764e+00,  3.1612e+00],
        [-6.9438e+00,  2.3286e+00],
        [ 5.7324e+00, -1.0348e+01],
        [ 7.2801e+00, -1.1895e+01],
        [ 6.9442e+00, -1.1559e+01],
        [-4.4202e-03, -4.6108e+00],
        [-5.5759e+00,  9.6066e-01],
        [ 4.8325e+00, -9.4478e+00],
        [ 7.2969e+00, -1.1912e+01],
        [ 4.1303e-01, -5.0282e+00],
        [ 2.4170e+00, -7.0322e+00],
        [-5.4900e+00,  8.7482e-01],
        [-6.8817e+00,  2.2665e+00],
        [-9.0429e+00,  4.4277e+00],
        [ 4.6125e+00, -9.2277e+00],
        [ 4.4780e+00, -9.0932e+00],
        [ 2.8530e+00, -7.4682e+00],
        [-1.9574e+00, -2.6578e+00],
        [ 1.3339e+00, -5.9491e+00],
        [ 1.8203e+00, -6.4355e+00],
        [-6.4191e+00,  1.8039e+00],
        [ 2.6750e+00, -7.2903e+00],
        [-7.1109e+00,  2.4956e+00],
        [ 6.7146e+00, -1.1330e+01],
        [-8.3406e+00,  3.7254e+00],
        [ 5.1578e+00, -9.7731e+00],
        [-5.5594e+00,  9.4423e-01],
        [-5.1875e+00,  5.7228e-01],
        [ 5.8041e+00, -1.0419e+01],
        [-7.5623e+00,  2.9470e+00],
        [ 2.2571e+00, -6.8723e+00],
        [-1.0326e+00, -3.5826e+00],
        [ 5.7178e+00, -1.0333e+01],
        [-4.3508e+00, -2.6444e-01],
        [ 2.4065e+00, -7.0217e+00],
        [ 1.2945e+00, -5.9097e+00],
        [-6.7399e+00,  2.1246e+00],
        [ 5.8125e+00, -1.0428e+01],
        [-7.3960e+00,  2.7807e+00],
        [ 9.6378e-01, -5.5790e+00],
        [ 4.4152e+00, -9.0304e+00],
        [-5.4651e+00,  8.4991e-01],
        [ 2.1888e+00, -6.8040e+00],
        [-5.9379e+00,  1.3227e+00],
        [-6.9553e+00,  2.3400e+00],
        [ 1.7270e+00, -6.3423e+00],
        [-6.8058e+00,  2.1906e+00],
        [-5.7965e+00,  1.1813e+00],
        [ 4.9282e+00, -9.5434e+00],
        [ 6.2594e+00, -1.0875e+01],
        [-9.2892e+00,  4.6740e+00],
        [ 3.4937e+00, -8.1089e+00],
        [-4.8013e+00,  1.8608e-01],
        [-7.3051e+00,  2.6899e+00],
        [-4.0486e+00, -5.6663e-01],
        [-8.9755e+00,  4.3603e+00],
        [-4.2994e-01, -4.1853e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7617, 0.2383],
        [0.2798, 0.7202]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5665, 0.4335], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3004, 0.0981],
         [0.2330, 0.1947]],

        [[0.7150, 0.1009],
         [0.1502, 0.1241]],

        [[0.0854, 0.1089],
         [0.1727, 0.4848]],

        [[0.2096, 0.0980],
         [0.7855, 0.9253]],

        [[0.1575, 0.1017],
         [0.7429, 0.6578]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 1
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9598877889442053
time is 2
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.844814436176263
time is 3
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448373142749671
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
Global Adjusted Rand Index: 0.8984868343518893
Average Adjusted Rand Index: 0.8982285104303095
Iteration 0: Loss = -24162.367415317865
Iteration 10: Loss = -11707.473186881089
Iteration 20: Loss = -11682.663940480252
Iteration 30: Loss = -11668.323322632497
Iteration 40: Loss = -11459.510855685707
Iteration 50: Loss = -11442.704305934863
Iteration 60: Loss = -11441.029500173201
Iteration 70: Loss = -11440.90435654967
Iteration 80: Loss = -11440.889410409527
Iteration 90: Loss = -11440.887857169619
Iteration 100: Loss = -11440.887703496775
Iteration 110: Loss = -11440.887718868842
1
Iteration 120: Loss = -11440.887731015244
2
Iteration 130: Loss = -11440.88772483818
3
Stopping early at iteration 129 due to no improvement.
pi: tensor([[0.5719, 0.4281],
        [0.4702, 0.5298]], dtype=torch.float64)
alpha: tensor([0.5334, 0.4666])
beta: tensor([[[0.2904, 0.0979],
         [0.5724, 0.1997]],

        [[0.1801, 0.1012],
         [0.5130, 0.6704]],

        [[0.7405, 0.1079],
         [0.5158, 0.7653]],

        [[0.7640, 0.1005],
         [0.4473, 0.1334]],

        [[0.3434, 0.1005],
         [0.8796, 0.3049]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 1
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9598877889442053
time is 2
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8823823075947004
time is 3
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 84
Adjusted Rand Index: 0.45728784888638285
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
Global Adjusted Rand Index: 0.39819276833990513
Average Adjusted Rand Index: 0.8282328432731744
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24160.911606694382
Iteration 100: Loss = -11706.63587363928
Iteration 200: Loss = -11698.503895168542
Iteration 300: Loss = -11695.645674112235
Iteration 400: Loss = -11693.345123613959
Iteration 500: Loss = -11684.073897474196
Iteration 600: Loss = -11662.255002968048
Iteration 700: Loss = -11583.557158724887
Iteration 800: Loss = -11541.948986914875
Iteration 900: Loss = -11529.842505868524
Iteration 1000: Loss = -11519.478109341484
Iteration 1100: Loss = -11517.8133233464
Iteration 1200: Loss = -11510.864335234319
Iteration 1300: Loss = -11510.085967104595
Iteration 1400: Loss = -11509.383964624374
Iteration 1500: Loss = -11509.05545793735
Iteration 1600: Loss = -11508.186921656194
Iteration 1700: Loss = -11506.242311425784
Iteration 1800: Loss = -11504.432869928292
Iteration 1900: Loss = -11502.377189432143
Iteration 2000: Loss = -11500.351738024483
Iteration 2100: Loss = -11498.448294713831
Iteration 2200: Loss = -11497.86316562748
Iteration 2300: Loss = -11497.55265217346
Iteration 2400: Loss = -11496.982882470624
Iteration 2500: Loss = -11494.437209932217
Iteration 2600: Loss = -11494.031874830835
Iteration 2700: Loss = -11490.506815637342
Iteration 2800: Loss = -11490.317131279451
Iteration 2900: Loss = -11490.302631246745
Iteration 3000: Loss = -11490.292233550015
Iteration 3100: Loss = -11490.262108259774
Iteration 3200: Loss = -11480.351405997304
Iteration 3300: Loss = -11480.331356916435
Iteration 3400: Loss = -11480.313222062281
Iteration 3500: Loss = -11480.306425888026
Iteration 3600: Loss = -11480.301877557204
Iteration 3700: Loss = -11480.298237441984
Iteration 3800: Loss = -11480.29503746727
Iteration 3900: Loss = -11480.291952977923
Iteration 4000: Loss = -11480.285155184656
Iteration 4100: Loss = -11477.747334689953
Iteration 4200: Loss = -11477.737174569349
Iteration 4300: Loss = -11474.448435277338
Iteration 4400: Loss = -11474.185753310909
Iteration 4500: Loss = -11474.154757945562
Iteration 4600: Loss = -11474.042218989667
Iteration 4700: Loss = -11473.175100175607
Iteration 4800: Loss = -11473.161011828754
Iteration 4900: Loss = -11473.14461338288
Iteration 5000: Loss = -11473.141149425011
Iteration 5100: Loss = -11473.138941475798
Iteration 5200: Loss = -11473.137916315312
Iteration 5300: Loss = -11473.136670359116
Iteration 5400: Loss = -11473.135286446079
Iteration 5500: Loss = -11473.136486641984
1
Iteration 5600: Loss = -11473.134327119085
Iteration 5700: Loss = -11473.13373672636
Iteration 5800: Loss = -11473.131596338926
Iteration 5900: Loss = -11473.128473932922
Iteration 6000: Loss = -11473.127839126671
Iteration 6100: Loss = -11473.127450267204
Iteration 6200: Loss = -11473.128967545874
1
Iteration 6300: Loss = -11473.129799002021
2
Iteration 6400: Loss = -11473.126439257341
Iteration 6500: Loss = -11473.127320666768
1
Iteration 6600: Loss = -11473.125764268698
Iteration 6700: Loss = -11473.130565152811
1
Iteration 6800: Loss = -11473.129119633348
2
Iteration 6900: Loss = -11473.126336272877
3
Iteration 7000: Loss = -11473.126301162836
4
Iteration 7100: Loss = -11473.124855038115
Iteration 7200: Loss = -11473.130145122755
1
Iteration 7300: Loss = -11473.1241322648
Iteration 7400: Loss = -11473.129572951524
1
Iteration 7500: Loss = -11473.120241596855
Iteration 7600: Loss = -11473.109664996871
Iteration 7700: Loss = -11473.114406249635
1
Iteration 7800: Loss = -11473.112267023653
2
Iteration 7900: Loss = -11473.11076798914
3
Iteration 8000: Loss = -11473.11108731081
4
Iteration 8100: Loss = -11473.107071936978
Iteration 8200: Loss = -11473.095192766794
Iteration 8300: Loss = -11473.093143439904
Iteration 8400: Loss = -11473.092165742675
Iteration 8500: Loss = -11473.090475518286
Iteration 8600: Loss = -11466.467821783854
Iteration 8700: Loss = -11466.393429236246
Iteration 8800: Loss = -11466.392283058882
Iteration 8900: Loss = -11466.21159926056
Iteration 9000: Loss = -11466.071571398541
Iteration 9100: Loss = -11466.070490612361
Iteration 9200: Loss = -11466.084607944169
1
Iteration 9300: Loss = -11466.079085704996
2
Iteration 9400: Loss = -11466.125819740371
3
Iteration 9500: Loss = -11466.080020723923
4
Iteration 9600: Loss = -11466.06948547854
Iteration 9700: Loss = -11465.834924855775
Iteration 9800: Loss = -11466.217484328728
1
Iteration 9900: Loss = -11465.821396864794
Iteration 10000: Loss = -11465.821359473375
Iteration 10100: Loss = -11465.821287349982
Iteration 10200: Loss = -11465.821154693213
Iteration 10300: Loss = -11465.821201257097
1
Iteration 10400: Loss = -11465.822416594367
2
Iteration 10500: Loss = -11464.607638041301
Iteration 10600: Loss = -11457.819215920023
Iteration 10700: Loss = -11457.828808727943
1
Iteration 10800: Loss = -11457.83282633843
2
Iteration 10900: Loss = -11457.818496134274
Iteration 11000: Loss = -11457.83951763387
1
Iteration 11100: Loss = -11457.821541239711
2
Iteration 11200: Loss = -11457.81843285301
Iteration 11300: Loss = -11457.82116161743
1
Iteration 11400: Loss = -11457.821017440063
2
Iteration 11500: Loss = -11457.817317021223
Iteration 11600: Loss = -11457.817036922826
Iteration 11700: Loss = -11457.816935566718
Iteration 11800: Loss = -11457.816899544276
Iteration 11900: Loss = -11457.817147926962
1
Iteration 12000: Loss = -11457.816829161186
Iteration 12100: Loss = -11457.817631757935
1
Iteration 12200: Loss = -11457.818321570729
2
Iteration 12300: Loss = -11457.816539746824
Iteration 12400: Loss = -11457.816079066431
Iteration 12500: Loss = -11457.830032427046
1
Iteration 12600: Loss = -11457.826478319901
2
Iteration 12700: Loss = -11457.826086035164
3
Iteration 12800: Loss = -11457.819074061743
4
Iteration 12900: Loss = -11457.816059644412
Iteration 13000: Loss = -11457.816438415273
1
Iteration 13100: Loss = -11457.845526913261
2
Iteration 13200: Loss = -11457.81554600897
Iteration 13300: Loss = -11457.818734626511
1
Iteration 13400: Loss = -11457.81555803321
2
Iteration 13500: Loss = -11457.824323122999
3
Iteration 13600: Loss = -11457.815533321933
Iteration 13700: Loss = -11457.813826761234
Iteration 13800: Loss = -11457.817172306057
1
Iteration 13900: Loss = -11457.814405627938
2
Iteration 14000: Loss = -11457.834415631962
3
Iteration 14100: Loss = -11457.81256847761
Iteration 14200: Loss = -11457.814327207516
1
Iteration 14300: Loss = -11457.810377209222
Iteration 14400: Loss = -11457.810807530239
1
Iteration 14500: Loss = -11457.802368374123
Iteration 14600: Loss = -11457.855911739149
1
Iteration 14700: Loss = -11457.80224594864
Iteration 14800: Loss = -11457.802798991996
1
Iteration 14900: Loss = -11457.846045972532
2
Iteration 15000: Loss = -11457.809777893304
3
Iteration 15100: Loss = -11457.780157586445
Iteration 15200: Loss = -11457.780140085835
Iteration 15300: Loss = -11457.792436542817
1
Iteration 15400: Loss = -11457.78138134044
2
Iteration 15500: Loss = -11457.781510946183
3
Iteration 15600: Loss = -11457.779854525117
Iteration 15700: Loss = -11457.779769929553
Iteration 15800: Loss = -11457.78339564919
1
Iteration 15900: Loss = -11457.77887555485
Iteration 16000: Loss = -11457.782891686551
1
Iteration 16100: Loss = -11457.791905639613
2
Iteration 16200: Loss = -11457.778851144054
Iteration 16300: Loss = -11457.885258913466
1
Iteration 16400: Loss = -11457.77754618451
Iteration 16500: Loss = -11457.799079086702
1
Iteration 16600: Loss = -11457.778673725954
2
Iteration 16700: Loss = -11457.777559130152
3
Iteration 16800: Loss = -11457.78498216871
4
Iteration 16900: Loss = -11457.777485577904
Iteration 17000: Loss = -11457.778678876577
1
Iteration 17100: Loss = -11457.777506739272
2
Iteration 17200: Loss = -11457.788268782988
3
Iteration 17300: Loss = -11457.860466792416
4
Iteration 17400: Loss = -11457.787726804734
5
Iteration 17500: Loss = -11457.77783419804
6
Iteration 17600: Loss = -11457.778361789235
7
Iteration 17700: Loss = -11457.91267627519
8
Iteration 17800: Loss = -11457.777842404186
9
Iteration 17900: Loss = -11457.779825578526
10
Stopping early at iteration 17900 due to no improvement.
tensor([[  1.7164,  -3.1027],
        [  0.3682,  -2.8943],
        [  3.8512,  -5.4134],
        [ -7.4029,   5.9284],
        [-10.3094,   7.2218],
        [-12.1566,   7.5414],
        [ -9.6363,   7.6411],
        [  0.2013,  -1.7873],
        [-10.3196,   8.9145],
        [ -9.6976,   8.2580],
        [ -5.9138,   3.1067],
        [  1.2599,  -3.2311],
        [ -8.9971,   4.3819],
        [  3.3585,  -4.7448],
        [ -6.1224,   4.7345],
        [-10.0079,   8.5488],
        [ -5.5962,   4.2082],
        [ -5.5778,   4.1870],
        [  1.3658,  -3.2949],
        [ -6.6294,   4.9070],
        [ -7.0867,   2.7058],
        [ -2.8926,   1.1225],
        [  3.6211,  -5.0717],
        [ -6.6355,   4.1019],
        [ -0.6309,  -2.4350],
        [-10.4282,   7.9544],
        [ -6.8832,   4.9023],
        [ -9.1029,   6.9535],
        [  0.8135,  -2.6333],
        [ -7.8947,   5.8402],
        [  0.1381,  -1.6073],
        [ -4.8739,   2.7798],
        [  3.6868,  -5.0739],
        [ -4.9965,   3.6001],
        [  3.5454,  -5.3402],
        [ -9.7505,   7.9784],
        [ -9.5605,   7.6031],
        [ -5.2116,   3.2589],
        [  5.4170,  -6.8418],
        [ -2.5527,   1.1616],
        [ -8.4850,   7.0372],
        [ -5.6893,   1.8352],
        [ -0.1110,  -1.4655],
        [  2.2233,  -3.6164],
        [  1.4124,  -2.7997],
        [ -8.8035,   7.1048],
        [ -7.2175,   5.7772],
        [ -8.7651,   5.8747],
        [ -4.7261,   3.3249],
        [ -1.2897,  -0.2149],
        [ -5.9996,   4.5657],
        [ -8.8302,   7.1074],
        [ -5.0370,   1.0398],
        [-10.2059,   7.6025],
        [  0.2672,  -2.0207],
        [  2.3551,  -3.8527],
        [  4.0378,  -5.7133],
        [ -8.5170,   6.8716],
        [ -8.2893,   6.6821],
        [ -8.8506,   7.1980],
        [ -3.8815,   1.9667],
        [ -4.9585,   3.5626],
        [ -5.9273,   4.5253],
        [  2.8192,  -5.5449],
        [ -6.5742,   5.0643],
        [  5.9598,  -7.6626],
        [ -8.3831,   5.6196],
        [  3.1623,  -4.5607],
        [ -9.2308,   6.9400],
        [ -0.4043,  -1.3631],
        [ -1.1227,  -0.6648],
        [ -9.7595,   7.7703],
        [  1.7428,  -3.5093],
        [ -9.7520,   7.8690],
        [ -3.3859,   1.9208],
        [ -9.5285,   8.0751],
        [ -0.0511,  -1.4751],
        [ -9.6062,   7.9522],
        [ -4.2933,   2.7974],
        [  3.9523,  -5.4600],
        [ -4.4776,   2.3185],
        [  2.0351,  -3.8283],
        [ -5.6317,   3.5657],
        [ -8.4462,   6.7632],
        [  0.7113,  -2.4103],
        [ -7.6129,   5.9254],
        [  1.0097,  -2.9832],
        [  0.9355,  -2.7108],
        [ -6.0469,   4.1967],
        [  2.1924,  -4.3815],
        [  1.0697,  -2.6348],
        [ -8.4607,   6.9868],
        [ -9.9539,   8.3546],
        [  5.2774,  -7.1033],
        [ -7.9888,   6.3691],
        [ -2.3820,   0.7714],
        [  0.7848,  -5.0888],
        [ -2.6628,   1.0308],
        [  3.3344,  -5.3234],
        [ -3.9928,   1.2265]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.4653, 0.5347],
        [0.6728, 0.3272]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3535, 0.6465], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2765, 0.0956],
         [0.5724, 0.2341]],

        [[0.1801, 0.1006],
         [0.5130, 0.6704]],

        [[0.7405, 0.1049],
         [0.5158, 0.7653]],

        [[0.7640, 0.0971],
         [0.4473, 0.1334]],

        [[0.3434, 0.0994],
         [0.8796, 0.3049]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 93
Adjusted Rand Index: 0.7367933557689339
time is 1
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9598877889442053
time is 2
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8823435719624108
time is 3
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448373142749671
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
Global Adjusted Rand Index: 0.04649386726846644
Average Adjusted Rand Index: 0.8689335336592338
Iteration 0: Loss = -22593.355062958755
Iteration 10: Loss = -11369.715851347179
Iteration 20: Loss = -11369.229320308941
Iteration 30: Loss = -11369.230594731782
1
Iteration 40: Loss = -11369.230597151205
2
Iteration 50: Loss = -11369.230594409712
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7505, 0.2495],
        [0.2942, 0.7058]], dtype=torch.float64)
alpha: tensor([0.5547, 0.4453])
beta: tensor([[[0.2948, 0.0981],
         [0.9135, 0.1901]],

        [[0.5955, 0.1014],
         [0.3481, 0.8961]],

        [[0.6667, 0.1087],
         [0.2277, 0.9870]],

        [[0.2230, 0.0983],
         [0.5424, 0.8219]],

        [[0.2478, 0.1018],
         [0.7848, 0.9262]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 1
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9598877889442053
time is 2
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8823823075947004
time is 3
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448373142749671
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
Global Adjusted Rand Index: 0.9061019342890484
Average Adjusted Rand Index: 0.905742084713997
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22593.12203013759
Iteration 100: Loss = -11736.214620606284
Iteration 200: Loss = -11710.773316880404
Iteration 300: Loss = -11702.917622610777
Iteration 400: Loss = -11700.446534908015
Iteration 500: Loss = -11698.69135750527
Iteration 600: Loss = -11697.894770939345
Iteration 700: Loss = -11696.65978252505
Iteration 800: Loss = -11690.717321019565
Iteration 900: Loss = -11688.128017208117
Iteration 1000: Loss = -11625.931606423012
Iteration 1100: Loss = -11503.98569447693
Iteration 1200: Loss = -11457.428494067568
Iteration 1300: Loss = -11432.308109262724
Iteration 1400: Loss = -11401.825139323679
Iteration 1500: Loss = -11394.35305280767
Iteration 1600: Loss = -11392.620117076982
Iteration 1700: Loss = -11392.50046702765
Iteration 1800: Loss = -11391.934287402906
Iteration 1900: Loss = -11381.65016929596
Iteration 2000: Loss = -11381.607065963562
Iteration 2100: Loss = -11381.580951263133
Iteration 2200: Loss = -11381.565014126501
Iteration 2300: Loss = -11381.542375633551
Iteration 2400: Loss = -11381.441606111463
Iteration 2500: Loss = -11378.086625270502
Iteration 2600: Loss = -11375.10911810773
Iteration 2700: Loss = -11375.108248780758
Iteration 2800: Loss = -11375.089663124038
Iteration 2900: Loss = -11375.077774119101
Iteration 3000: Loss = -11375.052120779597
Iteration 3100: Loss = -11375.033711978884
Iteration 3200: Loss = -11375.016930177193
Iteration 3300: Loss = -11374.970370724794
Iteration 3400: Loss = -11369.29893089119
Iteration 3500: Loss = -11369.292993923647
Iteration 3600: Loss = -11369.280091481602
Iteration 3700: Loss = -11369.263615558642
Iteration 3800: Loss = -11369.261157018776
Iteration 3900: Loss = -11369.259102876025
Iteration 4000: Loss = -11369.257269212521
Iteration 4100: Loss = -11369.254560612098
Iteration 4200: Loss = -11369.252599310266
Iteration 4300: Loss = -11369.25084840975
Iteration 4400: Loss = -11369.249413767555
Iteration 4500: Loss = -11369.24744285984
Iteration 4600: Loss = -11369.245919079203
Iteration 4700: Loss = -11369.245490744255
Iteration 4800: Loss = -11369.240550537515
Iteration 4900: Loss = -11369.16561696595
Iteration 5000: Loss = -11369.166452449987
1
Iteration 5100: Loss = -11369.163891729027
Iteration 5200: Loss = -11369.163278401407
Iteration 5300: Loss = -11369.16251334922
Iteration 5400: Loss = -11369.161859853848
Iteration 5500: Loss = -11369.16135097611
Iteration 5600: Loss = -11369.160636207307
Iteration 5700: Loss = -11369.1600467628
Iteration 5800: Loss = -11369.159520954132
Iteration 5900: Loss = -11369.158897060994
Iteration 6000: Loss = -11369.158194278229
Iteration 6100: Loss = -11369.157530773777
Iteration 6200: Loss = -11369.157437581958
Iteration 6300: Loss = -11369.156818221909
Iteration 6400: Loss = -11369.156658912181
Iteration 6500: Loss = -11369.15761840228
1
Iteration 6600: Loss = -11369.159371085436
2
Iteration 6700: Loss = -11369.31232861767
3
Iteration 6800: Loss = -11369.155561603508
Iteration 6900: Loss = -11369.155934719072
1
Iteration 7000: Loss = -11369.15514644068
Iteration 7100: Loss = -11369.155102128689
Iteration 7200: Loss = -11369.154849957182
Iteration 7300: Loss = -11369.15513425479
1
Iteration 7400: Loss = -11369.154534792144
Iteration 7500: Loss = -11369.154259015906
Iteration 7600: Loss = -11369.15473055284
1
Iteration 7700: Loss = -11369.170676518364
2
Iteration 7800: Loss = -11369.161397542612
3
Iteration 7900: Loss = -11369.155952388703
4
Iteration 8000: Loss = -11369.153906578531
Iteration 8100: Loss = -11369.220312572237
1
Iteration 8200: Loss = -11369.153377948167
Iteration 8300: Loss = -11369.154024143296
1
Iteration 8400: Loss = -11369.153185383018
Iteration 8500: Loss = -11369.153015014805
Iteration 8600: Loss = -11369.178826238032
1
Iteration 8700: Loss = -11368.77015871225
Iteration 8800: Loss = -11368.767590254956
Iteration 8900: Loss = -11368.764631359225
Iteration 9000: Loss = -11368.764310668417
Iteration 9100: Loss = -11368.780272703323
1
Iteration 9200: Loss = -11368.764138530478
Iteration 9300: Loss = -11368.766206758213
1
Iteration 9400: Loss = -11368.766676206598
2
Iteration 9500: Loss = -11368.768750607102
3
Iteration 9600: Loss = -11368.898260521604
4
Iteration 9700: Loss = -11368.764281175694
5
Iteration 9800: Loss = -11368.764700471133
6
Iteration 9900: Loss = -11368.770875794948
7
Iteration 10000: Loss = -11367.589201167268
Iteration 10100: Loss = -11367.649197923887
1
Iteration 10200: Loss = -11367.586343897508
Iteration 10300: Loss = -11367.587097370499
1
Iteration 10400: Loss = -11367.586211052449
Iteration 10500: Loss = -11367.586136910466
Iteration 10600: Loss = -11367.619085591612
1
Iteration 10700: Loss = -11367.585517478758
Iteration 10800: Loss = -11367.593987224145
1
Iteration 10900: Loss = -11367.598425688013
2
Iteration 11000: Loss = -11367.738398613414
3
Iteration 11100: Loss = -11367.585464475924
Iteration 11200: Loss = -11366.450495385705
Iteration 11300: Loss = -11366.221080839763
Iteration 11400: Loss = -11366.220965820628
Iteration 11500: Loss = -11366.24872141942
1
Iteration 11600: Loss = -11366.224161115188
2
Iteration 11700: Loss = -11366.244393282053
3
Iteration 11800: Loss = -11366.212293905899
Iteration 11900: Loss = -11366.207856740806
Iteration 12000: Loss = -11366.2103606691
1
Iteration 12100: Loss = -11366.208408596985
2
Iteration 12200: Loss = -11366.208024157171
3
Iteration 12300: Loss = -11366.23869994227
4
Iteration 12400: Loss = -11366.207456728313
Iteration 12500: Loss = -11366.20774662456
1
Iteration 12600: Loss = -11366.136762770642
Iteration 12700: Loss = -11366.133435447906
Iteration 12800: Loss = -11366.13745203428
1
Iteration 12900: Loss = -11366.185557770295
2
Iteration 13000: Loss = -11366.135496040459
3
Iteration 13100: Loss = -11366.13750752571
4
Iteration 13200: Loss = -11366.13079758958
Iteration 13300: Loss = -11366.12751839382
Iteration 13400: Loss = -11366.129164365138
1
Iteration 13500: Loss = -11366.130314885402
2
Iteration 13600: Loss = -11366.214425121729
3
Iteration 13700: Loss = -11366.127938871055
4
Iteration 13800: Loss = -11366.127465322283
Iteration 13900: Loss = -11366.12967635032
1
Iteration 14000: Loss = -11366.211841434635
2
Iteration 14100: Loss = -11366.127834425704
3
Iteration 14200: Loss = -11366.118060554818
Iteration 14300: Loss = -11366.073206847605
Iteration 14400: Loss = -11366.0733160125
1
Iteration 14500: Loss = -11366.073251108424
2
Iteration 14600: Loss = -11366.01696910531
Iteration 14700: Loss = -11366.02459856272
1
Iteration 14800: Loss = -11366.01758917478
2
Iteration 14900: Loss = -11366.007989246009
Iteration 15000: Loss = -11366.016836119858
1
Iteration 15100: Loss = -11366.007280592927
Iteration 15200: Loss = -11366.007513592707
1
Iteration 15300: Loss = -11366.01009777525
2
Iteration 15400: Loss = -11366.00706633973
Iteration 15500: Loss = -11366.009038631357
1
Iteration 15600: Loss = -11366.009732210976
2
Iteration 15700: Loss = -11366.028495787607
3
Iteration 15800: Loss = -11366.008077467135
4
Iteration 15900: Loss = -11366.007470702556
5
Iteration 16000: Loss = -11366.006890385945
Iteration 16100: Loss = -11366.006659357492
Iteration 16200: Loss = -11366.007230717834
1
Iteration 16300: Loss = -11366.006944516193
2
Iteration 16400: Loss = -11366.009941895378
3
Iteration 16500: Loss = -11365.990374599302
Iteration 16600: Loss = -11365.991421672814
1
Iteration 16700: Loss = -11365.991615563127
2
Iteration 16800: Loss = -11365.99200250871
3
Iteration 16900: Loss = -11365.990264217584
Iteration 17000: Loss = -11365.990522298403
1
Iteration 17100: Loss = -11365.989910473467
Iteration 17200: Loss = -11365.990189001706
1
Iteration 17300: Loss = -11366.014008070315
2
Iteration 17400: Loss = -11365.990544860077
3
Iteration 17500: Loss = -11365.990147030541
4
Iteration 17600: Loss = -11366.062271101062
5
Iteration 17700: Loss = -11365.990826542647
6
Iteration 17800: Loss = -11366.001487393894
7
Iteration 17900: Loss = -11365.989991395616
8
Iteration 18000: Loss = -11365.99192812566
9
Iteration 18100: Loss = -11365.995831720476
10
Stopping early at iteration 18100 due to no improvement.
tensor([[ -5.7902,   3.7145],
        [ -7.6378,   6.0501],
        [-10.1147,   8.5139],
        [  5.7754,  -7.1659],
        [  7.8610,  -9.2525],
        [  0.5130,  -1.9030],
        [  7.1946, -10.6557],
        [ -5.9029,   1.7878],
        [  6.3015,  -7.8134],
        [  7.4538,  -9.4769],
        [  3.3039,  -4.7243],
        [ -4.2671,   2.4147],
        [  4.3976,  -6.0925],
        [ -7.4438,   4.7175],
        [  3.5439,  -5.8964],
        [  2.6977,  -4.7614],
        [  3.6640,  -5.0550],
        [  3.5461,  -5.1133],
        [ -5.9966,   3.6012],
        [  4.7583,  -6.6794],
        [  1.7265,  -4.6231],
        [ -2.3560,   0.0433],
        [-10.4821,   8.9329],
        [  2.6478,  -5.1337],
        [ -4.9093,   2.3166],
        [ -3.8456,   0.6926],
        [  8.0323, -10.5703],
        [  5.5672,  -6.9572],
        [ -5.9820,   4.5957],
        [  4.2410,  -6.0603],
        [ -2.7881,   0.8904],
        [  0.7803,  -2.6824],
        [ -6.8697,   5.4832],
        [  7.9942, -10.0942],
        [ -8.2535,   6.0114],
        [  7.4609,  -9.3296],
        [  5.2652,  -9.0682],
        [  1.9673,  -4.1995],
        [ -8.3413,   6.9508],
        [ -4.3901,   1.7618],
        [  6.0344,  -8.8471],
        [  1.5570,  -2.9443],
        [ -3.5836,   2.0484],
        [ -6.2108,   4.7255],
        [ -5.6431,   3.6209],
        [  7.2364,  -9.4144],
        [  4.5089,  -6.5264],
        [  4.8231,  -6.5738],
        [  1.4831,  -3.1244],
        [ -3.9909,   2.5417],
        [  4.6044,  -5.9909],
        [  5.3244,  -9.9396],
        [  1.8563,  -3.5854],
        [  3.8019,  -5.6466],
        [ -4.2286,   2.1358],
        [ -6.5517,   2.5960],
        [ -7.4359,   6.0302],
        [  6.5223,  -7.9087],
        [  5.7342,  -7.9353],
        [  4.1132,  -6.1649],
        [ -1.9592,  -2.6560],
        [  2.6312,  -4.6551],
        [  3.3364,  -4.9186],
        [ -4.8009,   3.4144],
        [  4.2365,  -5.7283],
        [ -5.7249,   3.8713],
        [  5.8592,  -7.3386],
        [ -7.3034,   4.7158],
        [  6.7310,  -8.5621],
        [ -4.3762,   2.1403],
        [ -3.6143,   2.1868],
        [  7.2190, -11.6231],
        [ -5.9634,   4.5411],
        [  3.5410,  -5.5886],
        [  0.4403,  -2.1096],
        [  7.6159,  -9.3938],
        [ -2.9805,   1.1475],
        [  3.5402,  -5.8934],
        [  2.9004,  -4.3073],
        [ -5.7291,   3.1338],
        [  1.2828,  -2.6961],
        [ -5.9120,   4.2583],
        [  2.5330,  -3.9747],
        [  5.9357,  -7.4725],
        [ -3.9062,   2.4010],
        [  3.6728,  -5.3230],
        [ -4.3259,   2.9376],
        [ -5.5302,   3.7598],
        [  2.9995,  -5.0699],
        [ -5.3273,   3.6614],
        [-10.6805,   8.9066],
        [  6.7305,  -8.1267],
        [  7.2331,  -8.7520],
        [ -6.7187,   4.9588],
        [  4.9519,  -6.5858],
        [ -3.2744,   1.7109],
        [ -5.6902,   4.3017],
        [ -3.0273,   0.4553],
        [-10.0246,   8.5162],
        [  0.7564,  -2.9982]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7617, 0.2383],
        [0.2811, 0.7189]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5679, 0.4321], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2997, 0.0980],
         [0.9135, 0.1950]],

        [[0.5955, 0.1004],
         [0.3481, 0.8961]],

        [[0.6667, 0.1086],
         [0.2277, 0.9870]],

        [[0.2230, 0.0978],
         [0.5424, 0.8219]],

        [[0.2478, 0.1015],
         [0.7848, 0.9262]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 1
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9598877889442053
time is 2
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.844814436176263
time is 3
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448373142749671
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
Global Adjusted Rand Index: 0.8984868343518893
Average Adjusted Rand Index: 0.8982285104303095
Iteration 0: Loss = -27760.60725528991
Iteration 10: Loss = -11708.105389023192
Iteration 20: Loss = -11684.319359049043
Iteration 30: Loss = -11680.775209828862
Iteration 40: Loss = -11552.32609672552
Iteration 50: Loss = -11443.96252389484
Iteration 60: Loss = -11441.11347254545
Iteration 70: Loss = -11440.92762687171
Iteration 80: Loss = -11440.891809284696
Iteration 90: Loss = -11440.888303493057
Iteration 100: Loss = -11440.887858390064
Iteration 110: Loss = -11440.887793936963
Iteration 120: Loss = -11440.887777659169
Iteration 130: Loss = -11440.887768191758
Iteration 140: Loss = -11440.88773879942
Iteration 150: Loss = -11440.887765666957
1
Iteration 160: Loss = -11440.887745317888
2
Iteration 170: Loss = -11440.887761428396
3
Stopping early at iteration 169 due to no improvement.
pi: tensor([[0.5719, 0.4281],
        [0.4702, 0.5298]], dtype=torch.float64)
alpha: tensor([0.5334, 0.4666])
beta: tensor([[[0.2904, 0.0979],
         [0.3152, 0.1997]],

        [[0.4702, 0.1012],
         [0.2472, 0.4325]],

        [[0.1406, 0.1079],
         [0.0051, 0.7533]],

        [[0.6147, 0.1005],
         [0.3500, 0.0277]],

        [[0.9932, 0.1005],
         [0.8270, 0.9595]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 1
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9598877889442053
time is 2
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8823823075947004
time is 3
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 84
Adjusted Rand Index: 0.45728784888638285
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
Global Adjusted Rand Index: 0.39819276833990513
Average Adjusted Rand Index: 0.8282328432731744
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27760.999699368265
Iteration 100: Loss = -11720.430491043951
Iteration 200: Loss = -11709.560124133775
Iteration 300: Loss = -11704.63430449808
Iteration 400: Loss = -11700.300013583139
Iteration 500: Loss = -11698.217994589915
Iteration 600: Loss = -11697.41010631449
Iteration 700: Loss = -11696.365186613692
Iteration 800: Loss = -11694.79061110733
Iteration 900: Loss = -11685.129535644273
Iteration 1000: Loss = -11671.528002785302
Iteration 1100: Loss = -11631.487011884417
Iteration 1200: Loss = -11575.602384143871
Iteration 1300: Loss = -11563.58175299099
Iteration 1400: Loss = -11555.510794300086
Iteration 1500: Loss = -11547.297194980652
Iteration 1600: Loss = -11539.473086772901
Iteration 1700: Loss = -11533.76901416327
Iteration 1800: Loss = -11529.202751389868
Iteration 1900: Loss = -11527.62884695612
Iteration 2000: Loss = -11526.797199374168
Iteration 2100: Loss = -11525.17401122252
Iteration 2200: Loss = -11510.874013775692
Iteration 2300: Loss = -11510.197552416976
Iteration 2400: Loss = -11509.256037129733
Iteration 2500: Loss = -11500.84142743546
Iteration 2600: Loss = -11498.130901809296
Iteration 2700: Loss = -11492.397530804785
Iteration 2800: Loss = -11486.779596283273
Iteration 2900: Loss = -11486.722575215632
Iteration 3000: Loss = -11486.688289401864
Iteration 3100: Loss = -11486.667168734271
Iteration 3200: Loss = -11486.651703714006
Iteration 3300: Loss = -11486.638412342414
Iteration 3400: Loss = -11486.626339640423
Iteration 3500: Loss = -11486.613595599241
Iteration 3600: Loss = -11486.593758490282
Iteration 3700: Loss = -11486.499844277856
Iteration 3800: Loss = -11486.458034529283
Iteration 3900: Loss = -11486.436863190645
Iteration 4000: Loss = -11486.360219623162
Iteration 4100: Loss = -11486.3470643187
Iteration 4200: Loss = -11486.342711240237
Iteration 4300: Loss = -11486.338509240744
Iteration 4400: Loss = -11486.335097668967
Iteration 4500: Loss = -11486.337392153342
1
Iteration 4600: Loss = -11483.744468273739
Iteration 4700: Loss = -11483.687507118013
Iteration 4800: Loss = -11483.664869147382
Iteration 4900: Loss = -11483.649133600467
Iteration 5000: Loss = -11483.648250542417
Iteration 5100: Loss = -11483.644095107064
Iteration 5200: Loss = -11483.644143109632
1
Iteration 5300: Loss = -11483.641059001044
Iteration 5400: Loss = -11483.64236120453
1
Iteration 5500: Loss = -11483.646892028366
2
Iteration 5600: Loss = -11483.642222491311
3
Iteration 5700: Loss = -11483.636497831145
Iteration 5800: Loss = -11483.63682923801
1
Iteration 5900: Loss = -11483.637779608713
2
Iteration 6000: Loss = -11483.639079060098
3
Iteration 6100: Loss = -11483.635277222105
Iteration 6200: Loss = -11483.632051174483
Iteration 6300: Loss = -11483.631406608503
Iteration 6400: Loss = -11483.63020816225
Iteration 6500: Loss = -11483.629189409243
Iteration 6600: Loss = -11483.6299320585
1
Iteration 6700: Loss = -11483.627335388272
Iteration 6800: Loss = -11483.621060325891
Iteration 6900: Loss = -11483.607879036166
Iteration 7000: Loss = -11482.736453182195
Iteration 7100: Loss = -11482.741868731906
1
Iteration 7200: Loss = -11482.734884167516
Iteration 7300: Loss = -11482.738516789936
1
Iteration 7400: Loss = -11482.74153826997
2
Iteration 7500: Loss = -11482.73431599017
Iteration 7600: Loss = -11482.735403366438
1
Iteration 7700: Loss = -11482.732669015584
Iteration 7800: Loss = -11482.731942431305
Iteration 7900: Loss = -11482.731587344128
Iteration 8000: Loss = -11482.73033276483
Iteration 8100: Loss = -11482.729643980036
Iteration 8200: Loss = -11482.737128832016
1
Iteration 8300: Loss = -11482.727656351894
Iteration 8400: Loss = -11482.725847626123
Iteration 8500: Loss = -11482.72723937904
1
Iteration 8600: Loss = -11480.86491640067
Iteration 8700: Loss = -11479.319420265421
Iteration 8800: Loss = -11479.253299178163
Iteration 8900: Loss = -11479.251038138276
Iteration 9000: Loss = -11479.249760961917
Iteration 9100: Loss = -11479.232406303301
Iteration 9200: Loss = -11468.706786225624
Iteration 9300: Loss = -11468.707064576305
1
Iteration 9400: Loss = -11468.702858466995
Iteration 9500: Loss = -11468.70927147695
1
Iteration 9600: Loss = -11468.701420064626
Iteration 9700: Loss = -11468.837300630657
1
Iteration 9800: Loss = -11468.69701426693
Iteration 9900: Loss = -11468.696850433409
Iteration 10000: Loss = -11468.726719560153
1
Iteration 10100: Loss = -11468.696484457172
Iteration 10200: Loss = -11468.696183372487
Iteration 10300: Loss = -11468.695887752921
Iteration 10400: Loss = -11468.681356209767
Iteration 10500: Loss = -11468.519865759437
Iteration 10600: Loss = -11468.540410147196
1
Iteration 10700: Loss = -11468.517782202565
Iteration 10800: Loss = -11468.516981962011
Iteration 10900: Loss = -11468.564209810665
1
Iteration 11000: Loss = -11468.495639148181
Iteration 11100: Loss = -11468.49864203696
1
Iteration 11200: Loss = -11468.452285998152
Iteration 11300: Loss = -11468.831023833724
1
Iteration 11400: Loss = -11468.452185102695
Iteration 11500: Loss = -11468.452428182543
1
Iteration 11600: Loss = -11468.452342765833
2
Iteration 11700: Loss = -11468.529996226742
3
Iteration 11800: Loss = -11468.451861316435
Iteration 11900: Loss = -11468.459501842355
1
Iteration 12000: Loss = -11468.452762903898
2
Iteration 12100: Loss = -11468.445222346976
Iteration 12200: Loss = -11468.491149141833
1
Iteration 12300: Loss = -11468.445070535086
Iteration 12400: Loss = -11468.494498585478
1
Iteration 12500: Loss = -11468.444992441335
Iteration 12600: Loss = -11468.450394947213
1
Iteration 12700: Loss = -11468.444944807014
Iteration 12800: Loss = -11468.444878096267
Iteration 12900: Loss = -11468.454032543066
1
Iteration 13000: Loss = -11468.444794645364
Iteration 13100: Loss = -11468.444818582719
1
Iteration 13200: Loss = -11468.449599993643
2
Iteration 13300: Loss = -11468.53471764285
3
Iteration 13400: Loss = -11468.450891183995
4
Iteration 13500: Loss = -11468.451928135015
5
Iteration 13600: Loss = -11468.430716040675
Iteration 13700: Loss = -11468.431698350932
1
Iteration 13800: Loss = -11468.430536062402
Iteration 13900: Loss = -11468.43055057464
1
Iteration 14000: Loss = -11468.462485465956
2
Iteration 14100: Loss = -11468.430437715644
Iteration 14200: Loss = -11468.518074763553
1
Iteration 14300: Loss = -11468.496844167843
2
Iteration 14400: Loss = -11468.43377452206
3
Iteration 14500: Loss = -11468.434710828022
4
Iteration 14600: Loss = -11468.430543881243
5
Iteration 14700: Loss = -11468.430597250912
6
Iteration 14800: Loss = -11468.430318430812
Iteration 14900: Loss = -11468.433345125115
1
Iteration 15000: Loss = -11468.43023855432
Iteration 15100: Loss = -11468.430171040265
Iteration 15200: Loss = -11468.429567222085
Iteration 15300: Loss = -11468.420903275977
Iteration 15400: Loss = -11468.452932837643
1
Iteration 15500: Loss = -11468.42229236943
2
Iteration 15600: Loss = -11468.41997485321
Iteration 15700: Loss = -11468.421494706843
1
Iteration 15800: Loss = -11468.712652687467
2
Iteration 15900: Loss = -11468.419975110772
3
Iteration 16000: Loss = -11468.375272423498
Iteration 16100: Loss = -11468.555743675211
1
Iteration 16200: Loss = -11468.342826636033
Iteration 16300: Loss = -11468.344727604112
1
Iteration 16400: Loss = -11468.342792928155
Iteration 16500: Loss = -11468.355271062925
1
Iteration 16600: Loss = -11468.342546049833
Iteration 16700: Loss = -11468.345987512794
1
Iteration 16800: Loss = -11468.367441432647
2
Iteration 16900: Loss = -11468.34247614594
Iteration 17000: Loss = -11457.897367210437
Iteration 17100: Loss = -11457.888030736081
Iteration 17200: Loss = -11457.79921810923
Iteration 17300: Loss = -11457.79894174185
Iteration 17400: Loss = -11457.797723195767
Iteration 17500: Loss = -11457.762362724148
Iteration 17600: Loss = -11457.815685643707
1
Iteration 17700: Loss = -11457.761468335859
Iteration 17800: Loss = -11457.761407743847
Iteration 17900: Loss = -11457.761659306263
1
Iteration 18000: Loss = -11457.761412308748
2
Iteration 18100: Loss = -11457.793194405142
3
Iteration 18200: Loss = -11457.761385813294
Iteration 18300: Loss = -11457.762992959577
1
Iteration 18400: Loss = -11457.76656397563
2
Iteration 18500: Loss = -11457.761119046438
Iteration 18600: Loss = -11457.761856717916
1
Iteration 18700: Loss = -11457.76238081953
2
Iteration 18800: Loss = -11457.761047598196
Iteration 18900: Loss = -11457.760836282781
Iteration 19000: Loss = -11457.761085143275
1
Iteration 19100: Loss = -11457.765890332144
2
Iteration 19200: Loss = -11457.761593352077
3
Iteration 19300: Loss = -11457.760561627918
Iteration 19400: Loss = -11457.765023502407
1
Iteration 19500: Loss = -11457.763226615452
2
Iteration 19600: Loss = -11457.760976521093
3
Iteration 19700: Loss = -11457.760523173236
Iteration 19800: Loss = -11457.767388726817
1
Iteration 19900: Loss = -11457.76515894464
2
tensor([[ 1.6474e+00, -3.1517e+00],
        [ 7.4631e-01, -2.5073e+00],
        [ 3.9119e+00, -5.3134e+00],
        [-9.1111e+00,  7.7009e+00],
        [-9.0759e+00,  7.6198e+00],
        [-4.1632e+00,  2.2737e+00],
        [-9.9817e+00,  8.5200e+00],
        [-1.3211e+00, -3.2941e+00],
        [-8.4661e+00,  7.0650e+00],
        [-1.0157e+01,  8.7283e+00],
        [-5.5160e+00,  3.5250e+00],
        [ 1.5332e+00, -2.9663e+00],
        [-7.4657e+00,  5.9351e+00],
        [ 3.1037e+00, -4.9643e+00],
        [-9.7030e+00,  8.2238e+00],
        [-7.1030e+00,  5.4795e+00],
        [-5.9049e+00,  3.9338e+00],
        [-9.5160e+00,  7.9105e+00],
        [ 1.5359e+00, -3.1157e+00],
        [-6.4695e+00,  5.0730e+00],
        [-5.5895e+00,  4.1917e+00],
        [-2.7094e+00,  1.2156e+00],
        [ 4.4875e+00, -5.9914e+00],
        [-8.9921e+00,  7.2057e+00],
        [ 2.8052e-02, -1.8572e+00],
        [-2.4521e+00,  8.4408e-01],
        [-6.7946e+00,  4.9728e+00],
        [-1.0142e+01,  8.1747e+00],
        [ 6.2154e-02, -3.3958e+00],
        [-7.6094e+00,  6.1144e+00],
        [-5.6888e-01, -2.2813e+00],
        [-4.7033e+00,  2.8978e+00],
        [ 2.9326e+00, -5.8772e+00],
        [-5.0472e+00,  3.5776e+00],
        [ 3.6879e+00, -5.2026e+00],
        [-1.0950e+01,  9.3151e+00],
        [-9.8827e+00,  8.4867e+00],
        [-4.9114e+00,  3.5106e+00],
        [ 5.2845e+00, -6.9952e+00],
        [-2.6026e+00,  1.0382e+00],
        [-1.0470e+01,  8.9262e+00],
        [-5.3739e+00,  2.1902e+00],
        [-8.0704e-02, -1.3886e+00],
        [ 2.0749e+00, -3.7746e+00],
        [ 6.4025e+00, -8.1397e+00],
        [-8.4819e+00,  7.0193e+00],
        [-9.7615e+00,  8.3503e+00],
        [-8.5093e+00,  6.0568e+00],
        [-4.7323e+00,  3.3446e+00],
        [-1.2316e+00, -1.5644e-01],
        [-6.1483e+00,  4.3990e+00],
        [-1.0261e+01,  8.8582e+00],
        [-3.7696e+00,  2.3122e+00],
        [-6.0754e+00,  4.5637e+00],
        [ 4.3752e-01, -1.8471e+00],
        [ 2.0141e+00, -4.2310e+00],
        [ 3.9855e+00, -5.8242e+00],
        [-9.3933e+00,  6.1469e+00],
        [-1.0045e+01,  8.4265e+00],
        [-1.0601e+01,  8.4303e+00],
        [-3.6888e+00,  2.1592e+00],
        [-9.7990e+00,  7.6773e+00],
        [-1.0120e+01,  8.6987e+00],
        [ 6.4758e+00, -7.9078e+00],
        [-6.5891e+00,  5.0690e+00],
        [ 3.8125e+00, -5.4190e+00],
        [-7.9512e+00,  6.0665e+00],
        [ 3.1449e+00, -4.5338e+00],
        [-8.7594e+00,  7.3332e+00],
        [-9.7073e-01, -1.9432e+00],
        [-1.1120e+00, -6.8272e-01],
        [-1.0848e+01,  8.8695e+00],
        [ 1.7803e+00, -3.5713e+00],
        [-1.0539e+01,  8.3763e+00],
        [-3.3558e+00,  1.9693e+00],
        [-1.0482e+01,  9.0629e+00],
        [-1.4194e-03, -1.3900e+00],
        [-5.9934e+00,  4.6060e+00],
        [-5.4595e+00,  1.6532e+00],
        [ 4.0136e+00, -5.4020e+00],
        [-9.9224e+00,  8.4440e+00],
        [ 5.4529e+00, -6.8824e+00],
        [-5.3833e+00,  3.8197e+00],
        [-8.4055e+00,  6.9707e+00],
        [ 8.2160e-01, -2.4005e+00],
        [-7.7184e+00,  5.7849e+00],
        [ 1.2363e+00, -2.7306e+00],
        [ 3.0805e-01, -3.4109e+00],
        [-5.8340e+00,  4.4406e+00],
        [ 2.5878e+00, -3.9742e+00],
        [ 1.0936e+00, -2.5694e+00],
        [-9.1638e+00,  7.7541e+00],
        [-1.0192e+01,  8.1328e+00],
        [ 3.1069e+00, -7.7221e+00],
        [-1.0427e+01,  8.6292e+00],
        [-2.2524e+00,  8.1841e-01],
        [ 2.1577e+00, -3.6921e+00],
        [-2.5538e+00,  1.0560e+00],
        [ 3.5039e+00, -5.1583e+00],
        [-3.7216e+00,  1.4617e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.4659, 0.5341],
        [0.6731, 0.3269]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3546, 0.6454], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2763, 0.0955],
         [0.3152, 0.2343]],

        [[0.4702, 0.1005],
         [0.2472, 0.4325]],

        [[0.1406, 0.1048],
         [0.0051, 0.7533]],

        [[0.6147, 0.0972],
         [0.3500, 0.0277]],

        [[0.9932, 0.0993],
         [0.8270, 0.9595]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 93
Adjusted Rand Index: 0.7367933557689339
time is 1
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9598877889442053
time is 2
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8823435719624108
time is 3
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448373142749671
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
Global Adjusted Rand Index: 0.04649386726846644
Average Adjusted Rand Index: 0.8689335336592338
Iteration 0: Loss = -20449.367526767768
Iteration 10: Loss = -11369.33231717384
Iteration 20: Loss = -11369.233221519411
Iteration 30: Loss = -11369.230666631649
Iteration 40: Loss = -11369.230593241904
Iteration 50: Loss = -11369.230594382883
1
Iteration 60: Loss = -11369.230592556267
Iteration 70: Loss = -11369.230592556267
1
Iteration 80: Loss = -11369.230592556267
2
Iteration 90: Loss = -11369.230592556267
3
Stopping early at iteration 89 due to no improvement.
pi: tensor([[0.7058, 0.2942],
        [0.2495, 0.7505]], dtype=torch.float64)
alpha: tensor([0.4453, 0.5547])
beta: tensor([[[0.1901, 0.0981],
         [0.8611, 0.2948]],

        [[0.1978, 0.1014],
         [0.1067, 0.4724]],

        [[0.9594, 0.1087],
         [0.6455, 0.8505]],

        [[0.9850, 0.0983],
         [0.8418, 0.3303]],

        [[0.6635, 0.1018],
         [0.8301, 0.2437]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 1
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9598877889442053
time is 2
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8823823075947004
time is 3
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448373142749671
time is 4
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208023791611811
Global Adjusted Rand Index: 0.9061019342890484
Average Adjusted Rand Index: 0.905742084713997
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20449.21808418291
Iteration 100: Loss = -11642.510598179382
Iteration 200: Loss = -11539.708771904998
Iteration 300: Loss = -11522.91528256865
Iteration 400: Loss = -11487.420749032994
Iteration 500: Loss = -11461.794118486107
Iteration 600: Loss = -11453.582511403116
Iteration 700: Loss = -11452.358342588637
Iteration 800: Loss = -11448.615106784502
Iteration 900: Loss = -11403.204369948307
Iteration 1000: Loss = -11386.985120832702
Iteration 1100: Loss = -11386.682679536636
Iteration 1200: Loss = -11386.598504434009
Iteration 1300: Loss = -11386.552554790846
Iteration 1400: Loss = -11386.522123682098
Iteration 1500: Loss = -11386.49957718472
Iteration 1600: Loss = -11386.481189297647
Iteration 1700: Loss = -11386.465976760568
Iteration 1800: Loss = -11386.455537724818
Iteration 1900: Loss = -11386.447150741506
Iteration 2000: Loss = -11386.439613607832
Iteration 2100: Loss = -11386.432946498848
Iteration 2200: Loss = -11386.427429909923
Iteration 2300: Loss = -11386.422020629812
Iteration 2400: Loss = -11386.413982154309
Iteration 2500: Loss = -11386.30624682518
Iteration 2600: Loss = -11384.351032186363
Iteration 2700: Loss = -11384.34317872239
Iteration 2800: Loss = -11384.340341158326
Iteration 2900: Loss = -11384.338190249287
Iteration 3000: Loss = -11384.336457113632
Iteration 3100: Loss = -11384.3349068017
Iteration 3200: Loss = -11384.333384904041
Iteration 3300: Loss = -11384.33171577808
Iteration 3400: Loss = -11384.329303741486
Iteration 3500: Loss = -11384.314559705426
Iteration 3600: Loss = -11378.474841958569
Iteration 3700: Loss = -11378.471683682572
Iteration 3800: Loss = -11378.46983214263
Iteration 3900: Loss = -11378.468868383727
Iteration 4000: Loss = -11378.478116608536
1
Iteration 4100: Loss = -11378.467389884545
Iteration 4200: Loss = -11378.46676808096
Iteration 4300: Loss = -11378.466241214257
Iteration 4400: Loss = -11378.46574519225
Iteration 4500: Loss = -11378.465296151982
Iteration 4600: Loss = -11378.464872253477
Iteration 4700: Loss = -11378.464866232214
Iteration 4800: Loss = -11378.464122009516
Iteration 4900: Loss = -11378.463728682362
Iteration 5000: Loss = -11378.463347411858
Iteration 5100: Loss = -11378.463284360554
Iteration 5200: Loss = -11378.462545545097
Iteration 5300: Loss = -11378.461276785538
Iteration 5400: Loss = -11378.459394418054
Iteration 5500: Loss = -11378.47350351722
1
Iteration 5600: Loss = -11378.458687227034
Iteration 5700: Loss = -11378.458676861002
Iteration 5800: Loss = -11378.458385229676
Iteration 5900: Loss = -11378.467816009847
1
Iteration 6000: Loss = -11378.458736752867
2
Iteration 6100: Loss = -11378.457652216915
Iteration 6200: Loss = -11378.456424082688
Iteration 6300: Loss = -11378.457883733283
1
Iteration 6400: Loss = -11378.456892477745
2
Iteration 6500: Loss = -11378.455817515736
Iteration 6600: Loss = -11378.456600020096
1
Iteration 6700: Loss = -11378.468223166798
2
Iteration 6800: Loss = -11378.455392979211
Iteration 6900: Loss = -11378.456188172137
1
Iteration 7000: Loss = -11378.455177371883
Iteration 7100: Loss = -11378.45513728578
Iteration 7200: Loss = -11378.46889257578
1
Iteration 7300: Loss = -11378.455524692848
2
Iteration 7400: Loss = -11378.483728534758
3
Iteration 7500: Loss = -11378.460695640817
4
Iteration 7600: Loss = -11378.4540705995
Iteration 7700: Loss = -11378.45393499361
Iteration 7800: Loss = -11378.459027566862
1
Iteration 7900: Loss = -11378.457633964232
2
Iteration 8000: Loss = -11378.46463986417
3
Iteration 8100: Loss = -11378.453893801427
Iteration 8200: Loss = -11378.45393147992
1
Iteration 8300: Loss = -11378.59344292321
2
Iteration 8400: Loss = -11378.453610969984
Iteration 8500: Loss = -11378.453768994976
1
Iteration 8600: Loss = -11378.49569287709
2
Iteration 8700: Loss = -11378.454010252166
3
Iteration 8800: Loss = -11378.454674652501
4
Iteration 8900: Loss = -11378.454264438908
5
Iteration 9000: Loss = -11378.452396647277
Iteration 9100: Loss = -11378.451497652924
Iteration 9200: Loss = -11378.454177595231
1
Iteration 9300: Loss = -11378.454015602083
2
Iteration 9400: Loss = -11378.460804647893
3
Iteration 9500: Loss = -11378.45802722099
4
Iteration 9600: Loss = -11378.452771882987
5
Iteration 9700: Loss = -11378.418311253961
Iteration 9800: Loss = -11378.416329737442
Iteration 9900: Loss = -11378.41814803574
1
Iteration 10000: Loss = -11378.416215672085
Iteration 10100: Loss = -11378.416966737279
1
Iteration 10200: Loss = -11378.416254712904
2
Iteration 10300: Loss = -11378.416177495525
Iteration 10400: Loss = -11378.417178794394
1
Iteration 10500: Loss = -11378.425471983184
2
Iteration 10600: Loss = -11378.416156815103
Iteration 10700: Loss = -11378.427410710734
1
Iteration 10800: Loss = -11378.41682581992
2
Iteration 10900: Loss = -11378.415948743697
Iteration 11000: Loss = -11378.458828201394
1
Iteration 11100: Loss = -11378.41575185207
Iteration 11200: Loss = -11378.416363595179
1
Iteration 11300: Loss = -11367.422689529463
Iteration 11400: Loss = -11367.42208312758
Iteration 11500: Loss = -11367.426710984813
1
Iteration 11600: Loss = -11367.42528807362
2
Iteration 11700: Loss = -11367.420941148192
Iteration 11800: Loss = -11367.424614595438
1
Iteration 11900: Loss = -11367.453272596265
2
Iteration 12000: Loss = -11367.424575248553
3
Iteration 12100: Loss = -11367.42087098331
Iteration 12200: Loss = -11367.421727152027
1
Iteration 12300: Loss = -11367.44268487329
2
Iteration 12400: Loss = -11367.426294992802
3
Iteration 12500: Loss = -11367.42723603397
4
Iteration 12600: Loss = -11367.421253688633
5
Iteration 12700: Loss = -11367.421056656003
6
Iteration 12800: Loss = -11367.430442119583
7
Iteration 12900: Loss = -11367.420917294203
8
Iteration 13000: Loss = -11367.452160325762
9
Iteration 13100: Loss = -11367.42101802698
10
Stopping early at iteration 13100 due to no improvement.
tensor([[ 3.8575, -5.5775],
        [ 3.4661, -5.7514],
        [ 5.9144, -7.3883],
        [-7.1347,  5.6855],
        [-8.1677,  6.7671],
        [-2.3176,  0.1083],
        [-8.0630,  6.5562],
        [ 3.0757, -4.5358],
        [-7.6378,  6.2488],
        [-8.3751,  6.2940],
        [-4.7967,  3.2262],
        [ 2.4738, -4.1534],
        [-6.2465,  4.2913],
        [ 5.2934, -6.7827],
        [-5.4299,  3.9947],
        [-6.0457,  1.4305],
        [-5.0545,  3.6663],
        [-5.0427,  3.6001],
        [ 3.7908, -5.7266],
        [-6.4250,  4.9942],
        [-3.8831,  2.4572],
        [-0.1728, -2.5245],
        [ 5.3754, -6.9796],
        [-4.9616,  2.7512],
        [ 2.4296, -4.7424],
        [ 1.5707, -2.9646],
        [-6.2611,  4.6222],
        [-7.5839,  4.9156],
        [ 4.5886, -5.9810],
        [-5.9228,  4.3875],
        [ 0.9549, -2.6745],
        [-3.2297,  0.2790],
        [ 4.7218, -7.5816],
        [-4.4462,  2.9683],
        [ 6.0281, -8.1689],
        [-8.3208,  6.5076],
        [-7.7849,  6.3930],
        [-4.5424,  1.6369],
        [ 5.8916, -8.6678],
        [ 2.1808, -3.9036],
        [-9.0396,  5.4636],
        [-3.3263,  1.1886],
        [ 1.4322, -4.1355],
        [ 4.3392, -6.5226],
        [ 3.9333, -5.3245],
        [-8.1795,  6.7314],
        [-6.5800,  4.4387],
        [-7.7943,  6.4078],
        [-3.0480,  1.6087],
        [ 2.5511, -3.9561],
        [-6.0877,  4.4906],
        [-7.7746,  6.0895],
        [-3.5891,  1.8445],
        [-5.7040,  3.7315],
        [ 2.2561, -4.0345],
        [ 3.4408, -5.6490],
        [ 4.7124, -8.7426],
        [-8.0897,  5.9331],
        [-7.8690,  5.7238],
        [-6.3367,  3.9340],
        [-1.7543, -1.0123],
        [-4.4095,  2.8614],
        [-5.5291,  2.7230],
        [ 2.7749, -5.4387],
        [-5.6872,  4.3002],
        [ 3.9494, -5.6556],
        [-7.3611,  5.4149],
        [ 3.7678, -8.1698],
        [-8.4250,  6.5488],
        [ 2.5220, -3.9103],
        [ 2.1727, -3.5793],
        [-8.2281,  6.8334],
        [ 2.9877, -7.4653],
        [-5.9645,  3.1520],
        [-2.0363,  0.5675],
        [-8.2718,  6.8490],
        [ 1.2956, -2.8163],
        [-5.4412,  3.9985],
        [-4.3266,  2.8689],
        [ 3.5048, -5.3576],
        [-2.7230,  1.2570],
        [ 4.3342, -5.8314],
        [-4.6928,  1.8117],
        [-7.2967,  5.7125],
        [ 2.4308, -3.8739],
        [-5.5066,  3.4771],
        [ 2.4776, -4.7305],
        [ 2.7656, -6.4748],
        [-4.7509,  3.3342],
        [ 3.1987, -5.7767],
        [ 2.7653, -4.1543],
        [-7.9761,  5.8959],
        [-7.9236,  6.5373],
        [ 4.7863, -6.8272],
        [-8.0716,  3.4564],
        [ 0.1737, -4.7889],
        [ 4.0545, -5.8830],
        [ 0.7583, -2.7152],
        [ 5.9522, -7.3794],
        [-3.1175,  0.6785]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7140, 0.2860],
        [0.2418, 0.7582]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4339, 0.5661], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1949, 0.0982],
         [0.8611, 0.2999]],

        [[0.1978, 0.1006],
         [0.1067, 0.4724]],

        [[0.9594, 0.1077],
         [0.6455, 0.8505]],

        [[0.9850, 0.0981],
         [0.8418, 0.3303]],

        [[0.6635, 0.1017],
         [0.8301, 0.2437]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 1
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9598877889442053
time is 2
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8823823075947004
time is 3
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448373142749671
time is 4
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208023791611811
Global Adjusted Rand Index: 0.9061019342890484
Average Adjusted Rand Index: 0.905742084713997
11403.158788486695
new:  [0.04649386726846644, 0.8984868343518893, 0.04649386726846644, 0.9061019342890484] [0.8689335336592338, 0.8982285104303095, 0.8689335336592338, 0.905742084713997] [11457.779825578526, 11365.995831720476, 11457.761411052097, 11367.42101802698]
prior:  [0.39819276833990513, 0.9061019342890484, 0.39819276833990513, 0.9061019342890484] [0.8282328432731744, 0.905742084713997, 0.8282328432731744, 0.905742084713997] [11440.88772483818, 11369.230594409712, 11440.887761428396, 11369.230592556267]
-----------------------------------------------------------------------------------------
This iteration is 2
True Objective function: Loss = -11261.845722400833
Iteration 0: Loss = -19401.40302716299
Iteration 10: Loss = -11631.481799426007
Iteration 20: Loss = -11248.620095865148
Iteration 30: Loss = -11248.611690675278
Iteration 40: Loss = -11248.611464674497
Iteration 50: Loss = -11248.61144817149
Iteration 60: Loss = -11248.611448834383
1
Iteration 70: Loss = -11248.611453026935
2
Iteration 80: Loss = -11248.611453026786
3
Stopping early at iteration 79 due to no improvement.
pi: tensor([[0.7458, 0.2542],
        [0.2034, 0.7966]], dtype=torch.float64)
alpha: tensor([0.4520, 0.5480])
beta: tensor([[[0.1911, 0.1004],
         [0.6568, 0.2990]],

        [[0.7572, 0.0994],
         [0.4733, 0.2468]],

        [[0.1947, 0.1002],
         [0.0109, 0.4696]],

        [[0.7222, 0.1012],
         [0.6576, 0.1405]],

        [[0.9615, 0.0904],
         [0.6649, 0.3395]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208093606567974
time is 1
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448582102964589
time is 2
tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 4
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9368928970803924
Average Adjusted Rand Index: 0.9371333261237875
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19409.837501059956
Iteration 100: Loss = -11649.185386721847
Iteration 200: Loss = -11647.046206030851
Iteration 300: Loss = -11646.333836721169
Iteration 400: Loss = -11645.96633747242
Iteration 500: Loss = -11645.461489021964
Iteration 600: Loss = -11643.771562713964
Iteration 700: Loss = -11642.823444345762
Iteration 800: Loss = -11639.927038443559
Iteration 900: Loss = -11635.103683545343
Iteration 1000: Loss = -11593.62636727892
Iteration 1100: Loss = -11491.5728991958
Iteration 1200: Loss = -11426.613261032178
Iteration 1300: Loss = -11410.4572905183
Iteration 1400: Loss = -11371.073598108067
Iteration 1500: Loss = -11334.799329931919
Iteration 1600: Loss = -11310.069418346216
Iteration 1700: Loss = -11309.806247583236
Iteration 1800: Loss = -11309.036131216595
Iteration 1900: Loss = -11301.56928187175
Iteration 2000: Loss = -11268.196002759629
Iteration 2100: Loss = -11264.095815561803
Iteration 2200: Loss = -11253.625385526884
Iteration 2300: Loss = -11253.58459610562
Iteration 2400: Loss = -11253.566941733612
Iteration 2500: Loss = -11253.554084832798
Iteration 2600: Loss = -11253.545697741873
Iteration 2700: Loss = -11253.538565427196
Iteration 2800: Loss = -11253.51709080574
Iteration 2900: Loss = -11253.500314272591
Iteration 3000: Loss = -11253.48126341786
Iteration 3100: Loss = -11253.477112815632
Iteration 3200: Loss = -11253.473170653942
Iteration 3300: Loss = -11253.469674247404
Iteration 3400: Loss = -11253.46714506262
Iteration 3500: Loss = -11253.464460673382
Iteration 3600: Loss = -11253.464384693729
Iteration 3700: Loss = -11249.04066682258
Iteration 3800: Loss = -11249.013611828648
Iteration 3900: Loss = -11249.01683039429
1
Iteration 4000: Loss = -11249.010602160783
Iteration 4100: Loss = -11249.011817744564
1
Iteration 4200: Loss = -11249.00863505576
Iteration 4300: Loss = -11249.052473797654
1
Iteration 4400: Loss = -11249.029384594523
2
Iteration 4500: Loss = -11249.006362325725
Iteration 4600: Loss = -11249.007205404065
1
Iteration 4700: Loss = -11249.03294800447
2
Iteration 4800: Loss = -11249.005463744259
Iteration 4900: Loss = -11249.00562920976
1
Iteration 5000: Loss = -11249.089520320927
2
Iteration 5100: Loss = -11249.003210253544
Iteration 5200: Loss = -11249.030200997742
1
Iteration 5300: Loss = -11249.002379095013
Iteration 5400: Loss = -11249.002028735926
Iteration 5500: Loss = -11249.001686312366
Iteration 5600: Loss = -11249.001283884956
Iteration 5700: Loss = -11249.00092215257
Iteration 5800: Loss = -11249.000519299774
Iteration 5900: Loss = -11248.999980407567
Iteration 6000: Loss = -11249.016529052296
1
Iteration 6100: Loss = -11249.049072548902
2
Iteration 6200: Loss = -11248.992259771447
Iteration 6300: Loss = -11248.990691283352
Iteration 6400: Loss = -11249.013514741946
1
Iteration 6500: Loss = -11248.994517205276
2
Iteration 6600: Loss = -11248.990023840784
Iteration 6700: Loss = -11248.989896193489
Iteration 6800: Loss = -11248.991759338138
1
Iteration 6900: Loss = -11248.984283614554
Iteration 7000: Loss = -11248.984596243728
1
Iteration 7100: Loss = -11248.984186882923
Iteration 7200: Loss = -11248.989414313139
1
Iteration 7300: Loss = -11248.986121323727
2
Iteration 7400: Loss = -11248.985331002024
3
Iteration 7500: Loss = -11249.037348350948
4
Iteration 7600: Loss = -11248.995694632853
5
Iteration 7700: Loss = -11249.007399203058
6
Iteration 7800: Loss = -11248.990571944276
7
Iteration 7900: Loss = -11248.98064473218
Iteration 8000: Loss = -11248.979910508944
Iteration 8100: Loss = -11248.986879746137
1
Iteration 8200: Loss = -11248.999609875216
2
Iteration 8300: Loss = -11248.983006279615
3
Iteration 8400: Loss = -11248.980161162564
4
Iteration 8500: Loss = -11248.980452239583
5
Iteration 8600: Loss = -11248.979728458717
Iteration 8700: Loss = -11249.013560528028
1
Iteration 8800: Loss = -11248.975395128538
Iteration 8900: Loss = -11248.970703247556
Iteration 9000: Loss = -11248.974651262468
1
Iteration 9100: Loss = -11248.968447111345
Iteration 9200: Loss = -11248.968327585795
Iteration 9300: Loss = -11248.976333702383
1
Iteration 9400: Loss = -11248.968250939055
Iteration 9500: Loss = -11248.969546882472
1
Iteration 9600: Loss = -11248.96823877505
Iteration 9700: Loss = -11248.968562757009
1
Iteration 9800: Loss = -11248.968148173213
Iteration 9900: Loss = -11248.968188252427
1
Iteration 10000: Loss = -11248.977269201947
2
Iteration 10100: Loss = -11248.96830077202
3
Iteration 10200: Loss = -11248.968017066276
Iteration 10300: Loss = -11248.96867710122
1
Iteration 10400: Loss = -11248.968212174532
2
Iteration 10500: Loss = -11248.968106116252
3
Iteration 10600: Loss = -11249.050030039754
4
Iteration 10700: Loss = -11248.971858335372
5
Iteration 10800: Loss = -11248.96797213528
Iteration 10900: Loss = -11248.968088272639
1
Iteration 11000: Loss = -11248.972342543364
2
Iteration 11100: Loss = -11248.98071795475
3
Iteration 11200: Loss = -11248.967889981866
Iteration 11300: Loss = -11248.967871916215
Iteration 11400: Loss = -11249.036131131903
1
Iteration 11500: Loss = -11249.189751421553
2
Iteration 11600: Loss = -11248.96781190711
Iteration 11700: Loss = -11248.968699484078
1
Iteration 11800: Loss = -11249.034453421194
2
Iteration 11900: Loss = -11248.967776452402
Iteration 12000: Loss = -11248.968468661084
1
Iteration 12100: Loss = -11249.17992345977
2
Iteration 12200: Loss = -11248.9672243325
Iteration 12300: Loss = -11248.969759266476
1
Iteration 12400: Loss = -11248.967716450748
2
Iteration 12500: Loss = -11249.035458513988
3
Iteration 12600: Loss = -11248.975342879123
4
Iteration 12700: Loss = -11248.967259678544
5
Iteration 12800: Loss = -11248.968994129642
6
Iteration 12900: Loss = -11248.977149969665
7
Iteration 13000: Loss = -11248.96741899972
8
Iteration 13100: Loss = -11248.967283954591
9
Iteration 13200: Loss = -11248.969104847005
10
Stopping early at iteration 13200 due to no improvement.
tensor([[ -5.7665,   1.1512],
        [ -5.5330,   0.9178],
        [  1.5074,  -6.1227],
        [  6.5751, -11.1903],
        [ -6.6601,   2.0449],
        [  7.8665, -12.4817],
        [  1.5355,  -6.1508],
        [ -9.2400,   4.6248],
        [ -8.7477,   4.1325],
        [  5.2891,  -9.9043],
        [ -6.9646,   2.3493],
        [ -7.6971,   3.0819],
        [-10.0296,   5.4144],
        [  7.9033, -12.5186],
        [ -6.7213,   2.1061],
        [  2.9398,  -7.5550],
        [  2.8411,  -7.4563],
        [ -8.6251,   4.0099],
        [ -6.3295,   1.7143],
        [  0.5145,  -5.1297],
        [ -6.6114,   1.9962],
        [ -6.2073,   1.5921],
        [  4.1379,  -8.7531],
        [ -6.7365,   2.1213],
        [  1.6480,  -6.2632],
        [  7.5076, -12.1228],
        [ -5.2220,   0.6068],
        [ -0.6373,  -3.9779],
        [ -8.6123,   3.9971],
        [  1.7949,  -6.4101],
        [ -4.7037,   0.0885],
        [  5.3733,  -9.9885],
        [ -7.1164,   2.5011],
        [ -8.3401,   3.7249],
        [ -4.7044,   0.0892],
        [ -4.7742,   0.1589],
        [ -9.5212,   4.9060],
        [  4.7052,  -9.3204],
        [  5.5536, -10.1688],
        [  0.5345,  -5.1497],
        [ -4.6375,   0.0223],
        [ -9.7070,   5.0918],
        [  5.0997,  -9.7149],
        [ -9.8149,   5.1996],
        [  3.5604,  -8.1756],
        [  4.9204,  -9.5356],
        [  4.0758,  -8.6910],
        [  0.3974,  -5.0127],
        [ -3.2773,  -1.3380],
        [  7.7255, -12.3407],
        [  8.0468, -12.6621],
        [  8.2149, -12.8301],
        [  5.2100,  -9.8253],
        [ -8.9641,   4.3489],
        [  3.1330,  -7.7482],
        [  2.5849,  -7.2001],
        [ -6.3127,   1.6975],
        [  4.5309,  -9.1462],
        [  6.4524, -11.0676],
        [ -6.2432,   1.6279],
        [  4.9660,  -9.5812],
        [  3.2435,  -7.8587],
        [  4.7928,  -9.4080],
        [  4.9856,  -9.6009],
        [  3.3940,  -8.0092],
        [  6.7788, -11.3940],
        [  7.5082, -12.1234],
        [ -7.5991,   2.9839],
        [ -5.2957,   0.6805],
        [  5.5818, -10.1970],
        [ -8.5636,   3.9484],
        [ -3.4954,  -1.1198],
        [ -5.5029,   0.8877],
        [ -6.1884,   1.5731],
        [ -6.6583,   2.0431],
        [ -5.9586,   1.3434],
        [ -6.2943,   1.6791],
        [ -5.7585,   1.1433],
        [  5.9134, -10.5286],
        [  3.5199,  -8.1351],
        [ -7.7682,   3.1530],
        [  7.6346, -12.2498],
        [  4.6854,  -9.3006],
        [  7.5927, -12.2079],
        [ -6.3415,   1.7263],
        [  0.8080,  -5.4232],
        [  3.9811,  -8.5963],
        [ -4.5385,  -0.0767],
        [ -5.9477,   1.3324],
        [  4.9648,  -9.5800],
        [  1.4138,  -6.0290],
        [  0.8317,  -5.4469],
        [  0.2756,  -4.8908],
        [ -8.9723,   4.3571],
        [  1.5585,  -6.1737],
        [ -8.1368,   3.5216],
        [  2.0382,  -6.6534],
        [ -2.6589,  -1.9563],
        [  6.6919, -11.3071],
        [ -5.8246,   1.2094]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.8091, 0.1909],
        [0.2480, 0.7520]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5358, 0.4642], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3034, 0.1011],
         [0.6568, 0.1962]],

        [[0.7572, 0.0985],
         [0.4733, 0.2468]],

        [[0.1947, 0.0992],
         [0.0109, 0.4696]],

        [[0.7222, 0.1004],
         [0.6576, 0.1405]],

        [[0.9615, 0.0903],
         [0.6649, 0.3395]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448582102964589
time is 2
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 4
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9368938376534556
Average Adjusted Rand Index: 0.9371328820871806
Iteration 0: Loss = -23306.588142973626
Iteration 10: Loss = -11646.005065028288
Iteration 20: Loss = -11637.870703641716
Iteration 30: Loss = -11620.041923636916
Iteration 40: Loss = -11615.237550005475
Iteration 50: Loss = -11249.167290901687
Iteration 60: Loss = -11248.609543027775
Iteration 70: Loss = -11248.611436643729
1
Iteration 80: Loss = -11248.611456515082
2
Iteration 90: Loss = -11248.611450899827
3
Stopping early at iteration 89 due to no improvement.
pi: tensor([[0.7966, 0.2034],
        [0.2542, 0.7458]], dtype=torch.float64)
alpha: tensor([0.5480, 0.4520])
beta: tensor([[[0.2990, 0.1004],
         [0.5645, 0.1911]],

        [[0.0943, 0.0994],
         [0.7396, 0.2779]],

        [[0.7308, 0.1002],
         [0.7634, 0.6553]],

        [[0.6129, 0.1012],
         [0.7837, 0.2577]],

        [[0.5579, 0.0904],
         [0.9962, 0.5121]]], dtype=torch.float64)
time is 0
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208093606567974
time is 1
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448582102964589
time is 2
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 4
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9368928970803924
Average Adjusted Rand Index: 0.9371333261237875
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23306.393432543126
Iteration 100: Loss = -11669.745483734436
Iteration 200: Loss = -11645.59734394201
Iteration 300: Loss = -11639.581762354364
Iteration 400: Loss = -11626.507724345502
Iteration 500: Loss = -11590.521779692033
Iteration 600: Loss = -11493.986660351995
Iteration 700: Loss = -11388.374491335906
Iteration 800: Loss = -11373.359828445024
Iteration 900: Loss = -11352.928715749364
Iteration 1000: Loss = -11342.053655709913
Iteration 1100: Loss = -11332.952347717715
Iteration 1200: Loss = -11332.59761861646
Iteration 1300: Loss = -11332.41166809675
Iteration 1400: Loss = -11329.423391176235
Iteration 1500: Loss = -11314.001346356543
Iteration 1600: Loss = -11313.89590995419
Iteration 1700: Loss = -11313.83739036453
Iteration 1800: Loss = -11312.526407881805
Iteration 1900: Loss = -11309.762005619028
Iteration 2000: Loss = -11309.321336207027
Iteration 2100: Loss = -11309.19328505609
Iteration 2200: Loss = -11309.171349992826
Iteration 2300: Loss = -11309.145400938562
Iteration 2400: Loss = -11305.647382024163
Iteration 2500: Loss = -11305.607717658744
Iteration 2600: Loss = -11305.579419103828
Iteration 2700: Loss = -11283.301938227421
Iteration 2800: Loss = -11283.007033393427
Iteration 2900: Loss = -11279.119522034107
Iteration 3000: Loss = -11279.10930532997
Iteration 3100: Loss = -11279.101095236449
Iteration 3200: Loss = -11279.093871384504
Iteration 3300: Loss = -11279.087008334032
Iteration 3400: Loss = -11279.079657809869
Iteration 3500: Loss = -11279.071134511836
Iteration 3600: Loss = -11279.064945934846
Iteration 3700: Loss = -11279.061206767352
Iteration 3800: Loss = -11279.057653504657
Iteration 3900: Loss = -11279.053947745748
Iteration 4000: Loss = -11279.049252208893
Iteration 4100: Loss = -11279.036479520035
Iteration 4200: Loss = -11275.970322280718
Iteration 4300: Loss = -11275.963429465924
Iteration 4400: Loss = -11275.956416040603
Iteration 4500: Loss = -11275.89253008272
Iteration 4600: Loss = -11271.774468380636
Iteration 4700: Loss = -11269.689806600945
Iteration 4800: Loss = -11257.260335596837
Iteration 4900: Loss = -11257.067276708829
Iteration 5000: Loss = -11257.062273642401
Iteration 5100: Loss = -11257.059011762689
Iteration 5200: Loss = -11257.056861512598
Iteration 5300: Loss = -11257.055430930628
Iteration 5400: Loss = -11257.054190756287
Iteration 5500: Loss = -11257.05313369276
Iteration 5600: Loss = -11257.052026957464
Iteration 5700: Loss = -11257.050841186385
Iteration 5800: Loss = -11257.049450059378
Iteration 5900: Loss = -11257.04853346671
Iteration 6000: Loss = -11257.050189359712
1
Iteration 6100: Loss = -11257.04694925226
Iteration 6200: Loss = -11257.049005810519
1
Iteration 6300: Loss = -11256.672230129121
Iteration 6400: Loss = -11256.671981136222
Iteration 6500: Loss = -11256.674331154596
1
Iteration 6600: Loss = -11256.670159241947
Iteration 6700: Loss = -11256.67141211494
1
Iteration 6800: Loss = -11256.65928195708
Iteration 6900: Loss = -11256.656192910275
Iteration 7000: Loss = -11256.655551066362
Iteration 7100: Loss = -11256.65683677633
1
Iteration 7200: Loss = -11256.654163660533
Iteration 7300: Loss = -11256.654327051878
1
Iteration 7400: Loss = -11256.655791722213
2
Iteration 7500: Loss = -11256.660487494843
3
Iteration 7600: Loss = -11256.653793402822
Iteration 7700: Loss = -11256.653072176958
Iteration 7800: Loss = -11256.656357134094
1
Iteration 7900: Loss = -11256.655427114274
2
Iteration 8000: Loss = -11250.268687565915
Iteration 8100: Loss = -11250.264791688525
Iteration 8200: Loss = -11250.26318680413
Iteration 8300: Loss = -11250.313512828494
1
Iteration 8400: Loss = -11250.257645431575
Iteration 8500: Loss = -11250.258385561778
1
Iteration 8600: Loss = -11250.257335106693
Iteration 8700: Loss = -11250.257323462676
Iteration 8800: Loss = -11250.257079018838
Iteration 8900: Loss = -11250.257187436719
1
Iteration 9000: Loss = -11250.256879446102
Iteration 9100: Loss = -11250.265589899942
1
Iteration 9200: Loss = -11250.256580202147
Iteration 9300: Loss = -11250.273327207206
1
Iteration 9400: Loss = -11250.255671510768
Iteration 9500: Loss = -11250.255580366204
Iteration 9600: Loss = -11250.259183226344
1
Iteration 9700: Loss = -11250.255416706439
Iteration 9800: Loss = -11250.256662062924
1
Iteration 9900: Loss = -11250.259911934281
2
Iteration 10000: Loss = -11250.357139248501
3
Iteration 10100: Loss = -11250.170557519818
Iteration 10200: Loss = -11250.170828280236
1
Iteration 10300: Loss = -11250.168159965551
Iteration 10400: Loss = -11250.173747940687
1
Iteration 10500: Loss = -11250.186262281193
2
Iteration 10600: Loss = -11250.179100046253
3
Iteration 10700: Loss = -11250.166296954436
Iteration 10800: Loss = -11250.172096100825
1
Iteration 10900: Loss = -11250.301327306262
2
Iteration 11000: Loss = -11250.167357226328
3
Iteration 11100: Loss = -11250.166249951664
Iteration 11200: Loss = -11250.1660601223
Iteration 11300: Loss = -11250.177411398026
1
Iteration 11400: Loss = -11250.175567255908
2
Iteration 11500: Loss = -11250.16342817246
Iteration 11600: Loss = -11250.162160176249
Iteration 11700: Loss = -11250.162058396007
Iteration 11800: Loss = -11250.212151788543
1
Iteration 11900: Loss = -11250.160721866463
Iteration 12000: Loss = -11250.181485148174
1
Iteration 12100: Loss = -11250.161114877501
2
Iteration 12200: Loss = -11250.160683653368
Iteration 12300: Loss = -11250.161018348386
1
Iteration 12400: Loss = -11250.160955030671
2
Iteration 12500: Loss = -11250.160764329523
3
Iteration 12600: Loss = -11250.16056585423
Iteration 12700: Loss = -11250.173302891162
1
Iteration 12800: Loss = -11250.186833035292
2
Iteration 12900: Loss = -11250.170039711002
3
Iteration 13000: Loss = -11250.163769117036
4
Iteration 13100: Loss = -11250.178909845994
5
Iteration 13200: Loss = -11250.15702616713
Iteration 13300: Loss = -11250.156493639586
Iteration 13400: Loss = -11250.139187018889
Iteration 13500: Loss = -11250.13922909545
1
Iteration 13600: Loss = -11250.140968330119
2
Iteration 13700: Loss = -11250.143583623149
3
Iteration 13800: Loss = -11250.140884921915
4
Iteration 13900: Loss = -11250.145776597537
5
Iteration 14000: Loss = -11250.19390713676
6
Iteration 14100: Loss = -11250.138696408056
Iteration 14200: Loss = -11250.181177213368
1
Iteration 14300: Loss = -11250.138654139859
Iteration 14400: Loss = -11250.301177525207
1
Iteration 14500: Loss = -11250.138640196365
Iteration 14600: Loss = -11250.13861405956
Iteration 14700: Loss = -11250.13864185903
1
Iteration 14800: Loss = -11250.138544939633
Iteration 14900: Loss = -11250.140395602975
1
Iteration 15000: Loss = -11250.138522750947
Iteration 15100: Loss = -11250.14119294511
1
Iteration 15200: Loss = -11250.139857181252
2
Iteration 15300: Loss = -11250.13852711444
3
Iteration 15400: Loss = -11250.138273722823
Iteration 15500: Loss = -11250.137533132009
Iteration 15600: Loss = -11250.137526101596
Iteration 15700: Loss = -11250.137124767136
Iteration 15800: Loss = -11250.139880798977
1
Iteration 15900: Loss = -11250.137080026083
Iteration 16000: Loss = -11250.2710258097
1
Iteration 16100: Loss = -11250.137086573792
2
Iteration 16200: Loss = -11250.137103779505
3
Iteration 16300: Loss = -11250.13811387216
4
Iteration 16400: Loss = -11250.137198475739
5
Iteration 16500: Loss = -11250.140639085801
6
Iteration 16600: Loss = -11250.13825530367
7
Iteration 16700: Loss = -11250.137204592238
8
Iteration 16800: Loss = -11250.138170256252
9
Iteration 16900: Loss = -11250.214520381753
10
Stopping early at iteration 16900 due to no improvement.
tensor([[ -4.0502,   2.6538],
        [ -4.1184,   2.0856],
        [  7.3273,  -9.0130],
        [  5.8783,  -8.6523],
        [ -6.4365,   1.8806],
        [  5.7227,  -7.1360],
        [  2.9279,  -4.7705],
        [ -7.8371,   5.4256],
        [ -6.8188,   5.4211],
        [  7.0885,  -8.6697],
        [ -5.2241,   3.6698],
        [ -6.3722,   3.9714],
        [ -9.5252,   7.0788],
        [  3.0209,  -4.8878],
        [ -4.9731,   3.4328],
        [  4.5012,  -5.8975],
        [  2.0789,  -6.1831],
        [ -9.6592,   8.2727],
        [ -5.0846,   2.7852],
        [  1.0274,  -2.4830],
        [ -5.0994,   3.4202],
        [ -4.5689,   2.7441],
        [  4.1489,  -6.8324],
        [ -5.0126,   3.5671],
        [  2.9910,  -4.9123],
        [  4.9325,  -6.3209],
        [ -4.5609,   3.0521],
        [  0.7610,  -2.6524],
        [ -8.3262,   3.7110],
        [  2.6256,  -5.2963],
        [ -4.1938,   2.5092],
        [  7.0424,  -8.5957],
        [ -5.5720,   3.7867],
        [ -9.0962,   7.7082],
        [ -4.0695,   2.6327],
        [ -3.7555,   0.7959],
        [ -7.8746,   5.8763],
        [  8.6546, -10.1123],
        [  5.2522,  -6.6768],
        [  2.1222,  -3.6464],
        [ -2.9839,   1.5962],
        [ -7.9225,   6.4850],
        [  6.6680,  -8.0957],
        [ -7.8335,   6.4305],
        [  4.6018,  -6.9991],
        [  5.8618,  -8.5159],
        [  4.6368,  -6.0301],
        [  1.8510,  -3.4430],
        [ -2.4806,  -0.7567],
        [  7.7990,  -9.3358],
        [  8.2478,  -9.6482],
        [  4.8510,  -7.0169],
        [  7.3509,  -8.8052],
        [ -7.1191,   5.7297],
        [  4.2411,  -6.2785],
        [  7.8918, -10.1747],
        [ -5.3098,   2.5990],
        [  6.6813,  -8.7252],
        [  4.5572,  -6.3847],
        [ -4.6190,   2.9452],
        [  7.6457,  -9.0901],
        [  4.8124,  -6.4217],
        [  3.9545,  -5.3453],
        [  7.3654,  -9.5489],
        [  4.4979,  -6.2262],
        [  3.6424,  -5.1354],
        [  8.0925, -12.7077],
        [ -5.7749,   4.3459],
        [ -3.5328,   2.1415],
        [  8.3604, -10.1904],
        [ -7.7478,   4.4069],
        [ -1.7039,   0.3130],
        [ -3.9046,   2.2984],
        [ -4.4512,   3.0577],
        [ -4.8985,   3.3771],
        [ -4.5987,   2.3628],
        [ -4.7695,   2.9141],
        [ -4.0682,   2.6113],
        [  0.1291,  -1.9187],
        [  5.0570,  -6.4669],
        [ -6.7303,   3.7484],
        [  6.8697,  -8.7740],
        [  6.5836,  -7.9946],
        [  2.1823,  -5.0091],
        [ -4.6650,   3.0506],
        [  0.6761,  -3.4584],
        [  3.9189,  -6.3931],
        [ -3.4483,   0.7397],
        [ -5.4891,   1.4387],
        [  6.3773,  -7.7822],
        [  1.7100,  -3.6942],
        [-10.1300,   8.2217],
        [  1.7818,  -3.1918],
        [ -7.0461,   5.6526],
        [  2.0795,  -3.4823],
        [ -7.2181,   3.9756],
        [  3.1959,  -5.2900],
        [ -1.1375,  -0.4756],
        [  7.5529,  -8.9746],
        [ -4.7650,   1.8743]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.8077, 0.1923],
        [0.2530, 0.7470]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5279, 0.4721], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3048, 0.1038],
         [0.5645, 0.1954]],

        [[0.0943, 0.0982],
         [0.7396, 0.2779]],

        [[0.7308, 0.0988],
         [0.7634, 0.6553]],

        [[0.6129, 0.1004],
         [0.7837, 0.2577]],

        [[0.5579, 0.0899],
         [0.9962, 0.5121]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 1
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448582102964589
time is 2
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 4
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9368946998208005
Average Adjusted Rand Index: 0.9371325814615584
Iteration 0: Loss = -23022.484384582982
Iteration 10: Loss = -11251.87316961551
Iteration 20: Loss = -11248.612923568277
Iteration 30: Loss = -11248.611551117361
Iteration 40: Loss = -11248.61145234639
Iteration 50: Loss = -11248.61144608983
Iteration 60: Loss = -11248.611455471691
1
Iteration 70: Loss = -11248.611457301862
2
Iteration 80: Loss = -11248.611448443124
3
Stopping early at iteration 79 due to no improvement.
pi: tensor([[0.7966, 0.2034],
        [0.2542, 0.7458]], dtype=torch.float64)
alpha: tensor([0.5480, 0.4520])
beta: tensor([[[0.2990, 0.1004],
         [0.3661, 0.1911]],

        [[0.7123, 0.0994],
         [0.6051, 0.3805]],

        [[0.9816, 0.1002],
         [0.2772, 0.9244]],

        [[0.1543, 0.1012],
         [0.6766, 0.7959]],

        [[0.6156, 0.0904],
         [0.1097, 0.4814]]], dtype=torch.float64)
time is 0
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208093606567974
time is 1
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448582102964589
time is 2
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 4
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9368928970803924
Average Adjusted Rand Index: 0.9371333261237875
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23022.200894292368
Iteration 100: Loss = -11821.902731517492
Iteration 200: Loss = -11759.62226381123
Iteration 300: Loss = -11655.090080152439
Iteration 400: Loss = -11561.707066329436
Iteration 500: Loss = -11553.38372988068
Iteration 600: Loss = -11552.232829931598
Iteration 700: Loss = -11538.85472915089
Iteration 800: Loss = -11529.719968990501
Iteration 900: Loss = -11525.661806490385
Iteration 1000: Loss = -11493.432043614835
Iteration 1100: Loss = -11420.153565416755
Iteration 1200: Loss = -11312.487341609232
Iteration 1300: Loss = -11303.645539747717
Iteration 1400: Loss = -11293.869038547373
Iteration 1500: Loss = -11281.412131173382
Iteration 1600: Loss = -11280.980660011395
Iteration 1700: Loss = -11280.696464879933
Iteration 1800: Loss = -11273.684824424783
Iteration 1900: Loss = -11273.487833159534
Iteration 2000: Loss = -11273.353199403862
Iteration 2100: Loss = -11263.687682842597
Iteration 2200: Loss = -11261.49822140375
Iteration 2300: Loss = -11261.41162247928
Iteration 2400: Loss = -11255.149557378989
Iteration 2500: Loss = -11252.917484674406
Iteration 2600: Loss = -11252.900527060547
Iteration 2700: Loss = -11252.81131279191
Iteration 2800: Loss = -11252.805370337435
Iteration 2900: Loss = -11252.800538044676
Iteration 3000: Loss = -11252.7963895486
Iteration 3100: Loss = -11252.792765187416
Iteration 3200: Loss = -11252.78965179684
Iteration 3300: Loss = -11252.786772830386
Iteration 3400: Loss = -11252.784285670576
Iteration 3500: Loss = -11252.78306023039
Iteration 3600: Loss = -11252.780040011936
Iteration 3700: Loss = -11252.778212826319
Iteration 3800: Loss = -11252.79180544045
1
Iteration 3900: Loss = -11252.775479644335
Iteration 4000: Loss = -11252.773730167755
Iteration 4100: Loss = -11252.77523948846
1
Iteration 4200: Loss = -11252.771333897963
Iteration 4300: Loss = -11252.77031200757
Iteration 4400: Loss = -11252.769525075462
Iteration 4500: Loss = -11252.768557434812
Iteration 4600: Loss = -11252.768184435088
Iteration 4700: Loss = -11252.76701514751
Iteration 4800: Loss = -11252.76760939529
1
Iteration 4900: Loss = -11252.765721427111
Iteration 5000: Loss = -11252.77561239958
1
Iteration 5100: Loss = -11252.764651888474
Iteration 5200: Loss = -11252.76762071003
1
Iteration 5300: Loss = -11252.76366236495
Iteration 5400: Loss = -11252.764270735452
1
Iteration 5500: Loss = -11252.762867192645
Iteration 5600: Loss = -11252.771038068751
1
Iteration 5700: Loss = -11252.762139584729
Iteration 5800: Loss = -11252.76217308181
1
Iteration 5900: Loss = -11252.761549631176
Iteration 6000: Loss = -11252.762382047566
1
Iteration 6100: Loss = -11252.760998403197
Iteration 6200: Loss = -11252.760792482262
Iteration 6300: Loss = -11252.78005821469
1
Iteration 6400: Loss = -11252.760279179623
Iteration 6500: Loss = -11252.760844316903
1
Iteration 6600: Loss = -11252.759882786599
Iteration 6700: Loss = -11252.75980587763
Iteration 6800: Loss = -11252.759817759561
1
Iteration 6900: Loss = -11252.75936661365
Iteration 7000: Loss = -11252.760073927388
1
Iteration 7100: Loss = -11252.761137457255
2
Iteration 7200: Loss = -11252.759359004347
Iteration 7300: Loss = -11252.758773845215
Iteration 7400: Loss = -11252.75912417712
1
Iteration 7500: Loss = -11252.758672069856
Iteration 7600: Loss = -11252.759194230375
1
Iteration 7700: Loss = -11252.782539544727
2
Iteration 7800: Loss = -11252.758675916632
3
Iteration 7900: Loss = -11252.757957101681
Iteration 8000: Loss = -11252.75781466082
Iteration 8100: Loss = -11252.757704692538
Iteration 8200: Loss = -11252.757797265947
1
Iteration 8300: Loss = -11252.757293756787
Iteration 8400: Loss = -11252.757314304148
1
Iteration 8500: Loss = -11252.756762926861
Iteration 8600: Loss = -11252.760461577822
1
Iteration 8700: Loss = -11252.850369433305
2
Iteration 8800: Loss = -11252.75762721902
3
Iteration 8900: Loss = -11252.762049264864
4
Iteration 9000: Loss = -11252.681305134116
Iteration 9100: Loss = -11250.903482243804
Iteration 9200: Loss = -11250.893516482376
Iteration 9300: Loss = -11250.889544058016
Iteration 9400: Loss = -11250.890206949472
1
Iteration 9500: Loss = -11250.899917657858
2
Iteration 9600: Loss = -11250.88943747455
Iteration 9700: Loss = -11250.891253089223
1
Iteration 9800: Loss = -11250.89251899305
2
Iteration 9900: Loss = -11250.889715196274
3
Iteration 10000: Loss = -11250.889896616944
4
Iteration 10100: Loss = -11250.88944278284
5
Iteration 10200: Loss = -11250.889998259028
6
Iteration 10300: Loss = -11250.889588408243
7
Iteration 10400: Loss = -11250.91655706604
8
Iteration 10500: Loss = -11250.888884498361
Iteration 10600: Loss = -11250.89195991044
1
Iteration 10700: Loss = -11250.888905410964
2
Iteration 10800: Loss = -11250.913515267011
3
Iteration 10900: Loss = -11250.890529775143
4
Iteration 11000: Loss = -11250.888856102458
Iteration 11100: Loss = -11250.891620066119
1
Iteration 11200: Loss = -11250.898097665058
2
Iteration 11300: Loss = -11251.02797208005
3
Iteration 11400: Loss = -11250.891340698709
4
Iteration 11500: Loss = -11250.891843279944
5
Iteration 11600: Loss = -11250.967686981076
6
Iteration 11700: Loss = -11250.890275803078
7
Iteration 11800: Loss = -11250.888517359239
Iteration 11900: Loss = -11250.889200252042
1
Iteration 12000: Loss = -11250.88849129959
Iteration 12100: Loss = -11250.889044432195
1
Iteration 12200: Loss = -11250.890061763168
2
Iteration 12300: Loss = -11250.562174243347
Iteration 12400: Loss = -11250.561981525652
Iteration 12500: Loss = -11250.564598354622
1
Iteration 12600: Loss = -11250.569614668617
2
Iteration 12700: Loss = -11250.57030626824
3
Iteration 12800: Loss = -11250.561880912068
Iteration 12900: Loss = -11250.562168725957
1
Iteration 13000: Loss = -11250.849127767755
2
Iteration 13100: Loss = -11250.558949941045
Iteration 13200: Loss = -11247.052223077644
Iteration 13300: Loss = -11247.057593078189
1
Iteration 13400: Loss = -11247.058684925345
2
Iteration 13500: Loss = -11247.04305116376
Iteration 13600: Loss = -11247.051351159407
1
Iteration 13700: Loss = -11247.048444237113
2
Iteration 13800: Loss = -11247.043225644953
3
Iteration 13900: Loss = -11247.045447471282
4
Iteration 14000: Loss = -11247.217334407767
5
Iteration 14100: Loss = -11247.043019279385
Iteration 14200: Loss = -11247.043272012856
1
Iteration 14300: Loss = -11247.060768888088
2
Iteration 14400: Loss = -11247.050075931626
3
Iteration 14500: Loss = -11247.050903439798
4
Iteration 14600: Loss = -11247.04340735198
5
Iteration 14700: Loss = -11247.043079742696
6
Iteration 14800: Loss = -11247.043256468234
7
Iteration 14900: Loss = -11247.047642788006
8
Iteration 15000: Loss = -11247.043399438093
9
Iteration 15100: Loss = -11247.043016319045
Iteration 15200: Loss = -11247.04517496893
1
Iteration 15300: Loss = -11247.042865303469
Iteration 15400: Loss = -11247.04301495738
1
Iteration 15500: Loss = -11247.042784196918
Iteration 15600: Loss = -11247.044536908323
1
Iteration 15700: Loss = -11247.042736172092
Iteration 15800: Loss = -11247.138589049717
1
Iteration 15900: Loss = -11247.042653289614
Iteration 16000: Loss = -11247.042632863288
Iteration 16100: Loss = -11247.043935156034
1
Iteration 16200: Loss = -11247.042622325755
Iteration 16300: Loss = -11247.044389488061
1
Iteration 16400: Loss = -11245.3062029811
Iteration 16500: Loss = -11245.295918948063
Iteration 16600: Loss = -11245.321400724531
1
Iteration 16700: Loss = -11245.291371618356
Iteration 16800: Loss = -11245.291329872842
Iteration 16900: Loss = -11245.291128702676
Iteration 17000: Loss = -11245.291355143107
1
Iteration 17100: Loss = -11245.202509236362
Iteration 17200: Loss = -11245.193714431985
Iteration 17300: Loss = -11245.187797243645
Iteration 17400: Loss = -11245.246313776664
1
Iteration 17500: Loss = -11245.186422333232
Iteration 17600: Loss = -11245.182988175433
Iteration 17700: Loss = -11245.182648372887
Iteration 17800: Loss = -11244.22983266132
Iteration 17900: Loss = -11244.109747678209
Iteration 18000: Loss = -11244.1097461314
Iteration 18100: Loss = -11244.10981787078
1
Iteration 18200: Loss = -11244.109723076255
Iteration 18300: Loss = -11244.111158049205
1
Iteration 18400: Loss = -11244.109860631766
2
Iteration 18500: Loss = -11244.10996545492
3
Iteration 18600: Loss = -11244.110647236746
4
Iteration 18700: Loss = -11244.107528745764
Iteration 18800: Loss = -11244.108811606773
1
Iteration 18900: Loss = -11244.107101209176
Iteration 19000: Loss = -11244.200963949228
1
Iteration 19100: Loss = -11244.106923945652
Iteration 19200: Loss = -11244.10725194609
1
Iteration 19300: Loss = -11244.107548535007
2
Iteration 19400: Loss = -11244.110164624577
3
Iteration 19500: Loss = -11244.107038450966
4
Iteration 19600: Loss = -11244.107304170693
5
Iteration 19700: Loss = -11244.107352939865
6
Iteration 19800: Loss = -11244.106909017994
Iteration 19900: Loss = -11244.10690730033
tensor([[-10.1307,   6.9551],
        [ -3.9463,   2.5422],
        [  1.9851,  -5.4340],
        [  6.3574,  -8.4947],
        [ -8.3103,   5.1285],
        [  5.7237,  -7.1103],
        [  3.0131,  -4.6954],
        [ -8.4715,   6.9273],
        [-10.7160,   8.3226],
        [  8.0065,  -9.5374],
        [ -5.8835,   3.4178],
        [-12.1588,   7.5435],
        [ -9.6831,   8.2354],
        [  1.9902,  -5.8144],
        [ -9.5430,   7.5658],
        [  7.7038,  -9.0902],
        [  4.3170,  -6.0343],
        [ -7.1044,   5.5155],
        [ -7.7652,   6.2292],
        [  2.0994,  -3.4886],
        [ -5.5880,   3.3775],
        [ -5.1016,   2.6691],
        [  5.8463,  -7.2880],
        [ -5.1769,   3.7658],
        [  3.0439,  -4.7119],
        [  4.5612,  -6.7102],
        [ -3.6307,   2.2421],
        [  0.8765,  -2.4732],
        [ -6.9992,   5.5889],
        [  2.5331,  -5.2897],
        [ -7.7855,   5.6123],
        [  6.9817,  -9.0580],
        [ -9.6499,   7.6923],
        [ -7.8261,   4.2994],
        [ -8.1329,   6.2643],
        [ -3.4896,   1.3948],
        [ -9.7171,   8.0731],
        [  7.1935,  -8.7524],
        [  6.3585,  -7.7457],
        [  1.7288,  -3.7937],
        [-10.5544,   6.0903],
        [-11.1596,   8.7971],
        [  8.5562, -11.9467],
        [-10.4476,   9.0613],
        [  5.1011,  -6.4927],
        [  8.7262, -10.1645],
        [  9.0931, -10.7568],
        [  7.5580, -10.8239],
        [ -2.1912,  -0.0947],
        [  4.6684,  -6.3636],
        [  4.8138,  -6.3035],
        [  5.2751,  -6.7110],
        [  5.8103,  -7.2518],
        [ -9.8137,   8.4044],
        [  4.3704,  -6.1957],
        [  4.1884,  -5.6414],
        [ -5.4728,   2.8955],
        [  3.9853,  -5.4516],
        [  4.6642,  -6.1892],
        [ -9.7112,   7.8013],
        [  5.9645,  -9.8453],
        [  4.1559,  -6.9779],
        [  3.6657,  -5.4929],
        [  7.7509,  -9.1392],
        [  3.5259,  -8.1411],
        [  4.7434,  -6.1966],
        [  4.6573,  -6.1100],
        [ -9.3281,   7.7350],
        [ -4.3340,   1.8028],
        [  6.3355,  -7.7642],
        [ -9.7966,   8.2582],
        [ -2.0950,   0.2580],
        [ -4.1370,   2.7250],
        [ -4.8450,   3.1465],
        [ -8.9479,   7.5430],
        [ -4.4298,   3.0378],
        [ -7.5997,   6.1224],
        [ -9.1929,   7.7796],
        [  0.0343,  -1.6327],
        [  4.8055,  -6.8357],
        [-10.5507,   8.2099],
        [  0.9729,  -2.6435],
        [  6.2603,  -8.3874],
        [  2.8171,  -4.3919],
        [-10.0759,   7.7937],
        [  2.2364,  -4.0103],
        [  9.0989, -11.6404],
        [ -3.0333,   1.5653],
        [ -8.8165,   5.8259],
        [  8.8919, -11.8139],
        [  2.9777,  -4.4955],
        [  2.4304,  -3.8217],
        [  1.5610,  -3.3006],
        [ -9.8772,   8.4577],
        [  7.4637,  -8.9414],
        [ -6.6116,   5.0589],
        [  7.6251,  -9.0199],
        [ -1.6046,  -0.5408],
        [  7.1733,  -8.6818],
        [-10.0621,   8.2757]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.8081, 0.1919],
        [0.2471, 0.7529]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5336, 0.4664], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3062, 0.1011],
         [0.3661, 0.1957]],

        [[0.7123, 0.0984],
         [0.6051, 0.3805]],

        [[0.9816, 0.0990],
         [0.2772, 0.9244]],

        [[0.1543, 0.1013],
         [0.6766, 0.7959]],

        [[0.6156, 0.0906],
         [0.1097, 0.4814]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448582102964589
time is 2
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 4
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9446702630164284
Average Adjusted Rand Index: 0.9449712659255646
Iteration 0: Loss = -39987.94888065132
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.3991,    nan]],

        [[0.9893,    nan],
         [0.1388, 0.9433]],

        [[0.1937,    nan],
         [0.5046, 0.0667]],

        [[0.5045,    nan],
         [0.3821, 0.9748]],

        [[0.9635,    nan],
         [0.8600, 0.2112]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -39984.05679406322
Iteration 100: Loss = -11675.100961161968
Iteration 200: Loss = -11655.95985527578
Iteration 300: Loss = -11651.958500988183
Iteration 400: Loss = -11649.918723142056
Iteration 500: Loss = -11648.71081690279
Iteration 600: Loss = -11647.93172975153
Iteration 700: Loss = -11647.398317675073
Iteration 800: Loss = -11647.019846489155
Iteration 900: Loss = -11646.761026878086
Iteration 1000: Loss = -11646.591950562613
Iteration 1100: Loss = -11646.469518768383
Iteration 1200: Loss = -11646.371350378986
Iteration 1300: Loss = -11646.291160623186
Iteration 1400: Loss = -11646.273234666132
Iteration 1500: Loss = -11646.169536164429
Iteration 1600: Loss = -11646.12245278289
Iteration 1700: Loss = -11646.081817926992
Iteration 1800: Loss = -11646.046666336319
Iteration 1900: Loss = -11646.015122763629
Iteration 2000: Loss = -11645.987321335137
Iteration 2100: Loss = -11646.039303114547
1
Iteration 2200: Loss = -11645.939782364438
Iteration 2300: Loss = -11645.919165639914
Iteration 2400: Loss = -11645.900113844995
Iteration 2500: Loss = -11645.882630614808
Iteration 2600: Loss = -11645.8656849338
Iteration 2700: Loss = -11645.849768111844
Iteration 2800: Loss = -11645.834359058781
Iteration 2900: Loss = -11645.819253680495
Iteration 3000: Loss = -11645.804249956274
Iteration 3100: Loss = -11645.789144195744
Iteration 3200: Loss = -11645.773280445012
Iteration 3300: Loss = -11645.756672063999
Iteration 3400: Loss = -11645.738834517313
Iteration 3500: Loss = -11645.749358905354
1
Iteration 3600: Loss = -11645.697461622216
Iteration 3700: Loss = -11645.672593552512
Iteration 3800: Loss = -11645.651232917682
Iteration 3900: Loss = -11645.591199355076
Iteration 4000: Loss = -11644.044709580012
Iteration 4100: Loss = -11643.51034792825
Iteration 4200: Loss = -11643.328360690768
Iteration 4300: Loss = -11643.176941708634
Iteration 4400: Loss = -11643.005632123279
Iteration 4500: Loss = -11642.696029636892
Iteration 4600: Loss = -11640.031431916415
Iteration 4700: Loss = -11638.034344454592
Iteration 4800: Loss = -11634.93437310414
Iteration 4900: Loss = -11606.813708660578
Iteration 5000: Loss = -11542.947454774077
Iteration 5100: Loss = -11525.440944805325
Iteration 5200: Loss = -11512.469838226138
Iteration 5300: Loss = -11508.097465254092
Iteration 5400: Loss = -11502.296113150276
Iteration 5500: Loss = -11502.046458470088
Iteration 5600: Loss = -11501.895620859124
Iteration 5700: Loss = -11498.842372011097
Iteration 5800: Loss = -11495.48021809332
Iteration 5900: Loss = -11488.195271819326
Iteration 6000: Loss = -11488.090435206037
Iteration 6100: Loss = -11487.753022439096
Iteration 6200: Loss = -11487.740612703032
Iteration 6300: Loss = -11481.67589289357
Iteration 6400: Loss = -11481.179083132543
Iteration 6500: Loss = -11477.70633597581
Iteration 6600: Loss = -11472.856351252196
Iteration 6700: Loss = -11472.839865883718
Iteration 6800: Loss = -11472.826307767877
Iteration 6900: Loss = -11473.049844637462
1
Iteration 7000: Loss = -11472.810936228649
Iteration 7100: Loss = -11472.805420482693
Iteration 7200: Loss = -11472.803481415616
Iteration 7300: Loss = -11472.79625822636
Iteration 7400: Loss = -11472.791809952527
Iteration 7500: Loss = -11472.827349017249
1
Iteration 7600: Loss = -11472.77970375361
Iteration 7700: Loss = -11472.761423266706
Iteration 7800: Loss = -11468.900445313722
Iteration 7900: Loss = -11468.887990619332
Iteration 8000: Loss = -11468.88455293687
Iteration 8100: Loss = -11464.6748961947
Iteration 8200: Loss = -11464.673445982999
Iteration 8300: Loss = -11464.665456860392
Iteration 8400: Loss = -11458.51826310664
Iteration 8500: Loss = -11458.502499639595
Iteration 8600: Loss = -11448.87664349028
Iteration 8700: Loss = -11448.920121938176
1
Iteration 8800: Loss = -11448.727947631387
Iteration 8900: Loss = -11448.862223724345
1
Iteration 9000: Loss = -11448.723282714493
Iteration 9100: Loss = -11448.729099686027
1
Iteration 9200: Loss = -11448.721403573425
Iteration 9300: Loss = -11448.722604768838
1
Iteration 9400: Loss = -11448.719846703883
Iteration 9500: Loss = -11448.720756875577
1
Iteration 9600: Loss = -11448.730107094878
2
Iteration 9700: Loss = -11448.717464484163
Iteration 9800: Loss = -11448.71382289106
Iteration 9900: Loss = -11448.69569042316
Iteration 10000: Loss = -11448.69820284943
1
Iteration 10100: Loss = -11448.714184626602
2
Iteration 10200: Loss = -11440.048398809155
Iteration 10300: Loss = -11440.016592547943
Iteration 10400: Loss = -11440.012988363129
Iteration 10500: Loss = -11436.216146693721
Iteration 10600: Loss = -11436.228910164213
1
Iteration 10700: Loss = -11436.212076527727
Iteration 10800: Loss = -11436.211152968068
Iteration 10900: Loss = -11436.190471313379
Iteration 11000: Loss = -11435.842462789065
Iteration 11100: Loss = -11434.661134588672
Iteration 11200: Loss = -11434.52177590558
Iteration 11300: Loss = -11434.518976974976
Iteration 11400: Loss = -11434.519278917645
1
Iteration 11500: Loss = -11434.521972278995
2
Iteration 11600: Loss = -11434.510085432863
Iteration 11700: Loss = -11431.647458428279
Iteration 11800: Loss = -11429.692279664428
Iteration 11900: Loss = -11425.41456932354
Iteration 12000: Loss = -11424.398148534534
Iteration 12100: Loss = -11405.53298846332
Iteration 12200: Loss = -11404.953116241219
Iteration 12300: Loss = -11401.249714852942
Iteration 12400: Loss = -11401.246628250103
Iteration 12500: Loss = -11399.867386548403
Iteration 12600: Loss = -11399.649140517873
Iteration 12700: Loss = -11398.022878022224
Iteration 12800: Loss = -11397.882473530472
Iteration 12900: Loss = -11397.874230388437
Iteration 13000: Loss = -11397.875713931206
1
Iteration 13100: Loss = -11397.878726992825
2
Iteration 13200: Loss = -11397.873258788653
Iteration 13300: Loss = -11397.870134839204
Iteration 13400: Loss = -11397.791767124594
Iteration 13500: Loss = -11397.790237562589
Iteration 13600: Loss = -11397.791043862195
1
Iteration 13700: Loss = -11397.768508255875
Iteration 13800: Loss = -11397.769002931187
1
Iteration 13900: Loss = -11397.782117883991
2
Iteration 14000: Loss = -11397.767915980961
Iteration 14100: Loss = -11397.768981974858
1
Iteration 14200: Loss = -11397.768677924973
2
Iteration 14300: Loss = -11397.76798759268
3
Iteration 14400: Loss = -11397.76816102742
4
Iteration 14500: Loss = -11397.767599370582
Iteration 14600: Loss = -11397.767623007125
1
Iteration 14700: Loss = -11397.771380443222
2
Iteration 14800: Loss = -11397.7765693443
3
Iteration 14900: Loss = -11396.312061216062
Iteration 15000: Loss = -11396.249001624521
Iteration 15100: Loss = -11396.188804597157
Iteration 15200: Loss = -11396.189573731783
1
Iteration 15300: Loss = -11396.168954090788
Iteration 15400: Loss = -11396.16887832047
Iteration 15500: Loss = -11395.713241069549
Iteration 15600: Loss = -11395.862806972447
1
Iteration 15700: Loss = -11395.674131741815
Iteration 15800: Loss = -11395.741016139702
1
Iteration 15900: Loss = -11395.682933295948
2
Iteration 16000: Loss = -11395.698928005218
3
Iteration 16100: Loss = -11395.671514546917
Iteration 16200: Loss = -11395.682391735922
1
Iteration 16300: Loss = -11395.671491831466
Iteration 16400: Loss = -11395.67171625678
1
Iteration 16500: Loss = -11395.671646441857
2
Iteration 16600: Loss = -11395.73504577855
3
Iteration 16700: Loss = -11395.669829672537
Iteration 16800: Loss = -11395.673304574682
1
Iteration 16900: Loss = -11395.655923368779
Iteration 17000: Loss = -11396.080129399916
1
Iteration 17100: Loss = -11395.63967459857
Iteration 17200: Loss = -11395.63964935212
Iteration 17300: Loss = -11385.578532258836
Iteration 17400: Loss = -11382.667100420176
Iteration 17500: Loss = -11380.501965240335
Iteration 17600: Loss = -11380.455343239973
Iteration 17700: Loss = -11380.461677647856
1
Iteration 17800: Loss = -11380.531002297063
2
Iteration 17900: Loss = -11380.461764156258
3
Iteration 18000: Loss = -11380.451913357041
Iteration 18100: Loss = -11380.452753468191
1
Iteration 18200: Loss = -11380.480220383288
2
Iteration 18300: Loss = -11380.451215124667
Iteration 18400: Loss = -11380.449829410314
Iteration 18500: Loss = -11380.451874204662
1
Iteration 18600: Loss = -11380.497783980976
2
Iteration 18700: Loss = -11380.437239765546
Iteration 18800: Loss = -11369.60681138378
Iteration 18900: Loss = -11369.55940019416
Iteration 19000: Loss = -11369.5599450239
1
Iteration 19100: Loss = -11369.562175733325
2
Iteration 19200: Loss = -11369.55895806842
Iteration 19300: Loss = -11369.535697268451
Iteration 19400: Loss = -11369.559644466395
1
Iteration 19500: Loss = -11369.535483106645
Iteration 19600: Loss = -11369.53913335084
1
Iteration 19700: Loss = -11369.534960996933
Iteration 19800: Loss = -11369.535633065454
1
Iteration 19900: Loss = -11369.535618499895
2
tensor([[ -2.5774,  -0.4173],
        [ -2.3474,   0.3449],
        [  5.8739,  -7.2830],
        [  6.8411,  -8.3771],
        [ -3.7920,   0.9543],
        [  6.9815,  -8.3691],
        [  3.6434,  -5.0306],
        [ -5.2351,   3.6080],
        [ -7.6881,   5.5806],
        [  8.0069, -10.8044],
        [ -3.6030,   0.0995],
        [ -4.2612,   2.6344],
        [ -6.1174,   4.7201],
        [  3.2917,  -6.4288],
        [ -4.9984,   2.3719],
        [  4.7555,  -6.4664],
        [  4.8658,  -6.2758],
        [ -7.5381,   5.3453],
        [ -1.0406,  -2.2241],
        [  1.1646,  -5.2807],
        [ -5.1206,   1.6116],
        [ -2.2940,   0.8832],
        [  6.7539,  -8.4235],
        [  0.3924,  -1.7795],
        [  4.5400,  -6.4300],
        [  5.4355,  -8.3308],
        [ -4.9434,   3.2507],
        [  0.8456,  -3.6388],
        [ -3.4192,   2.0298],
        [  3.6790,  -5.4527],
        [ -0.9976,  -0.8517],
        [  6.5470,  -9.2135],
        [ -3.4604,   1.0198],
        [ -5.7606,   3.7634],
        [ -1.5531,  -0.1081],
        [ -2.3508,   0.7884],
        [ -6.0315,   4.6295],
        [  6.2122,  -8.2135],
        [  6.3028,  -7.9628],
        [  4.2749,  -6.1256],
        [ -0.2128,  -2.0738],
        [ -4.8824,   2.9666],
        [  7.1046, -11.6888],
        [ -7.0709,   5.6673],
        [  5.5129,  -9.0689],
        [  6.3154,  -7.8880],
        [  5.8911,  -7.8772],
        [  3.5999,  -6.2722],
        [  1.6450,  -3.5172],
        [  4.4771,  -5.9442],
        [  4.4930,  -5.9927],
        [  5.6163,  -7.2896],
        [  3.8819,  -8.4971],
        [ -7.6704,   6.2791],
        [  5.1279,  -7.6832],
        [  4.4874,  -6.0279],
        [ -3.3894,   1.8379],
        [  5.3733,  -7.0826],
        [  6.6552,  -8.6384],
        [  0.5767,  -2.1486],
        [  7.3683, -10.8689],
        [  5.3838,  -7.7373],
        [  5.3147,  -7.7683],
        [  7.1842,  -9.3939],
        [  6.8469,  -8.2629],
        [  6.5505,  -8.4221],
        [  5.8782,  -8.0401],
        [ -3.5751,   0.2021],
        [  0.4352,  -4.0329],
        [  6.2193,  -7.6071],
        [ -6.1049,   3.2894],
        [ -0.6383,  -0.8818],
        [  0.8903,  -2.5483],
        [  0.3069,  -2.2741],
        [ -3.1757,   1.3125],
        [  1.3087,  -2.7163],
        [  0.4014,  -1.9293],
        [ -2.0316,   0.6267],
        [  3.4212,  -4.8246],
        [  5.3904,  -6.8108],
        [ -4.5797,   1.1360],
        [  0.4944,  -1.9718],
        [  7.2336,  -8.6283],
        [  3.1390,  -7.6492],
        [ -1.7698,   0.2909],
        [  1.8195,  -6.4347],
        [  5.3608,  -7.3348],
        [  1.0103,  -2.5858],
        [ -2.2307,  -0.4420],
        [  6.6809,  -8.7578],
        [  4.6083,  -7.5720],
        [  3.0466,  -4.4944],
        [  1.9401,  -5.9999],
        [ -7.6978,   6.2353],
        [  3.1702,  -4.5613],
        [ -4.4362,   2.8975],
        [  6.5050,  -8.8514],
        [  6.1439,  -9.9362],
        [  6.7830,  -8.3291],
        [ -3.0461,   1.5380]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6395, 0.3605],
        [0.3187, 0.6813]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6602, 0.3398], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2274, 0.0985],
         [0.3991, 0.2893]],

        [[0.9893, 0.0980],
         [0.1388, 0.9433]],

        [[0.1937, 0.0990],
         [0.5046, 0.0667]],

        [[0.5045, 0.1004],
         [0.3821, 0.9748]],

        [[0.9635, 0.0900],
         [0.8600, 0.2112]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 12
Adjusted Rand Index: 0.573478092283831
time is 1
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 14
Adjusted Rand Index: 0.513031661031208
time is 2
tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 4
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.08577338229800929
Average Adjusted Rand Index: 0.8014635668246239
Iteration 0: Loss = -21441.560171249323
Iteration 10: Loss = -11273.481712685207
Iteration 20: Loss = -11248.601590203647
Iteration 30: Loss = -11248.61132774585
1
Iteration 40: Loss = -11248.61144986409
2
Iteration 50: Loss = -11248.611443871065
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7458, 0.2542],
        [0.2034, 0.7966]], dtype=torch.float64)
alpha: tensor([0.4520, 0.5480])
beta: tensor([[[0.1911, 0.1004],
         [0.8489, 0.2990]],

        [[0.2139, 0.0994],
         [0.4561, 0.7683]],

        [[0.4008, 0.1002],
         [0.0562, 0.4954]],

        [[0.1864, 0.1012],
         [0.5463, 0.9061]],

        [[0.0488, 0.0904],
         [0.9115, 0.4449]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208093606567974
time is 1
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448582102964589
time is 2
tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 4
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9368928970803924
Average Adjusted Rand Index: 0.9371333261237875
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21441.084280074152
Iteration 100: Loss = -11700.009669424175
Iteration 200: Loss = -11663.007242178774
Iteration 300: Loss = -11649.49961915825
Iteration 400: Loss = -11644.410171272068
Iteration 500: Loss = -11642.557880277287
Iteration 600: Loss = -11640.96769989677
Iteration 700: Loss = -11637.480790007554
Iteration 800: Loss = -11580.811731383048
Iteration 900: Loss = -11498.085031443492
Iteration 1000: Loss = -11473.734207876754
Iteration 1100: Loss = -11470.905436575697
Iteration 1200: Loss = -11469.442830613687
Iteration 1300: Loss = -11468.059926226815
Iteration 1400: Loss = -11462.19660921645
Iteration 1500: Loss = -11391.749979460596
Iteration 1600: Loss = -11370.727344523784
Iteration 1700: Loss = -11363.120237232259
Iteration 1800: Loss = -11362.898080213477
Iteration 1900: Loss = -11362.711208019258
Iteration 2000: Loss = -11362.650628625819
Iteration 2100: Loss = -11362.589324889326
Iteration 2200: Loss = -11362.554330091894
Iteration 2300: Loss = -11362.538674199019
Iteration 2400: Loss = -11362.526079463334
Iteration 2500: Loss = -11362.515570111713
Iteration 2600: Loss = -11362.506698610718
Iteration 2700: Loss = -11362.499305323496
Iteration 2800: Loss = -11362.492949125248
Iteration 2900: Loss = -11362.487401109078
Iteration 3000: Loss = -11362.48248697469
Iteration 3100: Loss = -11362.47815604789
Iteration 3200: Loss = -11362.473920987244
Iteration 3300: Loss = -11362.46948856691
Iteration 3400: Loss = -11362.466248817867
Iteration 3500: Loss = -11362.463440400224
Iteration 3600: Loss = -11362.460737651785
Iteration 3700: Loss = -11362.457789576994
Iteration 3800: Loss = -11362.455802240538
Iteration 3900: Loss = -11362.453173133135
Iteration 4000: Loss = -11362.447753298982
Iteration 4100: Loss = -11357.9523694214
Iteration 4200: Loss = -11357.927048295142
Iteration 4300: Loss = -11357.925353995253
Iteration 4400: Loss = -11357.924033759531
Iteration 4500: Loss = -11357.92282925904
Iteration 4600: Loss = -11357.921778522836
Iteration 4700: Loss = -11357.920778717911
Iteration 4800: Loss = -11357.919748635823
Iteration 4900: Loss = -11357.91932985435
Iteration 5000: Loss = -11357.918008019302
Iteration 5100: Loss = -11357.931543795374
1
Iteration 5200: Loss = -11357.917612687274
Iteration 5300: Loss = -11357.916211442915
Iteration 5400: Loss = -11357.922155428945
1
Iteration 5500: Loss = -11357.915133467366
Iteration 5600: Loss = -11357.914736433944
Iteration 5700: Loss = -11357.914006482213
Iteration 5800: Loss = -11357.908711711869
Iteration 5900: Loss = -11357.908602479654
Iteration 6000: Loss = -11357.911159824469
1
Iteration 6100: Loss = -11357.92339496824
2
Iteration 6200: Loss = -11357.841342586726
Iteration 6300: Loss = -11357.833876457054
Iteration 6400: Loss = -11357.831912703954
Iteration 6500: Loss = -11357.853593126063
1
Iteration 6600: Loss = -11357.826606403249
Iteration 6700: Loss = -11356.682659177803
Iteration 6800: Loss = -11356.682686747641
1
Iteration 6900: Loss = -11356.684224148521
2
Iteration 7000: Loss = -11356.682465472662
Iteration 7100: Loss = -11356.681341387311
Iteration 7200: Loss = -11356.68446644335
1
Iteration 7300: Loss = -11356.682031354081
2
Iteration 7400: Loss = -11356.691641788193
3
Iteration 7500: Loss = -11356.680402578577
Iteration 7600: Loss = -11356.68683347781
1
Iteration 7700: Loss = -11356.681994064646
2
Iteration 7800: Loss = -11356.86470490466
3
Iteration 7900: Loss = -11356.67924518712
Iteration 8000: Loss = -11356.712275833912
1
Iteration 8100: Loss = -11356.673124535933
Iteration 8200: Loss = -11356.672791763514
Iteration 8300: Loss = -11356.681958123598
1
Iteration 8400: Loss = -11356.669971407138
Iteration 8500: Loss = -11356.669451414
Iteration 8600: Loss = -11356.570963224454
Iteration 8700: Loss = -11356.569194193078
Iteration 8800: Loss = -11356.652805107231
1
Iteration 8900: Loss = -11356.568247761683
Iteration 9000: Loss = -11356.568459943828
1
Iteration 9100: Loss = -11356.568071790087
Iteration 9200: Loss = -11356.57132956711
1
Iteration 9300: Loss = -11356.570863493987
2
Iteration 9400: Loss = -11356.69241472524
3
Iteration 9500: Loss = -11356.567254739553
Iteration 9600: Loss = -11356.56737238015
1
Iteration 9700: Loss = -11356.657455164432
2
Iteration 9800: Loss = -11356.379429829254
Iteration 9900: Loss = -11356.390219144165
1
Iteration 10000: Loss = -11356.378816338594
Iteration 10100: Loss = -11356.381724464834
1
Iteration 10200: Loss = -11356.378784416665
Iteration 10300: Loss = -11356.3831517299
1
Iteration 10400: Loss = -11356.378736969897
Iteration 10500: Loss = -11356.388874517748
1
Iteration 10600: Loss = -11356.375623106234
Iteration 10700: Loss = -11356.379700680156
1
Iteration 10800: Loss = -11356.375664853987
2
Iteration 10900: Loss = -11356.383694145237
3
Iteration 11000: Loss = -11356.375488603311
Iteration 11100: Loss = -11356.375694354872
1
Iteration 11200: Loss = -11356.375457053511
Iteration 11300: Loss = -11356.377859194667
1
Iteration 11400: Loss = -11356.375488895279
2
Iteration 11500: Loss = -11356.381918813615
3
Iteration 11600: Loss = -11356.377861985538
4
Iteration 11700: Loss = -11356.461351725658
5
Iteration 11800: Loss = -11356.372990959751
Iteration 11900: Loss = -11356.376876194036
1
Iteration 12000: Loss = -11356.375711451907
2
Iteration 12100: Loss = -11356.394082618375
3
Iteration 12200: Loss = -11356.37487045723
4
Iteration 12300: Loss = -11356.373516927404
5
Iteration 12400: Loss = -11356.405560806557
6
Iteration 12500: Loss = -11356.25291820522
Iteration 12600: Loss = -11356.243423085887
Iteration 12700: Loss = -11356.244541796943
1
Iteration 12800: Loss = -11356.250339516002
2
Iteration 12900: Loss = -11356.250264823919
3
Iteration 13000: Loss = -11356.2425917971
Iteration 13100: Loss = -11356.251494072207
1
Iteration 13200: Loss = -11356.279923572038
2
Iteration 13300: Loss = -11356.242673342316
3
Iteration 13400: Loss = -11356.24239028489
Iteration 13500: Loss = -11356.25782023675
1
Iteration 13600: Loss = -11356.31376460531
2
Iteration 13700: Loss = -11356.242136720959
Iteration 13800: Loss = -11356.255622304054
1
Iteration 13900: Loss = -11356.211438741875
Iteration 14000: Loss = -11356.21447428199
1
Iteration 14100: Loss = -11356.21124581334
Iteration 14200: Loss = -11356.211198966235
Iteration 14300: Loss = -11356.20789162188
Iteration 14400: Loss = -11356.21884492479
1
Iteration 14500: Loss = -11356.206869656091
Iteration 14600: Loss = -11356.416546015553
1
Iteration 14700: Loss = -11356.211463418722
2
Iteration 14800: Loss = -11356.214958207593
3
Iteration 14900: Loss = -11356.20690960103
4
Iteration 15000: Loss = -11356.20907827448
5
Iteration 15100: Loss = -11356.226471079237
6
Iteration 15200: Loss = -11356.214054005739
7
Iteration 15300: Loss = -11356.206579664682
Iteration 15400: Loss = -11356.208806725444
1
Iteration 15500: Loss = -11356.208671352993
2
Iteration 15600: Loss = -11356.207922854555
3
Iteration 15700: Loss = -11356.222314668878
4
Iteration 15800: Loss = -11356.206576196328
Iteration 15900: Loss = -11356.206826814938
1
Iteration 16000: Loss = -11356.209642840855
2
Iteration 16100: Loss = -11356.220378966085
3
Iteration 16200: Loss = -11356.207919141238
4
Iteration 16300: Loss = -11356.187721816075
Iteration 16400: Loss = -11356.188207968815
1
Iteration 16500: Loss = -11356.18872358022
2
Iteration 16600: Loss = -11356.18785960248
3
Iteration 16700: Loss = -11356.119266644528
Iteration 16800: Loss = -11356.118448557698
Iteration 16900: Loss = -11356.119928992704
1
Iteration 17000: Loss = -11356.117446658613
Iteration 17100: Loss = -11356.118943059111
1
Iteration 17200: Loss = -11356.128965995022
2
Iteration 17300: Loss = -11356.117068507323
Iteration 17400: Loss = -11356.117429302853
1
Iteration 17500: Loss = -11356.117064434966
Iteration 17600: Loss = -11356.117286393435
1
Iteration 17700: Loss = -11356.1170641752
Iteration 17800: Loss = -11356.117794719139
1
Iteration 17900: Loss = -11356.117047755723
Iteration 18000: Loss = -11356.122515020965
1
Iteration 18100: Loss = -11356.28484323747
2
Iteration 18200: Loss = -11356.119991738851
3
Iteration 18300: Loss = -11356.11704592975
Iteration 18400: Loss = -11356.117159689042
1
Iteration 18500: Loss = -11356.125084707628
2
Iteration 18600: Loss = -11356.13993557583
3
Iteration 18700: Loss = -11356.116909062532
Iteration 18800: Loss = -11356.115786476872
Iteration 18900: Loss = -11356.1158802219
1
Iteration 19000: Loss = -11356.115715213346
Iteration 19100: Loss = -11356.119080845556
1
Iteration 19200: Loss = -11356.120692158145
2
Iteration 19300: Loss = -11356.12819126109
3
Iteration 19400: Loss = -11356.115723470539
4
Iteration 19500: Loss = -11356.152520035028
5
Iteration 19600: Loss = -11356.11575517737
6
Iteration 19700: Loss = -11356.120284198561
7
Iteration 19800: Loss = -11356.120501706238
8
Iteration 19900: Loss = -11356.11569471796
tensor([[  0.9838,  -4.8288],
        [  2.9689,  -4.6639],
        [-10.1423,   8.7516],
        [ -8.4035,   5.7313],
        [  3.3156,  -4.7371],
        [ -7.3684,   5.9440],
        [ -7.2645,   5.8721],
        [  5.4658,  -6.8768],
        [  4.7082,  -6.1833],
        [-10.4100,   8.8139],
        [  2.8396,  -5.7949],
        [  4.1823,  -5.9114],
        [  5.6138,  -7.2231],
        [ -4.0952,   2.6905],
        [  3.1715,  -4.9052],
        [ -6.9260,   5.0454],
        [ -6.1306,   4.2018],
        [  8.4369, -11.1665],
        [  1.5395,  -6.1547],
        [ -3.4133,   1.2967],
        [  6.9141, -11.5293],
        [  3.9038,  -5.6556],
        [-11.8081,   8.1485],
        [  3.8553,  -5.3426],
        [ -5.6765,   3.9813],
        [-10.8309,   9.3175],
        [  1.5946,  -3.5368],
        [ -1.8614,   0.3601],
        [  4.6975,  -6.0848],
        [ -4.1324,   2.6568],
        [ -0.0310,  -3.3854],
        [-11.1969,   9.3725],
        [  3.0378,  -5.2735],
        [  6.3282,  -9.7285],
        [  0.6721,  -2.6828],
        [  1.9858,  -4.7563],
        [  5.6118,  -8.4742],
        [-11.3453,   9.8765],
        [ -7.7958,   5.4075],
        [ -9.1841,   7.5362],
        [  1.4496,  -2.8491],
        [  5.9181,  -7.6181],
        [ -8.9570,   6.6839],
        [  6.0444,  -7.8949],
        [ -6.8031,   5.4143],
        [ -8.6138,   7.2100],
        [ -8.1404,   5.6401],
        [ -5.1557,   3.6071],
        [ -2.0287,   0.1951],
        [ -5.8638,   4.2613],
        [-10.7779,   9.2503],
        [ -6.9190,   5.4467],
        [-10.1606,   8.6439],
        [  5.0083,  -6.5206],
        [ -5.7389,   3.8770],
        [-10.4517,   8.9805],
        [  4.1118,  -5.6316],
        [ -9.4254,   7.4196],
        [-10.9406,   9.5530],
        [  2.3045,  -4.6339],
        [-10.2049,   8.6767],
        [ -7.3838,   5.4750],
        [ -8.9111,   7.4827],
        [-11.7282,   9.2744],
        [ -7.2159,   5.1720],
        [-11.9483,   9.1350],
        [ -9.7970,   7.5041],
        [  3.0617,  -5.6666],
        [  1.7589,  -4.2897],
        [-10.4345,   9.0404],
        [  4.9014,  -6.5493],
        [ -0.9220,  -0.5578],
        [  0.8189,  -2.3540],
        [  1.4093,  -4.8367],
        [  2.6555,  -4.0677],
        [  2.9219,  -4.4010],
        [  1.7114,  -3.1810],
        [  2.1072,  -3.7035],
        [ -1.6795,   0.1849],
        [ -6.7034,   3.9787],
        [  4.4961,  -6.1006],
        [ -2.0032,   0.3295],
        [ -8.6640,   6.0944],
        [ -9.3250,   7.9353],
        [  2.8527,  -4.5301],
        [ -3.4998,   1.8692],
        [ -7.8807,   6.3805],
        [ -0.9327,  -3.0057],
        [  2.4123,  -3.8109],
        [ -8.9230,   7.1308],
        [ -4.0114,   2.6139],
        [ -3.4578,   2.0496],
        [ -4.8103,   2.3090],
        [  5.9521,  -7.3648],
        [ -5.3646,   3.8390],
        [  8.8020, -10.3913],
        [ -6.0220,   4.5659],
        [ -3.0689,  -0.8122],
        [-12.1986,   7.9675],
        [  1.4958,  -2.9762]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.5417, 0.4583],
        [0.4897, 0.5103]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4468, 0.5532], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2140, 0.0995],
         [0.8489, 0.3000]],

        [[0.2139, 0.0990],
         [0.4561, 0.7683]],

        [[0.4008, 0.1001],
         [0.0562, 0.4954]],

        [[0.1864, 0.0971],
         [0.5463, 0.9061]],

        [[0.0488, 0.0902],
         [0.9115, 0.4449]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208093606567974
time is 1
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448582102964589
time is 2
tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 16
Adjusted Rand Index: 0.45696969696969697
time is 4
tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.40331381370214303
Average Adjusted Rand Index: 0.8365272655177269
11261.845722400833
new:  [0.9368946998208005, 0.9446702630164284, 0.08577338229800929, 0.40331381370214303] [0.9371325814615584, 0.9449712659255646, 0.8014635668246239, 0.8365272655177269] [11250.214520381753, 11244.129126644599, 11369.535014228872, 11356.115803652889]
prior:  [0.9368928970803924, 0.9368928970803924, 0.0, 0.9368928970803924] [0.9371333261237875, 0.9371333261237875, 0.0, 0.9371333261237875] [11248.611450899827, 11248.611448443124, nan, 11248.611443871065]
-----------------------------------------------------------------------------------------
This iteration is 3
True Objective function: Loss = -11047.855077549195
Iteration 0: Loss = -16957.89508107659
Iteration 10: Loss = -11278.253933514672
Iteration 20: Loss = -11272.845832792978
Iteration 30: Loss = -11129.219312388428
Iteration 40: Loss = -11030.445961862664
Iteration 50: Loss = -11030.451298434682
1
Iteration 60: Loss = -11030.450754645544
2
Iteration 70: Loss = -11030.450705651934
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[0.7016, 0.2984],
        [0.2638, 0.7362]], dtype=torch.float64)
alpha: tensor([0.4685, 0.5315])
beta: tensor([[[0.2931, 0.0960],
         [0.3329, 0.1929]],

        [[0.4503, 0.1109],
         [0.2846, 0.2609]],

        [[0.2004, 0.0966],
         [0.5609, 0.6885]],

        [[0.8989, 0.1044],
         [0.7728, 0.3606]],

        [[0.4538, 0.0901],
         [0.7762, 0.2832]]], dtype=torch.float64)
time is 0
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824214784112464
time is 1
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448373142749671
time is 2
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 3
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9291539174073066
Average Adjusted Rand Index: 0.9294509749048011
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17206.189533727415
Iteration 100: Loss = -11289.713404628033
Iteration 200: Loss = -11283.140753288415
Iteration 300: Loss = -11281.294231299684
Iteration 400: Loss = -11277.8574369567
Iteration 500: Loss = -11277.212132252038
Iteration 600: Loss = -11276.873393530255
Iteration 700: Loss = -11276.641210980208
Iteration 800: Loss = -11276.439723340985
Iteration 900: Loss = -11276.265507935304
Iteration 1000: Loss = -11276.114109780789
Iteration 1100: Loss = -11275.981684772238
Iteration 1200: Loss = -11275.866246799054
Iteration 1300: Loss = -11275.765916468972
Iteration 1400: Loss = -11275.678728975796
Iteration 1500: Loss = -11275.602944989309
Iteration 1600: Loss = -11275.536997052599
Iteration 1700: Loss = -11275.479515503765
Iteration 1800: Loss = -11275.429331406456
Iteration 1900: Loss = -11275.385285456181
Iteration 2000: Loss = -11275.34661325501
Iteration 2100: Loss = -11275.31249457166
Iteration 2200: Loss = -11275.282326871918
Iteration 2300: Loss = -11275.255557328683
Iteration 2400: Loss = -11275.231698568838
Iteration 2500: Loss = -11275.210451539177
Iteration 2600: Loss = -11275.191337007498
Iteration 2700: Loss = -11275.174215720963
Iteration 2800: Loss = -11275.158795047097
Iteration 2900: Loss = -11275.144894940526
Iteration 3000: Loss = -11275.13232257647
Iteration 3100: Loss = -11275.120878085952
Iteration 3200: Loss = -11275.110539967662
Iteration 3300: Loss = -11275.101081832445
Iteration 3400: Loss = -11275.09246898814
Iteration 3500: Loss = -11275.084570446315
Iteration 3600: Loss = -11275.077363643557
Iteration 3700: Loss = -11275.070743092096
Iteration 3800: Loss = -11275.06468564897
Iteration 3900: Loss = -11275.059080032714
Iteration 4000: Loss = -11275.053957635635
Iteration 4100: Loss = -11275.049181606666
Iteration 4200: Loss = -11275.044811018199
Iteration 4300: Loss = -11275.040766265822
Iteration 4400: Loss = -11275.036979786706
Iteration 4500: Loss = -11275.033493003471
Iteration 4600: Loss = -11275.030237208428
Iteration 4700: Loss = -11275.027245811403
Iteration 4800: Loss = -11275.024485321252
Iteration 4900: Loss = -11275.021826402879
Iteration 5000: Loss = -11275.01941634822
Iteration 5100: Loss = -11275.017155938984
Iteration 5200: Loss = -11275.015064165842
Iteration 5300: Loss = -11275.013082177395
Iteration 5400: Loss = -11275.011239201705
Iteration 5500: Loss = -11275.05754627003
1
Iteration 5600: Loss = -11275.007886962117
Iteration 5700: Loss = -11275.006426558844
Iteration 5800: Loss = -11275.00547499608
Iteration 5900: Loss = -11275.003681512331
Iteration 6000: Loss = -11275.0448709109
1
Iteration 6100: Loss = -11275.001330314833
Iteration 6200: Loss = -11275.00021344438
Iteration 6300: Loss = -11275.519671619433
1
Iteration 6400: Loss = -11274.998235742642
Iteration 6500: Loss = -11274.997363406063
Iteration 6600: Loss = -11274.996494502631
Iteration 6700: Loss = -11274.99628181476
Iteration 6800: Loss = -11274.995022611218
Iteration 6900: Loss = -11274.994309443682
Iteration 7000: Loss = -11275.003223000163
1
Iteration 7100: Loss = -11274.993067134299
Iteration 7200: Loss = -11274.992511234645
Iteration 7300: Loss = -11274.991934064898
Iteration 7400: Loss = -11274.994809002172
1
Iteration 7500: Loss = -11274.990987559444
Iteration 7600: Loss = -11274.99054047038
Iteration 7700: Loss = -11274.991201581728
1
Iteration 7800: Loss = -11274.98979500295
Iteration 7900: Loss = -11274.989389240289
Iteration 8000: Loss = -11274.990040552962
1
Iteration 8100: Loss = -11274.988744872111
Iteration 8200: Loss = -11274.988441773667
Iteration 8300: Loss = -11275.019550514944
1
Iteration 8400: Loss = -11274.987915652
Iteration 8500: Loss = -11274.988262511433
1
Iteration 8600: Loss = -11274.989880260488
2
Iteration 8700: Loss = -11274.988454345339
3
Iteration 8800: Loss = -11275.017194547905
4
Iteration 8900: Loss = -11274.986788207474
Iteration 9000: Loss = -11274.986737854639
Iteration 9100: Loss = -11274.987410103508
1
Iteration 9200: Loss = -11274.986707139818
Iteration 9300: Loss = -11274.986883323314
1
Iteration 9400: Loss = -11275.061984293494
2
Iteration 9500: Loss = -11274.986033901256
Iteration 9600: Loss = -11274.98585744551
Iteration 9700: Loss = -11274.986368802985
1
Iteration 9800: Loss = -11274.986136391917
2
Iteration 9900: Loss = -11274.985551389515
Iteration 10000: Loss = -11274.985546623953
Iteration 10100: Loss = -11274.985385912256
Iteration 10200: Loss = -11274.98655900662
1
Iteration 10300: Loss = -11274.985302348248
Iteration 10400: Loss = -11275.136495063382
1
Iteration 10500: Loss = -11274.985170025247
Iteration 10600: Loss = -11274.986300328415
1
Iteration 10700: Loss = -11274.98515790139
Iteration 10800: Loss = -11275.031544257783
1
Iteration 10900: Loss = -11274.985519133803
2
Iteration 11000: Loss = -11274.996747065314
3
Iteration 11100: Loss = -11274.984859792368
Iteration 11200: Loss = -11274.986182376648
1
Iteration 11300: Loss = -11274.984827272625
Iteration 11400: Loss = -11274.985347735736
1
Iteration 11500: Loss = -11274.984747556064
Iteration 11600: Loss = -11274.985783237198
1
Iteration 11700: Loss = -11274.984930004668
2
Iteration 11800: Loss = -11274.984953396404
3
Iteration 11900: Loss = -11274.985112086213
4
Iteration 12000: Loss = -11274.987986730375
5
Iteration 12100: Loss = -11274.984891952427
6
Iteration 12200: Loss = -11274.986591869556
7
Iteration 12300: Loss = -11274.98506075161
8
Iteration 12400: Loss = -11274.988611877525
9
Iteration 12500: Loss = -11274.985022900657
10
Stopping early at iteration 12500 due to no improvement.
tensor([[ -7.3576,   2.7424],
        [ -7.1273,   2.5121],
        [ -7.4810,   2.8658],
        [ -5.4945,   0.8792],
        [ -8.6837,   4.0685],
        [ -5.4849,   0.8697],
        [ -9.1806,   4.5654],
        [ -7.2032,   2.5879],
        [ -7.9528,   3.3375],
        [ -4.7483,   0.1330],
        [ -7.6699,   3.0547],
        [ -9.4598,   4.8446],
        [ -5.4888,   0.8735],
        [  1.3727,  -5.9879],
        [ -8.7457,   4.1305],
        [ -7.2676,   2.6523],
        [ -9.0722,   4.4569],
        [ -5.7022,   1.0870],
        [ -6.7694,   2.1541],
        [ -6.3195,   1.7043],
        [ -5.6662,   1.0510],
        [ -8.2612,   3.6459],
        [ -6.4695,   1.8543],
        [ -8.8407,   4.2255],
        [ -5.1913,   0.5760],
        [ -6.4761,   1.8608],
        [ -6.5993,   1.9841],
        [ -6.4326,   1.8174],
        [ -9.6363,   5.0211],
        [ -7.7516,   3.1364],
        [ -5.4053,   0.7901],
        [ -7.6382,   3.0229],
        [ -5.5923,   0.9770],
        [ -9.6567,   5.0415],
        [-10.2500,   5.6348],
        [ -5.8921,   1.2768],
        [ -8.8402,   4.2250],
        [ -6.7920,   2.1768],
        [ -8.6187,   4.0035],
        [ -6.8304,   2.2152],
        [ -7.1210,   2.5058],
        [ -7.9563,   3.3411],
        [ -6.2755,   1.6603],
        [ -7.7698,   3.1546],
        [ -4.9148,   0.2995],
        [ -4.7702,   0.1549],
        [ -7.8282,   3.2130],
        [ -7.1233,   2.5081],
        [ -7.4373,   2.8221],
        [ -5.7744,   1.1591],
        [ -6.0683,   1.4531],
        [ -7.3035,   2.6883],
        [ -7.8844,   3.2691],
        [ -6.1420,   1.5268],
        [ -8.3411,   3.7259],
        [ -9.4449,   4.8297],
        [ -7.1146,   2.4994],
        [  4.6125,  -9.2278],
        [-10.1678,   5.5526],
        [ -7.4781,   2.8629],
        [ -6.4067,   1.7915],
        [ -6.1275,   1.5123],
        [ -5.3257,   0.7105],
        [ -6.4428,   1.8276],
        [ -7.7869,   3.1717],
        [ -7.4252,   2.8100],
        [ -4.1826,  -0.4326],
        [ -8.2353,   3.6201],
        [ -6.6889,   2.0737],
        [ -2.9845,  -1.6307],
        [ -8.7239,   4.1087],
        [ -8.1877,   3.5724],
        [ -6.6458,   2.0306],
        [ -8.1299,   3.5146],
        [ -6.1057,   1.4905],
        [ -8.1821,   3.5669],
        [ -8.8161,   4.2009],
        [ -9.4525,   4.8373],
        [ -4.2978,  -0.3174],
        [ -9.6264,   5.0112],
        [ -5.0201,   0.4049],
        [ -8.8992,   4.2840],
        [-10.3918,   5.7766],
        [ -8.5776,   3.9624],
        [-10.0742,   5.4590],
        [ -8.1800,   3.5648],
        [ -8.6389,   4.0237],
        [ -5.7257,   1.1104],
        [ -7.4464,   2.8312],
        [ -8.0679,   3.4527],
        [ -7.9496,   3.3344],
        [ -5.9699,   1.3547],
        [ -6.7560,   2.1408],
        [ -4.2872,  -0.3280],
        [ -8.9273,   4.3121],
        [ -5.1837,   0.5684],
        [ -7.4270,   2.8118],
        [ -8.9164,   4.3012],
        [ -6.0381,   1.4229],
        [ -6.4630,   1.8477]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.5742e-01, 4.2575e-02],
        [4.7887e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0232, 0.9768], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[3.9892e-04, 9.5799e-02],
         [3.3287e-01, 1.6825e-01]],

        [[4.5026e-01, 2.2085e-01],
         [2.8457e-01, 2.6094e-01]],

        [[2.0043e-01, 2.9462e-01],
         [5.6088e-01, 6.8846e-01]],

        [[8.9894e-01, 2.8520e-01],
         [7.7278e-01, 3.6057e-01]],

        [[4.5380e-01, 1.9926e-01],
         [7.7618e-01, 2.8323e-01]]], dtype=torch.float64,
       grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.004144175314943718
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 41
Adjusted Rand Index: 0.01382061954603653
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.004144175314943718
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.0006667223888488251
Global Adjusted Rand Index: 0.0024783991517729827
Average Adjusted Rand Index: 0.003544026683938192
Iteration 0: Loss = -23158.153137623358
Iteration 10: Loss = -11290.567981583234
Iteration 20: Loss = -11286.862022907831
Iteration 30: Loss = -11279.532360362864
Iteration 40: Loss = -11275.637059512246
Iteration 50: Loss = -11232.986858039445
Iteration 60: Loss = -11030.455928671658
Iteration 70: Loss = -11030.45064302016
Iteration 80: Loss = -11030.450705553385
1
Iteration 90: Loss = -11030.450691407106
2
Iteration 100: Loss = -11030.450681899005
3
Stopping early at iteration 99 due to no improvement.
pi: tensor([[0.7016, 0.2984],
        [0.2638, 0.7362]], dtype=torch.float64)
alpha: tensor([0.4685, 0.5315])
beta: tensor([[[0.2931, 0.0960],
         [0.9678, 0.1929]],

        [[0.3175, 0.1109],
         [0.5611, 0.1432]],

        [[0.2432, 0.0966],
         [0.1505, 0.4386]],

        [[0.9175, 0.1044],
         [0.0310, 0.6510]],

        [[0.5827, 0.0901],
         [0.4582, 0.2654]]], dtype=torch.float64)
time is 0
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824214784112464
time is 1
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448373142749671
time is 2
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 3
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9291539174073066
Average Adjusted Rand Index: 0.9294509749048011
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23159.682816485172
Iteration 100: Loss = -11307.912347169033
Iteration 200: Loss = -11293.93572124055
Iteration 300: Loss = -11291.447994313276
Iteration 400: Loss = -11287.711926096843
Iteration 500: Loss = -11283.82075243451
Iteration 600: Loss = -11281.886940899998
Iteration 700: Loss = -11280.36895611351
Iteration 800: Loss = -11279.137101505454
Iteration 900: Loss = -11277.173354743069
Iteration 1000: Loss = -11271.892135731307
Iteration 1100: Loss = -11231.679723249925
Iteration 1200: Loss = -11207.027255421264
Iteration 1300: Loss = -11179.239145459795
Iteration 1400: Loss = -11175.999732609649
Iteration 1500: Loss = -11171.276170136322
Iteration 1600: Loss = -11160.13871016532
Iteration 1700: Loss = -11139.969012836948
Iteration 1800: Loss = -11133.61548757641
Iteration 1900: Loss = -11132.726645764364
Iteration 2000: Loss = -11132.63188815803
Iteration 2100: Loss = -11130.199536219998
Iteration 2200: Loss = -11120.248553320358
Iteration 2300: Loss = -11116.102774376222
Iteration 2400: Loss = -11113.521604850264
Iteration 2500: Loss = -11109.016927931338
Iteration 2600: Loss = -11103.887617550175
Iteration 2700: Loss = -11103.83095060634
Iteration 2800: Loss = -11103.797199246246
Iteration 2900: Loss = -11103.767235987054
Iteration 3000: Loss = -11103.714555743516
Iteration 3100: Loss = -11095.81483750931
Iteration 3200: Loss = -11095.691935477449
Iteration 3300: Loss = -11095.473260569788
Iteration 3400: Loss = -11091.508000576194
Iteration 3500: Loss = -11091.491234084258
Iteration 3600: Loss = -11091.479003986373
Iteration 3700: Loss = -11091.470472904217
Iteration 3800: Loss = -11091.464165596932
Iteration 3900: Loss = -11091.460594070166
Iteration 4000: Loss = -11091.455832803953
Iteration 4100: Loss = -11091.452596707513
Iteration 4200: Loss = -11091.452542236026
Iteration 4300: Loss = -11091.446900297
Iteration 4400: Loss = -11091.44371719788
Iteration 4500: Loss = -11091.455505262955
1
Iteration 4600: Loss = -11091.433167250027
Iteration 4700: Loss = -11091.431814387006
Iteration 4800: Loss = -11091.42912837001
Iteration 4900: Loss = -11091.426775391476
Iteration 5000: Loss = -11091.423883790168
Iteration 5100: Loss = -11091.419151285989
Iteration 5200: Loss = -11091.407325829205
Iteration 5300: Loss = -11091.307526230012
Iteration 5400: Loss = -11091.245359630142
Iteration 5500: Loss = -11091.241884290363
Iteration 5600: Loss = -11091.259482694031
1
Iteration 5700: Loss = -11091.238509773875
Iteration 5800: Loss = -11091.237238733927
Iteration 5900: Loss = -11091.236140054922
Iteration 6000: Loss = -11091.236629633317
1
Iteration 6100: Loss = -11091.23901431668
2
Iteration 6200: Loss = -11091.236744145937
3
Iteration 6300: Loss = -11091.246212819658
4
Iteration 6400: Loss = -11091.233775214001
Iteration 6500: Loss = -11091.281396150529
1
Iteration 6600: Loss = -11091.229736491514
Iteration 6700: Loss = -11091.22941331194
Iteration 6800: Loss = -11091.228734184762
Iteration 6900: Loss = -11091.230494208228
1
Iteration 7000: Loss = -11091.22719234607
Iteration 7100: Loss = -11091.22034833869
Iteration 7200: Loss = -11091.241051239964
1
Iteration 7300: Loss = -11091.267939950792
2
Iteration 7400: Loss = -11091.176913776468
Iteration 7500: Loss = -11091.175506044223
Iteration 7600: Loss = -11091.270020947044
1
Iteration 7700: Loss = -11091.173600314754
Iteration 7800: Loss = -11091.351333449866
1
Iteration 7900: Loss = -11091.16941892029
Iteration 8000: Loss = -11091.169242327045
Iteration 8100: Loss = -11091.167922747009
Iteration 8200: Loss = -11091.167677851421
Iteration 8300: Loss = -11091.167681067178
1
Iteration 8400: Loss = -11091.17648334546
2
Iteration 8500: Loss = -11091.167236363144
Iteration 8600: Loss = -11091.162299572949
Iteration 8700: Loss = -11091.153438822575
Iteration 8800: Loss = -11091.153324599318
Iteration 8900: Loss = -11091.285817801947
1
Iteration 9000: Loss = -11091.152867077246
Iteration 9100: Loss = -11091.18187307169
1
Iteration 9200: Loss = -11091.151884613568
Iteration 9300: Loss = -11091.161604224306
1
Iteration 9400: Loss = -11091.117692830145
Iteration 9500: Loss = -11091.117035992886
Iteration 9600: Loss = -11091.116768387614
Iteration 9700: Loss = -11091.116636457276
Iteration 9800: Loss = -11091.122158177444
1
Iteration 9900: Loss = -11091.116506469209
Iteration 10000: Loss = -11091.11644366657
Iteration 10100: Loss = -11091.15996254629
1
Iteration 10200: Loss = -11091.116264552822
Iteration 10300: Loss = -11091.143577551202
1
Iteration 10400: Loss = -11091.116104324192
Iteration 10500: Loss = -11091.116983420969
1
Iteration 10600: Loss = -11091.1441568609
2
Iteration 10700: Loss = -11091.225665822703
3
Iteration 10800: Loss = -11091.116051146739
Iteration 10900: Loss = -11091.11622911691
1
Iteration 11000: Loss = -11091.151432805882
2
Iteration 11100: Loss = -11091.1145714505
Iteration 11200: Loss = -11091.115056916873
1
Iteration 11300: Loss = -11091.115701352574
2
Iteration 11400: Loss = -11091.32522858114
3
Iteration 11500: Loss = -11091.114684742017
4
Iteration 11600: Loss = -11091.11997332171
5
Iteration 11700: Loss = -11091.116031405338
6
Iteration 11800: Loss = -11091.11415652585
Iteration 11900: Loss = -11091.20800554685
1
Iteration 12000: Loss = -11091.279404961168
2
Iteration 12100: Loss = -11091.114919143281
3
Iteration 12200: Loss = -11091.106615308832
Iteration 12300: Loss = -11091.107101627216
1
Iteration 12400: Loss = -11091.107827871201
2
Iteration 12500: Loss = -11091.113128050396
3
Iteration 12600: Loss = -11091.106452249502
Iteration 12700: Loss = -11091.109601274491
1
Iteration 12800: Loss = -11091.111239853037
2
Iteration 12900: Loss = -11091.111144849181
3
Iteration 13000: Loss = -11091.093065621646
Iteration 13100: Loss = -11091.097325182292
1
Iteration 13200: Loss = -11091.094555099433
2
Iteration 13300: Loss = -11091.091334878265
Iteration 13400: Loss = -11091.092269322005
1
Iteration 13500: Loss = -11091.093506238885
2
Iteration 13600: Loss = -11091.098507426816
3
Iteration 13700: Loss = -11091.091472205082
4
Iteration 13800: Loss = -11091.091303882295
Iteration 13900: Loss = -11091.087867872302
Iteration 14000: Loss = -11091.138675544562
1
Iteration 14100: Loss = -11091.07879318959
Iteration 14200: Loss = -11091.078231649011
Iteration 14300: Loss = -11091.097507079332
1
Iteration 14400: Loss = -11091.065668022617
Iteration 14500: Loss = -11091.068115366945
1
Iteration 14600: Loss = -11091.297251863642
2
Iteration 14700: Loss = -11091.035021178921
Iteration 14800: Loss = -11091.046238716011
1
Iteration 14900: Loss = -11091.049940610308
2
Iteration 15000: Loss = -11091.033791760907
Iteration 15100: Loss = -11091.181884955793
1
Iteration 15200: Loss = -11042.490784615577
Iteration 15300: Loss = -11035.510000632687
Iteration 15400: Loss = -11035.444466670662
Iteration 15500: Loss = -11031.776926060042
Iteration 15600: Loss = -11031.7855148908
1
Iteration 15700: Loss = -11031.752354912107
Iteration 15800: Loss = -11031.7432496372
Iteration 15900: Loss = -11026.73537737178
Iteration 16000: Loss = -11026.728497981821
Iteration 16100: Loss = -11026.728187375084
Iteration 16200: Loss = -11026.731149374105
1
Iteration 16300: Loss = -11026.72822103844
2
Iteration 16400: Loss = -11026.824690287149
3
Iteration 16500: Loss = -11026.723666842767
Iteration 16600: Loss = -11026.762636100244
1
Iteration 16700: Loss = -11026.723053961092
Iteration 16800: Loss = -11026.724231794478
1
Iteration 16900: Loss = -11026.733819390574
2
Iteration 17000: Loss = -11026.72318899935
3
Iteration 17100: Loss = -11026.721755242314
Iteration 17200: Loss = -11026.80642357876
1
Iteration 17300: Loss = -11026.715641955609
Iteration 17400: Loss = -11026.7589139385
1
Iteration 17500: Loss = -11026.713889440338
Iteration 17600: Loss = -11026.718310225826
1
Iteration 17700: Loss = -11026.723344506709
2
Iteration 17800: Loss = -11026.713308451912
Iteration 17900: Loss = -11026.711290212235
Iteration 18000: Loss = -11026.709491565678
Iteration 18100: Loss = -11026.73918603277
1
Iteration 18200: Loss = -11026.708419900326
Iteration 18300: Loss = -11026.708244545867
Iteration 18400: Loss = -11026.905944158401
1
Iteration 18500: Loss = -11026.702152182153
Iteration 18600: Loss = -11026.714408206322
1
Iteration 18700: Loss = -11026.701884980213
Iteration 18800: Loss = -11026.725044212075
1
Iteration 18900: Loss = -11026.745550502783
2
Iteration 19000: Loss = -11026.703380527431
3
Iteration 19100: Loss = -11026.701929680996
4
Iteration 19200: Loss = -11026.706694317421
5
Iteration 19300: Loss = -11026.701389048483
Iteration 19400: Loss = -11026.702428855087
1
Iteration 19500: Loss = -11026.702407798224
2
Iteration 19600: Loss = -11026.701176157047
Iteration 19700: Loss = -11026.704684768732
1
Iteration 19800: Loss = -11026.694483648855
Iteration 19900: Loss = -11026.68014918266
tensor([[ 6.7609, -8.5436],
        [-5.6667,  2.8869],
        [ 6.7529, -8.2179],
        [-6.1487,  3.8685],
        [-3.5069, -0.3955],
        [-1.3295, -0.0984],
        [ 6.9205, -8.3096],
        [-4.7604,  1.1882],
        [-7.3351,  5.1089],
        [-5.8560,  1.9960],
        [ 6.8961, -8.2858],
        [ 5.2259, -9.0970],
        [-7.7807,  6.1444],
        [-3.7406,  2.3512],
        [ 6.5513, -8.0919],
        [ 4.2103, -6.0120],
        [ 1.7926, -3.2251],
        [-8.6574,  6.8430],
        [ 6.3545, -9.0761],
        [-8.1188,  5.3655],
        [ 1.0947, -2.8357],
        [-4.8427,  3.2857],
        [-4.7106,  3.1675],
        [-3.7568,  2.3005],
        [-9.7050,  8.2472],
        [-7.7748,  6.3682],
        [-5.2193,  3.7836],
        [ 6.1428, -8.3025],
        [ 3.4059, -4.8235],
        [ 6.5009, -7.9056],
        [ 4.8797, -7.6089],
        [ 7.2455, -8.6331],
        [-6.1683,  4.7689],
        [-8.7071,  7.1929],
        [ 4.2204, -8.8357],
        [-6.5922,  5.1949],
        [ 2.2047, -3.6232],
        [ 6.0130, -9.4568],
        [-7.4112,  4.6950],
        [-3.0597, -0.4039],
        [-4.4843,  3.0168],
        [ 5.8378, -7.7736],
        [ 5.2275, -6.7968],
        [ 2.5423, -4.1051],
        [ 5.4780, -7.5839],
        [-3.9159,  2.4490],
        [-4.8556,  3.3309],
        [ 6.3163, -7.7873],
        [ 6.3855, -8.4055],
        [-5.0109,  3.6012],
        [-4.6536,  2.8133],
        [-4.7008,  3.1874],
        [ 6.3857, -7.7931],
        [-2.2824,  0.7024],
        [-2.7826,  1.3960],
        [-6.1311,  4.7258],
        [-2.3336,  0.8452],
        [-3.9714,  2.2572],
        [-6.5695,  5.0383],
        [ 1.4580, -3.5475],
        [-1.4860, -0.6371],
        [ 3.7615, -5.1815],
        [-5.8643,  4.2880],
        [-8.1697,  6.1902],
        [ 1.6430, -3.0451],
        [ 5.8705, -7.7841],
        [ 0.1681, -1.7493],
        [ 3.4084, -4.9479],
        [ 3.9951, -8.0052],
        [ 0.5542, -2.6005],
        [ 5.2810, -9.6949],
        [ 5.2291, -9.3484],
        [ 2.2166, -4.2106],
        [-6.1319,  3.9309],
        [-7.7260,  6.0310],
        [-7.3780,  5.9045],
        [ 1.8906, -4.4948],
        [-4.6724,  3.2232],
        [-6.3584,  3.6391],
        [-8.5135,  6.4870],
        [-3.3189,  1.9265],
        [-8.6365,  6.7175],
        [ 7.1434, -8.6633],
        [ 0.5082, -3.3301],
        [ 6.6074, -8.4032],
        [ 2.0173, -3.4583],
        [-1.2052, -0.4481],
        [-3.5013,  1.9855],
        [ 2.8657, -4.2537],
        [ 2.1961, -3.6575],
        [-8.9807,  7.5881],
        [-8.7383,  7.3485],
        [-7.2563,  4.7894],
        [-3.7888,  2.1302],
        [ 6.4060, -8.3867],
        [-3.1258,  1.3581],
        [ 5.5221, -7.9833],
        [ 2.1726, -3.7707],
        [-2.1098,  0.5189],
        [-5.2648,  3.0481]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7137, 0.2863],
        [0.2539, 0.7461]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4691, 0.5309], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3000, 0.0960],
         [0.9678, 0.1967]],

        [[0.3175, 0.1105],
         [0.5611, 0.1432]],

        [[0.2432, 0.0962],
         [0.1505, 0.4386]],

        [[0.9175, 0.1036],
         [0.0310, 0.6510]],

        [[0.5827, 0.0899],
         [0.4582, 0.2654]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824214784112464
time is 1
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
time is 2
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 3
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9524807523239673
Average Adjusted Rand Index: 0.9526444267184807
Iteration 0: Loss = -41871.715187480586
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.1670,    nan]],

        [[0.9168,    nan],
         [0.5268, 0.1612]],

        [[0.0486,    nan],
         [0.9247, 0.2215]],

        [[0.7239,    nan],
         [0.4467, 0.8094]],

        [[0.5264,    nan],
         [0.7680, 0.0382]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -41870.92497283905
Iteration 100: Loss = -11319.727190651474
Iteration 200: Loss = -11302.245004880897
Iteration 300: Loss = -11295.19588732132
Iteration 400: Loss = -11293.547561819809
Iteration 500: Loss = -11292.613894655895
Iteration 600: Loss = -11292.022044077827
Iteration 700: Loss = -11291.615586806898
Iteration 800: Loss = -11291.311625968732
Iteration 900: Loss = -11291.028959230116
Iteration 1000: Loss = -11290.766020481082
Iteration 1100: Loss = -11290.649132587045
Iteration 1200: Loss = -11290.545779481063
Iteration 1300: Loss = -11290.460226104058
Iteration 1400: Loss = -11290.4082999938
Iteration 1500: Loss = -11290.354422494322
Iteration 1600: Loss = -11290.296793417721
Iteration 1700: Loss = -11290.176186713496
Iteration 1800: Loss = -11289.875268941807
Iteration 1900: Loss = -11289.691713684515
Iteration 2000: Loss = -11289.508138999681
Iteration 2100: Loss = -11288.811368659482
Iteration 2200: Loss = -11288.272364937791
Iteration 2300: Loss = -11287.991136636407
Iteration 2400: Loss = -11287.804470979234
Iteration 2500: Loss = -11287.660091305133
Iteration 2600: Loss = -11287.53231976979
Iteration 2700: Loss = -11287.409290682292
Iteration 2800: Loss = -11287.288229744323
Iteration 2900: Loss = -11287.166965029583
Iteration 3000: Loss = -11287.036228792525
Iteration 3100: Loss = -11286.880998240778
Iteration 3200: Loss = -11286.686494331449
Iteration 3300: Loss = -11286.450445576631
Iteration 3400: Loss = -11286.178315466695
Iteration 3500: Loss = -11285.891634369522
Iteration 3600: Loss = -11285.632780740263
Iteration 3700: Loss = -11285.433509526329
Iteration 3800: Loss = -11285.264343941513
Iteration 3900: Loss = -11284.199492242498
Iteration 4000: Loss = -11283.512602849392
Iteration 4100: Loss = -11283.221634164443
Iteration 4200: Loss = -11283.059685194032
Iteration 4300: Loss = -11282.83161437444
Iteration 4400: Loss = -11278.949894679483
Iteration 4500: Loss = -11269.804564990622
Iteration 4600: Loss = -11232.694487351282
Iteration 4700: Loss = -11213.134377744389
Iteration 4800: Loss = -11212.417245164592
Iteration 4900: Loss = -11212.04426880987
Iteration 5000: Loss = -11209.430155093965
Iteration 5100: Loss = -11209.160695451827
Iteration 5200: Loss = -11204.761735458549
Iteration 5300: Loss = -11195.688869371812
Iteration 5400: Loss = -11188.99333098242
Iteration 5500: Loss = -11184.501326892321
Iteration 5600: Loss = -11182.463195986604
Iteration 5700: Loss = -11165.414821742073
Iteration 5800: Loss = -11162.734038171593
Iteration 5900: Loss = -11162.581243368892
Iteration 6000: Loss = -11162.554844783683
Iteration 6100: Loss = -11161.891465283257
Iteration 6200: Loss = -11160.428485215767
Iteration 6300: Loss = -11160.407204802867
Iteration 6400: Loss = -11160.389041766217
Iteration 6500: Loss = -11160.34578063396
Iteration 6600: Loss = -11160.336095986828
Iteration 6700: Loss = -11160.333027669518
Iteration 6800: Loss = -11160.329555469378
Iteration 6900: Loss = -11160.324291170296
Iteration 7000: Loss = -11160.320344814865
Iteration 7100: Loss = -11160.256102403417
Iteration 7200: Loss = -11160.248867790442
Iteration 7300: Loss = -11156.427493016097
Iteration 7400: Loss = -11156.380326844426
Iteration 7500: Loss = -11156.378193481438
Iteration 7600: Loss = -11156.376617645943
Iteration 7700: Loss = -11156.375421717035
Iteration 7800: Loss = -11156.371151594001
Iteration 7900: Loss = -11156.322687389591
Iteration 8000: Loss = -11156.295341363155
Iteration 8100: Loss = -11156.295684914463
1
Iteration 8200: Loss = -11156.288444367388
Iteration 8300: Loss = -11156.286977999853
Iteration 8400: Loss = -11156.279147299208
Iteration 8500: Loss = -11156.300606749955
1
Iteration 8600: Loss = -11156.276958245235
Iteration 8700: Loss = -11156.275958499473
Iteration 8800: Loss = -11156.28093516498
1
Iteration 8900: Loss = -11155.969297297559
Iteration 9000: Loss = -11155.940020526145
Iteration 9100: Loss = -11155.97342122902
1
Iteration 9200: Loss = -11155.937322786755
Iteration 9300: Loss = -11155.932036860551
Iteration 9400: Loss = -11155.940940871942
1
Iteration 9500: Loss = -11155.928281088598
Iteration 9600: Loss = -11155.928898875505
1
Iteration 9700: Loss = -11155.924996718764
Iteration 9800: Loss = -11155.925398416406
1
Iteration 9900: Loss = -11155.907036923243
Iteration 10000: Loss = -11155.915888207055
1
Iteration 10100: Loss = -11155.91209486343
2
Iteration 10200: Loss = -11155.894263683695
Iteration 10300: Loss = -11155.863194187661
Iteration 10400: Loss = -11155.862763532368
Iteration 10500: Loss = -11155.862369941864
Iteration 10600: Loss = -11155.861157175397
Iteration 10700: Loss = -11155.864852332348
1
Iteration 10800: Loss = -11155.854658063794
Iteration 10900: Loss = -11155.87701820784
1
Iteration 11000: Loss = -11155.83382727041
Iteration 11100: Loss = -11155.83372245879
Iteration 11200: Loss = -11155.833618640916
Iteration 11300: Loss = -11155.83387994337
1
Iteration 11400: Loss = -11155.958325658554
2
Iteration 11500: Loss = -11155.827573226601
Iteration 11600: Loss = -11155.828785012718
1
Iteration 11700: Loss = -11155.827411807959
Iteration 11800: Loss = -11155.827273460836
Iteration 11900: Loss = -11155.849889283905
1
Iteration 12000: Loss = -11155.826900226148
Iteration 12100: Loss = -11155.82684648709
Iteration 12200: Loss = -11155.833595345637
1
Iteration 12300: Loss = -11155.826592223008
Iteration 12400: Loss = -11155.835930954103
1
Iteration 12500: Loss = -11155.824834867713
Iteration 12600: Loss = -11155.824762660011
Iteration 12700: Loss = -11155.825823441797
1
Iteration 12800: Loss = -11155.834192908436
2
Iteration 12900: Loss = -11155.890646696798
3
Iteration 13000: Loss = -11155.822422639812
Iteration 13100: Loss = -11155.822062937448
Iteration 13200: Loss = -11155.824146058598
1
Iteration 13300: Loss = -11155.82194251063
Iteration 13400: Loss = -11155.826093914076
1
Iteration 13500: Loss = -11155.82167129992
Iteration 13600: Loss = -11155.821480379762
Iteration 13700: Loss = -11155.818081525777
Iteration 13800: Loss = -11155.814950273952
Iteration 13900: Loss = -11155.815024939791
1
Iteration 14000: Loss = -11155.814660237027
Iteration 14100: Loss = -11155.814568548432
Iteration 14200: Loss = -11155.869209241955
1
Iteration 14300: Loss = -11155.814157194749
Iteration 14400: Loss = -11155.831407870535
1
Iteration 14500: Loss = -11155.813992040077
Iteration 14600: Loss = -11155.814116445268
1
Iteration 14700: Loss = -11155.813869502847
Iteration 14800: Loss = -11155.81392611204
1
Iteration 14900: Loss = -11155.813829356935
Iteration 15000: Loss = -11155.814022834784
1
Iteration 15100: Loss = -11155.815193621824
2
Iteration 15200: Loss = -11155.818163673695
3
Iteration 15300: Loss = -11155.813671611171
Iteration 15400: Loss = -11155.813611714115
Iteration 15500: Loss = -11155.840601064487
1
Iteration 15600: Loss = -11155.823001337269
2
Iteration 15700: Loss = -11155.805130511877
Iteration 15800: Loss = -11155.80973745821
1
Iteration 15900: Loss = -11155.8050390461
Iteration 16000: Loss = -11155.810324355441
1
Iteration 16100: Loss = -11155.805052812333
2
Iteration 16200: Loss = -11155.809954850487
3
Iteration 16300: Loss = -11155.797455476264
Iteration 16400: Loss = -11156.053195766537
1
Iteration 16500: Loss = -11155.797478653149
2
Iteration 16600: Loss = -11155.810496168833
3
Iteration 16700: Loss = -11155.797446958522
Iteration 16800: Loss = -11155.79945070007
1
Iteration 16900: Loss = -11155.928927282956
2
Iteration 17000: Loss = -11155.796740130128
Iteration 17100: Loss = -11155.829822857067
1
Iteration 17200: Loss = -11155.796686824093
Iteration 17300: Loss = -11155.801526152367
1
Iteration 17400: Loss = -11155.79665421788
Iteration 17500: Loss = -11155.796684198967
1
Iteration 17600: Loss = -11155.796728622616
2
Iteration 17700: Loss = -11155.796757640277
3
Iteration 17800: Loss = -11155.796766961534
4
Iteration 17900: Loss = -11155.801934388724
5
Iteration 18000: Loss = -11155.799204378558
6
Iteration 18100: Loss = -11155.812451500944
7
Iteration 18200: Loss = -11155.799281212576
8
Iteration 18300: Loss = -11155.796570079197
Iteration 18400: Loss = -11155.796932513262
1
Iteration 18500: Loss = -11155.854137602146
2
Iteration 18600: Loss = -11155.7965358859
Iteration 18700: Loss = -11155.796408498443
Iteration 18800: Loss = -11155.797622407943
1
Iteration 18900: Loss = -11155.799002299917
2
Iteration 19000: Loss = -11155.80023565228
3
Iteration 19100: Loss = -11155.796359245862
Iteration 19200: Loss = -11155.829864721998
1
Iteration 19300: Loss = -11155.810619509102
2
Iteration 19400: Loss = -11155.796720099303
3
Iteration 19500: Loss = -11155.796388713892
4
Iteration 19600: Loss = -11155.796381345895
5
Iteration 19700: Loss = -11155.796497514033
6
Iteration 19800: Loss = -11155.798540234948
7
Iteration 19900: Loss = -11155.795944728745
tensor([[ -2.8990,   1.3999],
        [ -5.5662,   2.3289],
        [ -4.4815,   2.3262],
        [ -5.8756,   4.4815],
        [ -5.2773,   3.5772],
        [ -6.3259,   4.0425],
        [ -5.6171,   2.8397],
        [ -4.6367,   2.2716],
        [ -5.9307,   4.4346],
        [ -6.7398,   4.6543],
        [ -4.2076,   2.7461],
        [ -3.1912,   1.1207],
        [ -5.8927,   4.5044],
        [ -5.7880,   3.9382],
        [ -3.3584,   1.9214],
        [ -4.1202,   2.1675],
        [ -2.3782,   0.8821],
        [ -5.6377,   4.2323],
        [ -4.2324,   2.4142],
        [ -5.2927,   3.6176],
        [ -6.0539,   4.5772],
        [ -3.7054,   2.2405],
        [ -8.6130,   7.2234],
        [ -4.4854,   2.7332],
        [-10.4010,   7.8987],
        [ -5.1571,   3.7563],
        [ -4.9115,   3.5252],
        [ -4.2023,   2.6721],
        [ -2.5992,   0.8520],
        [ -4.6259,   2.3010],
        [ -8.3516,   5.8924],
        [ -4.2440,   2.5538],
        [ -6.1454,   3.7328],
        [ -5.4027,   1.4908],
        [ -3.4315,   1.3064],
        [ -5.4346,   3.9261],
        [ -3.0605,   1.6679],
        [ -4.7066,   3.3153],
        [ -5.3958,   3.9750],
        [ -4.7605,   2.9490],
        [ -4.6267,   3.2332],
        [ -4.5733,   2.1518],
        [ -5.1656,   3.1573],
        [ -3.2800,   1.4924],
        [ -5.9522,   4.4659],
        [ -7.5640,   3.8288],
        [ -5.4823,   1.4327],
        [-10.6040,   7.1883],
        [ -5.2894,   2.0280],
        [ -5.4688,   3.6073],
        [ -5.3792,   3.9869],
        [ -4.6053,   2.8062],
        [ -3.9028,   2.5109],
        [ -5.6567,   3.7051],
        [ -4.0830,   2.6338],
        [ -4.8195,   3.0927],
        [ -5.2830,   2.6606],
        [ -8.7349,   4.1197],
        [ -3.6163,   1.7885],
        [ -4.0644,   2.6781],
        [ -5.8290,   3.0389],
        [ -5.2256,   3.5786],
        [ -5.9075,   4.4532],
        [ -5.3899,   3.4724],
        [ -9.2549,   7.7495],
        [ -4.8114,   1.9643],
        [ -6.6415,   4.5732],
        [ -3.3942,   1.9511],
        [ -5.0929,   3.2662],
        [ -7.5525,   5.3897],
        [ -3.4261,   2.0362],
        [ -3.5925,   2.1918],
        [ -5.5407,   1.7809],
        [ -3.9241,   2.4322],
        [ -5.3890,   3.9502],
        [ -5.7457,   4.1738],
        [ -5.1603,   3.7466],
        [ -4.4779,   2.8933],
        [ -6.6260,   5.2388],
        [ -2.6224,   0.7815],
        [ -5.8729,   4.3645],
        [ -4.7220,   3.1278],
        [ -1.5008,  -0.0906],
        [ -5.0821,   3.6376],
        [ -3.1112,  -0.7643],
        [ -5.0726,   0.8511],
        [ -1.6028,   0.1680],
        [ -5.5153,   2.7746],
        [ -4.7732,   2.3325],
        [ -4.2525,   1.0408],
        [ -5.9216,   4.4270],
        [ -5.2687,   3.8337],
        [ -5.3122,   3.0878],
        [-10.1441,   8.0193],
        [ -2.9410,   1.3550],
        [ -9.3020,   7.6770],
        [ -4.3668,   2.8512],
        [ -4.7533,   3.1835],
        [ -9.0104,   6.4790],
        [ -5.4956,   3.4102]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.5111, 0.4889],
        [0.3521, 0.6479]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0066, 0.9934], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3124, 0.2697],
         [0.1670, 0.1827]],

        [[0.9168, 0.1162],
         [0.5268, 0.1612]],

        [[0.0486, 0.0968],
         [0.9247, 0.2215]],

        [[0.7239, 0.1053],
         [0.4467, 0.8094]],

        [[0.5264, 0.0979],
         [0.7680, 0.0382]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 7
Adjusted Rand Index: 0.737020183942648
time is 2
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 3
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 4
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 77
Adjusted Rand Index: 0.28546347370166186
Global Adjusted Rand Index: 0.21351225714732988
Average Adjusted Rand Index: 0.5806575142019877
Iteration 0: Loss = -20088.41322252246
Iteration 10: Loss = -11209.236892660947
Iteration 20: Loss = -11030.45238660148
Iteration 30: Loss = -11030.450722601936
Iteration 40: Loss = -11030.450700084179
Iteration 50: Loss = -11030.450684669748
Iteration 60: Loss = -11030.450677106846
Iteration 70: Loss = -11030.450678936413
1
Iteration 80: Loss = -11030.450677969142
2
Iteration 90: Loss = -11030.45067796893
3
Stopping early at iteration 89 due to no improvement.
pi: tensor([[0.7362, 0.2638],
        [0.2984, 0.7016]], dtype=torch.float64)
alpha: tensor([0.5315, 0.4685])
beta: tensor([[[0.1929, 0.0960],
         [0.4678, 0.2931]],

        [[0.6951, 0.1109],
         [0.0366, 0.3334]],

        [[0.0432, 0.0966],
         [0.2875, 0.5298]],

        [[0.9121, 0.1044],
         [0.8855, 0.3182]],

        [[0.2898, 0.0901],
         [0.0118, 0.0575]]], dtype=torch.float64)
time is 0
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824214784112464
time is 1
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448373142749671
time is 2
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 3
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9291539174073066
Average Adjusted Rand Index: 0.9294509749048011
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20088.043979804126
Iteration 100: Loss = -11332.714760080527
Iteration 200: Loss = -11307.0591298142
Iteration 300: Loss = -11299.126201119565
Iteration 400: Loss = -11289.744628093747
Iteration 500: Loss = -11288.924048417293
Iteration 600: Loss = -11288.473264505497
Iteration 700: Loss = -11288.063467489204
Iteration 800: Loss = -11287.587205697662
Iteration 900: Loss = -11287.188485467139
Iteration 1000: Loss = -11287.068758897454
Iteration 1100: Loss = -11286.992181035957
Iteration 1200: Loss = -11286.936549880062
Iteration 1300: Loss = -11286.885641234534
Iteration 1400: Loss = -11286.845677543128
Iteration 1500: Loss = -11286.809256950504
Iteration 1600: Loss = -11286.770488641512
Iteration 1700: Loss = -11286.734294871514
Iteration 1800: Loss = -11286.701687544488
Iteration 1900: Loss = -11286.671417051932
Iteration 2000: Loss = -11286.641463519532
Iteration 2100: Loss = -11286.612350820815
Iteration 2200: Loss = -11286.584332604138
Iteration 2300: Loss = -11286.555310786276
Iteration 2400: Loss = -11286.520388800498
Iteration 2500: Loss = -11286.470727795446
Iteration 2600: Loss = -11286.39353521154
Iteration 2700: Loss = -11286.29876793788
Iteration 2800: Loss = -11286.219351434305
Iteration 2900: Loss = -11286.15437029969
Iteration 3000: Loss = -11286.084119868268
Iteration 3100: Loss = -11286.005433722186
Iteration 3200: Loss = -11285.926414194866
Iteration 3300: Loss = -11285.86299299481
Iteration 3400: Loss = -11285.794777634466
Iteration 3500: Loss = -11285.884898947355
1
Iteration 3600: Loss = -11285.628226946494
Iteration 3700: Loss = -11285.475476798034
Iteration 3800: Loss = -11285.29193612983
Iteration 3900: Loss = -11285.008158446044
Iteration 4000: Loss = -11284.538222449844
Iteration 4100: Loss = -11282.710129402007
Iteration 4200: Loss = -11282.54735067339
Iteration 4300: Loss = -11282.516845146714
Iteration 4400: Loss = -11282.50501230783
Iteration 4500: Loss = -11282.49874955398
Iteration 4600: Loss = -11282.4946539946
Iteration 4700: Loss = -11282.491787550069
Iteration 4800: Loss = -11282.500240404379
1
Iteration 4900: Loss = -11282.48760094777
Iteration 5000: Loss = -11282.483987215066
Iteration 5100: Loss = -11282.474846286703
Iteration 5200: Loss = -11282.473164697005
Iteration 5300: Loss = -11282.472169593018
Iteration 5400: Loss = -11282.471356772636
Iteration 5500: Loss = -11282.470679781309
Iteration 5600: Loss = -11282.469994759378
Iteration 5700: Loss = -11282.469418555012
Iteration 5800: Loss = -11282.46897588031
Iteration 5900: Loss = -11282.468452380639
Iteration 6000: Loss = -11282.605789751571
1
Iteration 6100: Loss = -11282.467602289244
Iteration 6200: Loss = -11282.467246379838
Iteration 6300: Loss = -11282.467101777782
Iteration 6400: Loss = -11282.466596579732
Iteration 6500: Loss = -11282.466422681744
Iteration 6600: Loss = -11282.466110043884
Iteration 6700: Loss = -11282.475254126508
1
Iteration 6800: Loss = -11282.465546960018
Iteration 6900: Loss = -11282.46572720012
1
Iteration 7000: Loss = -11282.465125295617
Iteration 7100: Loss = -11282.464992130112
Iteration 7200: Loss = -11282.46471108013
Iteration 7300: Loss = -11282.464538355383
Iteration 7400: Loss = -11282.44362393536
Iteration 7500: Loss = -11282.444300990903
1
Iteration 7600: Loss = -11282.443237135136
Iteration 7700: Loss = -11282.443048102963
Iteration 7800: Loss = -11282.442978027395
Iteration 7900: Loss = -11282.456277008627
1
Iteration 8000: Loss = -11282.442651310892
Iteration 8100: Loss = -11282.44604090056
1
Iteration 8200: Loss = -11282.442443527658
Iteration 8300: Loss = -11282.451599006501
1
Iteration 8400: Loss = -11282.442302165842
Iteration 8500: Loss = -11282.809584704566
1
Iteration 8600: Loss = -11282.442119799412
Iteration 8700: Loss = -11282.442020877972
Iteration 8800: Loss = -11282.442064193723
1
Iteration 8900: Loss = -11282.441908359144
Iteration 9000: Loss = -11282.442453896256
1
Iteration 9100: Loss = -11282.441783242804
Iteration 9200: Loss = -11282.678147901785
1
Iteration 9300: Loss = -11282.44169529064
Iteration 9400: Loss = -11282.44165482658
Iteration 9500: Loss = -11282.44227026278
1
Iteration 9600: Loss = -11282.441508006319
Iteration 9700: Loss = -11282.441910962689
1
Iteration 9800: Loss = -11282.441453236508
Iteration 9900: Loss = -11282.441375003338
Iteration 10000: Loss = -11282.677980702008
1
Iteration 10100: Loss = -11282.441354717264
Iteration 10200: Loss = -11282.441255491432
Iteration 10300: Loss = -11282.483987359014
1
Iteration 10400: Loss = -11282.441202955926
Iteration 10500: Loss = -11282.444659044018
1
Iteration 10600: Loss = -11282.430611019161
Iteration 10700: Loss = -11282.440635398585
1
Iteration 10800: Loss = -11282.43539372595
2
Iteration 10900: Loss = -11282.447495141687
3
Iteration 11000: Loss = -11282.430805100277
4
Iteration 11100: Loss = -11282.430518463012
Iteration 11200: Loss = -11282.430938291576
1
Iteration 11300: Loss = -11282.618545120062
2
Iteration 11400: Loss = -11282.430400233417
Iteration 11500: Loss = -11282.607566954217
1
Iteration 11600: Loss = -11282.430379325142
Iteration 11700: Loss = -11282.432868281736
1
Iteration 11800: Loss = -11282.430327170941
Iteration 11900: Loss = -11282.430469025263
1
Iteration 12000: Loss = -11282.43029394774
Iteration 12100: Loss = -11282.437155931357
1
Iteration 12200: Loss = -11282.43028052179
Iteration 12300: Loss = -11282.446803073584
1
Iteration 12400: Loss = -11282.430285829842
2
Iteration 12500: Loss = -11282.430365651355
3
Iteration 12600: Loss = -11282.439673730612
4
Iteration 12700: Loss = -11282.479245350625
5
Iteration 12800: Loss = -11282.474264314278
6
Iteration 12900: Loss = -11282.430191261516
Iteration 13000: Loss = -11282.429736963926
Iteration 13100: Loss = -11282.430468156743
1
Iteration 13200: Loss = -11282.430928299276
2
Iteration 13300: Loss = -11282.431980864318
3
Iteration 13400: Loss = -11282.453295200565
4
Iteration 13500: Loss = -11282.438094878667
5
Iteration 13600: Loss = -11282.431131283232
6
Iteration 13700: Loss = -11282.438355588303
7
Iteration 13800: Loss = -11282.42963907621
Iteration 13900: Loss = -11282.429892978715
1
Iteration 14000: Loss = -11282.429706987763
2
Iteration 14100: Loss = -11282.430060936236
3
Iteration 14200: Loss = -11282.429707739611
4
Iteration 14300: Loss = -11282.429612689577
Iteration 14400: Loss = -11282.429611768044
Iteration 14500: Loss = -11282.429721995997
1
Iteration 14600: Loss = -11282.431567908045
2
Iteration 14700: Loss = -11282.429606030877
Iteration 14800: Loss = -11282.44685122662
1
Iteration 14900: Loss = -11282.435031154955
2
Iteration 15000: Loss = -11282.42959140719
Iteration 15100: Loss = -11282.431361122699
1
Iteration 15200: Loss = -11282.439118772041
2
Iteration 15300: Loss = -11282.429574341097
Iteration 15400: Loss = -11282.44032461502
1
Iteration 15500: Loss = -11282.43102361011
2
Iteration 15600: Loss = -11282.429901838512
3
Iteration 15700: Loss = -11282.444006860895
4
Iteration 15800: Loss = -11282.429581950959
5
Iteration 15900: Loss = -11282.429693604892
6
Iteration 16000: Loss = -11282.42953736338
Iteration 16100: Loss = -11282.42968371437
1
Iteration 16200: Loss = -11282.430480063906
2
Iteration 16300: Loss = -11282.433417662021
3
Iteration 16400: Loss = -11282.510190528776
4
Iteration 16500: Loss = -11282.429557438592
5
Iteration 16600: Loss = -11282.43030632727
6
Iteration 16700: Loss = -11282.471314440932
7
Iteration 16800: Loss = -11282.435321358797
8
Iteration 16900: Loss = -11282.446874965879
9
Iteration 17000: Loss = -11282.467340842555
10
Stopping early at iteration 17000 due to no improvement.
tensor([[-1.7170,  0.1924],
        [-3.2681,  1.8814],
        [-3.6542,  2.0218],
        [-4.6171,  2.7437],
        [-3.9715,  1.3811],
        [-4.6508,  3.1082],
        [-3.9939,  2.4905],
        [-3.6247,  2.1481],
        [-4.5700,  3.1729],
        [-5.3596,  3.9212],
        [-3.5288,  1.3865],
        [-2.1207,  0.7152],
        [-4.8729,  3.2913],
        [-4.5382,  3.1504],
        [-2.3219,  0.8256],
        [-3.0596,  1.6131],
        [-1.6652, -0.8375],
        [-4.4354,  3.0425],
        [-2.4920,  1.0788],
        [-4.2440,  2.7247],
        [-4.7640,  3.3421],
        [-2.6967,  1.2694],
        [-3.9227,  2.4178],
        [-3.2644,  1.2143],
        [-6.1673,  1.5521],
        [-4.6609,  2.7647],
        [-4.2834,  2.4180],
        [-3.0274,  0.8965],
        [-1.4544, -1.2124],
        [-3.0503,  1.5499],
        [-3.7757,  2.0344],
        [-4.4794,  0.4858],
        [-4.3249,  2.4366],
        [-3.1173,  0.8073],
        [-1.6031,  0.2135],
        [-3.6565,  2.2643],
        [-1.5461,  0.1561],
        [-4.4570,  2.9734],
        [-4.4718,  2.5200],
        [-3.1084,  1.3177],
        [-3.6538,  0.7033],
        [-3.2361,  1.0477],
        [-4.6133,  3.2268],
        [-3.5067, -0.1462],
        [-4.8585,  3.4187],
        [-5.1316,  3.7432],
        [-2.7477,  1.3549],
        [-3.8409,  2.1712],
        [-3.9987,  1.0464],
        [-4.2189,  2.8243],
        [-4.3991,  2.5552],
        [-4.7605,  0.4019],
        [-3.0084,  0.5746],
        [-5.0978,  1.4951],
        [-2.9957,  0.7673],
        [-3.7021,  2.2806],
        [-4.0959,  2.6543],
        [-5.7753,  4.3574],
        [-1.9290,  0.5396],
        [-4.3192, -0.2252],
        [-5.6027,  2.7939],
        [-4.5960,  2.8807],
        [-4.9754,  3.1571],
        [-4.1961,  1.7032],
        [-4.2420,  2.4703],
        [-3.2272,  1.6531],
        [-5.5607,  3.4760],
        [-3.0712,  1.4156],
        [-3.6347,  1.2309],
        [-6.4958,  4.9931],
        [-3.9828, -0.6324],
        [-3.2763,  0.9372],
        [-4.1288,  2.5062],
        [-3.0253, -0.5812],
        [-3.8508,  1.9765],
        [-5.8163,  2.2333],
        [-5.4592,  1.6061],
        [-2.8509,  1.4083],
        [-5.2632,  3.6484],
        [-1.6079, -1.6852],
        [-5.2260,  3.1817],
        [-3.2326,  0.8863],
        [ 0.0826, -2.2385],
        [-4.7852,  1.6434],
        [-1.6205, -0.4168],
        [-2.9116,  0.3647],
        [ 0.6607, -2.0484],
        [-4.5939,  1.8216],
        [-3.4479,  1.2636],
        [-2.6095,  1.2215],
        [-4.5193,  2.5268],
        [-4.3273,  1.7295],
        [-3.7367,  2.2505],
        [-4.9271,  3.2897],
        [-1.9267,  0.1220],
        [-6.3365,  2.5537],
        [-2.9603,  1.5733],
        [-3.8173,  2.2513],
        [-4.2941,  1.8987],
        [-3.9953,  2.5853]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.5031e-01, 4.9694e-02],
        [1.0000e+00, 1.3706e-07]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0469, 0.9531], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1722, 0.2568],
         [0.4678, 0.1593]],

        [[0.6951, 0.2900],
         [0.0366, 0.3334]],

        [[0.0432, 0.1315],
         [0.2875, 0.5298]],

        [[0.9121, 0.2514],
         [0.8855, 0.3182]],

        [[0.2898, 0.0879],
         [0.0118, 0.0575]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.002056465888134684
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.0013070675007002147
Global Adjusted Rand Index: -0.0019432205995196924
Average Adjusted Rand Index: 0.0008167707786587624
Iteration 0: Loss = -22562.15187854226
Iteration 10: Loss = -11290.5713828284
Iteration 20: Loss = -11290.556410317951
Iteration 30: Loss = -11284.18066719674
Iteration 40: Loss = -11278.885169009602
Iteration 50: Loss = -11274.45079020083
Iteration 60: Loss = -11184.514044709715
Iteration 70: Loss = -11030.436358705841
Iteration 80: Loss = -11030.451316243156
1
Iteration 90: Loss = -11030.45077335775
2
Iteration 100: Loss = -11030.45073777274
3
Stopping early at iteration 99 due to no improvement.
pi: tensor([[0.7016, 0.2984],
        [0.2638, 0.7362]], dtype=torch.float64)
alpha: tensor([0.4685, 0.5315])
beta: tensor([[[0.2931, 0.0960],
         [0.6150, 0.1929]],

        [[0.0824, 0.1109],
         [0.2208, 0.3097]],

        [[0.0893, 0.0966],
         [0.4214, 0.6985]],

        [[0.0182, 0.1044],
         [0.0866, 0.4337]],

        [[0.4738, 0.0901],
         [0.8244, 0.5297]]], dtype=torch.float64)
time is 0
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824214784112464
time is 1
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448373142749671
time is 2
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 3
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9291539174073066
Average Adjusted Rand Index: 0.9294509749048011
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22562.340010169948
Iteration 100: Loss = -11292.856692076435
Iteration 200: Loss = -11291.123332171826
Iteration 300: Loss = -11290.314358950904
Iteration 400: Loss = -11286.340916430363
Iteration 500: Loss = -11279.564266244026
Iteration 600: Loss = -11275.654331251944
Iteration 700: Loss = -11268.350836433025
Iteration 800: Loss = -11197.505486232081
Iteration 900: Loss = -11138.019141805724
Iteration 1000: Loss = -11121.97012878245
Iteration 1100: Loss = -11121.487579415556
Iteration 1200: Loss = -11108.01035336137
Iteration 1300: Loss = -11107.840162654407
Iteration 1400: Loss = -11107.759515181626
Iteration 1500: Loss = -11107.703573934956
Iteration 1600: Loss = -11107.655530701166
Iteration 1700: Loss = -11107.61151629008
Iteration 1800: Loss = -11107.583962795728
Iteration 1900: Loss = -11107.566337077724
Iteration 2000: Loss = -11107.55272247498
Iteration 2100: Loss = -11107.541161799001
Iteration 2200: Loss = -11107.530485531413
Iteration 2300: Loss = -11107.518389096578
Iteration 2400: Loss = -11107.498789861644
Iteration 2500: Loss = -11107.48308008937
Iteration 2600: Loss = -11106.768596391556
Iteration 2700: Loss = -11106.736662349882
Iteration 2800: Loss = -11101.945808707605
Iteration 2900: Loss = -11101.664665947023
Iteration 3000: Loss = -11101.653538573517
Iteration 3100: Loss = -11101.634898227476
Iteration 3200: Loss = -11101.616275852031
Iteration 3300: Loss = -11101.611607537598
Iteration 3400: Loss = -11101.608063810116
Iteration 3500: Loss = -11101.603771263104
Iteration 3600: Loss = -11101.534314428753
Iteration 3700: Loss = -11100.406907020593
Iteration 3800: Loss = -11100.38193114249
Iteration 3900: Loss = -11100.377050249486
Iteration 4000: Loss = -11100.374454701845
Iteration 4100: Loss = -11100.37168446345
Iteration 4200: Loss = -11100.369439923043
Iteration 4300: Loss = -11100.367012533216
Iteration 4400: Loss = -11100.30371575592
Iteration 4500: Loss = -11100.30020427588
Iteration 4600: Loss = -11100.299389439517
Iteration 4700: Loss = -11100.298570321107
Iteration 4800: Loss = -11100.297855680488
Iteration 4900: Loss = -11100.297174552685
Iteration 5000: Loss = -11100.296508953654
Iteration 5100: Loss = -11100.29576710078
Iteration 5200: Loss = -11100.294945586964
Iteration 5300: Loss = -11100.293870281506
Iteration 5400: Loss = -11100.29339539757
Iteration 5500: Loss = -11100.291675923316
Iteration 5600: Loss = -11100.29107190004
Iteration 5700: Loss = -11100.29052614522
Iteration 5800: Loss = -11100.290117958799
Iteration 5900: Loss = -11100.28963218492
Iteration 6000: Loss = -11100.293777339337
1
Iteration 6100: Loss = -11100.289031804521
Iteration 6200: Loss = -11100.288673331517
Iteration 6300: Loss = -11100.288380931837
Iteration 6400: Loss = -11100.287779115686
Iteration 6500: Loss = -11100.302147862343
1
Iteration 6600: Loss = -11100.259088922123
Iteration 6700: Loss = -11100.149194601636
Iteration 6800: Loss = -11100.130597404941
Iteration 6900: Loss = -11100.124257516667
Iteration 7000: Loss = -11100.122614897273
Iteration 7100: Loss = -11100.125180805644
1
Iteration 7200: Loss = -11100.122468298207
Iteration 7300: Loss = -11100.122035258773
Iteration 7400: Loss = -11100.121297701646
Iteration 7500: Loss = -11100.121103206147
Iteration 7600: Loss = -11100.120924102885
Iteration 7700: Loss = -11100.128318088955
1
Iteration 7800: Loss = -11100.119730893648
Iteration 7900: Loss = -11099.479001603479
Iteration 8000: Loss = -11099.470278687322
Iteration 8100: Loss = -11099.470843272808
1
Iteration 8200: Loss = -11099.47043739181
2
Iteration 8300: Loss = -11094.584843343286
Iteration 8400: Loss = -11094.592170979178
1
Iteration 8500: Loss = -11094.58245334231
Iteration 8600: Loss = -11094.573246448
Iteration 8700: Loss = -11094.573428114338
1
Iteration 8800: Loss = -11094.569444643743
Iteration 8900: Loss = -11094.568777282571
Iteration 9000: Loss = -11094.569002272627
1
Iteration 9100: Loss = -11094.59968122269
2
Iteration 9200: Loss = -11094.553193886104
Iteration 9300: Loss = -11094.553659759624
1
Iteration 9400: Loss = -11094.552994977927
Iteration 9500: Loss = -11094.554842881082
1
Iteration 9600: Loss = -11094.552813796263
Iteration 9700: Loss = -11094.552575862363
Iteration 9800: Loss = -11094.55254740746
Iteration 9900: Loss = -11094.552259981765
Iteration 10000: Loss = -11094.552189378175
Iteration 10100: Loss = -11094.552062814444
Iteration 10200: Loss = -11093.976684616498
Iteration 10300: Loss = -11093.990146096878
1
Iteration 10400: Loss = -11093.958556781305
Iteration 10500: Loss = -11093.9616299728
1
Iteration 10600: Loss = -11093.962917278626
2
Iteration 10700: Loss = -11093.958553761302
Iteration 10800: Loss = -11093.9586766716
1
Iteration 10900: Loss = -11093.982769406664
2
Iteration 11000: Loss = -11093.93596816038
Iteration 11100: Loss = -11093.93040877511
Iteration 11200: Loss = -11093.858984917953
Iteration 11300: Loss = -11040.270420445022
Iteration 11400: Loss = -11035.712909913329
Iteration 11500: Loss = -11027.106984912258
Iteration 11600: Loss = -11027.10176139374
Iteration 11700: Loss = -11027.098638118961
Iteration 11800: Loss = -11027.09355781247
Iteration 11900: Loss = -11026.768859676446
Iteration 12000: Loss = -11026.766184178296
Iteration 12100: Loss = -11026.768566602495
1
Iteration 12200: Loss = -11026.765425745902
Iteration 12300: Loss = -11026.764075605393
Iteration 12400: Loss = -11026.879848700306
1
Iteration 12500: Loss = -11026.762843868097
Iteration 12600: Loss = -11026.787230100446
1
Iteration 12700: Loss = -11026.76261806001
Iteration 12800: Loss = -11026.762433856577
Iteration 12900: Loss = -11026.7620477309
Iteration 13000: Loss = -11026.76042895786
Iteration 13100: Loss = -11026.764183244532
1
Iteration 13200: Loss = -11026.7582654663
Iteration 13300: Loss = -11026.758272352154
1
Iteration 13400: Loss = -11026.758208766756
Iteration 13500: Loss = -11026.75815508238
Iteration 13600: Loss = -11026.759440034031
1
Iteration 13700: Loss = -11026.753641069112
Iteration 13800: Loss = -11026.751612073773
Iteration 13900: Loss = -11026.752575447552
1
Iteration 14000: Loss = -11026.873451044043
2
Iteration 14100: Loss = -11026.738832179608
Iteration 14200: Loss = -11026.737589962515
Iteration 14300: Loss = -11026.739910273285
1
Iteration 14400: Loss = -11027.00039337823
2
Iteration 14500: Loss = -11026.73627912658
Iteration 14600: Loss = -11026.744775107793
1
Iteration 14700: Loss = -11026.73546185187
Iteration 14800: Loss = -11026.734955041247
Iteration 14900: Loss = -11026.741998873209
1
Iteration 15000: Loss = -11026.726242973664
Iteration 15100: Loss = -11026.74043217205
1
Iteration 15200: Loss = -11026.726224685446
Iteration 15300: Loss = -11026.83104034661
1
Iteration 15400: Loss = -11026.726185426616
Iteration 15500: Loss = -11026.747189636897
1
Iteration 15600: Loss = -11026.726186685673
2
Iteration 15700: Loss = -11026.74023769688
3
Iteration 15800: Loss = -11026.726438846861
4
Iteration 15900: Loss = -11026.72722937973
5
Iteration 16000: Loss = -11026.725427740872
Iteration 16100: Loss = -11026.721732234646
Iteration 16200: Loss = -11026.722044209213
1
Iteration 16300: Loss = -11026.721798872139
2
Iteration 16400: Loss = -11026.721508284254
Iteration 16500: Loss = -11026.734931446947
1
Iteration 16600: Loss = -11026.843529662156
2
Iteration 16700: Loss = -11026.674399140833
Iteration 16800: Loss = -11026.67810389331
1
Iteration 16900: Loss = -11026.685950805073
2
Iteration 17000: Loss = -11026.67372408135
Iteration 17100: Loss = -11026.67000648315
Iteration 17200: Loss = -11026.87838499226
1
Iteration 17300: Loss = -11026.67050866079
2
Iteration 17400: Loss = -11026.669151552192
Iteration 17500: Loss = -11026.783558802294
1
Iteration 17600: Loss = -11026.677003158286
2
Iteration 17700: Loss = -11026.668656987127
Iteration 17800: Loss = -11026.66932496177
1
Iteration 17900: Loss = -11026.868741380047
2
Iteration 18000: Loss = -11026.66844176619
Iteration 18100: Loss = -11026.680589724305
1
Iteration 18200: Loss = -11026.666747247964
Iteration 18300: Loss = -11026.69068009188
1
Iteration 18400: Loss = -11026.666708549252
Iteration 18500: Loss = -11026.668239186904
1
Iteration 18600: Loss = -11026.666476678547
Iteration 18700: Loss = -11026.667233976515
1
Iteration 18800: Loss = -11026.666331207456
Iteration 18900: Loss = -11026.681656458375
1
Iteration 19000: Loss = -11026.66630286175
Iteration 19100: Loss = -11026.877268314809
1
Iteration 19200: Loss = -11026.666233041797
Iteration 19300: Loss = -11026.751705215991
1
Iteration 19400: Loss = -11026.666153051365
Iteration 19500: Loss = -11026.667597216094
1
Iteration 19600: Loss = -11026.67342836419
2
Iteration 19700: Loss = -11026.666102543968
Iteration 19800: Loss = -11026.703021071376
1
Iteration 19900: Loss = -11026.6661567219
2
tensor([[ 7.3769, -9.2090],
        [-5.9230,  2.6277],
        [ 6.9278, -8.3434],
        [-5.7035,  4.3112],
        [-2.3637,  0.7474],
        [-1.3100, -0.0775],
        [ 5.1070, -8.0068],
        [-4.2371,  1.7106],
        [-6.9130,  5.5266],
        [-4.9605,  2.8890],
        [ 6.9641, -8.3664],
        [ 5.7336, -8.3624],
        [-7.6901,  6.1636],
        [-3.9887,  2.1010],
        [ 4.7269, -7.3977],
        [ 5.2720, -7.4485],
        [ 1.7833, -3.2221],
        [-9.1718,  6.1951],
        [ 7.0521, -8.5044],
        [-7.5127,  5.9374],
        [ 0.7024, -3.2264],
        [-4.7741,  3.3509],
        [-5.2255,  2.6487],
        [-3.7184,  2.3251],
        [-3.3165,  1.8767],
        [-8.0122,  6.1159],
        [-5.7537,  3.2477],
        [ 4.0348, -5.4783],
        [ 2.2597, -5.9787],
        [ 3.6580, -5.0805],
        [ 3.3518, -4.9713],
        [ 6.9057, -8.3771],
        [-6.6111,  4.3088],
        [-8.6591,  6.6269],
        [ 4.4717, -6.5451],
        [-6.9234,  4.8614],
        [ 2.2203, -3.6075],
        [ 4.6070, -9.1336],
        [-6.9117,  5.2026],
        [-2.5683,  0.0870],
        [-4.4801,  3.0191],
        [ 3.1094, -4.5478],
        [ 1.2825, -3.9093],
        [ 2.5923, -4.0422],
        [ 3.1861, -5.1170],
        [-4.1328,  2.2297],
        [-5.4885,  2.6982],
        [ 3.9531, -6.9521],
        [ 3.8785, -5.7194],
        [-5.0397,  3.5571],
        [-4.7259,  2.7389],
        [-5.1102,  2.7751],
        [ 5.2376, -8.9905],
        [-2.7285,  0.2568],
        [-2.8548,  1.3216],
        [-6.2891,  4.5644],
        [-2.3562,  0.8226],
        [-4.1074,  2.1196],
        [-6.4837,  5.0777],
        [ 1.8069, -3.1946],
        [-1.1233, -0.2751],
        [ 2.6987, -5.5491],
        [-5.8028,  4.3351],
        [-7.9545,  6.3590],
        [ 1.4953, -3.1899],
        [ 5.1881, -6.5809],
        [-0.3782, -2.2949],
        [ 2.6723, -5.6697],
        [ 1.1079, -5.7231],
        [ 0.6356, -2.5160],
        [ 6.6352, -8.7948],
        [ 4.6216, -6.0630],
        [ 2.0192, -4.4197],
        [-5.7239,  4.3358],
        [-7.8137,  5.9382],
        [-7.4396,  5.8392],
        [ 1.8013, -4.5811],
        [-4.6745,  3.2177],
        [-5.7558,  4.2396],
        [-8.8946,  6.8003],
        [-3.4807,  1.7652],
        [-9.1730,  5.3505],
        [ 7.4535, -9.0404],
        [ 1.0007, -2.8372],
        [ 7.5790, -8.9819],
        [ 1.6815, -3.7891],
        [-1.0709, -0.3156],
        [-3.4770,  1.9944],
        [ 4.2214, -6.1640],
        [ 2.0999, -3.7492],
        [-9.5951,  7.0904],
        [-9.8179,  6.2599],
        [-7.8372,  4.2024],
        [-4.3394,  1.5777],
        [ 5.8188, -8.5391],
        [-2.9955,  1.4864],
        [ 4.6822, -8.3324],
        [ 2.1035, -3.8497],
        [-2.0584,  0.5692],
        [-5.6393,  2.6627]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7137, 0.2863],
        [0.2539, 0.7461]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4691, 0.5309], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3000, 0.0960],
         [0.6150, 0.1967]],

        [[0.0824, 0.1105],
         [0.2208, 0.3097]],

        [[0.0893, 0.0961],
         [0.4214, 0.6985]],

        [[0.0182, 0.1036],
         [0.0866, 0.4337]],

        [[0.4738, 0.0899],
         [0.8244, 0.5297]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824214784112464
time is 1
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
time is 2
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 3
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9524807523239673
Average Adjusted Rand Index: 0.9526444267184807
11047.855077549195
new:  [0.9524807523239673, 0.21351225714732988, -0.0019432205995196924, 0.9524807523239673] [0.9526444267184807, 0.5806575142019877, 0.0008167707786587624, 0.9526444267184807] [11026.679705755007, 11155.795031512096, 11282.467340842555, 11026.666142409586]
prior:  [0.9291539174073066, 0.0, 0.9291539174073066, 0.9291539174073066] [0.9294509749048011, 0.0, 0.9294509749048011, 0.9294509749048011] [11030.450681899005, nan, 11030.45067796893, 11030.45073777274]
-----------------------------------------------------------------------------------------
This iteration is 4
True Objective function: Loss = -11166.107420627
Iteration 0: Loss = -19558.979571754244
Iteration 10: Loss = -11305.775624030139
Iteration 20: Loss = -11267.935082626707
Iteration 30: Loss = -11258.008874825555
Iteration 40: Loss = -11229.14320051035
Iteration 50: Loss = -11221.907832216824
Iteration 60: Loss = -11196.606380132436
Iteration 70: Loss = -11136.417160018013
Iteration 80: Loss = -11136.377194044102
Iteration 90: Loss = -11136.373440917296
Iteration 100: Loss = -11136.373124600414
Iteration 110: Loss = -11136.373123260964
Iteration 120: Loss = -11136.373110835708
Iteration 130: Loss = -11136.37310702626
Iteration 140: Loss = -11136.373117156461
1
Iteration 150: Loss = -11136.373110138795
2
Iteration 160: Loss = -11136.373106533154
Iteration 170: Loss = -11136.373113285576
1
Iteration 180: Loss = -11136.373106579373
2
Iteration 190: Loss = -11136.373113285577
3
Stopping early at iteration 189 due to no improvement.
pi: tensor([[0.7841, 0.2159],
        [0.2693, 0.7307]], dtype=torch.float64)
alpha: tensor([0.5864, 0.4136])
beta: tensor([[[0.1984, 0.0996],
         [0.6732, 0.2871]],

        [[0.2117, 0.1150],
         [0.2516, 0.8093]],

        [[0.2644, 0.1074],
         [0.6169, 0.4438]],

        [[0.9890, 0.0985],
         [0.3860, 0.9666]],

        [[0.8636, 0.1091],
         [0.1092, 0.6652]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.844331361923687
time is 1
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599857335115991
time is 2
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9206289602688308
time is 3
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 4
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
Global Adjusted Rand Index: 0.9136948049142537
Average Adjusted Rand Index: 0.913474059625672
tensor([[-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19646.6521932858
Iteration 100: Loss = -11220.74792921407
Iteration 200: Loss = -11189.176875965699
Iteration 300: Loss = -11181.537904982883
Iteration 400: Loss = -11181.19981401303
Iteration 500: Loss = -11180.887391943575
Iteration 600: Loss = -11180.770987457643
Iteration 700: Loss = -11180.724938546407
Iteration 800: Loss = -11180.694994809632
Iteration 900: Loss = -11178.84248682732
Iteration 1000: Loss = -11178.727759875901
Iteration 1100: Loss = -11178.70835334825
Iteration 1200: Loss = -11178.695563997038
Iteration 1300: Loss = -11178.68388874581
Iteration 1400: Loss = -11178.671795168717
Iteration 1500: Loss = -11178.657076964862
Iteration 1600: Loss = -11178.636776663987
Iteration 1700: Loss = -11178.606328540662
Iteration 1800: Loss = -11178.551443004593
Iteration 1900: Loss = -11178.412930287897
Iteration 2000: Loss = -11177.809034362177
Iteration 2100: Loss = -11130.724965016992
Iteration 2200: Loss = -11130.395781979307
Iteration 2300: Loss = -11130.332050769319
Iteration 2400: Loss = -11130.32226872749
Iteration 2500: Loss = -11128.66875644853
Iteration 2600: Loss = -11128.658449967896
Iteration 2700: Loss = -11128.648723786846
Iteration 2800: Loss = -11128.520926256138
Iteration 2900: Loss = -11128.519483122525
Iteration 3000: Loss = -11128.517061527837
Iteration 3100: Loss = -11128.516059601712
Iteration 3200: Loss = -11128.514497860475
Iteration 3300: Loss = -11128.508448116534
Iteration 3400: Loss = -11128.505482821494
Iteration 3500: Loss = -11128.503616678103
Iteration 3600: Loss = -11128.503306223636
Iteration 3700: Loss = -11128.502980629224
Iteration 3800: Loss = -11128.50322975789
1
Iteration 3900: Loss = -11128.502551578567
Iteration 4000: Loss = -11128.502300934346
Iteration 4100: Loss = -11128.50211174173
Iteration 4200: Loss = -11128.501933200196
Iteration 4300: Loss = -11128.501775174203
Iteration 4400: Loss = -11128.50159841425
Iteration 4500: Loss = -11128.501866990235
1
Iteration 4600: Loss = -11128.501272983132
Iteration 4700: Loss = -11128.502304465785
1
Iteration 4800: Loss = -11128.500377758406
Iteration 4900: Loss = -11128.491159172458
Iteration 5000: Loss = -11128.490606019901
Iteration 5100: Loss = -11128.49053244338
Iteration 5200: Loss = -11128.490452738502
Iteration 5300: Loss = -11128.490298250055
Iteration 5400: Loss = -11128.490110100747
Iteration 5500: Loss = -11128.48996498342
Iteration 5600: Loss = -11128.489496664872
Iteration 5700: Loss = -11128.490352583614
1
Iteration 5800: Loss = -11128.489423010718
Iteration 5900: Loss = -11128.489409609896
Iteration 6000: Loss = -11128.489643282053
1
Iteration 6100: Loss = -11128.489263365253
Iteration 6200: Loss = -11128.48927252477
1
Iteration 6300: Loss = -11128.489275438702
2
Iteration 6400: Loss = -11128.48913474162
Iteration 6500: Loss = -11128.489157074851
1
Iteration 6600: Loss = -11128.488985500859
Iteration 6700: Loss = -11128.504690640879
1
Iteration 6800: Loss = -11128.488856264708
Iteration 6900: Loss = -11128.488793725952
Iteration 7000: Loss = -11128.58819490584
1
Iteration 7100: Loss = -11128.488771258228
Iteration 7200: Loss = -11128.488683390562
Iteration 7300: Loss = -11128.539249946414
1
Iteration 7400: Loss = -11128.487884745977
Iteration 7500: Loss = -11128.48785033814
Iteration 7600: Loss = -11128.488539129217
1
Iteration 7700: Loss = -11128.487830940008
Iteration 7800: Loss = -11128.499424917292
1
Iteration 7900: Loss = -11128.497481986138
2
Iteration 8000: Loss = -11128.500771288436
3
Iteration 8100: Loss = -11128.514192618917
4
Iteration 8200: Loss = -11128.48580615566
Iteration 8300: Loss = -11128.48666648007
1
Iteration 8400: Loss = -11128.490635625267
2
Iteration 8500: Loss = -11128.540283904744
3
Iteration 8600: Loss = -11128.504158467778
4
Iteration 8700: Loss = -11128.608801775597
5
Iteration 8800: Loss = -11128.487271626938
6
Iteration 8900: Loss = -11128.485867283587
7
Iteration 9000: Loss = -11128.486419866358
8
Iteration 9100: Loss = -11128.48764511211
9
Iteration 9200: Loss = -11128.48571915351
Iteration 9300: Loss = -11128.486819006108
1
Iteration 9400: Loss = -11128.48593037491
2
Iteration 9500: Loss = -11128.485748392826
3
Iteration 9600: Loss = -11128.494348618075
4
Iteration 9700: Loss = -11128.48556257268
Iteration 9800: Loss = -11128.485667434883
1
Iteration 9900: Loss = -11128.485544112438
Iteration 10000: Loss = -11128.485991138776
1
Iteration 10100: Loss = -11128.602466912153
2
Iteration 10200: Loss = -11128.485505221079
Iteration 10300: Loss = -11128.489855564689
1
Iteration 10400: Loss = -11128.485495692736
Iteration 10500: Loss = -11128.486439009197
1
Iteration 10600: Loss = -11128.485511971625
2
Iteration 10700: Loss = -11128.541888439755
3
Iteration 10800: Loss = -11128.485488497892
Iteration 10900: Loss = -11128.485760215988
1
Iteration 11000: Loss = -11128.488686918
2
Iteration 11100: Loss = -11128.50063889877
3
Iteration 11200: Loss = -11128.594225800642
4
Iteration 11300: Loss = -11128.526743306049
5
Iteration 11400: Loss = -11128.487876171726
6
Iteration 11500: Loss = -11128.486609912232
7
Iteration 11600: Loss = -11128.48565120778
8
Iteration 11700: Loss = -11128.485541909893
9
Iteration 11800: Loss = -11128.489928599345
10
Stopping early at iteration 11800 due to no improvement.
tensor([[ 2.5426, -7.1579],
        [ 2.2969, -6.9121],
        [ 2.4143, -7.0295],
        [-4.5782, -0.0370],
        [-8.0989,  3.4837],
        [ 0.5740, -5.1892],
        [-7.7296,  3.1144],
        [-9.6704,  5.0552],
        [ 0.7601, -5.3754],
        [-3.9205, -0.6948],
        [-0.4485, -4.1668],
        [-4.3792, -0.2360],
        [ 3.2714, -7.8866],
        [-3.6505, -0.9647],
        [-0.9347, -3.6805],
        [-1.5335, -3.0817],
        [ 2.3247, -6.9399],
        [ 1.9190, -6.5342],
        [ 4.7037, -9.3189],
        [ 1.2203, -5.8355],
        [-2.9366, -1.6786],
        [ 2.2794, -6.8946],
        [-4.6896,  0.0744],
        [ 1.3312, -5.9464],
        [ 3.9284, -8.5436],
        [-1.2698, -3.3454],
        [-9.0624,  4.4471],
        [ 2.9123, -7.5275],
        [-5.9922,  1.3770],
        [ 4.7779, -9.3932],
        [-7.6555,  3.0403],
        [ 1.1425, -5.7577],
        [-6.4004,  1.7851],
        [-9.9225,  5.3073],
        [ 0.7698, -5.3850],
        [ 4.4688, -9.0841],
        [ 1.9624, -6.5776],
        [-5.3750,  0.7598],
        [-5.4064,  0.7912],
        [-4.0394, -0.5758],
        [ 0.8082, -5.4234],
        [-3.7491, -0.8661],
        [ 3.4865, -8.1018],
        [ 1.2293, -5.8445],
        [ 3.4823, -8.0975],
        [ 2.4178, -7.0330],
        [ 2.0471, -6.6623],
        [ 2.3810, -6.9963],
        [ 0.7018, -5.3170],
        [-0.0584, -4.5568],
        [-4.8807,  0.2655],
        [ 4.8883, -9.5035],
        [ 0.3516, -4.9668],
        [-7.2639,  2.6487],
        [-5.8076,  1.1924],
        [ 0.3911, -5.0063],
        [ 2.5586, -7.1738],
        [-8.1885,  3.5733],
        [-6.3037,  1.6885],
        [ 4.0718, -8.6871],
        [-6.3881,  1.7729],
        [ 2.5202, -7.1354],
        [-3.2582, -1.3570],
        [-6.2226,  1.6074],
        [ 1.5908, -6.2061],
        [-3.7597, -0.8556],
        [-6.4075,  1.7923],
        [-6.7381,  2.1229],
        [ 3.0062, -7.6214],
        [ 1.1094, -5.7246],
        [ 1.2427, -5.8579],
        [ 2.1781, -6.7933],
        [-3.2723, -1.3429],
        [-2.9724, -1.6429],
        [-6.9387,  2.3235],
        [ 1.3039, -5.9191],
        [ 0.2493, -4.8645],
        [-5.7713,  1.1561],
        [ 2.4304, -7.0456],
        [ 3.9216, -8.5369],
        [ 3.9048, -8.5200],
        [ 1.4041, -6.0193],
        [ 3.8971, -8.5123],
        [ 1.0471, -5.6623],
        [-1.2175, -3.3978],
        [ 3.9895, -8.6047],
        [-5.1711,  0.5559],
        [ 0.5275, -5.1428],
        [-6.2796,  1.6643],
        [-7.3151,  2.6998],
        [ 2.2225, -6.8377],
        [-7.7545,  3.1392],
        [ 1.0219, -5.6371],
        [ 2.8020, -7.4172],
        [ 1.0244, -5.6396],
        [ 3.1337, -7.7489],
        [ 3.0704, -7.6856],
        [ 2.6159, -7.2311],
        [ 1.9110, -6.5262],
        [-9.9196,  5.3043]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.8129, 0.1871],
        [0.2290, 0.7710]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6236, 0.3764], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2022, 0.0992],
         [0.6732, 0.2929]],

        [[0.2117, 0.1150],
         [0.2516, 0.8093]],

        [[0.2644, 0.1074],
         [0.6169, 0.4438]],

        [[0.9890, 0.0996],
         [0.3860, 0.9666]],

        [[0.8636, 0.1088],
         [0.1092, 0.6652]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9207884124763394
time is 2
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9206887570795217
time is 3
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.844846433231073
time is 4
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
Global Adjusted Rand Index: 0.9137126258860444
Average Adjusted Rand Index: 0.9137495690422354
Iteration 0: Loss = -23651.549250331223
Iteration 10: Loss = -11312.72509954416
Iteration 20: Loss = -11312.725099544163
1
Iteration 30: Loss = -11312.725099544172
2
Iteration 40: Loss = -11312.725099544197
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[1.0000e+00, 1.9322e-15],
        [1.0000e+00, 3.0049e-18]], dtype=torch.float64)
alpha: tensor([1.0000e+00, 1.9421e-15])
beta: tensor([[[0.1690, 0.1200],
         [0.8717, 0.1766]],

        [[0.5971, 0.2245],
         [0.7142, 0.2339]],

        [[0.1097, 0.1754],
         [0.7683, 0.3534]],

        [[0.5376, 0.1192],
         [0.0770, 0.2467]],

        [[0.9838, 0.2011],
         [0.0388, 0.1418]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 62
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 61
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23651.792108950376
Iteration 100: Loss = -11317.032380374098
Iteration 200: Loss = -11314.48902715482
Iteration 300: Loss = -11313.536249607536
Iteration 400: Loss = -11313.112315311726
Iteration 500: Loss = -11312.903933756892
Iteration 600: Loss = -11312.787083849462
Iteration 700: Loss = -11312.707343542492
Iteration 800: Loss = -11312.646964557727
Iteration 900: Loss = -11312.597107955375
Iteration 1000: Loss = -11312.553975001709
Iteration 1100: Loss = -11312.515630606384
Iteration 1200: Loss = -11312.48052205563
Iteration 1300: Loss = -11312.447296199793
Iteration 1400: Loss = -11312.41566267799
Iteration 1500: Loss = -11312.385897108263
Iteration 1600: Loss = -11312.357995672635
Iteration 1700: Loss = -11312.33175796743
Iteration 1800: Loss = -11312.306798548294
Iteration 1900: Loss = -11312.283080521065
Iteration 2000: Loss = -11312.260509536029
Iteration 2100: Loss = -11312.238391550967
Iteration 2200: Loss = -11312.216269922119
Iteration 2300: Loss = -11312.19429649015
Iteration 2400: Loss = -11312.172013585701
Iteration 2500: Loss = -11312.14913434483
Iteration 2600: Loss = -11312.1253990432
Iteration 2700: Loss = -11312.131180598406
1
Iteration 2800: Loss = -11312.074865401464
Iteration 2900: Loss = -11312.048240815833
Iteration 3000: Loss = -11312.02116328019
Iteration 3100: Loss = -11311.994380436852
Iteration 3200: Loss = -11311.967861560854
Iteration 3300: Loss = -11311.943203361492
Iteration 3400: Loss = -11311.92506292587
Iteration 3500: Loss = -11311.900565380118
Iteration 3600: Loss = -11311.883006060218
Iteration 3700: Loss = -11311.867854953189
Iteration 3800: Loss = -11311.855980504006
Iteration 3900: Loss = -11311.84378725939
Iteration 4000: Loss = -11311.834293746993
Iteration 4100: Loss = -11311.909870591986
1
Iteration 4200: Loss = -11311.819697101759
Iteration 4300: Loss = -11311.814565004357
Iteration 4400: Loss = -11311.810512861473
Iteration 4500: Loss = -11311.811636283714
1
Iteration 4600: Loss = -11311.805019181531
Iteration 4700: Loss = -11311.803176534535
Iteration 4800: Loss = -11311.802463414402
Iteration 4900: Loss = -11311.800639059367
Iteration 5000: Loss = -11311.799718319751
Iteration 5100: Loss = -11311.83740167966
1
Iteration 5200: Loss = -11311.798432856476
Iteration 5300: Loss = -11311.798020184873
Iteration 5400: Loss = -11311.797745528873
Iteration 5500: Loss = -11311.797369191687
Iteration 5600: Loss = -11311.804379709109
1
Iteration 5700: Loss = -11311.79690612932
Iteration 5800: Loss = -11311.796789580922
Iteration 5900: Loss = -11311.796676930147
Iteration 6000: Loss = -11311.796477069549
Iteration 6100: Loss = -11311.796433779627
Iteration 6200: Loss = -11311.796237310282
Iteration 6300: Loss = -11311.796143226018
Iteration 6400: Loss = -11311.79614415209
1
Iteration 6500: Loss = -11311.795923388414
Iteration 6600: Loss = -11311.795847437716
Iteration 6700: Loss = -11311.796420052817
1
Iteration 6800: Loss = -11311.795646613775
Iteration 6900: Loss = -11311.795555279445
Iteration 7000: Loss = -11311.981467295314
1
Iteration 7100: Loss = -11311.79539585443
Iteration 7200: Loss = -11311.795291992803
Iteration 7300: Loss = -11311.79516950224
Iteration 7400: Loss = -11311.795125247932
Iteration 7500: Loss = -11311.794966395137
Iteration 7600: Loss = -11311.794908053002
Iteration 7700: Loss = -11311.868891174516
1
Iteration 7800: Loss = -11311.794685372704
Iteration 7900: Loss = -11311.79459271192
Iteration 8000: Loss = -11311.794501449138
Iteration 8100: Loss = -11311.794997070689
1
Iteration 8200: Loss = -11311.794284875716
Iteration 8300: Loss = -11311.794192720541
Iteration 8400: Loss = -11311.800935560654
1
Iteration 8500: Loss = -11311.794033619788
Iteration 8600: Loss = -11311.79393696109
Iteration 8700: Loss = -11312.003740062002
1
Iteration 8800: Loss = -11311.79377081453
Iteration 8900: Loss = -11311.793704840133
Iteration 9000: Loss = -11311.793586501131
Iteration 9100: Loss = -11311.793950467872
1
Iteration 9200: Loss = -11311.793438076855
Iteration 9300: Loss = -11311.793374241604
Iteration 9400: Loss = -11311.799005755413
1
Iteration 9500: Loss = -11311.793244098684
Iteration 9600: Loss = -11311.793150084442
Iteration 9700: Loss = -11311.821933196617
1
Iteration 9800: Loss = -11311.793034880726
Iteration 9900: Loss = -11311.792988933563
Iteration 10000: Loss = -11311.825087944284
1
Iteration 10100: Loss = -11311.792830346938
Iteration 10200: Loss = -11311.792796398084
Iteration 10300: Loss = -11311.800074631019
1
Iteration 10400: Loss = -11311.79270877489
Iteration 10500: Loss = -11311.79259426926
Iteration 10600: Loss = -11311.792565530774
Iteration 10700: Loss = -11311.79277556509
1
Iteration 10800: Loss = -11311.792517305204
Iteration 10900: Loss = -11311.792429039684
Iteration 11000: Loss = -11311.793359649413
1
Iteration 11100: Loss = -11311.792338464853
Iteration 11200: Loss = -11311.792306175013
Iteration 11300: Loss = -11311.79256843995
1
Iteration 11400: Loss = -11311.792218908107
Iteration 11500: Loss = -11311.81022036782
1
Iteration 11600: Loss = -11311.792160176148
Iteration 11700: Loss = -11311.792119284299
Iteration 11800: Loss = -11311.79872733756
1
Iteration 11900: Loss = -11311.792053296838
Iteration 12000: Loss = -11311.792054638538
1
Iteration 12100: Loss = -11311.793198261636
2
Iteration 12200: Loss = -11311.791943196358
Iteration 12300: Loss = -11311.792775076487
1
Iteration 12400: Loss = -11311.791950893477
2
Iteration 12500: Loss = -11311.792008545266
3
Iteration 12600: Loss = -11311.889264269761
4
Iteration 12700: Loss = -11311.794939932695
5
Iteration 12800: Loss = -11311.791859405148
Iteration 12900: Loss = -11311.791897640844
1
Iteration 13000: Loss = -11311.7918252075
Iteration 13100: Loss = -11311.79249991501
1
Iteration 13200: Loss = -11311.800124098461
2
Iteration 13300: Loss = -11311.792065807074
3
Iteration 13400: Loss = -11311.791752954452
Iteration 13500: Loss = -11311.802603213066
1
Iteration 13600: Loss = -11311.855135957458
2
Iteration 13700: Loss = -11311.792536966013
3
Iteration 13800: Loss = -11311.794722550303
4
Iteration 13900: Loss = -11311.79547627555
5
Iteration 14000: Loss = -11311.792847430888
6
Iteration 14100: Loss = -11311.795053890473
7
Iteration 14200: Loss = -11311.853125037926
8
Iteration 14300: Loss = -11311.80583155294
9
Iteration 14400: Loss = -11311.988509591678
10
Stopping early at iteration 14400 due to no improvement.
tensor([[ 0.9249, -3.8382],
        [ 1.5479, -2.9680],
        [ 1.6279, -3.6167],
        [ 1.9081, -3.4429],
        [ 1.4321, -3.0301],
        [ 1.5781, -3.9211],
        [ 1.8666, -3.2729],
        [ 1.6168, -3.0958],
        [ 0.1174, -2.1311],
        [ 1.5424, -2.9358],
        [ 0.6539, -2.0474],
        [-0.3513, -4.2604],
        [ 1.2246, -2.6468],
        [ 0.9000, -2.7772],
        [ 1.8906, -3.3718],
        [ 2.0283, -3.4598],
        [ 1.3552, -3.0607],
        [-1.2602, -3.3550],
        [ 0.3806, -4.2747],
        [ 0.2621, -2.7673],
        [ 0.2081, -3.1016],
        [ 0.0590, -2.9790],
        [ 1.8961, -3.9171],
        [ 1.8810, -3.3752],
        [ 0.4728, -5.0880],
        [ 0.6657, -2.8816],
        [ 2.0501, -3.4405],
        [ 0.4120, -4.1375],
        [-0.0678, -4.5475],
        [ 0.9481, -2.3354],
        [ 1.2889, -3.1221],
        [ 1.1487, -3.8812],
        [ 1.0919, -3.4471],
        [ 1.3461, -4.2157],
        [ 0.0441, -2.6441],
        [ 1.6925, -3.6036],
        [-0.1705, -4.3715],
        [ 1.2147, -3.2411],
        [ 1.5698, -3.2551],
        [ 1.1573, -2.6555],
        [ 1.5062, -2.9354],
        [-0.2600, -3.9296],
        [ 1.1736, -2.6725],
        [ 1.2463, -2.6587],
        [ 2.3223, -3.7129],
        [ 0.8844, -2.3621],
        [ 1.0659, -2.5614],
        [ 1.8676, -3.7734],
        [ 0.9393, -2.4034],
        [ 1.4653, -3.2919],
        [ 0.9430, -2.3327],
        [ 1.4111, -3.0504],
        [ 0.5612, -1.9741],
        [ 1.3673, -2.7843],
        [ 1.2697, -3.7241],
        [ 1.1422, -4.1743],
        [ 2.0748, -3.5102],
        [ 0.9735, -2.3656],
        [ 0.7997, -3.0884],
        [ 0.9872, -2.4788],
        [ 0.6134, -2.7780],
        [ 1.3850, -3.9086],
        [ 0.8794, -2.3526],
        [ 0.7831, -2.8071],
        [ 1.5568, -2.9577],
        [ 0.9287, -2.4066],
        [ 1.3908, -2.8402],
        [ 2.5573, -4.0306],
        [ 0.8730, -3.5738],
        [ 0.6680, -3.7740],
        [-0.0467, -4.4459],
        [ 1.8010, -3.1958],
        [ 1.2186, -3.5255],
        [ 1.4941, -3.2299],
        [-1.1853, -3.4299],
        [ 1.3806, -2.8977],
        [ 1.4536, -3.3453],
        [ 1.9222, -3.3555],
        [ 0.2621, -3.6568],
        [ 1.0322, -2.5049],
        [ 0.6277, -2.1633],
        [ 0.0774, -4.6926],
        [ 1.6984, -3.2801],
        [ 1.6793, -3.1802],
        [ 2.1606, -3.6421],
        [ 2.2057, -4.1978],
        [ 0.4418, -2.0112],
        [ 0.5343, -2.1880],
        [ 1.2943, -2.6816],
        [ 1.0134, -2.4367],
        [ 0.7509, -2.3319],
        [ 1.9110, -3.3934],
        [ 1.5360, -2.9333],
        [ 0.2090, -4.8242],
        [ 0.5503, -1.9599],
        [ 1.9562, -3.4545],
        [ 0.2637, -3.4486],
        [ 1.2159, -4.3060],
        [ 0.7907, -3.3239],
        [ 2.6231, -4.0220]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.3674e-01, 6.3264e-02],
        [9.9935e-01, 6.5118e-04]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9797, 0.0203], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1688, 0.1352],
         [0.8717, 0.1838]],

        [[0.5971, 0.1974],
         [0.7142, 0.2339]],

        [[0.1097, 0.1777],
         [0.7683, 0.3534]],

        [[0.5376, 0.1485],
         [0.0770, 0.2467]],

        [[0.9838, 0.1881],
         [0.0388, 0.1418]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 62
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 61
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -28616.67626474894
Iteration 10: Loss = -11312.184807737543
Iteration 20: Loss = -11312.159482415673
Iteration 30: Loss = -11312.155042739652
Iteration 40: Loss = -11312.154099159403
Iteration 50: Loss = -11312.153795424138
Iteration 60: Loss = -11312.153727733043
Iteration 70: Loss = -11312.153678657103
Iteration 80: Loss = -11312.15366787169
Iteration 90: Loss = -11312.153573674033
Iteration 100: Loss = -11312.153645685197
1
Iteration 110: Loss = -11312.153594071999
2
Iteration 120: Loss = -11312.153568976413
Iteration 130: Loss = -11312.153515326167
Iteration 140: Loss = -11312.153503852227
Iteration 150: Loss = -11312.153491559136
Iteration 160: Loss = -11312.153472500091
Iteration 170: Loss = -11312.153442252706
Iteration 180: Loss = -11312.15341554621
Iteration 190: Loss = -11312.15345179834
1
Iteration 200: Loss = -11312.153370372025
Iteration 210: Loss = -11312.153391633412
1
Iteration 220: Loss = -11312.153319271214
Iteration 230: Loss = -11312.153279562512
Iteration 240: Loss = -11312.15330404138
1
Iteration 250: Loss = -11312.15330901831
2
Iteration 260: Loss = -11312.153237056631
Iteration 270: Loss = -11312.153234317318
Iteration 280: Loss = -11312.153240652955
1
Iteration 290: Loss = -11312.15321852206
Iteration 300: Loss = -11312.153166360153
Iteration 310: Loss = -11312.153142658903
Iteration 320: Loss = -11312.15314231536
Iteration 330: Loss = -11312.153148580526
1
Iteration 340: Loss = -11312.153095124155
Iteration 350: Loss = -11312.15309185159
Iteration 360: Loss = -11312.153078308458
Iteration 370: Loss = -11312.153076532342
Iteration 380: Loss = -11312.153019442683
Iteration 390: Loss = -11312.153000715156
Iteration 400: Loss = -11312.152988448464
Iteration 410: Loss = -11312.15295158484
Iteration 420: Loss = -11312.15293856081
Iteration 430: Loss = -11312.152949412546
1
Iteration 440: Loss = -11312.152889841385
Iteration 450: Loss = -11312.152876050213
Iteration 460: Loss = -11312.1528471806
Iteration 470: Loss = -11312.15286242628
1
Iteration 480: Loss = -11312.152783702382
Iteration 490: Loss = -11312.152810656118
1
Iteration 500: Loss = -11312.152760752291
Iteration 510: Loss = -11312.152759493942
Iteration 520: Loss = -11312.152705983932
Iteration 530: Loss = -11312.152742359996
1
Iteration 540: Loss = -11312.152667704197
Iteration 550: Loss = -11312.15265752239
Iteration 560: Loss = -11312.152619341874
Iteration 570: Loss = -11312.152580060074
Iteration 580: Loss = -11312.152642924917
1
Iteration 590: Loss = -11312.152582537301
2
Iteration 600: Loss = -11312.152547079082
Iteration 610: Loss = -11312.152544294131
Iteration 620: Loss = -11312.152550904493
1
Iteration 630: Loss = -11312.152512433293
Iteration 640: Loss = -11312.152486583658
Iteration 650: Loss = -11312.152465138151
Iteration 660: Loss = -11312.152446294605
Iteration 670: Loss = -11312.152450786742
1
Iteration 680: Loss = -11312.152412901098
Iteration 690: Loss = -11312.152340364462
Iteration 700: Loss = -11312.152394493722
1
Iteration 710: Loss = -11312.152363745645
2
Iteration 720: Loss = -11312.152308071161
Iteration 730: Loss = -11312.152329579329
1
Iteration 740: Loss = -11312.15231817556
2
Iteration 750: Loss = -11312.152241658596
Iteration 760: Loss = -11312.152232733613
Iteration 770: Loss = -11312.152219656002
Iteration 780: Loss = -11312.152204536458
Iteration 790: Loss = -11312.15216481595
Iteration 800: Loss = -11312.152199070006
1
Iteration 810: Loss = -11312.152173512117
2
Iteration 820: Loss = -11312.152136914714
Iteration 830: Loss = -11312.15210034132
Iteration 840: Loss = -11312.152071879556
Iteration 850: Loss = -11312.15203482495
Iteration 860: Loss = -11312.152075420581
1
Iteration 870: Loss = -11312.152015282154
Iteration 880: Loss = -11312.15200467608
Iteration 890: Loss = -11312.151958852346
Iteration 900: Loss = -11312.151977944763
1
Iteration 910: Loss = -11312.15193726301
Iteration 920: Loss = -11312.151854691421
Iteration 930: Loss = -11312.151922915244
1
Iteration 940: Loss = -11312.15188425098
2
Iteration 950: Loss = -11312.151865881926
3
Stopping early at iteration 949 due to no improvement.
pi: tensor([[0.1479, 0.8521],
        [0.4021, 0.5979]], dtype=torch.float64)
alpha: tensor([0.3206, 0.6794])
beta: tensor([[[0.1689, 0.1648],
         [0.7117, 0.1691]],

        [[0.2109, 0.1736],
         [0.3476, 0.2213]],

        [[0.5548, 0.1705],
         [0.6793, 0.0688]],

        [[0.4112, 0.1634],
         [0.3881, 0.7408]],

        [[0.7681, 0.1726],
         [0.7635, 0.7915]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 38
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -28616.40939331127
Iteration 100: Loss = -11515.553068428251
Iteration 200: Loss = -11457.97786468921
Iteration 300: Loss = -11407.671857096713
Iteration 400: Loss = -11344.6918430781
Iteration 500: Loss = -11325.105172862885
Iteration 600: Loss = -11316.741896147054
Iteration 700: Loss = -11313.898975280026
Iteration 800: Loss = -11313.359466733249
Iteration 900: Loss = -11313.08503660197
Iteration 1000: Loss = -11312.911031179981
Iteration 1100: Loss = -11312.78912314303
Iteration 1200: Loss = -11312.698495320594
Iteration 1300: Loss = -11312.62849707519
Iteration 1400: Loss = -11312.572786592485
Iteration 1500: Loss = -11312.527429005015
Iteration 1600: Loss = -11312.4899062025
Iteration 1700: Loss = -11312.458349369026
Iteration 1800: Loss = -11312.431509625814
Iteration 1900: Loss = -11312.408426472735
Iteration 2000: Loss = -11312.388480729407
Iteration 2100: Loss = -11312.371128190412
Iteration 2200: Loss = -11312.355947480508
Iteration 2300: Loss = -11312.34257922786
Iteration 2400: Loss = -11312.330692031976
Iteration 2500: Loss = -11312.320036503936
Iteration 2600: Loss = -11312.310584820487
Iteration 2700: Loss = -11312.302096841502
Iteration 2800: Loss = -11312.294384506069
Iteration 2900: Loss = -11312.287415352272
Iteration 3000: Loss = -11312.281036566797
Iteration 3100: Loss = -11312.275157512842
Iteration 3200: Loss = -11312.269765344212
Iteration 3300: Loss = -11312.264856713347
Iteration 3400: Loss = -11312.260351305167
Iteration 3500: Loss = -11312.256172813948
Iteration 3600: Loss = -11312.252309702342
Iteration 3700: Loss = -11312.248707760125
Iteration 3800: Loss = -11312.245328866156
Iteration 3900: Loss = -11312.242184286895
Iteration 4000: Loss = -11312.239271709357
Iteration 4100: Loss = -11312.236515549328
Iteration 4200: Loss = -11312.233951686425
Iteration 4300: Loss = -11312.23152598733
Iteration 4400: Loss = -11312.229183803336
Iteration 4500: Loss = -11312.227018321426
Iteration 4600: Loss = -11312.224967303446
Iteration 4700: Loss = -11312.223015433803
Iteration 4800: Loss = -11312.221197610941
Iteration 4900: Loss = -11312.219397131903
Iteration 5000: Loss = -11312.217773722707
Iteration 5100: Loss = -11312.216210798073
Iteration 5200: Loss = -11312.214690437646
Iteration 5300: Loss = -11312.21322846688
Iteration 5400: Loss = -11312.211850567064
Iteration 5500: Loss = -11312.21056119783
Iteration 5600: Loss = -11312.20927504318
Iteration 5700: Loss = -11312.210646147838
1
Iteration 5800: Loss = -11312.206896279551
Iteration 5900: Loss = -11312.205773257603
Iteration 6000: Loss = -11312.204658820106
Iteration 6100: Loss = -11312.203548227508
Iteration 6200: Loss = -11312.202446951795
Iteration 6300: Loss = -11312.201310994807
Iteration 6400: Loss = -11312.200306821906
Iteration 6500: Loss = -11312.199342069716
Iteration 6600: Loss = -11312.198557613874
Iteration 6700: Loss = -11312.19757425835
Iteration 6800: Loss = -11312.21236147044
1
Iteration 6900: Loss = -11312.19570727869
Iteration 7000: Loss = -11312.197769839826
1
Iteration 7100: Loss = -11312.193607054563
Iteration 7200: Loss = -11312.193489650157
Iteration 7300: Loss = -11312.191679845295
Iteration 7400: Loss = -11312.190114935427
Iteration 7500: Loss = -11312.188713597989
Iteration 7600: Loss = -11312.189251199561
1
Iteration 7700: Loss = -11312.187731764876
Iteration 7800: Loss = -11312.184669229477
Iteration 7900: Loss = -11312.183383027615
Iteration 8000: Loss = -11312.182882203178
Iteration 8100: Loss = -11312.183860818115
1
Iteration 8200: Loss = -11312.18043677621
Iteration 8300: Loss = -11312.17870424956
Iteration 8400: Loss = -11312.177554077718
Iteration 8500: Loss = -11312.176749563729
Iteration 8600: Loss = -11312.176007232492
Iteration 8700: Loss = -11312.175485787771
Iteration 8800: Loss = -11312.174504547487
Iteration 8900: Loss = -11312.175122519979
1
Iteration 9000: Loss = -11312.172975572983
Iteration 9100: Loss = -11312.172264063687
Iteration 9200: Loss = -11312.171229624935
Iteration 9300: Loss = -11312.1700263871
Iteration 9400: Loss = -11312.17228635319
1
Iteration 9500: Loss = -11312.165699316245
Iteration 9600: Loss = -11312.16101958485
Iteration 9700: Loss = -11312.23124523067
1
Iteration 9800: Loss = -11312.130331500355
Iteration 9900: Loss = -11312.106289993295
Iteration 10000: Loss = -11312.08733879046
Iteration 10100: Loss = -11312.073616748989
Iteration 10200: Loss = -11312.060403311016
Iteration 10300: Loss = -11312.050327478637
Iteration 10400: Loss = -11312.044824387247
Iteration 10500: Loss = -11312.049004078655
1
Iteration 10600: Loss = -11312.048729587978
2
Iteration 10700: Loss = -11312.124740642696
3
Iteration 10800: Loss = -11312.069245780503
4
Iteration 10900: Loss = -11312.148633741072
5
Iteration 11000: Loss = -11312.043790144306
Iteration 11100: Loss = -11312.043244411972
Iteration 11200: Loss = -11312.042751498595
Iteration 11300: Loss = -11312.04279402452
1
Iteration 11400: Loss = -11312.042695405376
Iteration 11500: Loss = -11312.043658356579
1
Iteration 11600: Loss = -11312.074362443125
2
Iteration 11700: Loss = -11312.044786280318
3
Iteration 11800: Loss = -11312.04235391831
Iteration 11900: Loss = -11312.134593351568
1
Iteration 12000: Loss = -11312.042371492187
2
Iteration 12100: Loss = -11312.0502044992
3
Iteration 12200: Loss = -11312.081138898393
4
Iteration 12300: Loss = -11312.042205620803
Iteration 12400: Loss = -11312.04293415655
1
Iteration 12500: Loss = -11312.042144632336
Iteration 12600: Loss = -11312.044729447374
1
Iteration 12700: Loss = -11312.042111608462
Iteration 12800: Loss = -11312.083647621936
1
Iteration 12900: Loss = -11312.041980649139
Iteration 13000: Loss = -11312.041938328317
Iteration 13100: Loss = -11312.042145548065
1
Iteration 13200: Loss = -11312.04190657233
Iteration 13300: Loss = -11312.045037011036
1
Iteration 13400: Loss = -11312.041884373568
Iteration 13500: Loss = -11312.041961721163
1
Iteration 13600: Loss = -11312.041974958382
2
Iteration 13700: Loss = -11312.041811675901
Iteration 13800: Loss = -11312.042052594968
1
Iteration 13900: Loss = -11312.047420127772
2
Iteration 14000: Loss = -11312.05047577924
3
Iteration 14100: Loss = -11312.041625343347
Iteration 14200: Loss = -11312.04764385109
1
Iteration 14300: Loss = -11312.054735643225
2
Iteration 14400: Loss = -11312.040411775119
Iteration 14500: Loss = -11311.975991810321
Iteration 14600: Loss = -11311.868409970279
Iteration 14700: Loss = -11311.838739144152
Iteration 14800: Loss = -11311.794496874383
Iteration 14900: Loss = -11311.788196831803
Iteration 15000: Loss = -11311.702863045844
Iteration 15100: Loss = -11311.69680519034
Iteration 15200: Loss = -11311.710751089362
1
Iteration 15300: Loss = -11311.657332942004
Iteration 15400: Loss = -11311.655338948454
Iteration 15500: Loss = -11311.654745419066
Iteration 15600: Loss = -11311.648994112138
Iteration 15700: Loss = -11311.63845427122
Iteration 15800: Loss = -11311.633679564475
Iteration 15900: Loss = -11311.633893883129
1
Iteration 16000: Loss = -11311.66360264212
2
Iteration 16100: Loss = -11311.635618726294
3
Iteration 16200: Loss = -11311.63161300296
Iteration 16300: Loss = -11311.697210741346
1
Iteration 16400: Loss = -11311.672134459091
2
Iteration 16500: Loss = -11311.753588383634
3
Iteration 16600: Loss = -11311.740070237385
4
Iteration 16700: Loss = -11311.636599356078
5
Iteration 16800: Loss = -11311.629799170136
Iteration 16900: Loss = -11311.629387934123
Iteration 17000: Loss = -11311.62858093281
Iteration 17100: Loss = -11311.64135914658
1
Iteration 17200: Loss = -11311.636479767092
2
Iteration 17300: Loss = -11311.628518224697
Iteration 17400: Loss = -11311.653478860238
1
Iteration 17500: Loss = -11311.62853423142
2
Iteration 17600: Loss = -11311.629246069691
3
Iteration 17700: Loss = -11311.628856994748
4
Iteration 17800: Loss = -11311.628586358564
5
Iteration 17900: Loss = -11311.630447419353
6
Iteration 18000: Loss = -11311.628356539457
Iteration 18100: Loss = -11311.630230282399
1
Iteration 18200: Loss = -11311.741100840718
2
Iteration 18300: Loss = -11311.790035377351
3
Iteration 18400: Loss = -11311.63686579306
4
Iteration 18500: Loss = -11311.691979606143
5
Iteration 18600: Loss = -11311.644324037161
6
Iteration 18700: Loss = -11311.629465341646
7
Iteration 18800: Loss = -11311.628330449377
Iteration 18900: Loss = -11311.628647616417
1
Iteration 19000: Loss = -11311.629896502984
2
Iteration 19100: Loss = -11311.628187576827
Iteration 19200: Loss = -11311.628304460532
1
Iteration 19300: Loss = -11311.628926462541
2
Iteration 19400: Loss = -11311.628172531653
Iteration 19500: Loss = -11311.628630939995
1
Iteration 19600: Loss = -11311.639203000244
2
Iteration 19700: Loss = -11311.63856271861
3
Iteration 19800: Loss = -11311.655051244496
4
Iteration 19900: Loss = -11311.629099717375
5
tensor([[-7.6577,  5.8948],
        [-7.5584,  6.1102],
        [-7.6316,  5.9417],
        [-7.6276,  5.9548],
        [-8.0334,  5.6290],
        [-7.4954,  6.0674],
        [-8.1579,  5.2092],
        [-8.0583,  5.5356],
        [-7.6466,  6.0257],
        [-7.5183,  6.1129],
        [-9.1363,  4.5211],
        [-7.5823,  6.0940],
        [-9.1377,  4.5225],
        [-8.9727,  4.6965],
        [-7.5364,  5.9168],
        [-7.5309,  6.0020],
        [-8.2186,  5.3782],
        [-7.7431,  5.8880],
        [-8.2448,  5.3618],
        [-7.6152,  6.0325],
        [-7.9156,  5.7333],
        [-7.5037,  6.1173],
        [-7.8977,  5.6475],
        [-8.0361,  5.5297],
        [-7.4708,  6.0843],
        [-7.5155,  6.1126],
        [-7.5423,  6.0036],
        [-7.5123,  6.0853],
        [-7.5334,  6.0861],
        [-7.7625,  5.8925],
        [-8.6075,  4.9463],
        [-8.4522,  5.1426],
        [-7.6079,  6.0229],
        [-7.7592,  5.8294],
        [-7.5241,  6.1194],
        [-7.6298,  5.9442],
        [-8.5224,  5.1273],
        [-7.6743,  5.9684],
        [-7.5121,  6.0983],
        [-7.5729,  6.0318],
        [-7.5491,  6.0427],
        [-7.5521,  6.1108],
        [-7.5142,  6.1197],
        [-7.5412,  6.1061],
        [-8.6753,  4.8406],
        [-7.4946,  6.1052],
        [-7.5200,  6.1316],
        [-7.4860,  6.0352],
        [-7.8817,  5.7752],
        [-7.5798,  6.0338],
        [-7.7491,  5.8703],
        [-7.5245,  6.0330],
        [-7.5376,  6.1120],
        [-8.0705,  5.5789],
        [-8.2356,  5.2018],
        [-7.5290,  6.0398],
        [-7.4876,  6.1013],
        [-8.1727,  5.4923],
        [-7.6104,  6.0441],
        [-8.2123,  5.4314],
        [-7.7070,  5.9787],
        [-7.6929,  5.8844],
        [-7.7194,  5.9912],
        [-7.6014,  5.9492],
        [-7.5184,  6.1283],
        [-7.6855,  5.9621],
        [-8.1721,  5.4898],
        [-7.7007,  5.7874],
        [-7.4755,  6.0761],
        [-7.5356,  6.0990],
        [-8.7215,  4.8981],
        [-7.6675,  5.9290],
        [-7.5006,  6.1140],
        [-7.8663,  5.7617],
        [-8.0543,  5.6133],
        [-7.6082,  6.0867],
        [-7.7588,  5.9365],
        [-7.9437,  5.6433],
        [-7.6912,  5.9851],
        [-7.5216,  6.1266],
        [-7.6536,  6.0187],
        [-7.4935,  6.1055],
        [-7.4815,  6.0942],
        [-7.5471,  5.8344],
        [-7.4554,  6.0688],
        [-7.5114,  5.9573],
        [-7.8946,  5.7768],
        [-7.5487,  6.1116],
        [-7.5887,  6.0790],
        [-7.6763,  5.7895],
        [-9.1389,  4.5237],
        [-7.5439,  6.0470],
        [-7.7389,  5.9180],
        [-7.5884,  5.8981],
        [-7.6132,  6.0514],
        [-7.6007,  5.9379],
        [-7.6244,  6.0467],
        [-8.9336,  4.6244],
        [-7.6361,  6.0037],
        [-7.7626,  5.7556]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.9152, 0.0848],
        [0.9770, 0.0230]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.2570e-06, 1.0000e+00], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1722, 0.1687],
         [0.7117, 0.1670]],

        [[0.2109, 0.2042],
         [0.3476, 0.2213]],

        [[0.5548, 0.1710],
         [0.6793, 0.0688]],

        [[0.4112, 0.1452],
         [0.3881, 0.7408]],

        [[0.7681, 0.1773],
         [0.7635, 0.7915]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 38
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 61
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.006376326519735513
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -43232.30905362228
Iteration 10: Loss = -11312.190163671297
Iteration 20: Loss = -11312.16402219182
Iteration 30: Loss = -11312.155867395642
Iteration 40: Loss = -11312.15299314685
Iteration 50: Loss = -11312.151762002051
Iteration 60: Loss = -11312.151263659298
Iteration 70: Loss = -11312.150989301603
Iteration 80: Loss = -11312.150811017944
Iteration 90: Loss = -11312.150740770023
Iteration 100: Loss = -11312.150688554588
Iteration 110: Loss = -11312.15059462689
Iteration 120: Loss = -11312.150527928607
Iteration 130: Loss = -11312.150526660444
Iteration 140: Loss = -11312.150482166244
Iteration 150: Loss = -11312.150405719523
Iteration 160: Loss = -11312.150381935884
Iteration 170: Loss = -11312.150319222621
Iteration 180: Loss = -11312.150243431113
Iteration 190: Loss = -11312.150263355466
1
Iteration 200: Loss = -11312.150152367332
Iteration 210: Loss = -11312.150144320252
Iteration 220: Loss = -11312.150078634004
Iteration 230: Loss = -11312.150049364865
Iteration 240: Loss = -11312.150042973979
Iteration 250: Loss = -11312.149982594181
Iteration 260: Loss = -11312.149904992752
Iteration 270: Loss = -11312.14987957318
Iteration 280: Loss = -11312.149848424882
Iteration 290: Loss = -11312.149814355893
Iteration 300: Loss = -11312.149734217184
Iteration 310: Loss = -11312.149730029156
Iteration 320: Loss = -11312.149668600814
Iteration 330: Loss = -11312.149642481681
Iteration 340: Loss = -11312.149572532186
Iteration 350: Loss = -11312.14951460939
Iteration 360: Loss = -11312.149512529972
Iteration 370: Loss = -11312.14946439921
Iteration 380: Loss = -11312.149403472324
Iteration 390: Loss = -11312.149371024945
Iteration 400: Loss = -11312.14932336534
Iteration 410: Loss = -11312.14925526986
Iteration 420: Loss = -11312.149227584227
Iteration 430: Loss = -11312.149203025627
Iteration 440: Loss = -11312.149133280644
Iteration 450: Loss = -11312.149099143471
Iteration 460: Loss = -11312.149057875822
Iteration 470: Loss = -11312.148999674393
Iteration 480: Loss = -11312.148966239798
Iteration 490: Loss = -11312.14890850462
Iteration 500: Loss = -11312.148886712672
Iteration 510: Loss = -11312.148851753425
Iteration 520: Loss = -11312.148773948515
Iteration 530: Loss = -11312.148715489879
Iteration 540: Loss = -11312.148653310223
Iteration 550: Loss = -11312.14866763861
1
Iteration 560: Loss = -11312.148604506114
Iteration 570: Loss = -11312.148533863872
Iteration 580: Loss = -11312.148496982105
Iteration 590: Loss = -11312.14851744486
1
Iteration 600: Loss = -11312.148438798822
Iteration 610: Loss = -11312.14838720173
Iteration 620: Loss = -11312.14831264074
Iteration 630: Loss = -11312.148269237987
Iteration 640: Loss = -11312.148228640768
Iteration 650: Loss = -11312.14814107729
Iteration 660: Loss = -11312.148180379449
1
Iteration 670: Loss = -11312.14810864135
Iteration 680: Loss = -11312.14807151237
Iteration 690: Loss = -11312.147993626497
Iteration 700: Loss = -11312.14797726414
Iteration 710: Loss = -11312.147904999369
Iteration 720: Loss = -11312.147893659963
Iteration 730: Loss = -11312.14784300975
Iteration 740: Loss = -11312.147762944867
Iteration 750: Loss = -11312.147736090883
Iteration 760: Loss = -11312.147674308386
Iteration 770: Loss = -11312.147670548471
Iteration 780: Loss = -11312.147581504474
Iteration 790: Loss = -11312.147525540664
Iteration 800: Loss = -11312.147475698972
Iteration 810: Loss = -11312.147468538718
Iteration 820: Loss = -11312.147424103057
Iteration 830: Loss = -11312.147335306521
Iteration 840: Loss = -11312.147297293053
Iteration 850: Loss = -11312.147278523673
Iteration 860: Loss = -11312.147235024353
Iteration 870: Loss = -11312.147187634107
Iteration 880: Loss = -11312.14712621309
Iteration 890: Loss = -11312.147039515448
Iteration 900: Loss = -11312.147011926
Iteration 910: Loss = -11312.146972243356
Iteration 920: Loss = -11312.14691626756
Iteration 930: Loss = -11312.146872615544
Iteration 940: Loss = -11312.146833636963
Iteration 950: Loss = -11312.14677091055
Iteration 960: Loss = -11312.146775195286
1
Iteration 970: Loss = -11312.146685736714
Iteration 980: Loss = -11312.146677399562
Iteration 990: Loss = -11312.14659329633
Iteration 1000: Loss = -11312.146550272033
Iteration 1010: Loss = -11312.146492545866
Iteration 1020: Loss = -11312.1464365015
Iteration 1030: Loss = -11312.146423734608
Iteration 1040: Loss = -11312.146349734081
Iteration 1050: Loss = -11312.146294322925
Iteration 1060: Loss = -11312.146278583397
Iteration 1070: Loss = -11312.146202917509
Iteration 1080: Loss = -11312.146161750334
Iteration 1090: Loss = -11312.146129733137
Iteration 1100: Loss = -11312.14604391385
Iteration 1110: Loss = -11312.146001221186
Iteration 1120: Loss = -11312.145994327166
Iteration 1130: Loss = -11312.145927603297
Iteration 1140: Loss = -11312.14590990321
Iteration 1150: Loss = -11312.145801561484
Iteration 1160: Loss = -11312.145761002146
Iteration 1170: Loss = -11312.14569473254
Iteration 1180: Loss = -11312.145661943563
Iteration 1190: Loss = -11312.145640230381
Iteration 1200: Loss = -11312.145560719899
Iteration 1210: Loss = -11312.145536782815
Iteration 1220: Loss = -11312.145450333743
Iteration 1230: Loss = -11312.145467120425
1
Iteration 1240: Loss = -11312.145380159864
Iteration 1250: Loss = -11312.14529562276
Iteration 1260: Loss = -11312.14524820054
Iteration 1270: Loss = -11312.145237838802
Iteration 1280: Loss = -11312.145199137103
Iteration 1290: Loss = -11312.145181796906
Iteration 1300: Loss = -11312.145054118144
Iteration 1310: Loss = -11312.145031285549
Iteration 1320: Loss = -11312.144990953591
Iteration 1330: Loss = -11312.144898863136
Iteration 1340: Loss = -11312.144877886245
Iteration 1350: Loss = -11312.144809540605
Iteration 1360: Loss = -11312.144736138842
Iteration 1370: Loss = -11312.144746825945
1
Iteration 1380: Loss = -11312.144656431357
Iteration 1390: Loss = -11312.144586619677
Iteration 1400: Loss = -11312.144575696519
Iteration 1410: Loss = -11312.144482092426
Iteration 1420: Loss = -11312.144447511242
Iteration 1430: Loss = -11312.144401951846
Iteration 1440: Loss = -11312.144381183452
Iteration 1450: Loss = -11312.144297619363
Iteration 1460: Loss = -11312.14426275338
Iteration 1470: Loss = -11312.144195326078
Iteration 1480: Loss = -11312.144169268238
Iteration 1490: Loss = -11312.144082045079
Iteration 1500: Loss = -11312.144031315926
Iteration 1510: Loss = -11312.144020694148
Iteration 1520: Loss = -11312.143928071666
Iteration 1530: Loss = -11312.143892062486
Iteration 1540: Loss = -11312.143816529027
Iteration 1550: Loss = -11312.143798591538
Iteration 1560: Loss = -11312.143700627994
Iteration 1570: Loss = -11312.143656659684
Iteration 1580: Loss = -11312.143613030235
Iteration 1590: Loss = -11312.14356262668
Iteration 1600: Loss = -11312.143522794699
Iteration 1610: Loss = -11312.143431349648
Iteration 1620: Loss = -11312.143400510005
Iteration 1630: Loss = -11312.1433124523
Iteration 1640: Loss = -11312.143303386081
Iteration 1650: Loss = -11312.143205764613
Iteration 1660: Loss = -11312.143171676418
Iteration 1670: Loss = -11312.14312668027
Iteration 1680: Loss = -11312.14306347711
Iteration 1690: Loss = -11312.143040020015
Iteration 1700: Loss = -11312.142947704257
Iteration 1710: Loss = -11312.14289434386
Iteration 1720: Loss = -11312.142847035318
Iteration 1730: Loss = -11312.142784056157
Iteration 1740: Loss = -11312.142723213583
Iteration 1750: Loss = -11312.142671778643
Iteration 1760: Loss = -11312.142578221126
Iteration 1770: Loss = -11312.14253241428
Iteration 1780: Loss = -11312.142510583853
Iteration 1790: Loss = -11312.142442266053
Iteration 1800: Loss = -11312.142368631907
Iteration 1810: Loss = -11312.142327384965
Iteration 1820: Loss = -11312.14225718755
Iteration 1830: Loss = -11312.142186526626
Iteration 1840: Loss = -11312.1421158853
Iteration 1850: Loss = -11312.14205453204
Iteration 1860: Loss = -11312.142000642376
Iteration 1870: Loss = -11312.141938886349
Iteration 1880: Loss = -11312.141889505303
Iteration 1890: Loss = -11312.141826741743
Iteration 1900: Loss = -11312.141763989212
Iteration 1910: Loss = -11312.141688712249
Iteration 1920: Loss = -11312.141644814483
Iteration 1930: Loss = -11312.141580129437
Iteration 1940: Loss = -11312.141530584695
Iteration 1950: Loss = -11312.141468648571
Iteration 1960: Loss = -11312.141352658507
Iteration 1970: Loss = -11312.14131452104
Iteration 1980: Loss = -11312.14122940741
Iteration 1990: Loss = -11312.141138166682
Iteration 2000: Loss = -11312.141112844663
Iteration 2010: Loss = -11312.14107086648
Iteration 2020: Loss = -11312.140965498727
Iteration 2030: Loss = -11312.14092361834
Iteration 2040: Loss = -11312.140842103083
Iteration 2050: Loss = -11312.140782209632
Iteration 2060: Loss = -11312.14067993846
Iteration 2070: Loss = -11312.140631264476
Iteration 2080: Loss = -11312.1405928712
Iteration 2090: Loss = -11312.14044526227
Iteration 2100: Loss = -11312.140433984709
Iteration 2110: Loss = -11312.140366119322
Iteration 2120: Loss = -11312.140276871052
Iteration 2130: Loss = -11312.140203745636
Iteration 2140: Loss = -11312.140129447524
Iteration 2150: Loss = -11312.140025557297
Iteration 2160: Loss = -11312.139962790188
Iteration 2170: Loss = -11312.139930025927
Iteration 2180: Loss = -11312.139838445686
Iteration 2190: Loss = -11312.139754262824
Iteration 2200: Loss = -11312.139656191459
Iteration 2210: Loss = -11312.139579400928
Iteration 2220: Loss = -11312.139508472213
Iteration 2230: Loss = -11312.139402929079
Iteration 2240: Loss = -11312.139349375044
Iteration 2250: Loss = -11312.139258350822
Iteration 2260: Loss = -11312.139187635592
Iteration 2270: Loss = -11312.13912112019
Iteration 2280: Loss = -11312.139010899653
Iteration 2290: Loss = -11312.138932080883
Iteration 2300: Loss = -11312.138845976871
Iteration 2310: Loss = -11312.1387548896
Iteration 2320: Loss = -11312.138668512682
Iteration 2330: Loss = -11312.138560854164
Iteration 2340: Loss = -11312.138501110867
Iteration 2350: Loss = -11312.138396172564
Iteration 2360: Loss = -11312.138292046557
Iteration 2370: Loss = -11312.138225144023
Iteration 2380: Loss = -11312.13809382478
Iteration 2390: Loss = -11312.138035541537
Iteration 2400: Loss = -11312.137934488132
Iteration 2410: Loss = -11312.137851519592
Iteration 2420: Loss = -11312.13773023465
Iteration 2430: Loss = -11312.137637133654
Iteration 2440: Loss = -11312.137508791238
Iteration 2450: Loss = -11312.137422773307
Iteration 2460: Loss = -11312.137317867799
Iteration 2470: Loss = -11312.137207403179
Iteration 2480: Loss = -11312.137106449023
Iteration 2490: Loss = -11312.137010956309
Iteration 2500: Loss = -11312.13687146162
Iteration 2510: Loss = -11312.136775093193
Iteration 2520: Loss = -11312.13666713909
Iteration 2530: Loss = -11312.136582007057
Iteration 2540: Loss = -11312.136423317246
Iteration 2550: Loss = -11312.13631941739
Iteration 2560: Loss = -11312.136213832955
Iteration 2570: Loss = -11312.136081883551
Iteration 2580: Loss = -11312.135979979566
Iteration 2590: Loss = -11312.135826117858
Iteration 2600: Loss = -11312.135744699981
Iteration 2610: Loss = -11312.135591526612
Iteration 2620: Loss = -11312.13545640949
Iteration 2630: Loss = -11312.135302467257
Iteration 2640: Loss = -11312.135189147304
Iteration 2650: Loss = -11312.135041898506
Iteration 2660: Loss = -11312.134941908324
Iteration 2670: Loss = -11312.134772773623
Iteration 2680: Loss = -11312.134653314637
Iteration 2690: Loss = -11312.134508764313
Iteration 2700: Loss = -11312.134346263001
Iteration 2710: Loss = -11312.134206413377
Iteration 2720: Loss = -11312.13403338368
Iteration 2730: Loss = -11312.133894763092
Iteration 2740: Loss = -11312.133730321613
Iteration 2750: Loss = -11312.133551568382
Iteration 2760: Loss = -11312.133437715729
Iteration 2770: Loss = -11312.133235888043
Iteration 2780: Loss = -11312.133070337284
Iteration 2790: Loss = -11312.132912918853
Iteration 2800: Loss = -11312.132730898456
Iteration 2810: Loss = -11312.132547543733
Iteration 2820: Loss = -11312.132396782035
Iteration 2830: Loss = -11312.132209238882
Iteration 2840: Loss = -11312.131977039875
Iteration 2850: Loss = -11312.131832634133
Iteration 2860: Loss = -11312.13161127951
Iteration 2870: Loss = -11312.131413748866
Iteration 2880: Loss = -11312.131215193773
Iteration 2890: Loss = -11312.13098453501
Iteration 2900: Loss = -11312.130804514152
Iteration 2910: Loss = -11312.13058796405
Iteration 2920: Loss = -11312.130373206177
Iteration 2930: Loss = -11312.130154043762
Iteration 2940: Loss = -11312.129940964967
Iteration 2950: Loss = -11312.12967761982
Iteration 2960: Loss = -11312.129488865468
Iteration 2970: Loss = -11312.129224660139
Iteration 2980: Loss = -11312.128987954018
Iteration 2990: Loss = -11312.128695502666
Iteration 3000: Loss = -11312.128461251252
Iteration 3010: Loss = -11312.128216800544
Iteration 3020: Loss = -11312.127929929178
Iteration 3030: Loss = -11312.127634225082
Iteration 3040: Loss = -11312.127348481872
Iteration 3050: Loss = -11312.12710087981
Iteration 3060: Loss = -11312.12681025048
Iteration 3070: Loss = -11312.126502403358
Iteration 3080: Loss = -11312.126203473947
Iteration 3090: Loss = -11312.125905138264
Iteration 3100: Loss = -11312.125563158992
Iteration 3110: Loss = -11312.125223331335
Iteration 3120: Loss = -11312.12487888516
Iteration 3130: Loss = -11312.124537556545
Iteration 3140: Loss = -11312.124228581422
Iteration 3150: Loss = -11312.123841945338
Iteration 3160: Loss = -11312.123476796416
Iteration 3170: Loss = -11312.123111060517
Iteration 3180: Loss = -11312.122718571864
Iteration 3190: Loss = -11312.122328009564
Iteration 3200: Loss = -11312.121914723124
Iteration 3210: Loss = -11312.121514934075
Iteration 3220: Loss = -11312.121129796842
Iteration 3230: Loss = -11312.120674715876
Iteration 3240: Loss = -11312.120240739085
Iteration 3250: Loss = -11312.119767244778
Iteration 3260: Loss = -11312.119331429696
Iteration 3270: Loss = -11312.118853113438
Iteration 3280: Loss = -11312.118341774183
Iteration 3290: Loss = -11312.11785853244
Iteration 3300: Loss = -11312.117329416229
Iteration 3310: Loss = -11312.116791094488
Iteration 3320: Loss = -11312.116310330633
Iteration 3330: Loss = -11312.115732335813
Iteration 3340: Loss = -11312.115111365756
Iteration 3350: Loss = -11312.114617316822
Iteration 3360: Loss = -11312.114001884765
Iteration 3370: Loss = -11312.113412061153
Iteration 3380: Loss = -11312.112770739532
Iteration 3390: Loss = -11312.112123511257
Iteration 3400: Loss = -11312.111454561284
Iteration 3410: Loss = -11312.11078417199
Iteration 3420: Loss = -11312.11012169601
Iteration 3430: Loss = -11312.109378018817
Iteration 3440: Loss = -11312.108667042623
Iteration 3450: Loss = -11312.107959968944
Iteration 3460: Loss = -11312.107194276676
Iteration 3470: Loss = -11312.106386531432
Iteration 3480: Loss = -11312.105637174245
Iteration 3490: Loss = -11312.104839523337
Iteration 3500: Loss = -11312.103996396077
Iteration 3510: Loss = -11312.103199833391
Iteration 3520: Loss = -11312.102354887566
Iteration 3530: Loss = -11312.101467538345
Iteration 3540: Loss = -11312.100619053681
Iteration 3550: Loss = -11312.099749199111
Iteration 3560: Loss = -11312.098820329855
Iteration 3570: Loss = -11312.09796421968
Iteration 3580: Loss = -11312.097064974236
Iteration 3590: Loss = -11312.096138417473
Iteration 3600: Loss = -11312.095197442064
Iteration 3610: Loss = -11312.09430048716
Iteration 3620: Loss = -11312.093387729992
Iteration 3630: Loss = -11312.092460670245
Iteration 3640: Loss = -11312.091516775467
Iteration 3650: Loss = -11312.090652379053
Iteration 3660: Loss = -11312.089728766845
Iteration 3670: Loss = -11312.088825690258
Iteration 3680: Loss = -11312.087976761934
Iteration 3690: Loss = -11312.087113073512
Iteration 3700: Loss = -11312.086252989659
Iteration 3710: Loss = -11312.08543461237
Iteration 3720: Loss = -11312.084660356983
Iteration 3730: Loss = -11312.083911563392
Iteration 3740: Loss = -11312.083145426575
Iteration 3750: Loss = -11312.082427146615
Iteration 3760: Loss = -11312.081778563357
Iteration 3770: Loss = -11312.081121735268
Iteration 3780: Loss = -11312.080493131381
Iteration 3790: Loss = -11312.079921502522
Iteration 3800: Loss = -11312.079384831055
Iteration 3810: Loss = -11312.078835560957
Iteration 3820: Loss = -11312.078415025804
Iteration 3830: Loss = -11312.077928197132
Iteration 3840: Loss = -11312.07750626351
Iteration 3850: Loss = -11312.077142319651
Iteration 3860: Loss = -11312.076795891046
Iteration 3870: Loss = -11312.076455582279
Iteration 3880: Loss = -11312.076159506354
Iteration 3890: Loss = -11312.07588549753
Iteration 3900: Loss = -11312.075618546181
Iteration 3910: Loss = -11312.075372282054
Iteration 3920: Loss = -11312.075183770903
Iteration 3930: Loss = -11312.075005923705
Iteration 3940: Loss = -11312.074809699572
Iteration 3950: Loss = -11312.074640144094
Iteration 3960: Loss = -11312.074495934443
Iteration 3970: Loss = -11312.074406218648
Iteration 3980: Loss = -11312.074280083492
Iteration 3990: Loss = -11312.074155171576
Iteration 4000: Loss = -11312.074073273323
Iteration 4010: Loss = -11312.073997356549
Iteration 4020: Loss = -11312.073864792683
Iteration 4030: Loss = -11312.07382834517
Iteration 4040: Loss = -11312.073762167036
Iteration 4050: Loss = -11312.073685621943
Iteration 4060: Loss = -11312.0736257428
Iteration 4070: Loss = -11312.073571231102
Iteration 4080: Loss = -11312.073536232383
Iteration 4090: Loss = -11312.073489839151
Iteration 4100: Loss = -11312.073426020263
Iteration 4110: Loss = -11312.073424534577
Iteration 4120: Loss = -11312.073390001946
Iteration 4130: Loss = -11312.073378811714
Iteration 4140: Loss = -11312.073333156626
Iteration 4150: Loss = -11312.0733184091
Iteration 4160: Loss = -11312.073320210273
1
Iteration 4170: Loss = -11312.073257397837
Iteration 4180: Loss = -11312.073242833178
Iteration 4190: Loss = -11312.073255735157
1
Iteration 4200: Loss = -11312.073230254218
Iteration 4210: Loss = -11312.073218561134
Iteration 4220: Loss = -11312.07320287566
Iteration 4230: Loss = -11312.073202144249
Iteration 4240: Loss = -11312.073186216216
Iteration 4250: Loss = -11312.07317424395
Iteration 4260: Loss = -11312.073179579187
1
Iteration 4270: Loss = -11312.073168385165
Iteration 4280: Loss = -11312.073157314566
Iteration 4290: Loss = -11312.0731709542
1
Iteration 4300: Loss = -11312.07313059069
Iteration 4310: Loss = -11312.073175848145
1
Iteration 4320: Loss = -11312.073136107325
2
Iteration 4330: Loss = -11312.073135116962
3
Stopping early at iteration 4329 due to no improvement.
pi: tensor([[0.9424, 0.0576],
        [0.9799, 0.0201]], dtype=torch.float64)
alpha: tensor([0.9445, 0.0555])
beta: tensor([[[0.1691, 0.1522],
         [0.2094, 0.1701]],

        [[0.8459, 0.1922],
         [0.5453, 0.0849]],

        [[0.1038, 0.1727],
         [0.0288, 0.6586]],

        [[0.7747, 0.1428],
         [0.2299, 0.7101]],

        [[0.1597, 0.1822],
         [0.8248, 0.2817]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 62
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 61
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -43232.201501308075
Iteration 100: Loss = -11460.535885154644
Iteration 200: Loss = -11411.41678842234
Iteration 300: Loss = -11381.83442556387
Iteration 400: Loss = -11364.264626402253
Iteration 500: Loss = -11351.07349694839
Iteration 600: Loss = -11332.158964848768
Iteration 700: Loss = -11326.631897589747
Iteration 800: Loss = -11324.21095357379
Iteration 900: Loss = -11323.799642060583
Iteration 1000: Loss = -11323.54408101115
Iteration 1100: Loss = -11323.383924159174
Iteration 1200: Loss = -11323.272340730218
Iteration 1300: Loss = -11323.169453446922
Iteration 1400: Loss = -11323.100787369052
Iteration 1500: Loss = -11323.048631646408
Iteration 1600: Loss = -11323.005713445325
Iteration 1700: Loss = -11322.969855649764
Iteration 1800: Loss = -11322.939567391712
Iteration 1900: Loss = -11322.913680273545
Iteration 2000: Loss = -11322.891324537615
Iteration 2100: Loss = -11322.871873715167
Iteration 2200: Loss = -11322.854792954051
Iteration 2300: Loss = -11322.83973709938
Iteration 2400: Loss = -11322.826265303007
Iteration 2500: Loss = -11322.81425515738
Iteration 2600: Loss = -11322.803446194293
Iteration 2700: Loss = -11322.793720314537
Iteration 2800: Loss = -11322.784940901905
Iteration 2900: Loss = -11322.777021253965
Iteration 3000: Loss = -11322.769833506518
Iteration 3100: Loss = -11322.763318059777
Iteration 3200: Loss = -11322.757391089539
Iteration 3300: Loss = -11322.751868941778
Iteration 3400: Loss = -11322.746910154814
Iteration 3500: Loss = -11322.742172086966
Iteration 3600: Loss = -11322.737914437857
Iteration 3700: Loss = -11322.73388453739
Iteration 3800: Loss = -11322.730205249483
Iteration 3900: Loss = -11322.726545169386
Iteration 4000: Loss = -11322.722747663793
Iteration 4100: Loss = -11322.711580339617
Iteration 4200: Loss = -11322.708770308824
Iteration 4300: Loss = -11322.70782457337
Iteration 4400: Loss = -11322.703772936931
Iteration 4500: Loss = -11322.701519648754
Iteration 4600: Loss = -11322.699447716903
Iteration 4700: Loss = -11322.697469077384
Iteration 4800: Loss = -11322.696367408193
Iteration 4900: Loss = -11322.69378256157
Iteration 5000: Loss = -11322.692165710856
Iteration 5100: Loss = -11322.690523045709
Iteration 5200: Loss = -11322.694182031253
1
Iteration 5300: Loss = -11322.687801218248
Iteration 5400: Loss = -11322.68641012758
Iteration 5500: Loss = -11322.686345197955
Iteration 5600: Loss = -11322.69138450039
1
Iteration 5700: Loss = -11322.682770826525
Iteration 5800: Loss = -11322.68143786943
Iteration 5900: Loss = -11322.684527762953
1
Iteration 6000: Loss = -11322.680096315886
Iteration 6100: Loss = -11322.682638893943
1
Iteration 6200: Loss = -11322.677907637322
Iteration 6300: Loss = -11322.67697793164
Iteration 6400: Loss = -11322.676105988978
Iteration 6500: Loss = -11322.675549472458
Iteration 6600: Loss = -11322.674243490395
Iteration 6700: Loss = -11322.628910869385
Iteration 6800: Loss = -11322.612416516866
Iteration 6900: Loss = -11322.609152761957
Iteration 7000: Loss = -11322.6078722801
Iteration 7100: Loss = -11322.606646425125
Iteration 7200: Loss = -11322.606345586752
Iteration 7300: Loss = -11322.606278563271
Iteration 7400: Loss = -11322.606468917578
1
Iteration 7500: Loss = -11322.60677148499
2
Iteration 7600: Loss = -11322.607757661248
3
Iteration 7700: Loss = -11322.604388016922
Iteration 7800: Loss = -11322.604419358418
1
Iteration 7900: Loss = -11322.604147009537
Iteration 8000: Loss = -11322.614144409701
1
Iteration 8100: Loss = -11322.602897441348
Iteration 8200: Loss = -11322.602854417783
Iteration 8300: Loss = -11322.607378989127
1
Iteration 8400: Loss = -11322.603219561219
2
Iteration 8500: Loss = -11322.60667018217
3
Iteration 8600: Loss = -11322.601923944429
Iteration 8700: Loss = -11322.60275946654
1
Iteration 8800: Loss = -11322.601011558167
Iteration 8900: Loss = -11322.615069450763
1
Iteration 9000: Loss = -11322.600733556365
Iteration 9100: Loss = -11322.60131395078
1
Iteration 9200: Loss = -11322.602818801417
2
Iteration 9300: Loss = -11322.595391529694
Iteration 9400: Loss = -11322.452178847563
Iteration 9500: Loss = -11322.451681578921
Iteration 9600: Loss = -11322.451398898022
Iteration 9700: Loss = -11322.451309699634
Iteration 9800: Loss = -11322.451804262668
1
Iteration 9900: Loss = -11322.451025147515
Iteration 10000: Loss = -11322.452046843944
1
Iteration 10100: Loss = -11322.45077820125
Iteration 10200: Loss = -11322.451928707018
1
Iteration 10300: Loss = -11322.45058647544
Iteration 10400: Loss = -11322.45279980815
1
Iteration 10500: Loss = -11322.450415492876
Iteration 10600: Loss = -11322.670885453757
1
Iteration 10700: Loss = -11322.45025308089
Iteration 10800: Loss = -11322.450217587237
Iteration 10900: Loss = -11322.450173436666
Iteration 11000: Loss = -11322.544430006883
1
Iteration 11100: Loss = -11322.44997577233
Iteration 11200: Loss = -11322.696996154576
1
Iteration 11300: Loss = -11322.449839861658
Iteration 11400: Loss = -11322.44979589069
Iteration 11500: Loss = -11322.45005513273
1
Iteration 11600: Loss = -11322.449705797751
Iteration 11700: Loss = -11322.49843019954
1
Iteration 11800: Loss = -11322.449629311453
Iteration 11900: Loss = -11322.449532968949
Iteration 12000: Loss = -11322.435478742133
Iteration 12100: Loss = -11322.427552934252
Iteration 12200: Loss = -11322.427499248652
Iteration 12300: Loss = -11322.428798984354
1
Iteration 12400: Loss = -11322.42738281107
Iteration 12500: Loss = -11322.427366000236
Iteration 12600: Loss = -11322.427377065122
1
Iteration 12700: Loss = -11322.427310064513
Iteration 12800: Loss = -11322.44026388476
1
Iteration 12900: Loss = -11322.427225981368
Iteration 13000: Loss = -11322.429286680072
1
Iteration 13100: Loss = -11322.427242914311
2
Iteration 13200: Loss = -11322.427165045801
Iteration 13300: Loss = -11322.43060788337
1
Iteration 13400: Loss = -11322.427152635117
Iteration 13500: Loss = -11322.4271181333
Iteration 13600: Loss = -11322.427154844678
1
Iteration 13700: Loss = -11322.427099573912
Iteration 13800: Loss = -11322.512477939508
1
Iteration 13900: Loss = -11322.427033021675
Iteration 14000: Loss = -11322.42702076834
Iteration 14100: Loss = -11322.427292839608
1
Iteration 14200: Loss = -11322.426984106112
Iteration 14300: Loss = -11322.569216015929
1
Iteration 14400: Loss = -11322.426976685027
Iteration 14500: Loss = -11322.481357356237
1
Iteration 14600: Loss = -11322.426922096927
Iteration 14700: Loss = -11322.454726369724
1
Iteration 14800: Loss = -11322.426920080285
Iteration 14900: Loss = -11322.435171056995
1
Iteration 15000: Loss = -11322.426965974835
2
Iteration 15100: Loss = -11322.439967299806
3
Iteration 15200: Loss = -11322.42689428151
Iteration 15300: Loss = -11322.428123742637
1
Iteration 15400: Loss = -11322.426904749347
2
Iteration 15500: Loss = -11322.42712824379
3
Iteration 15600: Loss = -11322.426898692009
4
Iteration 15700: Loss = -11322.427021156587
5
Iteration 15800: Loss = -11322.535686880328
6
Iteration 15900: Loss = -11322.426839959671
Iteration 16000: Loss = -11322.464604359839
1
Iteration 16100: Loss = -11322.426849135942
2
Iteration 16200: Loss = -11322.441205231653
3
Iteration 16300: Loss = -11322.426845457421
4
Iteration 16400: Loss = -11322.483646744378
5
Iteration 16500: Loss = -11322.426827027079
Iteration 16600: Loss = -11322.42685631102
1
Iteration 16700: Loss = -11321.152751645082
Iteration 16800: Loss = -11321.131866185968
Iteration 16900: Loss = -11321.11778017058
Iteration 17000: Loss = -11321.139912777271
1
Iteration 17100: Loss = -11321.115470395638
Iteration 17200: Loss = -11321.129483880388
1
Iteration 17300: Loss = -11320.518601149966
Iteration 17400: Loss = -11320.230907715473
Iteration 17500: Loss = -11319.727760991607
Iteration 17600: Loss = -11319.727704839772
Iteration 17700: Loss = -11319.662206756437
Iteration 17800: Loss = -11319.649897527968
Iteration 17900: Loss = -11319.65079937004
1
Iteration 18000: Loss = -11318.815169572852
Iteration 18100: Loss = -11317.342526668232
Iteration 18200: Loss = -11316.173656949279
Iteration 18300: Loss = -11314.247692304585
Iteration 18400: Loss = -11312.935997747214
Iteration 18500: Loss = -11310.08104663715
Iteration 18600: Loss = -11307.015979327281
Iteration 18700: Loss = -11300.017640759594
Iteration 18800: Loss = -11297.854665878183
Iteration 18900: Loss = -11297.610971430053
Iteration 19000: Loss = -11297.573517162451
Iteration 19100: Loss = -11297.572602844833
Iteration 19200: Loss = -11297.56847080556
Iteration 19300: Loss = -11297.517343446552
Iteration 19400: Loss = -11297.497961163539
Iteration 19500: Loss = -11297.464192555537
Iteration 19600: Loss = -11297.414620964313
Iteration 19700: Loss = -11297.413188306937
Iteration 19800: Loss = -11297.413990230349
1
Iteration 19900: Loss = -11297.418674405433
2
tensor([[-2.2279e+00,  8.0980e-01],
        [-3.1738e+00,  7.7563e-01],
        [-4.3218e+00,  2.9077e+00],
        [ 8.9254e+00, -1.0563e+01],
        [ 3.5114e+00, -5.0580e+00],
        [ 7.5356e+00, -1.0285e+01],
        [ 4.3034e+00, -5.6927e+00],
        [ 9.1649e+00, -1.1093e+01],
        [-2.7878e+00,  1.3167e+00],
        [ 2.5362e+00, -4.0913e+00],
        [-9.6880e-02, -1.4624e+00],
        [ 2.1011e+00, -5.1366e+00],
        [-2.5499e+00, -1.5666e-03],
        [ 1.3448e+00, -2.7925e+00],
        [ 1.8493e-01, -1.7764e+00],
        [-2.2830e-01, -3.0982e+00],
        [ 9.5223e-01, -2.3461e+00],
        [ 7.5435e-01, -2.2516e+00],
        [-5.0662e+00,  3.6799e+00],
        [-3.7504e+00,  4.8346e-01],
        [ 2.4379e+00, -5.0081e+00],
        [-1.8667e+00,  3.9858e-01],
        [ 4.2453e-01, -1.8156e+00],
        [-2.2638e+00,  8.0921e-01],
        [-5.8637e+00,  4.4666e+00],
        [ 3.9108e+00, -5.6023e+00],
        [ 4.9880e+00, -7.1775e+00],
        [-3.3572e+00,  1.7562e+00],
        [ 2.4341e+00, -5.2633e+00],
        [-4.4810e+00,  2.5686e+00],
        [ 9.2632e+00, -1.1023e+01],
        [ 1.3186e+00, -2.7276e+00],
        [ 3.5065e+00, -4.9375e+00],
        [ 5.4067e+00, -6.8144e+00],
        [-8.1935e-01, -2.8359e+00],
        [-4.4022e+00,  3.0157e+00],
        [-2.1674e+00,  7.7759e-01],
        [ 1.3195e+00, -4.3606e+00],
        [ 3.5086e+00, -5.3914e+00],
        [ 1.3392e+00, -3.3243e+00],
        [ 9.1795e-01, -2.3221e+00],
        [ 8.7198e+00, -1.0870e+01],
        [-2.3348e+00,  9.1452e-01],
        [-3.1633e+00,  1.2509e+00],
        [-4.6877e+00,  2.4555e+00],
        [-4.6215e+00,  3.2343e+00],
        [-1.3399e+00, -4.4309e-01],
        [-3.4244e+00,  1.6001e+00],
        [-2.6043e+00,  1.1688e+00],
        [ 1.3341e+00, -3.1873e+00],
        [ 3.5448e+00, -5.1214e+00],
        [-4.2390e+00,  2.8514e+00],
        [ 6.9309e-01, -2.4725e+00],
        [ 2.7937e+00, -4.1800e+00],
        [ 3.5761e+00, -5.0244e+00],
        [-1.8414e-01, -1.9086e+00],
        [-3.4856e+00,  1.3427e+00],
        [ 3.9745e+00, -6.8004e+00],
        [ 4.8143e+00, -6.3440e+00],
        [-2.2845e+00,  2.2590e-01],
        [ 2.5886e+00, -3.9780e+00],
        [ 6.2597e-02, -1.9193e+00],
        [-1.3826e-01, -3.9011e+00],
        [ 3.5615e+00, -4.9879e+00],
        [-2.9867e+00,  1.5951e+00],
        [ 9.0345e+00, -1.0562e+01],
        [ 9.7379e-01, -4.5633e+00],
        [ 1.9308e+00, -3.3397e+00],
        [-2.2520e+00,  7.9715e-01],
        [-1.8771e-01, -1.5381e+00],
        [ 1.4322e+00, -3.1083e+00],
        [-3.5756e+00,  6.6263e-01],
        [-2.0293e+00,  6.2743e-01],
        [ 1.3208e+00, -2.7837e+00],
        [ 3.2741e+00, -4.6967e+00],
        [-2.5945e+00,  1.1927e+00],
        [-1.0312e+00, -8.6742e-01],
        [ 2.4443e+00, -3.8312e+00],
        [-4.2098e+00,  2.8179e+00],
        [-3.6531e+00, -9.2646e-01],
        [-2.7482e+00, -1.4546e+00],
        [-9.7049e-02, -1.2972e+00],
        [-3.6064e+00,  1.5901e+00],
        [-3.5212e+00,  2.0040e+00],
        [-2.9676e+00,  8.9174e-01],
        [-5.4723e+00,  2.8043e+00],
        [ 2.4990e+00, -4.1092e+00],
        [-9.5605e-01, -8.6715e-01],
        [ 9.8552e-01, -3.4949e+00],
        [ 2.8279e+00, -5.1584e+00],
        [ 3.6790e-02, -1.4431e+00],
        [ 3.6188e+00, -5.0056e+00],
        [-2.1398e+00,  7.1994e-01],
        [-8.8822e-01, -4.9828e-01],
        [-1.2917e+00, -4.9571e-01],
        [-2.2819e+00,  8.6682e-01],
        [-3.8405e+00,  1.9756e+00],
        [-3.1924e+00,  4.6085e-01],
        [-4.4638e+00,  2.7846e+00],
        [ 3.6956e+00, -6.8175e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[6.6960e-01, 3.3040e-01],
        [1.0000e+00, 6.0839e-08]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5602, 0.4398], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1929, 0.1100],
         [0.2094, 0.2703]],

        [[0.8459, 0.1500],
         [0.5453, 0.0849]],

        [[0.1038, 0.1069],
         [0.0288, 0.6586]],

        [[0.7747, 0.1178],
         [0.2299, 0.7101]],

        [[0.1597, 0.1129],
         [0.8248, 0.2817]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 19
Adjusted Rand Index: 0.3781818181818182
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.019775482996808744
time is 2
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 93
Adjusted Rand Index: 0.7366601698966646
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.03211046845853326
time is 4
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 25
Adjusted Rand Index: 0.24196687024629002
Global Adjusted Rand Index: 9.984613345996704e-05
Average Adjusted Rand Index: 0.27382876875729945
Iteration 0: Loss = -21137.80611952913
Iteration 10: Loss = -11312.151966282343
Iteration 20: Loss = -11312.151933618086
Iteration 30: Loss = -11312.151862265633
Iteration 40: Loss = -11312.151847082057
Iteration 50: Loss = -11312.151806466612
Iteration 60: Loss = -11312.151783469668
Iteration 70: Loss = -11312.151764172399
Iteration 80: Loss = -11312.151749196279
Iteration 90: Loss = -11312.15172954832
Iteration 100: Loss = -11312.151738467004
1
Iteration 110: Loss = -11312.151768343994
2
Iteration 120: Loss = -11312.15170674535
Iteration 130: Loss = -11312.151750773075
1
Iteration 140: Loss = -11312.151756533898
2
Iteration 150: Loss = -11312.151737714441
3
Stopping early at iteration 149 due to no improvement.
pi: tensor([[0.4899, 0.5101],
        [0.5396, 0.4604]], dtype=torch.float64)
alpha: tensor([0.5141, 0.4859])
beta: tensor([[[0.1692, 0.1654],
         [0.1479, 0.1689]],

        [[0.2216, 0.1730],
         [0.2957, 0.3057]],

        [[0.9957, 0.1704],
         [0.7733, 0.7111]],

        [[0.4966, 0.1642],
         [0.8914, 0.4735]],

        [[0.9844, 0.1722],
         [0.0674, 0.7725]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 62
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 61
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21137.58965305037
Iteration 100: Loss = -11316.224208913722
Iteration 200: Loss = -11269.133643305893
Iteration 300: Loss = -11235.136896498656
Iteration 400: Loss = -11234.040306852923
Iteration 500: Loss = -11233.809705084723
Iteration 600: Loss = -11233.667906096605
Iteration 700: Loss = -11233.34856781424
Iteration 800: Loss = -11228.219612843219
Iteration 900: Loss = -11227.084539514557
Iteration 1000: Loss = -11226.79523331896
Iteration 1100: Loss = -11226.414004743414
Iteration 1200: Loss = -11222.139261971717
Iteration 1300: Loss = -11209.178354146708
Iteration 1400: Loss = -11203.685907085039
Iteration 1500: Loss = -11201.700261737587
Iteration 1600: Loss = -11189.188904133009
Iteration 1700: Loss = -11189.14940527398
Iteration 1800: Loss = -11189.136427689431
Iteration 1900: Loss = -11189.108629622666
Iteration 2000: Loss = -11173.812253508313
Iteration 2100: Loss = -11173.793634899856
Iteration 2200: Loss = -11173.791589251356
Iteration 2300: Loss = -11173.78726153225
Iteration 2400: Loss = -11173.785753843056
Iteration 2500: Loss = -11173.776586359609
Iteration 2600: Loss = -11171.602789534081
Iteration 2700: Loss = -11171.56602135963
Iteration 2800: Loss = -11171.493103335904
Iteration 2900: Loss = -11171.491174199347
Iteration 3000: Loss = -11171.490273191579
Iteration 3100: Loss = -11171.499068000925
1
Iteration 3200: Loss = -11171.495647738653
2
Iteration 3300: Loss = -11171.495321376755
3
Iteration 3400: Loss = -11171.486104215066
Iteration 3500: Loss = -11171.470546749097
Iteration 3600: Loss = -11171.46978336572
Iteration 3700: Loss = -11171.464617466472
Iteration 3800: Loss = -11167.68484278275
Iteration 3900: Loss = -11167.54311631596
Iteration 4000: Loss = -11167.542483617453
Iteration 4100: Loss = -11167.542029862709
Iteration 4200: Loss = -11167.541880797458
Iteration 4300: Loss = -11167.54181546734
Iteration 4400: Loss = -11167.55718536048
1
Iteration 4500: Loss = -11167.541515083843
Iteration 4600: Loss = -11167.541631029013
1
Iteration 4700: Loss = -11167.541244447142
Iteration 4800: Loss = -11167.541151385361
Iteration 4900: Loss = -11167.540925320465
Iteration 5000: Loss = -11167.538984703124
Iteration 5100: Loss = -11167.537925157154
Iteration 5200: Loss = -11167.538093312653
1
Iteration 5300: Loss = -11167.53765773939
Iteration 5400: Loss = -11167.537631609255
Iteration 5500: Loss = -11167.537461824668
Iteration 5600: Loss = -11167.541060143587
1
Iteration 5700: Loss = -11167.536874358666
Iteration 5800: Loss = -11167.536156201302
Iteration 5900: Loss = -11167.536125480636
Iteration 6000: Loss = -11167.536078303865
Iteration 6100: Loss = -11167.540691226961
1
Iteration 6200: Loss = -11167.536001696591
Iteration 6300: Loss = -11167.53597893529
Iteration 6400: Loss = -11167.535991875891
1
Iteration 6500: Loss = -11167.5359501382
Iteration 6600: Loss = -11167.536109211896
1
Iteration 6700: Loss = -11167.536149687292
2
Iteration 6800: Loss = -11167.5359587642
3
Iteration 6900: Loss = -11167.536821439402
4
Iteration 7000: Loss = -11167.53802730931
5
Iteration 7100: Loss = -11167.535689000482
Iteration 7200: Loss = -11167.536026918844
1
Iteration 7300: Loss = -11167.53513348142
Iteration 7400: Loss = -11167.517346771134
Iteration 7500: Loss = -11167.519463009467
1
Iteration 7600: Loss = -11167.518383226083
2
Iteration 7700: Loss = -11167.531777302906
3
Iteration 7800: Loss = -11167.51647015421
Iteration 7900: Loss = -11167.51640774948
Iteration 8000: Loss = -11167.516758700434
1
Iteration 8100: Loss = -11167.569056121814
2
Iteration 8200: Loss = -11167.516404032402
Iteration 8300: Loss = -11167.51703811015
1
Iteration 8400: Loss = -11167.51653209287
2
Iteration 8500: Loss = -11167.516388122995
Iteration 8600: Loss = -11167.572449911851
1
Iteration 8700: Loss = -11167.51638190183
Iteration 8800: Loss = -11167.536816654207
1
Iteration 8900: Loss = -11167.516352205726
Iteration 9000: Loss = -11167.531202791155
1
Iteration 9100: Loss = -11167.516338385503
Iteration 9200: Loss = -11167.516334305943
Iteration 9300: Loss = -11167.516349903133
1
Iteration 9400: Loss = -11167.516330147973
Iteration 9500: Loss = -11167.529159357327
1
Iteration 9600: Loss = -11167.516356225718
2
Iteration 9700: Loss = -11167.658598009544
3
Iteration 9800: Loss = -11167.516339931197
4
Iteration 9900: Loss = -11167.549479359257
5
Iteration 10000: Loss = -11167.516326886827
Iteration 10100: Loss = -11167.517030791954
1
Iteration 10200: Loss = -11167.519173847135
2
Iteration 10300: Loss = -11167.516324695458
Iteration 10400: Loss = -11167.516321369942
Iteration 10500: Loss = -11167.516319894465
Iteration 10600: Loss = -11167.519401710622
1
Iteration 10700: Loss = -11167.516517630058
2
Iteration 10800: Loss = -11167.521417601592
3
Iteration 10900: Loss = -11167.525717548771
4
Iteration 11000: Loss = -11167.516235308292
Iteration 11100: Loss = -11167.517080357527
1
Iteration 11200: Loss = -11167.525693585654
2
Iteration 11300: Loss = -11167.524187712484
3
Iteration 11400: Loss = -11167.52022463387
4
Iteration 11500: Loss = -11167.516330941242
5
Iteration 11600: Loss = -11167.516978616779
6
Iteration 11700: Loss = -11167.523083576401
7
Iteration 11800: Loss = -11167.539250960765
8
Iteration 11900: Loss = -11167.516139829899
Iteration 12000: Loss = -11167.51802665473
1
Iteration 12100: Loss = -11167.620457930325
2
Iteration 12200: Loss = -11167.51827535055
3
Iteration 12300: Loss = -11167.5214756302
4
Iteration 12400: Loss = -11167.517425350861
5
Iteration 12500: Loss = -11167.582386320599
6
Iteration 12600: Loss = -11167.541245714623
7
Iteration 12700: Loss = -11167.518066165352
8
Iteration 12800: Loss = -11167.51734136516
9
Iteration 12900: Loss = -11167.541251797506
10
Stopping early at iteration 12900 due to no improvement.
tensor([[-4.8418e+00,  3.4547e+00],
        [-4.7118e+00,  3.2006e+00],
        [-5.3528e+00,  3.1007e+00],
        [ 1.3215e+00, -3.4943e+00],
        [ 5.4000e+00, -6.8919e+00],
        [-3.2716e+00,  1.8776e+00],
        [ 4.6126e+00, -7.1995e+00],
        [ 6.8223e+00, -9.2943e+00],
        [-2.6848e+00,  1.2212e+00],
        [ 1.6694e+00, -3.2752e+00],
        [-1.5301e+00,  3.8902e-03],
        [ 1.8070e+00, -3.5066e+00],
        [-7.0852e+00,  2.4700e+00],
        [ 8.5077e-01, -2.9995e+00],
        [-1.8239e+00,  3.9265e-01],
        [-1.3083e+00, -5.3328e-01],
        [-4.3561e+00,  2.2395e+00],
        [-3.5356e+00,  1.7057e+00],
        [-7.2024e+00,  5.7205e+00],
        [-2.8375e+00,  1.4490e+00],
        [ 6.6173e-01, -2.1471e+00],
        [-4.1951e+00,  2.7404e+00],
        [ 1.7006e+00, -3.3499e+00],
        [-3.9553e+00,  2.1861e+00],
        [-6.7521e+00,  4.9447e+00],
        [-2.5835e+00, -1.7043e+00],
        [ 5.8556e+00, -7.5104e+00],
        [-5.2046e+00,  3.8120e+00],
        [ 3.5261e+00, -4.9201e+00],
        [-7.1137e+00,  5.3090e+00],
        [ 4.6622e+00, -6.0950e+00],
        [-3.2724e+00,  1.8400e+00],
        [ 3.5498e+00, -4.9907e+00],
        [ 6.0795e+00, -8.3011e+00],
        [-4.2198e+00,  1.1805e-01],
        [-7.2490e+00,  5.7913e+00],
        [-5.3858e+00,  1.7765e+00],
        [ 2.5216e+00, -4.5452e+00],
        [ 2.3038e+00, -4.0224e+00],
        [ 2.1672e+00, -4.7257e+00],
        [-3.3978e+00,  1.8693e+00],
        [-2.1812e-01, -4.1304e+00],
        [-5.8811e+00,  4.3605e+00],
        [-2.3694e+00,  9.0979e-01],
        [-6.2170e+00,  4.7950e+00],
        [-4.5726e+00,  2.9821e+00],
        [-4.0673e+00,  2.6659e+00],
        [-4.4422e+00,  2.6682e+00],
        [-2.6108e+00,  1.1956e+00],
        [-2.5131e+00,  8.1847e-01],
        [ 2.8446e+00, -4.2702e+00],
        [-7.1767e+00,  5.7670e+00],
        [-2.8529e+00, -6.2995e-02],
        [ 3.5351e+00, -7.2636e+00],
        [ 2.3042e+00, -4.9398e+00],
        [-3.1358e+00,  1.3785e+00],
        [-5.3541e+00,  3.9642e+00],
        [ 5.8613e+00, -7.2770e+00],
        [ 3.5210e+00, -5.9374e+00],
        [-6.9004e+00,  4.1415e+00],
        [ 3.8254e+00, -5.2119e+00],
        [-5.3198e+00,  3.5251e+00],
        [ 9.1328e-01, -2.3499e+00],
        [ 3.5154e+00, -5.5110e+00],
        [-3.9653e+00,  2.5710e+00],
        [ 1.8854e+00, -3.3753e+00],
        [ 3.6717e+00, -5.2879e+00],
        [ 3.2131e+00, -4.8211e+00],
        [-6.0331e+00,  2.9060e+00],
        [-3.8751e+00,  1.9732e+00],
        [-3.2625e+00,  1.8577e+00],
        [-5.0142e+00,  3.1196e+00],
        [ 6.0518e-02, -2.3792e+00],
        [ 2.3484e-01, -1.8510e+00],
        [ 4.7370e+00, -6.1254e+00],
        [-4.1106e+00,  1.8239e+00],
        [-2.8962e+00,  1.5094e+00],
        [ 2.8206e+00, -4.2122e+00],
        [-4.7852e+00,  3.0392e+00],
        [-6.1756e+00,  4.1353e+00],
        [-5.8641e+00,  4.4535e+00],
        [-3.6918e+00,  2.1063e+00],
        [-5.9275e+00,  4.5231e+00],
        [-4.1712e+00,  1.8403e+00],
        [-1.8837e+00, -9.6652e-02],
        [-7.1229e+00,  4.9778e+00],
        [ 2.4667e+00, -5.8186e+00],
        [-2.6594e+00,  1.1512e+00],
        [ 3.5337e+00, -5.2383e+00],
        [ 4.8207e+00, -6.2680e+00],
        [-4.3492e+00,  2.9425e+00],
        [ 4.6989e+00, -6.1085e+00],
        [-3.7863e+00,  1.5748e+00],
        [-5.3472e+00,  3.9562e+00],
        [-2.7106e+00,  1.3127e+00],
        [-5.2715e+00,  2.5271e+00],
        [-5.3607e+00,  3.9557e+00],
        [-5.2248e+00,  3.6552e+00],
        [-3.9805e+00,  2.4873e+00],
        [ 6.6509e+00, -8.3346e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.1645, 0.8355],
        [0.7324, 0.2676]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3929, 0.6071], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2468, 0.1000],
         [0.1479, 0.2268]],

        [[0.2216, 0.1169],
         [0.2957, 0.3057]],

        [[0.9957, 0.1071],
         [0.7733, 0.7111]],

        [[0.4966, 0.0971],
         [0.8914, 0.4735]],

        [[0.9844, 0.1069],
         [0.0674, 0.7725]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 92
Adjusted Rand Index: 0.7026660163709976
time is 2
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8447122004349823
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 95
Adjusted Rand Index: 0.8080740404436667
time is 4
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 6
Adjusted Rand Index: 0.7721314419105764
Global Adjusted Rand Index: 0.04292479918305753
Average Adjusted Rand Index: 0.8255167398320445
11166.107420627
new:  [0.0, -0.006376326519735513, 9.984613345996704e-05, 0.04292479918305753] [0.0, 0.0, 0.27382876875729945, 0.8255167398320445] [11311.988509591678, 11311.628316430855, 11297.440659852566, 11167.541251797506]
prior:  [0.0, 0.0, 0.0, 0.0] [0.0, 0.0, 0.0, 0.0] [11312.725099544197, 11312.151865881926, 11312.073135116962, 11312.151737714441]
-----------------------------------------------------------------------------------------
This iteration is 5
True Objective function: Loss = -11156.275019948027
Iteration 0: Loss = -17416.79862782646
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.6448,    nan]],

        [[0.6370,    nan],
         [0.1243, 0.9194]],

        [[0.2708,    nan],
         [0.3520, 0.8638]],

        [[0.6026,    nan],
         [0.4801, 0.7474]],

        [[0.2072,    nan],
         [0.6746, 0.0663]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18449.977018651207
Iteration 100: Loss = -11397.744470295152
Iteration 200: Loss = -11395.083323569843
Iteration 300: Loss = -11394.104099808932
Iteration 400: Loss = -11393.626579420235
Iteration 500: Loss = -11393.349542622518
Iteration 600: Loss = -11393.174337120436
Iteration 700: Loss = -11393.05695026409
Iteration 800: Loss = -11392.975184705085
Iteration 900: Loss = -11392.916924795269
Iteration 1000: Loss = -11392.874970808572
Iteration 1100: Loss = -11392.844406689635
Iteration 1200: Loss = -11392.821453567307
Iteration 1300: Loss = -11392.80343779765
Iteration 1400: Loss = -11392.788611760745
Iteration 1500: Loss = -11392.77563161647
Iteration 1600: Loss = -11392.763251843127
Iteration 1700: Loss = -11392.749378430919
Iteration 1800: Loss = -11392.728388635085
Iteration 1900: Loss = -11392.672563393699
Iteration 2000: Loss = -11392.386363436442
Iteration 2100: Loss = -11391.677360429665
Iteration 2200: Loss = -11390.927846814353
Iteration 2300: Loss = -11390.594953083924
Iteration 2400: Loss = -11387.074635962292
Iteration 2500: Loss = -11371.455028113352
Iteration 2600: Loss = -11343.938228019591
Iteration 2700: Loss = -11337.752124572104
Iteration 2800: Loss = -11337.231709958898
Iteration 2900: Loss = -11337.08758024489
Iteration 3000: Loss = -11337.046941114702
Iteration 3100: Loss = -11337.005236024293
Iteration 3200: Loss = -11336.98693639072
Iteration 3300: Loss = -11336.949782157575
Iteration 3400: Loss = -11335.815695936853
Iteration 3500: Loss = -11334.711750093038
Iteration 3600: Loss = -11334.537791055098
Iteration 3700: Loss = -11334.414955627719
Iteration 3800: Loss = -11333.953735471116
Iteration 3900: Loss = -11333.904711830637
Iteration 4000: Loss = -11333.858819410789
Iteration 4100: Loss = -11333.762777335964
Iteration 4200: Loss = -11333.731707829285
Iteration 4300: Loss = -11333.726116736927
Iteration 4400: Loss = -11333.707667426348
Iteration 4500: Loss = -11333.681334174798
Iteration 4600: Loss = -11333.652298384657
Iteration 4700: Loss = -11333.624992523934
Iteration 4800: Loss = -11333.614647101158
Iteration 4900: Loss = -11333.596874669114
Iteration 5000: Loss = -11333.568339703914
Iteration 5100: Loss = -11333.423112525536
Iteration 5200: Loss = -11331.075007724417
Iteration 5300: Loss = -11330.766473865395
Iteration 5400: Loss = -11253.716589015914
Iteration 5500: Loss = -11169.351541461305
Iteration 5600: Loss = -11161.329747478198
Iteration 5700: Loss = -11155.629790814713
Iteration 5800: Loss = -11153.561356471044
Iteration 5900: Loss = -11152.896933512031
Iteration 6000: Loss = -11152.811425138625
Iteration 6100: Loss = -11152.805558898654
Iteration 6200: Loss = -11151.047185128908
Iteration 6300: Loss = -11151.063815230656
1
Iteration 6400: Loss = -11149.546275365054
Iteration 6500: Loss = -11146.536443556197
Iteration 6600: Loss = -11146.42997626362
Iteration 6700: Loss = -11146.412045680749
Iteration 6800: Loss = -11146.410093291606
Iteration 6900: Loss = -11146.404221788296
Iteration 7000: Loss = -11146.274453108354
Iteration 7100: Loss = -11146.25833484022
Iteration 7200: Loss = -11146.253173879171
Iteration 7300: Loss = -11146.254440044782
1
Iteration 7400: Loss = -11146.260234116458
2
Iteration 7500: Loss = -11146.258255072982
3
Iteration 7600: Loss = -11146.259819593537
4
Iteration 7700: Loss = -11146.253225633867
5
Iteration 7800: Loss = -11146.251653302152
Iteration 7900: Loss = -11146.24994173434
Iteration 8000: Loss = -11146.250329630144
1
Iteration 8100: Loss = -11146.337901039396
2
Iteration 8200: Loss = -11146.251344656852
3
Iteration 8300: Loss = -11146.247666768977
Iteration 8400: Loss = -11146.246452183992
Iteration 8500: Loss = -11146.249097820695
1
Iteration 8600: Loss = -11146.226735777065
Iteration 8700: Loss = -11141.473337440786
Iteration 8800: Loss = -11141.518886443051
1
Iteration 8900: Loss = -11141.450490340178
Iteration 9000: Loss = -11141.448227158611
Iteration 9100: Loss = -11141.450409576726
1
Iteration 9200: Loss = -11141.448282757307
2
Iteration 9300: Loss = -11141.447943918602
Iteration 9400: Loss = -11141.453863259549
1
Iteration 9500: Loss = -11141.452242237723
2
Iteration 9600: Loss = -11141.490821831023
3
Iteration 9700: Loss = -11141.449600350867
4
Iteration 9800: Loss = -11141.446961086733
Iteration 9900: Loss = -11141.428544856111
Iteration 10000: Loss = -11141.449515202492
1
Iteration 10100: Loss = -11141.410263774787
Iteration 10200: Loss = -11141.413336899008
1
Iteration 10300: Loss = -11141.411025466088
2
Iteration 10400: Loss = -11141.414741110986
3
Iteration 10500: Loss = -11141.442097232712
4
Iteration 10600: Loss = -11141.396790928738
Iteration 10700: Loss = -11141.393042889153
Iteration 10800: Loss = -11141.389400001768
Iteration 10900: Loss = -11141.385237242599
Iteration 11000: Loss = -11141.386300668946
1
Iteration 11100: Loss = -11141.385817437502
2
Iteration 11200: Loss = -11141.392232133921
3
Iteration 11300: Loss = -11141.39017590593
4
Iteration 11400: Loss = -11141.384674562023
Iteration 11500: Loss = -11141.386437580533
1
Iteration 11600: Loss = -11141.407811347224
2
Iteration 11700: Loss = -11141.384107687307
Iteration 11800: Loss = -11141.384536665686
1
Iteration 11900: Loss = -11141.384549420605
2
Iteration 12000: Loss = -11141.384699755328
3
Iteration 12100: Loss = -11141.380967646946
Iteration 12200: Loss = -11137.144544231478
Iteration 12300: Loss = -11137.14454389453
Iteration 12400: Loss = -11137.146288133697
1
Iteration 12500: Loss = -11137.165890225553
2
Iteration 12600: Loss = -11137.143545311248
Iteration 12700: Loss = -11137.144879363292
1
Iteration 12800: Loss = -11137.143256050402
Iteration 12900: Loss = -11137.142844202383
Iteration 13000: Loss = -11137.147064380442
1
Iteration 13100: Loss = -11137.144622184536
2
Iteration 13200: Loss = -11137.136257338947
Iteration 13300: Loss = -11137.136461033138
1
Iteration 13400: Loss = -11137.141060382597
2
Iteration 13500: Loss = -11137.164414618266
3
Iteration 13600: Loss = -11137.136196060294
Iteration 13700: Loss = -11137.136351178904
1
Iteration 13800: Loss = -11137.155654214394
2
Iteration 13900: Loss = -11137.136177404564
Iteration 14000: Loss = -11137.140713028019
1
Iteration 14100: Loss = -11137.138343662276
2
Iteration 14200: Loss = -11137.136025491709
Iteration 14300: Loss = -11137.138650355197
1
Iteration 14400: Loss = -11137.139785978696
2
Iteration 14500: Loss = -11137.133361576418
Iteration 14600: Loss = -11137.127881055174
Iteration 14700: Loss = -11137.130677168543
1
Iteration 14800: Loss = -11137.151354849235
2
Iteration 14900: Loss = -11137.140349776864
3
Iteration 15000: Loss = -11137.140484678912
4
Iteration 15100: Loss = -11137.146039907282
5
Iteration 15200: Loss = -11137.127957860585
6
Iteration 15300: Loss = -11137.128586276727
7
Iteration 15400: Loss = -11137.129069668947
8
Iteration 15500: Loss = -11137.138301555948
9
Iteration 15600: Loss = -11137.129578625698
10
Stopping early at iteration 15600 due to no improvement.
tensor([[-10.2480,   5.6328],
        [-11.5877,   6.9725],
        [ -0.4171,  -4.1981],
        [  3.1172,  -7.7324],
        [ -9.4047,   4.7895],
        [-11.4911,   6.8759],
        [-10.9663,   6.3511],
        [  2.0886,  -6.7038],
        [-11.7735,   7.1583],
        [  3.5193,  -8.1346],
        [  2.0746,  -6.6899],
        [-10.8988,   6.2836],
        [-11.4707,   6.8555],
        [ -0.5027,  -4.1125],
        [  2.7383,  -7.3535],
        [-12.0097,   7.3945],
        [ -8.6737,   4.0585],
        [  2.5103,  -7.1255],
        [-11.2861,   6.6709],
        [ -0.5081,  -4.1071],
        [ -5.2767,   0.6615],
        [  0.2735,  -4.8887],
        [-11.3995,   6.7843],
        [-11.3224,   6.7072],
        [  1.9963,  -6.6116],
        [-10.4187,   5.8035],
        [ -9.4760,   4.8608],
        [-10.5168,   5.9016],
        [  4.2808,  -8.8960],
        [  1.9101,  -6.5253],
        [ -4.5918,  -0.0234],
        [-11.5744,   6.9592],
        [ -4.7721,   0.1569],
        [  2.9099,  -7.5251],
        [  2.8411,  -7.4564],
        [ -5.3867,   0.7714],
        [ -1.6382,  -2.9770],
        [ -8.6121,   3.9969],
        [  2.6710,  -7.2862],
        [ -6.0168,   1.4015],
        [ -6.2295,   1.6142],
        [  3.1786,  -7.7938],
        [  2.8178,  -7.4330],
        [ -0.3572,  -4.2581],
        [-11.9173,   7.3021],
        [  3.1454,  -7.7607],
        [  2.9665,  -7.5817],
        [ -6.4817,   1.8665],
        [  3.2999,  -7.9151],
        [  2.5008,  -7.1160],
        [  3.6948,  -8.3100],
        [  3.5750,  -8.1902],
        [ -0.0557,  -4.5595],
        [ -0.0808,  -4.5344],
        [ -6.2857,   1.6705],
        [ -6.0455,   1.4303],
        [  2.7799,  -7.3951],
        [  3.8305,  -8.4457],
        [-10.4418,   5.8266],
        [  3.4440,  -8.0592],
        [-11.0479,   6.4327],
        [  0.6958,  -5.3110],
        [  0.6050,  -5.2202],
        [ -7.6606,   3.0454],
        [ -9.1979,   4.5827],
        [ -0.1794,  -4.4359],
        [ -1.1410,  -3.4742],
        [  0.6490,  -5.2643],
        [ -9.9188,   5.3036],
        [ -8.7245,   4.1093],
        [ -7.1312,   2.5159],
        [ -6.8812,   2.2660],
        [ -9.0666,   4.4514],
        [-11.3078,   6.6926],
        [ -2.5559,  -2.0593],
        [  2.9512,  -7.5664],
        [-11.7267,   7.1114],
        [  0.7463,  -5.3615],
        [-11.6986,   7.0833],
        [ -6.6919,   2.0766],
        [  3.5929,  -8.2081],
        [ -7.3148,   2.6995],
        [ -8.2574,   3.6422],
        [  1.9059,  -6.5211],
        [-11.0804,   6.4652],
        [-11.4222,   6.8070],
        [-11.7975,   7.1823],
        [  3.4010,  -8.0162],
        [  1.0028,  -5.6180],
        [ -0.1052,  -4.5100],
        [ -7.5190,   2.9038],
        [-11.5242,   6.9089],
        [  1.5928,  -6.2080],
        [  0.6889,  -5.3041],
        [  2.5271,  -7.1423],
        [  0.8348,  -5.4501],
        [  0.4061,  -5.0213],
        [ -0.9017,  -3.7135],
        [-11.5766,   6.9614],
        [ -2.9946,  -1.6206]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7303, 0.2697],
        [0.2479, 0.7521]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4902, 0.5098], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2064, 0.0981],
         [0.6448, 0.2829]],

        [[0.6370, 0.1048],
         [0.1243, 0.9194]],

        [[0.2708, 0.1098],
         [0.3520, 0.8638]],

        [[0.6026, 0.0875],
         [0.4801, 0.7474]],

        [[0.2072, 0.0983],
         [0.6746, 0.0663]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824200963560147
time is 2
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
Global Adjusted Rand Index: 0.9446732505061082
Average Adjusted Rand Index: 0.9446435793997917
Iteration 0: Loss = -26852.950132141083
Iteration 10: Loss = -11388.362203716973
Iteration 20: Loss = -11387.801152193255
Iteration 30: Loss = -11387.379205000496
Iteration 40: Loss = -11386.989931912884
Iteration 50: Loss = -11386.616108134232
Iteration 60: Loss = -11386.243964668603
Iteration 70: Loss = -11385.857014915631
Iteration 80: Loss = -11385.433233745349
Iteration 90: Loss = -11384.935022263957
Iteration 100: Loss = -11384.264350132611
Iteration 110: Loss = -11382.865188891043
Iteration 120: Loss = -11335.791077542639
Iteration 130: Loss = -11142.801568255954
Iteration 140: Loss = -11139.57664938456
Iteration 150: Loss = -11139.578610836852
1
Iteration 160: Loss = -11139.578640802827
2
Iteration 170: Loss = -11139.578648592988
3
Stopping early at iteration 169 due to no improvement.
pi: tensor([[0.7306, 0.2694],
        [0.2916, 0.7084]], dtype=torch.float64)
alpha: tensor([0.5149, 0.4851])
beta: tensor([[[0.2776, 0.0984],
         [0.3563, 0.2021]],

        [[0.8968, 0.1053],
         [0.8739, 0.7364]],

        [[0.7916, 0.1098],
         [0.6995, 0.0676]],

        [[0.8378, 0.0875],
         [0.5118, 0.4581]],

        [[0.0888, 0.0983],
         [0.5517, 0.7173]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 2
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208
time is 3
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
Global Adjusted Rand Index: 0.9214427761527915
Average Adjusted Rand Index: 0.9212892570982858
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26852.935429840923
Iteration 100: Loss = -11448.709657696076
Iteration 200: Loss = -11431.014159025819
Iteration 300: Loss = -11418.706664047475
Iteration 400: Loss = -11410.10583120751
Iteration 500: Loss = -11402.37038983709
Iteration 600: Loss = -11397.176244656262
Iteration 700: Loss = -11393.71965113153
Iteration 800: Loss = -11392.780349808461
Iteration 900: Loss = -11392.21733966123
Iteration 1000: Loss = -11391.744577685622
Iteration 1100: Loss = -11391.199365359558
Iteration 1200: Loss = -11387.045193787959
Iteration 1300: Loss = -11340.284055111875
Iteration 1400: Loss = -11339.589354565629
Iteration 1500: Loss = -11339.38082204161
Iteration 1600: Loss = -11339.26807726571
Iteration 1700: Loss = -11339.195035552155
Iteration 1800: Loss = -11339.143045826075
Iteration 1900: Loss = -11339.10391892509
Iteration 2000: Loss = -11339.073343604277
Iteration 2100: Loss = -11339.048668328187
Iteration 2200: Loss = -11339.028400536494
Iteration 2300: Loss = -11339.01142618679
Iteration 2400: Loss = -11338.99708570894
Iteration 2500: Loss = -11338.98479246444
Iteration 2600: Loss = -11338.974186054784
Iteration 2700: Loss = -11338.96485570205
Iteration 2800: Loss = -11338.956721159046
Iteration 2900: Loss = -11338.949463276127
Iteration 3000: Loss = -11338.943050145403
Iteration 3100: Loss = -11338.937341624209
Iteration 3200: Loss = -11338.93216336897
Iteration 3300: Loss = -11338.927535526262
Iteration 3400: Loss = -11338.923368020376
Iteration 3500: Loss = -11338.919567209054
Iteration 3600: Loss = -11338.916064145635
Iteration 3700: Loss = -11338.91290892294
Iteration 3800: Loss = -11338.90998860373
Iteration 3900: Loss = -11338.9073024682
Iteration 4000: Loss = -11338.90484802672
Iteration 4100: Loss = -11338.90253408898
Iteration 4200: Loss = -11338.90041062874
Iteration 4300: Loss = -11338.89844370319
Iteration 4400: Loss = -11338.89654640736
Iteration 4500: Loss = -11338.894761036858
Iteration 4600: Loss = -11338.89306272741
Iteration 4700: Loss = -11338.891350148228
Iteration 4800: Loss = -11338.88968610565
Iteration 4900: Loss = -11338.887880258664
Iteration 5000: Loss = -11338.885684474517
Iteration 5100: Loss = -11338.881990027436
Iteration 5200: Loss = -11338.866600391517
Iteration 5300: Loss = -11338.78349274016
Iteration 5400: Loss = -11338.701444152592
Iteration 5500: Loss = -11338.700073182357
Iteration 5600: Loss = -11338.699129852665
Iteration 5700: Loss = -11338.697837647367
Iteration 5800: Loss = -11338.695525365052
Iteration 5900: Loss = -11338.685555265029
Iteration 6000: Loss = -11338.653981590976
Iteration 6100: Loss = -11338.651293267552
Iteration 6200: Loss = -11338.646118832103
Iteration 6300: Loss = -11338.604708350847
Iteration 6400: Loss = -11338.554611552656
Iteration 6500: Loss = -11338.533105425127
Iteration 6600: Loss = -11338.516934859877
Iteration 6700: Loss = -11338.485455490129
Iteration 6800: Loss = -11338.447023462255
Iteration 6900: Loss = -11338.379077143618
Iteration 7000: Loss = -11338.272456648958
Iteration 7100: Loss = -11338.237530646818
Iteration 7200: Loss = -11338.204934455107
Iteration 7300: Loss = -11338.14591987783
Iteration 7400: Loss = -11338.053669561634
Iteration 7500: Loss = -11337.994269402372
Iteration 7600: Loss = -11337.98372068844
Iteration 7700: Loss = -11337.954580362137
Iteration 7800: Loss = -11337.912590634232
Iteration 7900: Loss = -11337.878097965118
Iteration 8000: Loss = -11337.827998503772
Iteration 8100: Loss = -11337.783049725349
Iteration 8200: Loss = -11337.731960553172
Iteration 8300: Loss = -11337.690683905163
Iteration 8400: Loss = -11337.472015253961
Iteration 8500: Loss = -11337.450763804687
Iteration 8600: Loss = -11337.449926437057
Iteration 8700: Loss = -11337.443299359924
Iteration 8800: Loss = -11337.443077050835
Iteration 8900: Loss = -11337.443062343515
Iteration 9000: Loss = -11337.442817640893
Iteration 9100: Loss = -11337.442707851043
Iteration 9200: Loss = -11337.443054461124
1
Iteration 9300: Loss = -11337.451942189353
2
Iteration 9400: Loss = -11337.442733399334
3
Iteration 9500: Loss = -11337.442430746276
Iteration 9600: Loss = -11337.443138168024
1
Iteration 9700: Loss = -11337.442376005467
Iteration 9800: Loss = -11337.442228724121
Iteration 9900: Loss = -11337.44280333033
1
Iteration 10000: Loss = -11337.442102267161
Iteration 10100: Loss = -11337.470127528737
1
Iteration 10200: Loss = -11337.442028654383
Iteration 10300: Loss = -11337.441994906641
Iteration 10400: Loss = -11337.596533548469
1
Iteration 10500: Loss = -11337.441905314035
Iteration 10600: Loss = -11337.441844450223
Iteration 10700: Loss = -11337.4433899844
1
Iteration 10800: Loss = -11337.44181121048
Iteration 10900: Loss = -11337.44197866496
1
Iteration 11000: Loss = -11337.44165880347
Iteration 11100: Loss = -11337.443646814781
1
Iteration 11200: Loss = -11337.443160178651
2
Iteration 11300: Loss = -11337.44128803646
Iteration 11400: Loss = -11337.448775109373
1
Iteration 11500: Loss = -11337.479717998578
2
Iteration 11600: Loss = -11337.441125624306
Iteration 11700: Loss = -11337.44028175825
Iteration 11800: Loss = -11337.44035122001
1
Iteration 11900: Loss = -11337.44070402242
2
Iteration 12000: Loss = -11337.451973
3
Iteration 12100: Loss = -11337.452464318068
4
Iteration 12200: Loss = -11337.493479106379
5
Iteration 12300: Loss = -11337.443545718865
6
Iteration 12400: Loss = -11337.440378763924
7
Iteration 12500: Loss = -11337.439759423289
Iteration 12600: Loss = -11337.439884780597
1
Iteration 12700: Loss = -11337.439140376104
Iteration 12800: Loss = -11337.439513454105
1
Iteration 12900: Loss = -11337.438641335024
Iteration 13000: Loss = -11337.438844411527
1
Iteration 13100: Loss = -11337.439051458026
2
Iteration 13200: Loss = -11337.440731922274
3
Iteration 13300: Loss = -11337.440964702297
4
Iteration 13400: Loss = -11337.444631508097
5
Iteration 13500: Loss = -11337.438619983348
Iteration 13600: Loss = -11337.441890426702
1
Iteration 13700: Loss = -11337.438435208964
Iteration 13800: Loss = -11337.438354890357
Iteration 13900: Loss = -11337.5152023476
1
Iteration 14000: Loss = -11337.436006603137
Iteration 14100: Loss = -11335.673973477416
Iteration 14200: Loss = -11335.640379060751
Iteration 14300: Loss = -11335.634995115217
Iteration 14400: Loss = -11335.695454495477
1
Iteration 14500: Loss = -11335.632033816675
Iteration 14600: Loss = -11335.684423169707
1
Iteration 14700: Loss = -11335.628979031184
Iteration 14800: Loss = -11335.628183112258
Iteration 14900: Loss = -11335.625550186329
Iteration 15000: Loss = -11335.625374836316
Iteration 15100: Loss = -11335.642511553455
1
Iteration 15200: Loss = -11335.68323690101
2
Iteration 15300: Loss = -11335.625054706263
Iteration 15400: Loss = -11335.626322844837
1
Iteration 15500: Loss = -11335.62496032985
Iteration 15600: Loss = -11335.62495083444
Iteration 15700: Loss = -11335.624891904043
Iteration 15800: Loss = -11335.624902675403
1
Iteration 15900: Loss = -11335.625001524912
2
Iteration 16000: Loss = -11335.633474497781
3
Iteration 16100: Loss = -11335.687297846249
4
Iteration 16200: Loss = -11335.62484636232
Iteration 16300: Loss = -11335.626785617971
1
Iteration 16400: Loss = -11335.631579823057
2
Iteration 16500: Loss = -11335.684532449292
3
Iteration 16600: Loss = -11335.627178822275
4
Iteration 16700: Loss = -11335.627861460896
5
Iteration 16800: Loss = -11335.628002527175
6
Iteration 16900: Loss = -11335.625485214443
7
Iteration 17000: Loss = -11335.62473322478
Iteration 17100: Loss = -11335.628521287614
1
Iteration 17200: Loss = -11335.62595913623
2
Iteration 17300: Loss = -11335.625266871972
3
Iteration 17400: Loss = -11335.643775513556
4
Iteration 17500: Loss = -11335.624649679901
Iteration 17600: Loss = -11335.626814212244
1
Iteration 17700: Loss = -11335.62461307766
Iteration 17800: Loss = -11335.660597941765
1
Iteration 17900: Loss = -11335.624607189966
Iteration 18000: Loss = -11335.626125335995
1
Iteration 18100: Loss = -11335.64977024669
2
Iteration 18200: Loss = -11335.679327120479
3
Iteration 18300: Loss = -11335.627529426034
4
Iteration 18400: Loss = -11335.625404220158
5
Iteration 18500: Loss = -11335.6261479
6
Iteration 18600: Loss = -11335.642586136975
7
Iteration 18700: Loss = -11335.637526469134
8
Iteration 18800: Loss = -11335.625386864069
9
Iteration 18900: Loss = -11335.62596034305
10
Stopping early at iteration 18900 due to no improvement.
tensor([[ -0.3422,  -1.1995],
        [  2.4321,  -3.8485],
        [ -3.8299,   2.4426],
        [ -6.6467,   4.6536],
        [  4.0803,  -5.7071],
        [  3.4266,  -5.6904],
        [  1.1870,  -2.5738],
        [ -6.2189,   4.8268],
        [  5.3710,  -7.3874],
        [ -7.6556,   4.7741],
        [ -4.6906,   3.3017],
        [  1.9991,  -3.8517],
        [  3.7468,  -5.3787],
        [ -3.0773,   1.6748],
        [ -6.4795,   4.0506],
        [  7.3741,  -8.7731],
        [  5.2506,  -6.7004],
        [ -6.0390,   3.7151],
        [  2.8264,  -4.2676],
        [ -3.1033,   1.6212],
        [  2.2419,  -3.6282],
        [ -3.7280,   1.3599],
        [ -0.0971,  -1.5391],
        [  1.4251,  -2.8373],
        [ -4.9874,   2.4116],
        [  2.7363,  -4.1531],
        [  6.0733,  -7.4619],
        [  6.3880,  -7.8874],
        [ -8.4363,   6.0823],
        [ -3.9626,   2.1953],
        [  0.5977,  -4.2298],
        [  2.2759,  -3.8588],
        [ -0.1934,  -2.8506],
        [ -7.3939,   3.9062],
        [ -6.5190,   3.2290],
        [  5.1506,  -6.8845],
        [ -1.2441,  -0.7768],
        [  4.1272,  -6.3955],
        [ -7.2437,   4.6544],
        [  2.1080,  -3.7009],
        [  2.0778,  -3.6733],
        [ -7.4099,   2.7946],
        [ -6.4907,   4.9714],
        [ -4.5074,   1.6339],
        [  7.8778,  -9.4024],
        [ -5.8050,   3.6923],
        [ -7.4092,   5.7044],
        [  3.2320,  -5.1956],
        [ -6.6205,   4.0850],
        [ -5.5954,   3.9929],
        [ -7.3369,   5.3965],
        [ -6.6301,   4.1058],
        [ -4.4803,   1.7514],
        [ -2.9557,   0.3513],
        [  3.2120,  -4.7401],
        [  2.7818,  -5.0969],
        [ -6.6282,   5.1571],
        [ -7.1128,   4.7670],
        [  0.7208,  -2.9225],
        [ -6.9985,   4.7435],
        [  5.0121,  -6.4593],
        [ -4.0021,   2.2966],
        [ -4.9537,   3.3577],
        [  3.5753,  -5.1490],
        [  6.7860,  -8.2095],
        [ -3.1829,   0.1609],
        [ -2.9005,   1.4511],
        [ -4.3702,   2.8816],
        [  6.5703,  -8.2923],
        [  5.1906,  -6.6004],
        [  2.6372,  -4.1430],
        [  2.9115,  -4.4929],
        [  7.0181,  -8.4045],
        [  3.6094,  -5.6358],
        [ -1.5525,  -0.0673],
        [ -6.2143,   4.3478],
        [  7.1844, -11.1188],
        [ -4.1968,   2.8068],
        [  6.5180,  -7.9316],
        [  2.4957,  -7.0360],
        [ -6.9292,   4.9171],
        [  3.3149,  -4.7700],
        [  1.7408,  -3.9268],
        [ -5.8586,   4.4250],
        [  4.2255,  -6.4633],
        [  3.1735,  -4.5610],
        [  5.4883,  -6.9023],
        [ -6.4562,   4.6935],
        [ -4.8162,   3.0838],
        [ -3.6375,   0.3582],
        [  3.7546,  -6.9572],
        [  7.7229, -11.2743],
        [ -4.5589,   3.1650],
        [ -4.1558,   2.1116],
        [ -5.2415,   3.7051],
        [ -5.0068,   2.6129],
        [ -4.6450,   3.1087],
        [ -4.5741,   1.0133],
        [  3.0941,  -4.5477],
        [ -0.3267,  -1.6809]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.0726, 0.9274],
        [0.0338, 0.9662]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4984, 0.5016], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3058, 0.0991],
         [0.3563, 0.1756]],

        [[0.8968, 0.0896],
         [0.8739, 0.7364]],

        [[0.7916, 0.2047],
         [0.6995, 0.0676]],

        [[0.8378, 0.0889],
         [0.5118, 0.4581]],

        [[0.0888, 0.2547],
         [0.5517, 0.7173]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.004166919759460609
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.008125898971292667
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
Global Adjusted Rand Index: 0.021951170905119322
Average Adjusted Rand Index: 0.20186588525812504
Iteration 0: Loss = -23665.95223889674
Iteration 10: Loss = -11392.400299057997
Iteration 20: Loss = -11389.694786259546
Iteration 30: Loss = -11388.645903078303
Iteration 40: Loss = -11388.099655339194
Iteration 50: Loss = -11387.782854246932
Iteration 60: Loss = -11387.5920398629
Iteration 70: Loss = -11387.473539419734
Iteration 80: Loss = -11387.39725827562
Iteration 90: Loss = -11387.331591863023
Iteration 100: Loss = -11387.120990805523
Iteration 110: Loss = -11386.70957697814
Iteration 120: Loss = -11386.32784153675
Iteration 130: Loss = -11385.94673264141
Iteration 140: Loss = -11385.534080259204
Iteration 150: Loss = -11385.057865507855
Iteration 160: Loss = -11384.443054331137
Iteration 170: Loss = -11383.356514912022
Iteration 180: Loss = -11369.002020428366
Iteration 190: Loss = -11147.819930925938
Iteration 200: Loss = -11139.573492668276
Iteration 210: Loss = -11139.578601723524
1
Iteration 220: Loss = -11139.578663149516
2
Iteration 230: Loss = -11139.57863665489
3
Stopping early at iteration 229 due to no improvement.
pi: tensor([[0.7084, 0.2916],
        [0.2694, 0.7306]], dtype=torch.float64)
alpha: tensor([0.4851, 0.5149])
beta: tensor([[[0.2021, 0.0984],
         [0.7381, 0.2776]],

        [[0.4860, 0.1053],
         [0.2355, 0.1917]],

        [[0.2740, 0.1098],
         [0.7164, 0.3353]],

        [[0.4351, 0.0875],
         [0.3867, 0.5131]],

        [[0.5487, 0.0983],
         [0.6309, 0.1562]]], dtype=torch.float64)
time is 0
tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 2
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208
time is 3
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
Global Adjusted Rand Index: 0.9214427761527915
Average Adjusted Rand Index: 0.9212892570982858
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23667.08083396799
Iteration 100: Loss = -11404.902481600542
Iteration 200: Loss = -11398.307352461952
Iteration 300: Loss = -11395.777053355734
Iteration 400: Loss = -11394.554048917587
Iteration 500: Loss = -11393.875174782383
Iteration 600: Loss = -11393.447980806279
Iteration 700: Loss = -11393.154400225489
Iteration 800: Loss = -11392.938030333471
Iteration 900: Loss = -11392.767398943159
Iteration 1000: Loss = -11392.613427290997
Iteration 1100: Loss = -11392.4522522209
Iteration 1200: Loss = -11392.256549524935
Iteration 1300: Loss = -11392.020858473441
Iteration 1400: Loss = -11391.80524594575
Iteration 1500: Loss = -11391.608963057424
Iteration 1600: Loss = -11391.41132643426
Iteration 1700: Loss = -11391.182606431274
Iteration 1800: Loss = -11390.840905199919
Iteration 1900: Loss = -11390.00810459294
Iteration 2000: Loss = -11388.83313499165
Iteration 2100: Loss = -11388.249845454762
Iteration 2200: Loss = -11387.939678574281
Iteration 2300: Loss = -11387.02109434245
Iteration 2400: Loss = -11385.519227819419
Iteration 2500: Loss = -11329.609757834285
Iteration 2600: Loss = -11298.002658379846
Iteration 2700: Loss = -11281.689605316704
Iteration 2800: Loss = -11281.178420656297
Iteration 2900: Loss = -11281.106737101662
Iteration 3000: Loss = -11281.073179088524
Iteration 3100: Loss = -11281.040601378549
Iteration 3200: Loss = -11281.009241765774
Iteration 3300: Loss = -11279.377175280568
Iteration 3400: Loss = -11279.314487570591
Iteration 3500: Loss = -11277.262664008189
Iteration 3600: Loss = -11277.243083612591
Iteration 3700: Loss = -11277.162669587866
Iteration 3800: Loss = -11277.146538268118
Iteration 3900: Loss = -11276.989122939105
Iteration 4000: Loss = -11276.980972244382
Iteration 4100: Loss = -11276.974447840512
Iteration 4200: Loss = -11276.968605115286
Iteration 4300: Loss = -11276.96287102782
Iteration 4400: Loss = -11276.963089190647
1
Iteration 4500: Loss = -11276.951792340122
Iteration 4600: Loss = -11276.950584896422
Iteration 4700: Loss = -11276.926159857863
Iteration 4800: Loss = -11276.896738504423
Iteration 4900: Loss = -11276.748962666234
Iteration 5000: Loss = -11275.950400330341
Iteration 5100: Loss = -11272.294883741288
Iteration 5200: Loss = -11259.005443744061
Iteration 5300: Loss = -11250.170143835958
Iteration 5400: Loss = -11203.308846453689
Iteration 5500: Loss = -11199.068446823701
Iteration 5600: Loss = -11179.752679939174
Iteration 5700: Loss = -11161.409453495404
Iteration 5800: Loss = -11161.34499331743
Iteration 5900: Loss = -11158.37560824392
Iteration 6000: Loss = -11158.34366982475
Iteration 6100: Loss = -11157.744125513476
Iteration 6200: Loss = -11157.717157603773
Iteration 6300: Loss = -11157.559610371125
Iteration 6400: Loss = -11146.479861648431
Iteration 6500: Loss = -11146.332932965433
Iteration 6600: Loss = -11135.982610090015
Iteration 6700: Loss = -11135.973729111425
Iteration 6800: Loss = -11135.969681647477
Iteration 6900: Loss = -11135.97547634847
1
Iteration 7000: Loss = -11135.971666404146
2
Iteration 7100: Loss = -11135.968428195558
Iteration 7200: Loss = -11135.963193302887
Iteration 7300: Loss = -11135.966012137009
1
Iteration 7400: Loss = -11135.961754108408
Iteration 7500: Loss = -11135.963153983706
1
Iteration 7600: Loss = -11135.959427663383
Iteration 7700: Loss = -11135.95877097721
Iteration 7800: Loss = -11135.958701123975
Iteration 7900: Loss = -11135.959415601708
1
Iteration 8000: Loss = -11135.969124519112
2
Iteration 8100: Loss = -11135.957135198105
Iteration 8200: Loss = -11135.957227064351
1
Iteration 8300: Loss = -11136.001503016261
2
Iteration 8400: Loss = -11135.95709116439
Iteration 8500: Loss = -11135.954107384425
Iteration 8600: Loss = -11135.954312014357
1
Iteration 8700: Loss = -11135.954085306555
Iteration 8800: Loss = -11135.969684673952
1
Iteration 8900: Loss = -11135.988867827882
2
Iteration 9000: Loss = -11135.944689209944
Iteration 9100: Loss = -11135.94111869312
Iteration 9200: Loss = -11135.943109581489
1
Iteration 9300: Loss = -11135.940216122546
Iteration 9400: Loss = -11135.939868633413
Iteration 9500: Loss = -11135.950389043546
1
Iteration 9600: Loss = -11135.939525946507
Iteration 9700: Loss = -11135.940835696632
1
Iteration 9800: Loss = -11135.939400955522
Iteration 9900: Loss = -11135.939606750722
1
Iteration 10000: Loss = -11135.939316060174
Iteration 10100: Loss = -11135.939358011767
1
Iteration 10200: Loss = -11135.961299856674
2
Iteration 10300: Loss = -11135.9618118642
3
Iteration 10400: Loss = -11136.016699011776
4
Iteration 10500: Loss = -11135.941254333746
5
Iteration 10600: Loss = -11136.189811082817
6
Iteration 10700: Loss = -11135.938244958843
Iteration 10800: Loss = -11136.046377046998
1
Iteration 10900: Loss = -11135.938123727934
Iteration 11000: Loss = -11135.938280041468
1
Iteration 11100: Loss = -11135.979078503926
2
Iteration 11200: Loss = -11136.011330287402
3
Iteration 11300: Loss = -11135.938028068298
Iteration 11400: Loss = -11135.939252912125
1
Iteration 11500: Loss = -11135.942620815966
2
Iteration 11600: Loss = -11135.94028686765
3
Iteration 11700: Loss = -11135.93804362783
4
Iteration 11800: Loss = -11135.967120972555
5
Iteration 11900: Loss = -11135.959717483154
6
Iteration 12000: Loss = -11135.948363173065
7
Iteration 12100: Loss = -11135.940413973323
8
Iteration 12200: Loss = -11135.937631441779
Iteration 12300: Loss = -11135.934200096382
Iteration 12400: Loss = -11135.960478936038
1
Iteration 12500: Loss = -11135.932164455253
Iteration 12600: Loss = -11135.932387802271
1
Iteration 12700: Loss = -11135.957217271354
2
Iteration 12800: Loss = -11135.932121740427
Iteration 12900: Loss = -11135.936924872307
1
Iteration 13000: Loss = -11135.932074182874
Iteration 13100: Loss = -11135.939960990974
1
Iteration 13200: Loss = -11135.93373181019
2
Iteration 13300: Loss = -11135.935161002926
3
Iteration 13400: Loss = -11135.932777800788
4
Iteration 13500: Loss = -11135.933730229926
5
Iteration 13600: Loss = -11135.958820798416
6
Iteration 13700: Loss = -11135.933828189753
7
Iteration 13800: Loss = -11135.934075548543
8
Iteration 13900: Loss = -11135.93233279226
9
Iteration 14000: Loss = -11136.160604340168
10
Stopping early at iteration 14000 due to no improvement.
tensor([[ -0.0531,  -2.3172],
        [  3.5993,  -5.0820],
        [ -2.7978,   1.2070],
        [ -6.1102,   4.7192],
        [  4.9518,  -6.6413],
        [  4.5178,  -6.7847],
        [  0.8698,  -5.3210],
        [ -5.5858,   3.2334],
        [  5.9207,  -8.1676],
        [ -6.5284,   5.1373],
        [ -5.4929,   3.2248],
        [  2.4568,  -5.1793],
        [  3.9481,  -6.9069],
        [ -3.4102,   0.2623],
        [ -6.5386,   3.5454],
        [  6.1600,  -7.7856],
        [  3.9489,  -8.5147],
        [ -5.7117,   3.9068],
        [  3.5987,  -5.0168],
        [ -2.5261,   1.1388],
        [  2.2879,  -3.6908],
        [ -3.3002,   1.9099],
        [  0.3553,  -2.9541],
        [  2.4122,  -4.5933],
        [ -4.9905,   3.5757],
        [  3.3219,  -6.1984],
        [  6.3634,  -8.6019],
        [  5.8164,  -7.5730],
        [ -7.1034,   5.3703],
        [ -4.8768,   3.4739],
        [  1.4842,  -3.0602],
        [  3.6165,  -5.0584],
        [  1.6582,  -3.1888],
        [ -6.2478,   4.5948],
        [ -5.8460,   4.4204],
        [  2.0867,  -4.1170],
        [ -1.3295,  -0.0586],
        [  5.5091,  -6.9713],
        [ -5.8668,   4.1183],
        [  2.6874,  -4.7288],
        [  2.9824,  -4.6785],
        [ -6.3659,   4.5759],
        [ -5.8530,   4.3990],
        [ -3.8202,   0.3476],
        [  5.6586,  -8.7153],
        [ -6.1489,   4.7049],
        [ -6.0892,   4.6958],
        [  3.1057,  -5.2894],
        [ -7.4183,   5.8620],
        [ -5.6262,   4.1701],
        [ -6.4337,   5.0336],
        [ -6.6388,   5.0697],
        [ -2.9861,   1.5357],
        [ -3.1295,   1.2759],
        [  3.2984,  -4.7014],
        [  3.0548,  -4.4774],
        [ -6.1969,   3.9985],
        [ -6.7199,   5.3299],
        [  2.2949,  -4.0486],
        [ -6.4856,   4.9975],
        [  5.2126,  -6.9088],
        [ -4.3532,   1.6387],
        [ -3.7255,   2.3391],
        [  4.3515,  -5.9258],
        [  6.0957,  -7.5770],
        [ -3.2584,   0.9468],
        [ -1.8893,   0.4602],
        [ -3.8078,   2.1556],
        [  6.1758,  -7.5762],
        [  5.5640,  -6.9535],
        [  4.1066,  -5.5232],
        [  3.7784,  -5.3686],
        [  5.5843,  -8.0029],
        [  4.8181,  -6.2044],
        [ -0.6687,  -1.1589],
        [ -7.1249,   5.6808],
        [  6.8084,  -8.6078],
        [ -3.7775,   2.3909],
        [  6.4531,  -7.8562],
        [  3.6958,  -5.1003],
        [ -6.6580,   5.1444],
        [  4.1639,  -5.7600],
        [  2.9022,  -5.5538],
        [ -6.0331,   2.4085],
        [  4.9382,  -6.5244],
        [  3.6224,  -6.4615],
        [  6.0452,  -7.7127],
        [ -6.9656,   5.5097],
        [ -4.1467,   2.6656],
        [ -2.9870,   1.5892],
        [  4.4567,  -5.8470],
        [  6.7968, -10.6100],
        [ -4.8362,   2.9598],
        [ -3.8151,   2.1594],
        [ -5.5680,   4.0618],
        [ -3.8690,   2.4260],
        [ -3.5315,   1.9296],
        [ -2.4783,   0.4210],
        [  4.1526,  -5.6567],
        [ -0.1089,  -1.5378]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7426, 0.2574],
        [0.2713, 0.7287]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5016, 0.4984], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2864, 0.0993],
         [0.7381, 0.2040]],

        [[0.4860, 0.1062],
         [0.2355, 0.1917]],

        [[0.2740, 0.1102],
         [0.7164, 0.3353]],

        [[0.4351, 0.0876],
         [0.3867, 0.5131]],

        [[0.5487, 0.0991],
         [0.6309, 0.1562]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824200963560147
time is 2
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
Global Adjusted Rand Index: 0.936897667120417
Average Adjusted Rand Index: 0.936642795729521
Iteration 0: Loss = -19318.706421823787
Iteration 10: Loss = -11390.784168055794
Iteration 20: Loss = -11389.087771674236
Iteration 30: Loss = -11388.34422819399
Iteration 40: Loss = -11387.926056561735
Iteration 50: Loss = -11387.667663117074
Iteration 60: Loss = -11387.419194870285
Iteration 70: Loss = -11387.061871286938
Iteration 80: Loss = -11386.686746362264
Iteration 90: Loss = -11386.316771297259
Iteration 100: Loss = -11385.934693158548
Iteration 110: Loss = -11385.520429014074
Iteration 120: Loss = -11385.041328733456
Iteration 130: Loss = -11384.419580972604
Iteration 140: Loss = -11383.299738669422
Iteration 150: Loss = -11366.350795535005
Iteration 160: Loss = -11146.776032377757
Iteration 170: Loss = -11139.574327027827
Iteration 180: Loss = -11139.57862665034
1
Iteration 190: Loss = -11139.578652209193
2
Iteration 200: Loss = -11139.578645183807
3
Stopping early at iteration 199 due to no improvement.
pi: tensor([[0.7306, 0.2694],
        [0.2916, 0.7084]], dtype=torch.float64)
alpha: tensor([0.5149, 0.4851])
beta: tensor([[[0.2776, 0.0984],
         [0.8685, 0.2021]],

        [[0.3826, 0.1053],
         [0.6210, 0.3615]],

        [[0.7528, 0.1098],
         [0.0293, 0.3566]],

        [[0.5973, 0.0875],
         [0.2276, 0.4090]],

        [[0.1716, 0.0983],
         [0.6278, 0.8254]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 2
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208
time is 3
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
Global Adjusted Rand Index: 0.9214427761527915
Average Adjusted Rand Index: 0.9212892570982858
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19318.446135644972
Iteration 100: Loss = -11397.979099874728
Iteration 200: Loss = -11392.872665256727
Iteration 300: Loss = -11392.44607041981
Iteration 400: Loss = -11391.992732727484
Iteration 500: Loss = -11391.145564149823
Iteration 600: Loss = -11390.328373204165
Iteration 700: Loss = -11389.705805110058
Iteration 800: Loss = -11389.472422503668
Iteration 900: Loss = -11389.310899302931
Iteration 1000: Loss = -11389.13951336061
Iteration 1100: Loss = -11388.936724659583
Iteration 1200: Loss = -11388.724277626428
Iteration 1300: Loss = -11388.438875175745
Iteration 1400: Loss = -11348.25395140283
Iteration 1500: Loss = -11338.449743432962
Iteration 1600: Loss = -11338.262324651372
Iteration 1700: Loss = -11338.179185001816
Iteration 1800: Loss = -11338.024135252179
Iteration 1900: Loss = -11337.792683856142
Iteration 2000: Loss = -11337.655673429754
Iteration 2100: Loss = -11337.53307797039
Iteration 2200: Loss = -11337.28959025781
Iteration 2300: Loss = -11335.780129746368
Iteration 2400: Loss = -11335.716404661149
Iteration 2500: Loss = -11335.693504258534
Iteration 2600: Loss = -11335.701425076157
1
Iteration 2700: Loss = -11335.668410048133
Iteration 2800: Loss = -11335.652271344266
Iteration 2900: Loss = -11335.643522286357
Iteration 3000: Loss = -11335.63803602116
Iteration 3100: Loss = -11335.641832694579
1
Iteration 3200: Loss = -11335.634917350486
Iteration 3300: Loss = -11335.63373366846
Iteration 3400: Loss = -11335.682339685893
1
Iteration 3500: Loss = -11335.631985940214
Iteration 3600: Loss = -11335.631273167932
Iteration 3700: Loss = -11335.63074493211
Iteration 3800: Loss = -11335.630039404452
Iteration 3900: Loss = -11335.62971083884
Iteration 4000: Loss = -11335.629113223711
Iteration 4100: Loss = -11335.628631737234
Iteration 4200: Loss = -11335.628314581747
Iteration 4300: Loss = -11335.629018852773
1
Iteration 4400: Loss = -11335.627707426194
Iteration 4500: Loss = -11335.642381993184
1
Iteration 4600: Loss = -11335.627216875717
Iteration 4700: Loss = -11335.627716732837
1
Iteration 4800: Loss = -11335.626802530789
Iteration 4900: Loss = -11335.626743078588
Iteration 5000: Loss = -11335.626502304289
Iteration 5100: Loss = -11335.626365867683
Iteration 5200: Loss = -11335.626832319565
1
Iteration 5300: Loss = -11335.626090868589
Iteration 5400: Loss = -11335.695191204386
1
Iteration 5500: Loss = -11335.62590782529
Iteration 5600: Loss = -11335.627795737659
1
Iteration 5700: Loss = -11335.625722511333
Iteration 5800: Loss = -11335.625647709883
Iteration 5900: Loss = -11335.635962537406
1
Iteration 6000: Loss = -11335.625505514723
Iteration 6100: Loss = -11335.625434085789
Iteration 6200: Loss = -11335.625504026426
1
Iteration 6300: Loss = -11335.62579867134
2
Iteration 6400: Loss = -11335.673818679634
3
Iteration 6500: Loss = -11335.625237010529
Iteration 6600: Loss = -11335.62517626652
Iteration 6700: Loss = -11335.62673934893
1
Iteration 6800: Loss = -11335.625085838705
Iteration 6900: Loss = -11335.703492078204
1
Iteration 7000: Loss = -11335.625038172891
Iteration 7100: Loss = -11335.65639871874
1
Iteration 7200: Loss = -11335.624998959318
Iteration 7300: Loss = -11335.654629519411
1
Iteration 7400: Loss = -11335.62623343516
2
Iteration 7500: Loss = -11335.646096478771
3
Iteration 7600: Loss = -11335.625238273544
4
Iteration 7700: Loss = -11335.659709975745
5
Iteration 7800: Loss = -11335.624976102841
Iteration 7900: Loss = -11335.624915562883
Iteration 8000: Loss = -11335.624835690813
Iteration 8100: Loss = -11335.62483553215
Iteration 8200: Loss = -11335.624791139919
Iteration 8300: Loss = -11335.64883251025
1
Iteration 8400: Loss = -11335.62476049297
Iteration 8500: Loss = -11335.624769291895
1
Iteration 8600: Loss = -11335.63496494267
2
Iteration 8700: Loss = -11335.624934380941
3
Iteration 8800: Loss = -11335.691817954583
4
Iteration 8900: Loss = -11335.624715266586
Iteration 9000: Loss = -11335.624711113369
Iteration 9100: Loss = -11335.625533921806
1
Iteration 9200: Loss = -11335.624683664997
Iteration 9300: Loss = -11335.626111259538
1
Iteration 9400: Loss = -11335.624647780744
Iteration 9500: Loss = -11335.761700701374
1
Iteration 9600: Loss = -11335.624666039805
2
Iteration 9700: Loss = -11335.624692747328
3
Iteration 9800: Loss = -11335.653043010321
4
Iteration 9900: Loss = -11335.624630194841
Iteration 10000: Loss = -11335.624631298086
1
Iteration 10100: Loss = -11335.670964823255
2
Iteration 10200: Loss = -11335.624619684968
Iteration 10300: Loss = -11335.624609030732
Iteration 10400: Loss = -11335.636217921845
1
Iteration 10500: Loss = -11335.624635746177
2
Iteration 10600: Loss = -11335.624595381323
Iteration 10700: Loss = -11335.627918284239
1
Iteration 10800: Loss = -11335.624658793748
2
Iteration 10900: Loss = -11335.624683794527
3
Iteration 11000: Loss = -11335.721093149785
4
Iteration 11100: Loss = -11335.634680996289
5
Iteration 11200: Loss = -11335.624651735228
6
Iteration 11300: Loss = -11335.625614326726
7
Iteration 11400: Loss = -11335.63476618672
8
Iteration 11500: Loss = -11335.627210500861
9
Iteration 11600: Loss = -11335.636456081764
10
Stopping early at iteration 11600 due to no improvement.
tensor([[-0.2891, -1.1529],
        [ 2.3172, -3.9632],
        [-3.8394,  2.4337],
        [-6.3496,  4.9604],
        [ 3.7549, -6.0356],
        [ 3.7981, -5.3212],
        [ 1.1015, -2.6660],
        [-7.8272,  3.2120],
        [ 5.2657, -7.6691],
        [-6.8204,  5.4340],
        [-5.0167,  2.9754],
        [ 2.1409, -3.7104],
        [ 3.8712, -5.2575],
        [-3.1155,  1.6364],
        [-5.9797,  4.5533],
        [ 6.5081, -8.5167],
        [ 5.0484, -6.9254],
        [-5.6334,  4.1247],
        [ 2.6388, -4.4550],
        [-3.1117,  1.6119],
        [ 1.4050, -4.4649],
        [-3.6409,  1.4442],
        [-0.0134, -1.4627],
        [ 1.3274, -2.9406],
        [-5.1591,  2.2393],
        [ 2.5744, -4.3152],
        [ 4.8320, -8.2574],
        [ 5.9861, -8.0529],
        [-8.0194,  5.3327],
        [-5.0930,  1.0636],
        [ 1.6644, -3.1639],
        [ 2.1457, -3.9887],
        [ 0.6344, -2.0270],
        [-6.5474,  4.7578],
        [-5.8252,  3.8750],
        [ 5.3006, -6.6883],
        [-1.3571, -0.8969],
        [ 4.5570, -5.9799],
        [-8.2946,  3.6794],
        [ 2.0170, -3.7927],
        [ 2.1130, -3.6385],
        [-6.4716,  3.7356],
        [-6.4353,  5.0245],
        [-4.0125,  2.1295],
        [ 6.9485, -9.2781],
        [-5.5868,  3.9048],
        [-7.3931,  5.2160],
        [ 3.4774, -4.9508],
        [-6.1334,  4.4830],
        [-5.5675,  4.0207],
        [-7.1448,  5.6658],
        [-6.1259,  4.6110],
        [-3.8195,  2.4130],
        [-2.3559,  0.9461],
        [ 3.0357, -4.9167],
        [ 2.8808, -4.9976],
        [-7.5465,  4.2314],
        [-6.9482,  4.9063],
        [ 0.7571, -2.8919],
        [-6.5799,  5.1147],
        [ 4.6794, -6.8255],
        [-4.2544,  2.0447],
        [-6.4608,  1.8456],
        [ 3.1610, -5.5584],
        [ 5.5259, -9.6609],
        [-3.1761,  0.1621],
        [-2.9405,  1.4048],
        [-4.3466,  2.9050],
        [ 5.4015, -8.9607],
        [ 5.1284, -6.7061],
        [ 2.6765, -4.1027],
        [ 2.9406, -4.4640],
        [ 6.0328, -7.4268],
        [ 3.9042, -5.3396],
        [-1.6493, -0.1700],
        [-6.3991,  3.9406],
        [ 7.0702, -8.6573],
        [-4.4624,  2.5418],
        [ 6.1465, -7.9864],
        [ 3.7477, -5.1607],
        [-6.6181,  5.1995],
        [ 3.3496, -4.7406],
        [ 1.7389, -3.9284],
        [-6.1225,  4.2079],
        [ 3.0372, -7.6524],
        [ 2.9239, -4.8095],
        [ 5.5347, -6.9364],
        [-6.5240,  4.5951],
        [-4.6465,  3.2538],
        [-2.6979,  1.2946],
        [ 4.1685, -6.5237],
        [ 6.9327, -8.4333],
        [-4.8946,  2.8306],
        [-4.1154,  2.1529],
        [-6.0057,  2.9424],
        [-4.5362,  3.0836],
        [-4.6386,  3.1119],
        [-3.4872,  2.1006],
        [ 3.0225, -4.6190],
        [-0.0989, -1.4596]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.0724, 0.9276],
        [0.0338, 0.9662]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4970, 0.5030], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3066, 0.0993],
         [0.8685, 0.1755]],

        [[0.3826, 0.0895],
         [0.6210, 0.3615]],

        [[0.7528, 0.2046],
         [0.0293, 0.3566]],

        [[0.5973, 0.0889],
         [0.2276, 0.4090]],

        [[0.1716, 0.2546],
         [0.6278, 0.8254]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.004166919759460609
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.008125898971292667
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
Global Adjusted Rand Index: 0.021951170905119322
Average Adjusted Rand Index: 0.20186588525812504
Iteration 0: Loss = -45467.79004646777
Iteration 10: Loss = -11381.894146006378
Iteration 20: Loss = -11144.380870395864
Iteration 30: Loss = -11139.578255583985
Iteration 40: Loss = -11139.578635019596
1
Iteration 50: Loss = -11139.578640489393
2
Iteration 60: Loss = -11139.578645675361
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7306, 0.2694],
        [0.2916, 0.7084]], dtype=torch.float64)
alpha: tensor([0.5149, 0.4851])
beta: tensor([[[0.2776, 0.0984],
         [0.0464, 0.2021]],

        [[0.9861, 0.1053],
         [0.0818, 0.3233]],

        [[0.4407, 0.1098],
         [0.2003, 0.6250]],

        [[0.9029, 0.0875],
         [0.7007, 0.5140]],

        [[0.6454, 0.0983],
         [0.9844, 0.0788]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 1
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 2
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208
time is 3
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
Global Adjusted Rand Index: 0.9214427761527915
Average Adjusted Rand Index: 0.9212892570982858
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -45463.25319473019
Iteration 100: Loss = -11409.777210154276
Iteration 200: Loss = -11278.108123933102
Iteration 300: Loss = -11231.793202306473
Iteration 400: Loss = -11225.452862527836
Iteration 500: Loss = -11214.56717946796
Iteration 600: Loss = -11211.808989530375
Iteration 700: Loss = -11211.586192520965
Iteration 800: Loss = -11210.406342458065
Iteration 900: Loss = -11205.047971622753
Iteration 1000: Loss = -11204.833940806502
Iteration 1100: Loss = -11204.74484071594
Iteration 1200: Loss = -11204.614586239737
Iteration 1300: Loss = -11204.589305767873
Iteration 1400: Loss = -11204.577430469211
Iteration 1500: Loss = -11204.567690882137
Iteration 1600: Loss = -11204.55629586438
Iteration 1700: Loss = -11204.542483043953
Iteration 1800: Loss = -11204.537174541258
Iteration 1900: Loss = -11204.532607914209
Iteration 2000: Loss = -11204.52845908935
Iteration 2100: Loss = -11204.524507741136
Iteration 2200: Loss = -11204.520684553154
Iteration 2300: Loss = -11204.517396800898
Iteration 2400: Loss = -11204.514679557671
Iteration 2500: Loss = -11204.51195592998
Iteration 2600: Loss = -11204.509058132202
Iteration 2700: Loss = -11204.506959600247
Iteration 2800: Loss = -11204.504998234273
Iteration 2900: Loss = -11204.500399954473
Iteration 3000: Loss = -11204.491271091008
Iteration 3100: Loss = -11204.489638173842
Iteration 3200: Loss = -11204.48418885253
Iteration 3300: Loss = -11204.473165971182
Iteration 3400: Loss = -11204.471191959678
Iteration 3500: Loss = -11204.435632438232
Iteration 3600: Loss = -11204.104296722
Iteration 3700: Loss = -11204.101283363036
Iteration 3800: Loss = -11204.100159487765
Iteration 3900: Loss = -11204.099051597213
Iteration 4000: Loss = -11204.097958561784
Iteration 4100: Loss = -11204.097215046953
Iteration 4200: Loss = -11204.096718570054
Iteration 4300: Loss = -11204.101301186796
1
Iteration 4400: Loss = -11204.09602838141
Iteration 4500: Loss = -11204.095612290583
Iteration 4600: Loss = -11204.085562608709
Iteration 4700: Loss = -11204.089525660882
1
Iteration 4800: Loss = -11204.085115860624
Iteration 4900: Loss = -11204.084637411906
Iteration 5000: Loss = -11204.083210663845
Iteration 5100: Loss = -11204.082176144866
Iteration 5200: Loss = -11204.081957905782
Iteration 5300: Loss = -11204.081769007747
Iteration 5400: Loss = -11204.082313728826
1
Iteration 5500: Loss = -11204.082102174089
2
Iteration 5600: Loss = -11204.089010403372
3
Iteration 5700: Loss = -11204.084185590886
4
Iteration 5800: Loss = -11204.08456290164
5
Iteration 5900: Loss = -11204.080911386754
Iteration 6000: Loss = -11204.080850776343
Iteration 6100: Loss = -11204.080737890603
Iteration 6200: Loss = -11204.082041252645
1
Iteration 6300: Loss = -11204.084476892722
2
Iteration 6400: Loss = -11204.085536056042
3
Iteration 6500: Loss = -11204.086697108865
4
Iteration 6600: Loss = -11204.080635004966
Iteration 6700: Loss = -11204.07900550448
Iteration 6800: Loss = -11204.077527869078
Iteration 6900: Loss = -11204.078276247816
1
Iteration 7000: Loss = -11204.083612359265
2
Iteration 7100: Loss = -11204.079358383613
3
Iteration 7200: Loss = -11204.083909973528
4
Iteration 7300: Loss = -11204.084950700286
5
Iteration 7400: Loss = -11204.087869910869
6
Iteration 7500: Loss = -11204.086474875112
7
Iteration 7600: Loss = -11204.077520143246
Iteration 7700: Loss = -11204.079690527053
1
Iteration 7800: Loss = -11204.077666747718
2
Iteration 7900: Loss = -11204.080704511856
3
Iteration 8000: Loss = -11204.076061326648
Iteration 8100: Loss = -11204.077779433537
1
Iteration 8200: Loss = -11204.0772582703
2
Iteration 8300: Loss = -11204.07602822864
Iteration 8400: Loss = -11203.871923087001
Iteration 8500: Loss = -11203.878347835422
1
Iteration 8600: Loss = -11203.871827776456
Iteration 8700: Loss = -11203.871661659135
Iteration 8800: Loss = -11203.872872845775
1
Iteration 8900: Loss = -11203.871284763814
Iteration 9000: Loss = -11203.871477597273
1
Iteration 9100: Loss = -11203.874710764658
2
Iteration 9200: Loss = -11203.870478461653
Iteration 9300: Loss = -11203.88842830155
1
Iteration 9400: Loss = -11203.87049583745
2
Iteration 9500: Loss = -11203.87049943207
3
Iteration 9600: Loss = -11203.88153535652
4
Iteration 9700: Loss = -11203.869332702443
Iteration 9800: Loss = -11203.87038634315
1
Iteration 9900: Loss = -11203.810488503303
Iteration 10000: Loss = -11203.802490374073
Iteration 10100: Loss = -11203.820416474493
1
Iteration 10200: Loss = -11203.799252006616
Iteration 10300: Loss = -11203.847084907316
1
Iteration 10400: Loss = -11203.798702551148
Iteration 10500: Loss = -11203.798313133067
Iteration 10600: Loss = -11203.844838294199
1
Iteration 10700: Loss = -11203.875392826336
2
Iteration 10800: Loss = -11197.822475452333
Iteration 10900: Loss = -11197.82188584753
Iteration 11000: Loss = -11197.904622337881
1
Iteration 11100: Loss = -11197.828060040516
2
Iteration 11200: Loss = -11197.850771214695
3
Iteration 11300: Loss = -11197.82126413114
Iteration 11400: Loss = -11197.823354899494
1
Iteration 11500: Loss = -11197.888530250286
2
Iteration 11600: Loss = -11197.862776652077
3
Iteration 11700: Loss = -11197.831740880456
4
Iteration 11800: Loss = -11197.821432038141
5
Iteration 11900: Loss = -11197.821640069973
6
Iteration 12000: Loss = -11197.827679398111
7
Iteration 12100: Loss = -11197.92666651131
8
Iteration 12200: Loss = -11197.87557875654
9
Iteration 12300: Loss = -11197.82621718179
10
Stopping early at iteration 12300 due to no improvement.
tensor([[-1.2490, -0.5288],
        [-4.8808,  3.3262],
        [ 1.9307, -3.4147],
        [ 4.4188, -6.5585],
        [-6.4594,  4.3183],
        [-6.6690,  4.0783],
        [-5.4096,  2.7583],
        [ 3.4789, -5.0368],
        [-7.1125,  4.7256],
        [ 4.6283, -6.3319],
        [ 1.7691, -3.6489],
        [-3.7774,  1.9976],
        [-7.3858,  4.2352],
        [-0.6686, -1.0717],
        [ 3.3054, -5.7936],
        [-8.9342,  7.3878],
        [-9.5348,  6.6919],
        [ 2.6353, -4.0989],
        [-6.2228,  4.6017],
        [ 0.4252, -1.9259],
        [-4.7705,  2.7953],
        [-1.5619, -0.0870],
        [-3.1514,  1.7631],
        [-4.1828,  2.4797],
        [ 2.4332, -5.5650],
        [-6.2213,  3.0461],
        [-8.0068,  6.5441],
        [-8.8427,  5.7122],
        [ 7.1073, -8.5346],
        [ 3.3846, -5.3782],
        [-4.3063,  2.4534],
        [-4.4297,  2.5833],
        [-3.5801,  2.1713],
        [ 3.8764, -7.9456],
        [ 1.5064, -6.1217],
        [-6.4968,  1.8815],
        [-2.5000,  0.7002],
        [-7.6485,  6.2470],
        [ 4.4795, -5.8965],
        [-5.2677,  3.2124],
        [-4.6466,  1.9984],
        [ 2.8846, -4.3509],
        [ 4.3511, -5.8202],
        [ 0.7983, -2.1846],
        [-9.7886,  7.0468],
        [ 3.4928, -5.4990],
        [ 5.4114, -6.8680],
        [-6.2006,  4.6265],
        [ 2.4583, -3.8472],
        [ 2.5409, -3.9441],
        [ 4.1955, -7.2602],
        [ 4.8025, -6.2566],
        [ 0.9631, -2.5662],
        [-2.7645, -0.4966],
        [-6.6610,  4.5989],
        [-6.2363,  4.6765],
        [ 4.9617, -6.3517],
        [ 4.6209, -7.6410],
        [-3.3155,  1.9230],
        [ 2.9464, -4.8487],
        [-7.1900,  5.7745],
        [ 0.9247, -3.2981],
        [ 2.8614, -4.5837],
        [-6.0036,  4.5956],
        [-8.5502,  6.8703],
        [-2.6360,  1.2082],
        [-1.5001, -1.2091],
        [ 0.8976, -3.2794],
        [-8.3049,  6.5529],
        [-7.6867,  5.7489],
        [-7.8740,  4.4607],
        [-5.8964,  4.4379],
        [-8.6323,  6.7931],
        [-7.0091,  5.3942],
        [-0.4934, -1.2245],
        [ 4.1068, -5.8559],
        [-9.6463,  7.7258],
        [ 2.6577, -4.0685],
        [-9.4208,  7.5845],
        [-5.9711,  3.7548],
        [ 4.4713, -6.6726],
        [-5.4696,  3.3604],
        [-5.5741,  3.9407],
        [ 6.4539, -8.0813],
        [-8.9901,  5.1935],
        [-5.4917,  4.0522],
        [-8.4512,  6.7690],
        [ 4.8206, -6.2351],
        [ 2.2027, -3.5997],
        [ 1.6875, -3.6221],
        [-7.3842,  4.2000],
        [-8.5997,  7.1961],
        [ 2.2522, -4.0095],
        [-1.8913, -1.8582],
        [ 2.9573, -4.5426],
        [ 1.9167, -4.6022],
        [ 1.1787, -4.7530],
        [-0.8412, -3.7740],
        [-5.4271,  3.8478],
        [-1.7998,  0.1594]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.4385, 0.5615],
        [0.6280, 0.3720]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4475, 0.5525], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2333, 0.0964],
         [0.0464, 0.2622]],

        [[0.9861, 0.1024],
         [0.0818, 0.3233]],

        [[0.4407, 0.1099],
         [0.2003, 0.6250]],

        [[0.9029, 0.0879],
         [0.7007, 0.5140]],

        [[0.6454, 0.0956],
         [0.9844, 0.0788]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 94
Adjusted Rand Index: 0.7721314419105764
time is 1
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 6
Adjusted Rand Index: 0.7721426378272603
time is 2
tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824127248460211
time is 3
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 4
tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448573745636614
Global Adjusted Rand Index: 0.03648730285905462
Average Adjusted Rand Index: 0.8384689625484901
11156.275019948027
new:  [0.021951170905119322, 0.936897667120417, 0.021951170905119322, 0.03648730285905462] [0.20186588525812504, 0.936642795729521, 0.20186588525812504, 0.8384689625484901] [11335.62596034305, 11136.160604340168, 11335.636456081764, 11197.82621718179]
prior:  [0.9214427761527915, 0.9214427761527915, 0.9214427761527915, 0.9214427761527915] [0.9212892570982858, 0.9212892570982858, 0.9212892570982858, 0.9212892570982858] [11139.578648592988, 11139.57863665489, 11139.578645183807, 11139.578645675361]
-----------------------------------------------------------------------------------------
This iteration is 6
True Objective function: Loss = -11073.925390049195
Iteration 0: Loss = -26450.94299415214
Iteration 10: Loss = -11288.111703063216
Iteration 20: Loss = -11067.675017450492
Iteration 30: Loss = -11043.007079683342
Iteration 40: Loss = -11043.006991341075
Iteration 50: Loss = -11043.006978746096
Iteration 60: Loss = -11043.006978745381
Iteration 70: Loss = -11043.00697874538
Iteration 80: Loss = -11043.00697874538
1
Iteration 90: Loss = -11043.00697874538
2
Iteration 100: Loss = -11043.00697874538
3
Stopping early at iteration 99 due to no improvement.
pi: tensor([[0.6922, 0.3078],
        [0.2726, 0.7274]], dtype=torch.float64)
alpha: tensor([0.4563, 0.5437])
beta: tensor([[[0.2910, 0.1015],
         [0.7978, 0.2015]],

        [[0.0521, 0.0991],
         [0.8335, 0.1031]],

        [[0.2860, 0.0899],
         [0.8176, 0.4288]],

        [[0.0754, 0.0952],
         [0.0477, 0.7426]],

        [[0.2672, 0.1026],
         [0.4706, 0.0877]]], dtype=torch.float64)
time is 0
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9207702484198148
time is 1
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 2
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448387581811067
time is 3
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
Global Adjusted Rand Index: 0.9061114589584036
Average Adjusted Rand Index: 0.905767827130212
tensor([[-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26312.57422767121
Iteration 100: Loss = -11303.995825631811
Iteration 200: Loss = -11302.830769827742
Iteration 300: Loss = -11302.319937945398
Iteration 400: Loss = -11302.049173499256
Iteration 500: Loss = -11301.87720193968
Iteration 600: Loss = -11301.755402706187
Iteration 700: Loss = -11301.668106101964
Iteration 800: Loss = -11301.60803241491
Iteration 900: Loss = -11301.56803215393
Iteration 1000: Loss = -11301.541218419581
Iteration 1100: Loss = -11301.521861662644
Iteration 1200: Loss = -11301.506806080495
Iteration 1300: Loss = -11301.494556530391
Iteration 1400: Loss = -11301.484300858534
Iteration 1500: Loss = -11301.475597706678
Iteration 1600: Loss = -11301.46810830459
Iteration 1700: Loss = -11301.461627969618
Iteration 1800: Loss = -11301.45592847027
Iteration 1900: Loss = -11301.450892790488
Iteration 2000: Loss = -11301.446429828788
Iteration 2100: Loss = -11301.442451235083
Iteration 2200: Loss = -11301.438892396645
Iteration 2300: Loss = -11301.435632358854
Iteration 2400: Loss = -11301.432723615324
Iteration 2500: Loss = -11301.4300711071
Iteration 2600: Loss = -11301.427638228508
Iteration 2700: Loss = -11301.42541802208
Iteration 2800: Loss = -11301.4233971555
Iteration 2900: Loss = -11301.421530214267
Iteration 3000: Loss = -11301.419769468364
Iteration 3100: Loss = -11301.41812889169
Iteration 3200: Loss = -11301.416613453155
Iteration 3300: Loss = -11301.415179360556
Iteration 3400: Loss = -11301.41384078895
Iteration 3500: Loss = -11301.412571757288
Iteration 3600: Loss = -11301.411299391135
Iteration 3700: Loss = -11301.410117478408
Iteration 3800: Loss = -11301.408947196289
Iteration 3900: Loss = -11301.407778346242
Iteration 4000: Loss = -11301.406615423937
Iteration 4100: Loss = -11301.405409301098
Iteration 4200: Loss = -11301.404205376048
Iteration 4300: Loss = -11301.402874403693
Iteration 4400: Loss = -11301.40392719347
1
Iteration 4500: Loss = -11301.399683372418
Iteration 4600: Loss = -11301.397616096707
Iteration 4700: Loss = -11301.395045741132
Iteration 4800: Loss = -11301.3947560859
Iteration 4900: Loss = -11301.387131252528
Iteration 5000: Loss = -11301.38099150809
Iteration 5100: Loss = -11301.373238649274
Iteration 5200: Loss = -11301.36593733639
Iteration 5300: Loss = -11301.35924605592
Iteration 5400: Loss = -11301.354858769584
Iteration 5500: Loss = -11301.350875532196
Iteration 5600: Loss = -11301.346787971865
Iteration 5700: Loss = -11301.342082863253
Iteration 5800: Loss = -11301.336157147221
Iteration 5900: Loss = -11301.327457960986
Iteration 6000: Loss = -11301.307484419633
Iteration 6100: Loss = -11300.128108496447
Iteration 6200: Loss = -11298.1633459932
Iteration 6300: Loss = -11297.60283910644
Iteration 6400: Loss = -11297.559549973741
Iteration 6500: Loss = -11297.543375585616
Iteration 6600: Loss = -11297.535300020327
Iteration 6700: Loss = -11297.530101892638
Iteration 6800: Loss = -11297.526912995807
Iteration 6900: Loss = -11297.523789932626
Iteration 7000: Loss = -11297.521723974785
Iteration 7100: Loss = -11297.570433328316
1
Iteration 7200: Loss = -11297.518722118617
Iteration 7300: Loss = -11297.51758871828
Iteration 7400: Loss = -11297.516696829365
Iteration 7500: Loss = -11297.560984046917
1
Iteration 7600: Loss = -11297.515190983378
Iteration 7700: Loss = -11297.514568822598
Iteration 7800: Loss = -11297.514064459128
Iteration 7900: Loss = -11297.51691735437
1
Iteration 8000: Loss = -11297.513176399498
Iteration 8100: Loss = -11297.512804056156
Iteration 8200: Loss = -11297.512455000597
Iteration 8300: Loss = -11297.51376236912
1
Iteration 8400: Loss = -11297.511918743352
Iteration 8500: Loss = -11297.511660826603
Iteration 8600: Loss = -11297.51145876597
Iteration 8700: Loss = -11297.941740334676
1
Iteration 8800: Loss = -11297.511088323616
Iteration 8900: Loss = -11297.510942335492
Iteration 9000: Loss = -11297.51080127782
Iteration 9100: Loss = -11297.510857060339
1
Iteration 9200: Loss = -11297.510527747863
Iteration 9300: Loss = -11297.510428414027
Iteration 9400: Loss = -11297.510315069805
Iteration 9500: Loss = -11297.510282887995
Iteration 9600: Loss = -11297.51010017122
Iteration 9700: Loss = -11297.510029413304
Iteration 9800: Loss = -11297.509951623828
Iteration 9900: Loss = -11297.509873194898
Iteration 10000: Loss = -11297.509772391075
Iteration 10100: Loss = -11297.509716905251
Iteration 10200: Loss = -11297.509694301787
Iteration 10300: Loss = -11297.5096115558
Iteration 10400: Loss = -11297.509529845725
Iteration 10500: Loss = -11297.509469336894
Iteration 10600: Loss = -11297.536176657854
1
Iteration 10700: Loss = -11297.509343309157
Iteration 10800: Loss = -11297.50928454409
Iteration 10900: Loss = -11297.50926439836
Iteration 11000: Loss = -11297.509632368865
1
Iteration 11100: Loss = -11297.509143221429
Iteration 11200: Loss = -11297.509101374375
Iteration 11300: Loss = -11297.50909086917
Iteration 11400: Loss = -11297.509622477188
1
Iteration 11500: Loss = -11297.50901771022
Iteration 11600: Loss = -11297.508973740894
Iteration 11700: Loss = -11297.508951514299
Iteration 11800: Loss = -11297.513134293833
1
Iteration 11900: Loss = -11297.508898955894
Iteration 12000: Loss = -11297.508885162657
Iteration 12100: Loss = -11297.508880367894
Iteration 12200: Loss = -11297.510027131812
1
Iteration 12300: Loss = -11297.508848151396
Iteration 12400: Loss = -11297.508827424206
Iteration 12500: Loss = -11297.508827571968
1
Iteration 12600: Loss = -11297.740041472625
2
Iteration 12700: Loss = -11297.508792954874
Iteration 12800: Loss = -11297.508777886444
Iteration 12900: Loss = -11297.508890450757
1
Iteration 13000: Loss = -11297.508758738948
Iteration 13100: Loss = -11297.516403469739
1
Iteration 13200: Loss = -11297.510803132913
2
Iteration 13300: Loss = -11297.508695515482
Iteration 13400: Loss = -11297.5109567
1
Iteration 13500: Loss = -11297.508644219148
Iteration 13600: Loss = -11297.508687841335
1
Iteration 13700: Loss = -11297.50893868416
2
Iteration 13800: Loss = -11297.510887212029
3
Iteration 13900: Loss = -11297.795268176931
4
Iteration 14000: Loss = -11297.508644522772
5
Iteration 14100: Loss = -11297.50863197388
Iteration 14200: Loss = -11297.508830894916
1
Iteration 14300: Loss = -11297.508626789415
Iteration 14400: Loss = -11297.623648069732
1
Iteration 14500: Loss = -11297.508607860611
Iteration 14600: Loss = -11297.510693661447
1
Iteration 14700: Loss = -11297.508832530208
2
Iteration 14800: Loss = -11297.508587218308
Iteration 14900: Loss = -11297.510129875385
1
Iteration 15000: Loss = -11297.50864301182
2
Iteration 15100: Loss = -11297.509227211931
3
Iteration 15200: Loss = -11297.508597111515
4
Iteration 15300: Loss = -11297.50911209621
5
Iteration 15400: Loss = -11297.508556020108
Iteration 15500: Loss = -11297.508718793579
1
Iteration 15600: Loss = -11297.509014491921
2
Iteration 15700: Loss = -11297.508555151227
Iteration 15800: Loss = -11297.584722152
1
Iteration 15900: Loss = -11297.508544167593
Iteration 16000: Loss = -11297.509365532233
1
Iteration 16100: Loss = -11297.508599643355
2
Iteration 16200: Loss = -11297.50852897168
Iteration 16300: Loss = -11297.508556509409
1
Iteration 16400: Loss = -11297.50852034767
Iteration 16500: Loss = -11297.510423704136
1
Iteration 16600: Loss = -11297.508521165848
2
Iteration 16700: Loss = -11297.508555752649
3
Iteration 16800: Loss = -11297.508623516262
4
Iteration 16900: Loss = -11297.508509305026
Iteration 17000: Loss = -11297.530719766479
1
Iteration 17100: Loss = -11297.50852183022
2
Iteration 17200: Loss = -11297.50852274921
3
Iteration 17300: Loss = -11297.508507855751
Iteration 17400: Loss = -11297.50955014945
1
Iteration 17500: Loss = -11297.50850889858
2
Iteration 17600: Loss = -11297.510346337614
3
Iteration 17700: Loss = -11297.527406574061
4
Iteration 17800: Loss = -11297.508507406927
Iteration 17900: Loss = -11297.510327412732
1
Iteration 18000: Loss = -11297.50855454529
2
Iteration 18100: Loss = -11297.50851111751
3
Iteration 18200: Loss = -11297.508619368758
4
Iteration 18300: Loss = -11297.508512905879
5
Iteration 18400: Loss = -11297.53257417144
6
Iteration 18500: Loss = -11297.508492744992
Iteration 18600: Loss = -11297.508484389904
Iteration 18700: Loss = -11297.508633676018
1
Iteration 18800: Loss = -11297.508493909958
2
Iteration 18900: Loss = -11297.535305603145
3
Iteration 19000: Loss = -11297.508492588513
4
Iteration 19100: Loss = -11297.520211283152
5
Iteration 19200: Loss = -11297.508497490053
6
Iteration 19300: Loss = -11297.5749738554
7
Iteration 19400: Loss = -11297.508485777433
8
Iteration 19500: Loss = -11297.5109734104
9
Iteration 19600: Loss = -11297.508623963386
10
Stopping early at iteration 19600 due to no improvement.
tensor([[-11.3574,   6.7422],
        [-12.3614,   7.7462],
        [ -5.4089,   0.7937],
        [-12.0323,   7.4171],
        [-11.4952,   6.8800],
        [ -8.6833,   4.0681],
        [-10.6759,   6.0607],
        [-11.6369,   7.0217],
        [-11.1961,   6.5809],
        [-11.1409,   6.5256],
        [-10.8674,   6.2521],
        [ -0.0186,  -4.5966],
        [-11.9767,   7.3614],
        [-10.5616,   5.9464],
        [-12.4506,   7.8354],
        [-10.6874,   6.0722],
        [-11.5968,   6.9816],
        [-11.5988,   6.9836],
        [ -6.7880,   2.1728],
        [-10.4704,   5.8552],
        [-10.4062,   5.7909],
        [-12.2453,   7.6301],
        [-11.5357,   6.9205],
        [-10.7214,   6.1061],
        [-10.6662,   6.0510],
        [ -7.0007,   2.3855],
        [ -9.1069,   4.4917],
        [-11.1775,   6.5623],
        [-10.9098,   6.2946],
        [-11.2144,   6.5992],
        [-10.5388,   5.9236],
        [-11.0876,   6.4724],
        [ -5.3351,   0.7199],
        [-11.3873,   6.7721],
        [-10.9998,   6.3846],
        [-11.3022,   6.6870],
        [-11.0491,   6.4338],
        [-11.0788,   6.4635],
        [ -9.5579,   4.9427],
        [-12.0897,   7.4745],
        [-10.8165,   6.2013],
        [-11.5323,   6.9171],
        [-11.0165,   6.4013],
        [-10.6269,   6.0117],
        [ -9.9318,   5.3166],
        [-10.9432,   6.3280],
        [-10.9265,   6.3113],
        [-11.1301,   6.5148],
        [-10.9321,   6.3169],
        [-10.9808,   6.3656],
        [-12.2804,   7.6652],
        [ -4.5639,  -0.0513],
        [-11.3012,   6.6860],
        [-11.0694,   6.4542],
        [-11.1662,   6.5510],
        [-11.9119,   7.2967],
        [-10.9598,   6.3446],
        [-10.8493,   6.2341],
        [-10.8873,   6.2720],
        [-11.3999,   6.7847],
        [-10.8097,   6.1945],
        [ -4.9347,   0.3195],
        [ -6.0316,   1.4164],
        [ -5.7456,   1.1303],
        [ -7.1727,   2.5575],
        [-11.3335,   6.7182],
        [-10.9800,   6.3648],
        [-12.2720,   7.6568],
        [-12.0704,   7.4552],
        [-10.7321,   6.1169],
        [ -7.8475,   3.2323],
        [-10.8428,   6.2276],
        [-12.0622,   7.4469],
        [-11.1862,   6.5710],
        [ -7.5907,   2.9755],
        [-11.2104,   6.5952],
        [-10.7199,   6.1046],
        [-11.0904,   6.4752],
        [-10.9638,   6.3486],
        [-10.7816,   6.1664],
        [-10.5042,   5.8890],
        [-11.0801,   6.4649],
        [-10.2405,   5.6253],
        [-10.8887,   6.2735],
        [-11.0334,   6.4182],
        [-10.7230,   6.1078],
        [-12.0199,   7.4046],
        [-11.0901,   6.4749],
        [-10.5048,   5.8896],
        [ -0.0800,  -4.5352],
        [ -7.1433,   2.5281],
        [-11.0567,   6.4415],
        [-11.0844,   6.4692],
        [ -2.0158,  -2.5994],
        [-10.1003,   5.4851],
        [-10.9816,   6.3664],
        [ -5.6675,   1.0523],
        [-12.3274,   7.7122],
        [ -8.9694,   4.3542],
        [-10.7524,   6.1372]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 5.0932e-07],
        [1.7229e-09, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0264, 0.9736], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4812, 0.1109],
         [0.7978, 0.1709]],

        [[0.0521, 0.2569],
         [0.8335, 0.1031]],

        [[0.2860, 0.1340],
         [0.8176, 0.4288]],

        [[0.0754, 0.1458],
         [0.0477, 0.7426]],

        [[0.2672, 0.1438],
         [0.4706, 0.0877]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.012378759859606748
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.01575757575757576
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.002764517368444376
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.002764517368444376
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.01575757575757576
Global Adjusted Rand Index: 0.005643475871484754
Average Adjusted Rand Index: 0.007672975327573902
Iteration 0: Loss = -32520.112897960305
Iteration 10: Loss = -11301.655221396213
Iteration 20: Loss = -11301.655221396373
1
Iteration 30: Loss = -11301.655221682386
2
Iteration 40: Loss = -11301.655337655246
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[1.0000e+00, 2.6000e-08],
        [1.0000e+00, 4.6381e-22]], dtype=torch.float64)
alpha: tensor([1.0000e+00, 2.3210e-08])
beta: tensor([[[0.1688, 0.1222],
         [0.0313, 0.1173]],

        [[0.0657, 0.0695],
         [0.0145, 0.7483]],

        [[0.7005, 0.2189],
         [0.0660, 0.2624]],

        [[0.2222, 0.1787],
         [0.3226, 0.3542]],

        [[0.4678, 0.2895],
         [0.0080, 0.9142]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -32521.265081844584
Iteration 100: Loss = -11379.243466370093
Iteration 200: Loss = -11345.284979790122
Iteration 300: Loss = -11331.640772952687
Iteration 400: Loss = -11318.765947706726
Iteration 500: Loss = -11309.377998562299
Iteration 600: Loss = -11303.707826635458
Iteration 700: Loss = -11299.036862260462
Iteration 800: Loss = -11296.988329754493
Iteration 900: Loss = -11294.731551210023
Iteration 1000: Loss = -11292.507277807008
Iteration 1100: Loss = -11291.99019750621
Iteration 1200: Loss = -11291.650338643662
Iteration 1300: Loss = -11291.416381271876
Iteration 1400: Loss = -11291.233246277272
Iteration 1500: Loss = -11291.06939906393
Iteration 1600: Loss = -11290.955942052677
Iteration 1700: Loss = -11290.86096394443
Iteration 1800: Loss = -11290.780197964832
Iteration 1900: Loss = -11290.710964065682
Iteration 2000: Loss = -11290.650964830911
Iteration 2100: Loss = -11290.598661816959
Iteration 2200: Loss = -11290.552657343553
Iteration 2300: Loss = -11290.511993452606
Iteration 2400: Loss = -11290.47584651616
Iteration 2500: Loss = -11290.443564284573
Iteration 2600: Loss = -11290.414579412685
Iteration 2700: Loss = -11290.388569184375
Iteration 2800: Loss = -11290.365022950326
Iteration 2900: Loss = -11290.343710720754
Iteration 3000: Loss = -11290.324303427324
Iteration 3100: Loss = -11290.306691655915
Iteration 3200: Loss = -11290.290528787735
Iteration 3300: Loss = -11290.275749893384
Iteration 3400: Loss = -11290.262190839972
Iteration 3500: Loss = -11290.249665411397
Iteration 3600: Loss = -11290.238162239742
Iteration 3700: Loss = -11290.22751182153
Iteration 3800: Loss = -11290.217656098546
Iteration 3900: Loss = -11290.20858301958
Iteration 4000: Loss = -11290.200118182449
Iteration 4100: Loss = -11290.192250324671
Iteration 4200: Loss = -11290.189943730586
Iteration 4300: Loss = -11290.178154105091
Iteration 4400: Loss = -11290.171801459497
Iteration 4500: Loss = -11290.165937040287
Iteration 4600: Loss = -11290.16044529172
Iteration 4700: Loss = -11290.155205194167
Iteration 4800: Loss = -11290.150388450824
Iteration 4900: Loss = -11290.162104381825
1
Iteration 5000: Loss = -11290.141656063093
Iteration 5100: Loss = -11290.137667947673
Iteration 5200: Loss = -11290.133990479077
Iteration 5300: Loss = -11290.130910079528
Iteration 5400: Loss = -11290.127191438101
Iteration 5500: Loss = -11290.124128811643
Iteration 5600: Loss = -11290.121276288895
Iteration 5700: Loss = -11290.118461708731
Iteration 5800: Loss = -11290.115911275325
Iteration 5900: Loss = -11290.113917577117
Iteration 6000: Loss = -11290.111258227322
Iteration 6100: Loss = -11290.109106067266
Iteration 6200: Loss = -11290.168419179841
1
Iteration 6300: Loss = -11290.105221039325
Iteration 6400: Loss = -11290.103475669603
Iteration 6500: Loss = -11290.105202574045
1
Iteration 6600: Loss = -11290.100178728046
Iteration 6700: Loss = -11290.098676570764
Iteration 6800: Loss = -11290.097222429924
Iteration 6900: Loss = -11290.095859846422
Iteration 7000: Loss = -11290.098600405983
1
Iteration 7100: Loss = -11290.09355220707
Iteration 7200: Loss = -11290.092346280997
Iteration 7300: Loss = -11290.093051405358
1
Iteration 7400: Loss = -11290.09898624025
2
Iteration 7500: Loss = -11290.09410065552
3
Iteration 7600: Loss = -11290.088376193336
Iteration 7700: Loss = -11290.108626900635
1
Iteration 7800: Loss = -11290.086817417241
Iteration 7900: Loss = -11290.091364962882
1
Iteration 8000: Loss = -11290.0852484372
Iteration 8100: Loss = -11290.091152612014
1
Iteration 8200: Loss = -11290.08395691667
Iteration 8300: Loss = -11290.083434271437
Iteration 8400: Loss = -11290.08390035583
1
Iteration 8500: Loss = -11290.08273894457
Iteration 8600: Loss = -11290.081790827777
Iteration 8700: Loss = -11290.08397364797
1
Iteration 8800: Loss = -11290.080866244956
Iteration 8900: Loss = -11290.152422851164
1
Iteration 9000: Loss = -11290.080087576756
Iteration 9100: Loss = -11290.096839859958
1
Iteration 9200: Loss = -11290.078119089345
Iteration 9300: Loss = -11290.079244772263
1
Iteration 9400: Loss = -11290.077309992996
Iteration 9500: Loss = -11290.079532534653
1
Iteration 9600: Loss = -11290.080836933019
2
Iteration 9700: Loss = -11290.07952672069
3
Iteration 9800: Loss = -11290.075686232783
Iteration 9900: Loss = -11290.0748501993
Iteration 10000: Loss = -11290.077533561507
1
Iteration 10100: Loss = -11290.057943266149
Iteration 10200: Loss = -11289.946044674121
Iteration 10300: Loss = -11289.717396499835
Iteration 10400: Loss = -11289.554578622745
Iteration 10500: Loss = -11289.519164121704
Iteration 10600: Loss = -11289.434779318914
Iteration 10700: Loss = -11289.428175836327
Iteration 10800: Loss = -11289.407662112928
Iteration 10900: Loss = -11289.391670591402
Iteration 11000: Loss = -11289.362573579823
Iteration 11100: Loss = -11289.379114431345
1
Iteration 11200: Loss = -11289.308229108867
Iteration 11300: Loss = -11289.308934970979
1
Iteration 11400: Loss = -11289.26020577623
Iteration 11500: Loss = -11289.21862129354
Iteration 11600: Loss = -11289.203867452099
Iteration 11700: Loss = -11289.189037572576
Iteration 11800: Loss = -11289.174279796222
Iteration 11900: Loss = -11289.189929635173
1
Iteration 12000: Loss = -11289.150558457073
Iteration 12100: Loss = -11289.144712548472
Iteration 12200: Loss = -11289.136985711913
Iteration 12300: Loss = -11289.132845431732
Iteration 12400: Loss = -11289.20476802662
1
Iteration 12500: Loss = -11289.124963106919
Iteration 12600: Loss = -11289.126601584721
1
Iteration 12700: Loss = -11289.121338218416
Iteration 12800: Loss = -11289.119332728294
Iteration 12900: Loss = -11289.119652839794
1
Iteration 13000: Loss = -11289.128131572608
2
Iteration 13100: Loss = -11289.116850238239
Iteration 13200: Loss = -11289.116351338367
Iteration 13300: Loss = -11289.131035985998
1
Iteration 13400: Loss = -11289.115948361716
Iteration 13500: Loss = -11289.140743844457
1
Iteration 13600: Loss = -11289.115608003136
Iteration 13700: Loss = -11289.224950162332
1
Iteration 13800: Loss = -11289.115358010173
Iteration 13900: Loss = -11289.115250363855
Iteration 14000: Loss = -11289.118439335112
1
Iteration 14100: Loss = -11289.115121528865
Iteration 14200: Loss = -11289.115053228816
Iteration 14300: Loss = -11289.115335770404
1
Iteration 14400: Loss = -11289.114939496561
Iteration 14500: Loss = -11289.132927787774
1
Iteration 14600: Loss = -11289.114733192278
Iteration 14700: Loss = -11289.115040684605
1
Iteration 14800: Loss = -11289.145993316113
2
Iteration 14900: Loss = -11289.142090651929
3
Iteration 15000: Loss = -11289.113490283224
Iteration 15100: Loss = -11289.11274002784
Iteration 15200: Loss = -11289.176945027953
1
Iteration 15300: Loss = -11289.112899559326
2
Iteration 15400: Loss = -11289.112219729843
Iteration 15500: Loss = -11289.120076158428
1
Iteration 15600: Loss = -11289.116430419563
2
Iteration 15700: Loss = -11289.113007785545
3
Iteration 15800: Loss = -11289.112038631989
Iteration 15900: Loss = -11289.11314193369
1
Iteration 16000: Loss = -11289.11205090953
2
Iteration 16100: Loss = -11289.112160447512
3
Iteration 16200: Loss = -11289.12416043073
4
Iteration 16300: Loss = -11289.111952609856
Iteration 16400: Loss = -11289.112426309264
1
Iteration 16500: Loss = -11289.112120479052
2
Iteration 16600: Loss = -11289.111981702335
3
Iteration 16700: Loss = -11289.151292008146
4
Iteration 16800: Loss = -11289.113222561644
5
Iteration 16900: Loss = -11289.112399966469
6
Iteration 17000: Loss = -11289.11444960332
7
Iteration 17100: Loss = -11289.159057609555
8
Iteration 17200: Loss = -11289.11512103098
9
Iteration 17300: Loss = -11289.117034279447
10
Stopping early at iteration 17300 due to no improvement.
tensor([[ 1.7909, -3.2121],
        [-2.7900,  1.0565],
        [ 3.5463, -4.9451],
        [-3.1491,  1.5714],
        [ 0.2266, -2.3657],
        [ 3.6154, -6.1451],
        [ 3.1129, -5.2525],
        [ 0.4472, -1.9354],
        [ 0.3400, -2.0590],
        [-0.8595, -0.7134],
        [ 3.2102, -4.7511],
        [ 0.2853, -4.8145],
        [ 4.3519, -5.8794],
        [-5.1629,  2.0016],
        [-5.5665,  3.8964],
        [ 0.3808, -2.5540],
        [-2.6693,  0.6642],
        [ 4.3017, -5.8515],
        [ 2.8467, -4.7597],
        [ 3.3174, -4.7245],
        [ 1.2149, -2.6016],
        [-0.1920, -2.9450],
        [ 2.6599, -4.2998],
        [ 3.1554, -4.9809],
        [ 3.4734, -5.8471],
        [ 4.3068, -5.7646],
        [-1.7129, -0.2275],
        [-2.0528, -0.0804],
        [ 4.4773, -7.5131],
        [ 3.6640, -5.7032],
        [ 1.9181, -4.4325],
        [-6.0047,  4.4011],
        [ 0.7056, -4.4978],
        [-3.3552,  1.9670],
        [ 1.8745, -3.2840],
        [ 5.7124, -7.0990],
        [-0.0404, -1.3489],
        [ 3.7671, -6.3059],
        [ 1.7751, -3.6229],
        [ 3.5466, -4.9450],
        [ 5.0374, -6.9272],
        [ 1.5356, -2.9964],
        [ 0.8720, -2.7584],
        [ 2.5621, -3.9738],
        [-0.0605, -4.3352],
        [ 3.3585, -5.0426],
        [ 3.6164, -6.5998],
        [ 4.2228, -5.6116],
        [ 1.8585, -3.7063],
        [ 3.3304, -4.7996],
        [ 3.3178, -4.7046],
        [ 3.4542, -4.8467],
        [ 3.5766, -4.9838],
        [ 2.6659, -4.3800],
        [-4.9276,  3.4223],
        [ 1.9376, -3.5744],
        [ 2.1565, -3.7087],
        [-2.5487,  0.9855],
        [-6.9798,  5.5334],
        [-6.3457,  4.5180],
        [ 3.4926, -5.0139],
        [ 1.4675, -2.8673],
        [ 4.5843, -6.4311],
        [ 3.4786, -4.8655],
        [ 0.6064, -4.8880],
        [-3.2861,  1.7770],
        [-3.2798,  1.4560],
        [ 1.7914, -4.2525],
        [ 0.6071, -3.3406],
        [ 4.5682, -6.1284],
        [ 4.5419, -5.9330],
        [-3.4138,  1.7238],
        [ 2.6758, -5.5902],
        [ 1.0944, -3.4400],
        [ 0.9924, -3.5265],
        [-5.4127,  3.7073],
        [-3.1325,  1.7418],
        [ 4.8545, -6.2678],
        [ 4.1558, -6.3306],
        [ 3.5480, -5.3564],
        [ 2.8422, -5.6554],
        [ 3.8811, -5.8504],
        [ 4.9292, -6.5594],
        [ 1.6973, -3.0873],
        [ 2.4145, -3.9873],
        [ 2.3521, -4.6507],
        [ 2.4475, -3.8941],
        [ 0.0726, -1.5506],
        [ 5.1161, -7.3024],
        [ 3.9028, -5.3243],
        [ 4.7124, -6.9021],
        [ 3.3184, -5.2036],
        [ 2.6091, -4.4155],
        [ 1.4601, -2.9734],
        [ 4.2699, -5.8598],
        [ 3.6832, -5.0728],
        [-0.0476, -3.4060],
        [ 1.0397, -2.4355],
        [ 5.1179, -6.6569],
        [ 3.2755, -6.1673]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.7892e-01, 2.1083e-02],
        [1.0000e+00, 1.7377e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8111, 0.1889], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1687, 0.1285],
         [0.0313, 0.4779]],

        [[0.0657, 0.2403],
         [0.0145, 0.7483]],

        [[0.7005, 0.2259],
         [0.0660, 0.2624]],

        [[0.2222, 0.2001],
         [0.3226, 0.3542]],

        [[0.4678, 0.2582],
         [0.0080, 0.9142]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 77
Adjusted Rand Index: 0.2761490729387285
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.009696969696969697
Global Adjusted Rand Index: 0.023223897416870986
Average Adjusted Rand Index: 0.05716920852713964
Iteration 0: Loss = -22470.427823263188
Iteration 10: Loss = -11300.856542706948
Iteration 20: Loss = -11300.804706049496
Iteration 30: Loss = -11300.78894299756
Iteration 40: Loss = -11300.78279815279
Iteration 50: Loss = -11300.777920402832
Iteration 60: Loss = -11300.771574770935
Iteration 70: Loss = -11300.76958137589
Iteration 80: Loss = -11300.769696861897
1
Iteration 90: Loss = -11300.770278228087
2
Iteration 100: Loss = -11300.770948901278
3
Stopping early at iteration 99 due to no improvement.
pi: tensor([[0.9686, 0.0314],
        [0.9825, 0.0175]], dtype=torch.float64)
alpha: tensor([0.9693, 0.0307])
beta: tensor([[[0.1661, 0.1901],
         [0.1757, 0.2719]],

        [[0.6133, 0.2045],
         [0.4757, 0.0092]],

        [[0.4057, 0.2107],
         [0.1466, 0.0230]],

        [[0.3025, 0.1914],
         [0.9654, 0.7788]],

        [[0.6165, 0.2444],
         [0.2065, 0.2608]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
Global Adjusted Rand Index: 0.0009285052269176978
Average Adjusted Rand Index: 0.0008888888888888889
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22470.326543423187
Iteration 100: Loss = -11374.624077618879
Iteration 200: Loss = -11334.350927180678
Iteration 300: Loss = -11321.68176228028
Iteration 400: Loss = -11306.288178361396
Iteration 500: Loss = -11304.239917167351
Iteration 600: Loss = -11303.296005414793
Iteration 700: Loss = -11302.746099704036
Iteration 800: Loss = -11302.397162619971
Iteration 900: Loss = -11302.16299999174
Iteration 1000: Loss = -11301.998311193418
Iteration 1100: Loss = -11301.871975270253
Iteration 1200: Loss = -11301.76141963444
Iteration 1300: Loss = -11301.638004376984
Iteration 1400: Loss = -11301.399773188466
Iteration 1500: Loss = -11300.934921285558
Iteration 1600: Loss = -11300.68309935538
Iteration 1700: Loss = -11300.51661588482
Iteration 1800: Loss = -11300.385292564555
Iteration 1900: Loss = -11300.254864388906
Iteration 2000: Loss = -11300.110159134392
Iteration 2100: Loss = -11299.823556254361
Iteration 2200: Loss = -11299.432470695818
Iteration 2300: Loss = -11299.230189310225
Iteration 2400: Loss = -11299.167036070972
Iteration 2500: Loss = -11299.113078284903
Iteration 2600: Loss = -11299.056355360726
Iteration 2700: Loss = -11298.938449599953
Iteration 2800: Loss = -11298.897552044582
Iteration 2900: Loss = -11298.842651806675
Iteration 3000: Loss = -11298.770146373487
Iteration 3100: Loss = -11298.714188078024
Iteration 3200: Loss = -11298.621756565353
Iteration 3300: Loss = -11298.578671821488
Iteration 3400: Loss = -11298.535670993046
Iteration 3500: Loss = -11298.500516611311
Iteration 3600: Loss = -11298.461586672327
Iteration 3700: Loss = -11298.442663451779
Iteration 3800: Loss = -11298.418734028919
Iteration 3900: Loss = -11298.778074294574
1
Iteration 4000: Loss = -11298.317629352088
Iteration 4100: Loss = -11298.306185919291
Iteration 4200: Loss = -11298.295458882358
Iteration 4300: Loss = -11298.283431591852
Iteration 4400: Loss = -11298.265528257862
Iteration 4500: Loss = -11298.107503001576
Iteration 4600: Loss = -11298.042860197193
Iteration 4700: Loss = -11298.021890500253
Iteration 4800: Loss = -11298.03005353743
1
Iteration 4900: Loss = -11298.000238614775
Iteration 5000: Loss = -11297.98843818984
Iteration 5100: Loss = -11297.98715635229
Iteration 5200: Loss = -11297.963040861141
Iteration 5300: Loss = -11297.942558096713
Iteration 5400: Loss = -11297.913103401272
Iteration 5500: Loss = -11298.189125148505
1
Iteration 5600: Loss = -11297.884372909462
Iteration 5700: Loss = -11297.875623642669
Iteration 5800: Loss = -11297.895194724346
1
Iteration 5900: Loss = -11297.8595287588
Iteration 6000: Loss = -11297.851204967765
Iteration 6100: Loss = -11297.894494422486
1
Iteration 6200: Loss = -11297.835722104404
Iteration 6300: Loss = -11297.83148078058
Iteration 6400: Loss = -11297.827307639338
Iteration 6500: Loss = -11297.82334875008
Iteration 6600: Loss = -11297.818908300393
Iteration 6700: Loss = -11297.813960385105
Iteration 6800: Loss = -11297.809437219583
Iteration 6900: Loss = -11297.804383106079
Iteration 7000: Loss = -11297.800502624194
Iteration 7100: Loss = -11297.798636705129
Iteration 7200: Loss = -11297.79873531597
1
Iteration 7300: Loss = -11297.795620124885
Iteration 7400: Loss = -11297.793211595186
Iteration 7500: Loss = -11297.793736982165
1
Iteration 7600: Loss = -11297.790772531258
Iteration 7700: Loss = -11297.789732146008
Iteration 7800: Loss = -11297.794259726616
1
Iteration 7900: Loss = -11297.789232505505
Iteration 8000: Loss = -11297.788854194267
Iteration 8100: Loss = -11297.788943208712
1
Iteration 8200: Loss = -11297.787451841834
Iteration 8300: Loss = -11297.785570044005
Iteration 8400: Loss = -11297.785864010717
1
Iteration 8500: Loss = -11297.785408958269
Iteration 8600: Loss = -11297.785344089649
Iteration 8700: Loss = -11297.80148837932
1
Iteration 8800: Loss = -11297.785138402374
Iteration 8900: Loss = -11297.784891602285
Iteration 9000: Loss = -11297.785751479512
1
Iteration 9100: Loss = -11297.782984423737
Iteration 9200: Loss = -11297.778636756575
Iteration 9300: Loss = -11297.778663467452
1
Iteration 9400: Loss = -11297.873266125389
2
Iteration 9500: Loss = -11297.777984354992
Iteration 9600: Loss = -11297.798302650648
1
Iteration 9700: Loss = -11297.777632857184
Iteration 9800: Loss = -11297.997964152579
1
Iteration 9900: Loss = -11297.777242054162
Iteration 10000: Loss = -11297.815349753111
1
Iteration 10100: Loss = -11297.776923813011
Iteration 10200: Loss = -11297.839357901068
1
Iteration 10300: Loss = -11297.776938986903
2
Iteration 10400: Loss = -11297.805238789586
3
Iteration 10500: Loss = -11297.776811612015
Iteration 10600: Loss = -11297.778154861355
1
Iteration 10700: Loss = -11297.777388960863
2
Iteration 10800: Loss = -11297.776233629587
Iteration 10900: Loss = -11297.807644542227
1
Iteration 11000: Loss = -11297.775793346102
Iteration 11100: Loss = -11297.810011143629
1
Iteration 11200: Loss = -11297.775774936807
Iteration 11300: Loss = -11297.775738701323
Iteration 11400: Loss = -11297.775695320248
Iteration 11500: Loss = -11297.77487959085
Iteration 11600: Loss = -11297.783130217096
1
Iteration 11700: Loss = -11297.774709837564
Iteration 11800: Loss = -11297.774722654744
1
Iteration 11900: Loss = -11297.832533131124
2
Iteration 12000: Loss = -11297.802119831622
3
Iteration 12100: Loss = -11297.774528889731
Iteration 12200: Loss = -11297.775462822134
1
Iteration 12300: Loss = -11297.853452629846
2
Iteration 12400: Loss = -11297.774406029095
Iteration 12500: Loss = -11297.7806669545
1
Iteration 12600: Loss = -11297.774397593052
Iteration 12700: Loss = -11297.774440549567
1
Iteration 12800: Loss = -11297.797420461415
2
Iteration 12900: Loss = -11297.78254662035
3
Iteration 13000: Loss = -11297.774321452795
Iteration 13100: Loss = -11297.775039387354
1
Iteration 13200: Loss = -11297.774158693463
Iteration 13300: Loss = -11297.774337659212
1
Iteration 13400: Loss = -11297.774015473253
Iteration 13500: Loss = -11297.774172763975
1
Iteration 13600: Loss = -11297.774131830385
2
Iteration 13700: Loss = -11297.77411447926
3
Iteration 13800: Loss = -11297.775683969794
4
Iteration 13900: Loss = -11297.781738210087
5
Iteration 14000: Loss = -11297.774003697408
Iteration 14100: Loss = -11297.873714286416
1
Iteration 14200: Loss = -11297.773996088523
Iteration 14300: Loss = -11297.781638316284
1
Iteration 14400: Loss = -11297.775907439182
2
Iteration 14500: Loss = -11297.775593105465
3
Iteration 14600: Loss = -11297.782716759515
4
Iteration 14700: Loss = -11297.774976864437
5
Iteration 14800: Loss = -11297.774276217831
6
Iteration 14900: Loss = -11297.787819553394
7
Iteration 15000: Loss = -11297.77398581764
Iteration 15100: Loss = -11297.774099474082
1
Iteration 15200: Loss = -11297.774294936513
2
Iteration 15300: Loss = -11297.773997261287
3
Iteration 15400: Loss = -11297.813170955902
4
Iteration 15500: Loss = -11297.77504008913
5
Iteration 15600: Loss = -11297.774192698631
6
Iteration 15700: Loss = -11297.774370653762
7
Iteration 15800: Loss = -11297.774569075413
8
Iteration 15900: Loss = -11297.778415669982
9
Iteration 16000: Loss = -11297.773909825513
Iteration 16100: Loss = -11297.774517458332
1
Iteration 16200: Loss = -11297.774962027741
2
Iteration 16300: Loss = -11297.77394606704
3
Iteration 16400: Loss = -11297.77799911744
4
Iteration 16500: Loss = -11297.776620196375
5
Iteration 16600: Loss = -11297.781058997867
6
Iteration 16700: Loss = -11297.774588967664
7
Iteration 16800: Loss = -11297.775517660577
8
Iteration 16900: Loss = -11297.796700069512
9
Iteration 17000: Loss = -11297.775679867165
10
Stopping early at iteration 17000 due to no improvement.
tensor([[-9.0514e-01, -3.3392e+00],
        [ 4.7081e-01, -1.9413e+00],
        [-8.4526e-01, -2.0299e+00],
        [ 1.0621e-01, -1.7267e+00],
        [-5.9930e-02, -1.3699e+00],
        [-3.2103e-02, -1.5202e+00],
        [ 1.9832e-01, -1.6039e+00],
        [-3.5802e-01, -2.2566e+00],
        [-2.7441e-01, -2.2104e+00],
        [-1.3664e-01, -1.9335e+00],
        [-4.9057e-01, -2.0779e+00],
        [-3.3065e-01, -1.6713e+00],
        [-1.8311e-01, -1.3826e+00],
        [-1.2923e+00, -2.9461e+00],
        [ 3.0667e-01, -1.9397e+00],
        [-1.5031e+00, -3.3344e-01],
        [-5.3181e-01, -1.7921e+00],
        [-1.2129e+00, -2.2109e-01],
        [-2.9367e-01, -1.2586e+00],
        [ 2.8037e-01, -1.6738e+00],
        [-1.1484e+00, -3.0603e-01],
        [ 2.3202e-01, -1.7497e+00],
        [-5.2417e-01, -1.0424e+00],
        [-8.0916e-02, -1.7404e+00],
        [-1.2265e-01, -1.3649e+00],
        [-5.2661e-02, -1.6760e+00],
        [-1.2015e-01, -1.8767e+00],
        [-2.7903e-01, -2.1758e+00],
        [ 1.1581e-01, -1.5066e+00],
        [-8.8209e-01, -1.9734e+00],
        [-1.9670e-01, -1.4075e+00],
        [ 2.9666e-01, -1.6856e+00],
        [-2.3300e-01, -1.7539e+00],
        [-2.0065e-01, -2.3701e+00],
        [ 9.7527e-02, -1.5132e+00],
        [-6.3867e-01, -2.8087e+00],
        [-2.0611e-01, -2.0182e+00],
        [-2.2981e-01, -2.1108e+00],
        [-1.6112e-01, -1.9849e+00],
        [ 1.8973e-01, -1.5839e+00],
        [ 1.0831e-01, -1.4952e+00],
        [-8.1365e-02, -1.3050e+00],
        [-6.1948e-01, -2.4741e+00],
        [-1.0089e-01, -1.3614e+00],
        [-3.5315e-01, -1.3388e+00],
        [-1.0482e+00, -1.3569e+00],
        [-1.0705e-01, -1.7497e+00],
        [ 2.1178e-01, -1.6375e+00],
        [-6.5373e-01, -1.7469e+00],
        [ 1.3962e-01, -1.5375e+00],
        [-1.2200e-01, -2.3244e+00],
        [-1.9466e-02, -1.7989e+00],
        [ 2.8761e-01, -1.9519e+00],
        [ 3.7241e-02, -1.4650e+00],
        [-2.8281e-02, -1.6819e+00],
        [ 3.4438e-02, -1.6031e+00],
        [-1.4809e+00, -3.1343e+00],
        [-4.4431e-02, -1.3482e+00],
        [ 2.6266e-01, -1.7298e+00],
        [ 4.3406e-01, -1.8903e+00],
        [ 4.0217e-02, -1.4582e+00],
        [-6.7320e-02, -1.3190e+00],
        [-8.7634e-01, -2.2096e+00],
        [-5.4155e-02, -1.8116e+00],
        [ 1.8395e-01, -1.5703e+00],
        [-3.6824e-01, -1.1681e+00],
        [-7.7074e-01, -2.4105e+00],
        [-1.8773e-01, -1.3314e+00],
        [ 1.7541e-01, -1.6402e+00],
        [-7.2215e-01, -9.7593e-01],
        [-1.2190e-01, -1.2923e+00],
        [-3.6633e-01, -1.8459e+00],
        [ 2.3985e-01, -1.6582e+00],
        [ 3.0955e-01, -1.7020e+00],
        [ 4.6641e-02, -1.8202e+00],
        [ 1.6740e-01, -1.6962e+00],
        [-1.5399e+00, -1.6663e+00],
        [-6.5973e-02, -1.3331e+00],
        [-8.1583e-01, -2.4657e+00],
        [-1.3999e-02, -1.4182e+00],
        [-7.3925e-01, -1.3866e+00],
        [-1.3790e+00, -6.9279e-01],
        [-1.2119e-02, -1.4309e+00],
        [-9.4486e-01, -2.1382e+00],
        [-1.1824e-01, -1.8094e+00],
        [ 2.2488e-03, -1.8162e+00],
        [-1.0107e+00, -2.7125e+00],
        [ 1.1194e-01, -1.7526e+00],
        [-1.4442e+00, -2.5789e+00],
        [-1.0317e+00, -2.3796e+00],
        [-1.6884e-01, -1.6576e+00],
        [-2.1733e-01, -1.9640e+00],
        [ 4.9331e-02, -1.7706e+00],
        [-3.8152e-01, -1.7596e+00],
        [-1.8570e-01, -1.2061e+00],
        [-4.2546e-02, -1.3453e+00],
        [-6.2356e-01, -2.3555e+00],
        [ 1.7128e-01, -1.9065e+00],
        [-1.2222e+00, -3.3592e+00],
        [-6.8059e-01, -1.2133e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.9928, 0.0072],
        [0.6950, 0.3050]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7974, 0.2026], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1727, 0.1617],
         [0.1757, 0.1466]],

        [[0.6133, 0.0978],
         [0.4757, 0.0092]],

        [[0.4057, 0.1952],
         [0.1466, 0.0230]],

        [[0.3025, 0.1720],
         [0.9654, 0.7788]],

        [[0.6165, 0.2670],
         [0.2065, 0.2608]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: -0.021712907117008445
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.009696969696969697
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00512830579860434
Average Adjusted Rand Index: -0.006281975362795627
Iteration 0: Loss = -25294.09352213743
Iteration 10: Loss = -11300.83688472818
Iteration 20: Loss = -11299.02798636797
Iteration 30: Loss = -11298.847057887326
Iteration 40: Loss = -11298.77840876995
Iteration 50: Loss = -11298.759262768712
Iteration 60: Loss = -11298.75782003973
Iteration 70: Loss = -11298.75803273157
1
Iteration 80: Loss = -11298.758149050253
2
Iteration 90: Loss = -11298.75818691989
3
Stopping early at iteration 89 due to no improvement.
pi: tensor([[0.9709, 0.0291],
        [0.9651, 0.0349]], dtype=torch.float64)
alpha: tensor([0.9707, 0.0293])
beta: tensor([[[0.1694, 0.1288],
         [0.0429, 0.1405]],

        [[0.2676, 0.0877],
         [0.2234, 0.7494]],

        [[0.6520, 0.1932],
         [0.6164, 0.6244]],

        [[0.1917, 0.1663],
         [0.7584, 0.6377]],

        [[0.3262, 0.2531],
         [0.2577, 0.3031]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.006464646464646465
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
Global Adjusted Rand Index: -0.0006117518216548744
Average Adjusted Rand Index: -0.00040404040404040404
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25293.839124011753
Iteration 100: Loss = -11361.50271738607
Iteration 200: Loss = -11332.5807462207
Iteration 300: Loss = -11322.829018412165
Iteration 400: Loss = -11316.338503292278
Iteration 500: Loss = -11308.657000000345
Iteration 600: Loss = -11304.202815283383
Iteration 700: Loss = -11303.109030923755
Iteration 800: Loss = -11302.620012427455
Iteration 900: Loss = -11302.286995698341
Iteration 1000: Loss = -11302.069684308282
Iteration 1100: Loss = -11301.918277719673
Iteration 1200: Loss = -11301.801375787089
Iteration 1300: Loss = -11301.708147898338
Iteration 1400: Loss = -11301.632412555678
Iteration 1500: Loss = -11301.569828595546
Iteration 1600: Loss = -11301.51737417979
Iteration 1700: Loss = -11301.472759626884
Iteration 1800: Loss = -11301.434268413168
Iteration 1900: Loss = -11301.40128366732
Iteration 2000: Loss = -11301.370626701217
Iteration 2100: Loss = -11301.343368247377
Iteration 2200: Loss = -11301.318322645433
Iteration 2300: Loss = -11301.294088861325
Iteration 2400: Loss = -11301.270648926222
Iteration 2500: Loss = -11301.246534275504
Iteration 2600: Loss = -11301.22044222975
Iteration 2700: Loss = -11301.249890331368
1
Iteration 2800: Loss = -11301.105043739582
Iteration 2900: Loss = -11301.052487375138
Iteration 3000: Loss = -11300.99077253185
Iteration 3100: Loss = -11300.960098922158
Iteration 3200: Loss = -11300.908041426675
Iteration 3300: Loss = -11300.878409564
Iteration 3400: Loss = -11300.854751069912
Iteration 3500: Loss = -11300.831810128948
Iteration 3600: Loss = -11300.80978959303
Iteration 3700: Loss = -11300.789190158279
Iteration 3800: Loss = -11300.770159798725
Iteration 3900: Loss = -11300.751535057034
Iteration 4000: Loss = -11300.737293967826
Iteration 4100: Loss = -11300.724865959332
Iteration 4200: Loss = -11300.714074544805
Iteration 4300: Loss = -11300.704437371818
Iteration 4400: Loss = -11300.695713940673
Iteration 4500: Loss = -11300.68712490687
Iteration 4600: Loss = -11300.677338620097
Iteration 4700: Loss = -11300.661585957276
Iteration 4800: Loss = -11300.632137724368
Iteration 4900: Loss = -11300.606555969376
Iteration 5000: Loss = -11300.594718571254
Iteration 5100: Loss = -11300.588991732928
Iteration 5200: Loss = -11300.585847437465
Iteration 5300: Loss = -11300.583845888572
Iteration 5400: Loss = -11300.582408238135
Iteration 5500: Loss = -11300.581043102544
Iteration 5600: Loss = -11300.579662355074
Iteration 5700: Loss = -11300.579574439214
Iteration 5800: Loss = -11300.576137083164
Iteration 5900: Loss = -11300.574538616012
Iteration 6000: Loss = -11300.637000817842
1
Iteration 6100: Loss = -11300.572090073549
Iteration 6200: Loss = -11300.570492747032
Iteration 6300: Loss = -11300.852047788358
1
Iteration 6400: Loss = -11300.566604598635
Iteration 6500: Loss = -11300.566030385742
Iteration 6600: Loss = -11300.56561949088
Iteration 6700: Loss = -11300.566219545275
1
Iteration 6800: Loss = -11300.56441137688
Iteration 6900: Loss = -11300.563842971787
Iteration 7000: Loss = -11300.564239648149
1
Iteration 7100: Loss = -11300.563146798206
Iteration 7200: Loss = -11300.562824250563
Iteration 7300: Loss = -11300.757540765328
1
Iteration 7400: Loss = -11300.562401011592
Iteration 7500: Loss = -11300.563760890484
1
Iteration 7600: Loss = -11300.561980353907
Iteration 7700: Loss = -11300.56178297088
Iteration 7800: Loss = -11300.56157136167
Iteration 7900: Loss = -11300.561348636904
Iteration 8000: Loss = -11300.5611122908
Iteration 8100: Loss = -11300.560864443558
Iteration 8200: Loss = -11300.561570479284
1
Iteration 8300: Loss = -11300.560287134169
Iteration 8400: Loss = -11300.658958640102
1
Iteration 8500: Loss = -11300.559805322417
Iteration 8600: Loss = -11300.559503831917
Iteration 8700: Loss = -11300.560016188292
1
Iteration 8800: Loss = -11300.55902412296
Iteration 8900: Loss = -11300.559086369783
1
Iteration 9000: Loss = -11300.558369672672
Iteration 9100: Loss = -11300.574946342915
1
Iteration 9200: Loss = -11300.557674426513
Iteration 9300: Loss = -11300.55744216842
Iteration 9400: Loss = -11300.556989167579
Iteration 9500: Loss = -11300.565002206724
1
Iteration 9600: Loss = -11300.55634065966
Iteration 9700: Loss = -11300.571209817774
1
Iteration 9800: Loss = -11300.556628298227
2
Iteration 9900: Loss = -11300.555756118041
Iteration 10000: Loss = -11300.55574211024
Iteration 10100: Loss = -11300.558109108179
1
Iteration 10200: Loss = -11300.555339503138
Iteration 10300: Loss = -11300.555731770946
1
Iteration 10400: Loss = -11300.573246930438
2
Iteration 10500: Loss = -11300.555200209556
Iteration 10600: Loss = -11300.560058247256
1
Iteration 10700: Loss = -11300.558716269703
2
Iteration 10800: Loss = -11300.564233135317
3
Iteration 10900: Loss = -11300.602008611719
4
Iteration 11000: Loss = -11300.55551140548
5
Iteration 11100: Loss = -11300.554353690932
Iteration 11200: Loss = -11300.554675099715
1
Iteration 11300: Loss = -11300.554892809445
2
Iteration 11400: Loss = -11300.554221230717
Iteration 11500: Loss = -11300.605087792097
1
Iteration 11600: Loss = -11300.570593304548
2
Iteration 11700: Loss = -11300.563789338465
3
Iteration 11800: Loss = -11300.55416514634
Iteration 11900: Loss = -11300.574611895256
1
Iteration 12000: Loss = -11300.576109225887
2
Iteration 12100: Loss = -11300.560154025494
3
Iteration 12200: Loss = -11300.554928955304
4
Iteration 12300: Loss = -11300.554911535699
5
Iteration 12400: Loss = -11300.557322649143
6
Iteration 12500: Loss = -11300.55362860867
Iteration 12600: Loss = -11300.557203510405
1
Iteration 12700: Loss = -11300.553633124371
2
Iteration 12800: Loss = -11300.553933218365
3
Iteration 12900: Loss = -11300.553557844498
Iteration 13000: Loss = -11300.573898348843
1
Iteration 13100: Loss = -11300.553493242776
Iteration 13200: Loss = -11300.558493490795
1
Iteration 13300: Loss = -11300.553710929265
2
Iteration 13400: Loss = -11300.570621041075
3
Iteration 13500: Loss = -11300.576604933327
4
Iteration 13600: Loss = -11300.554033141152
5
Iteration 13700: Loss = -11300.556592890887
6
Iteration 13800: Loss = -11300.561960459934
7
Iteration 13900: Loss = -11300.553343377926
Iteration 14000: Loss = -11300.553497345012
1
Iteration 14100: Loss = -11300.558269655472
2
Iteration 14200: Loss = -11300.553290176915
Iteration 14300: Loss = -11300.553314767303
1
Iteration 14400: Loss = -11300.553357908497
2
Iteration 14500: Loss = -11300.553246430682
Iteration 14600: Loss = -11300.555926837827
1
Iteration 14700: Loss = -11300.553250436202
2
Iteration 14800: Loss = -11300.553209935637
Iteration 14900: Loss = -11300.560457800235
1
Iteration 15000: Loss = -11300.55333629862
2
Iteration 15100: Loss = -11300.553282591753
3
Iteration 15200: Loss = -11300.553155110823
Iteration 15300: Loss = -11300.904080509217
1
Iteration 15400: Loss = -11300.553162651207
2
Iteration 15500: Loss = -11300.553168140537
3
Iteration 15600: Loss = -11300.553455503967
4
Iteration 15700: Loss = -11300.55316348085
5
Iteration 15800: Loss = -11300.5641142346
6
Iteration 15900: Loss = -11300.55316481296
7
Iteration 16000: Loss = -11300.561113245856
8
Iteration 16100: Loss = -11300.556935802731
9
Iteration 16200: Loss = -11300.566362425207
10
Stopping early at iteration 16200 due to no improvement.
tensor([[-4.2038,  2.3965],
        [-4.4273,  2.2048],
        [-1.6817,  0.0492],
        [-4.5642, -0.0510],
        [-3.0822, -0.1349],
        [-2.3265,  0.7044],
        [-3.2432,  0.6575],
        [-3.0866,  1.6762],
        [-4.3537,  0.4184],
        [-3.1162,  1.7295],
        [-2.8280,  0.7585],
        [-1.7527,  0.3126],
        [-2.7990,  1.2464],
        [-2.6812,  1.1334],
        [-3.9701,  2.0498],
        [-2.7035, -0.0140],
        [-3.8517,  1.9632],
        [-2.9809,  1.2426],
        [-1.3917, -0.1924],
        [-3.2733,  1.4244],
        [-1.7188,  0.1761],
        [-3.4049,  1.8746],
        [-2.2943,  0.5274],
        [-2.5891,  1.1958],
        [-3.1460, -0.4657],
        [-2.4215,  1.0341],
        [-2.9855,  1.2982],
        [-3.6779,  2.2081],
        [-3.0786,  0.6328],
        [-2.6610, -0.5326],
        [-1.7633,  0.3698],
        [-3.2577,  1.8696],
        [-2.3390,  0.7955],
        [-3.6330,  2.2151],
        [-2.4461,  1.0061],
        [-3.8511,  2.4643],
        [-3.4252,  2.0338],
        [-3.8013,  0.8233],
        [-2.9958,  1.4735],
        [-3.0526,  1.4514],
        [-2.9149,  0.4459],
        [-2.1637,  0.7684],
        [-3.0958,  1.3582],
        [-2.0369,  0.5034],
        [-1.8004,  0.4116],
        [-2.7484,  1.3578],
        [-2.6364,  1.1442],
        [-4.2209,  0.8953],
        [-3.6853,  1.4606],
        [-2.6064,  1.1571],
        [-4.5816,  1.2415],
        [-2.6796,  1.1173],
        [-4.8000,  1.2627],
        [-4.2176,  1.3938],
        [-4.7449,  0.4578],
        [-2.7746,  0.9784],
        [-2.5430,  1.1566],
        [-2.5525,  1.0014],
        [-3.7304,  1.3336],
        [-4.0070,  2.4962],
        [-2.2684,  0.8283],
        [-1.8612,  0.3378],
        [-3.1866, -0.8289],
        [-2.7884,  1.3767],
        [-2.9995,  1.0180],
        [-1.9961,  0.5752],
        [-3.0655,  1.1437],
        [-4.1895,  2.2783],
        [-3.0661,  1.3257],
        [-3.0597,  0.1144],
        [-1.7001,  0.0935],
        [-3.1639,  0.5126],
        [-3.2981,  1.8909],
        [-3.6799,  1.2868],
        [-3.0413,  1.2808],
        [-3.2016,  1.6619],
        [-2.8321,  1.3842],
        [-3.4758,  1.2585],
        [-2.9834,  1.4358],
        [-3.1973, -0.3227],
        [-1.9568,  0.1435],
        [-3.4200,  1.8489],
        [-2.9180, -0.0922],
        [-3.3600,  1.7839],
        [-3.5471,  0.6401],
        [-3.9784,  0.4528],
        [-2.8353,  1.4385],
        [-2.9350,  1.4288],
        [-2.5180,  1.1315],
        [-1.6596,  0.2685],
        [-2.2791,  0.8216],
        [-2.9502,  1.2524],
        [-3.2461,  1.2674],
        [-2.0790,  0.3457],
        [-1.9298, -0.5002],
        [-2.2206,  0.6206],
        [-2.9792,  1.2039],
        [-3.4579,  2.0303],
        [-3.3536,  1.9671],
        [-3.2038, -0.1090]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.7924e-01, 2.0758e-02],
        [9.9999e-01, 5.0392e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0350, 0.9650], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1691, 0.1287],
         [0.0429, 0.1722]],

        [[0.2676, 0.2597],
         [0.2234, 0.7494]],

        [[0.6520, 0.2071],
         [0.6164, 0.6244]],

        [[0.1917, 0.1805],
         [0.7584, 0.6377]],

        [[0.3262, 0.2612],
         [0.2577, 0.3031]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
Global Adjusted Rand Index: -0.004358509047233172
Average Adjusted Rand Index: 0.0008888888888888889
Iteration 0: Loss = -18184.305398282486
Iteration 10: Loss = -11299.621742165673
Iteration 20: Loss = -11299.286526224629
Iteration 30: Loss = -11298.67173376828
Iteration 40: Loss = -11297.056058900536
Iteration 50: Loss = -11294.18043354295
Iteration 60: Loss = -11071.786283107906
Iteration 70: Loss = -11043.006229593166
Iteration 80: Loss = -11043.00697305572
1
Iteration 90: Loss = -11043.006973586887
2
Iteration 100: Loss = -11043.00697282774
3
Stopping early at iteration 99 due to no improvement.
pi: tensor([[0.7274, 0.2726],
        [0.3078, 0.6922]], dtype=torch.float64)
alpha: tensor([0.5437, 0.4563])
beta: tensor([[[0.2015, 0.1015],
         [0.8162, 0.2910]],

        [[0.3894, 0.0991],
         [0.6193, 0.0325]],

        [[0.6713, 0.0899],
         [0.5504, 0.4428]],

        [[0.8460, 0.0952],
         [0.8850, 0.6840]],

        [[0.6453, 0.1026],
         [0.2967, 0.1120]]], dtype=torch.float64)
time is 0
tensor([0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9207702484198148
time is 1
tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 2
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448387581811067
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
Global Adjusted Rand Index: 0.9061114589584036
Average Adjusted Rand Index: 0.905767827130212
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18184.152153627456
Iteration 100: Loss = -11353.74891378972
Iteration 200: Loss = -11309.851020836286
Iteration 300: Loss = -11299.951896895343
Iteration 400: Loss = -11298.975161247028
Iteration 500: Loss = -11298.656985300659
Iteration 600: Loss = -11298.48360473218
Iteration 700: Loss = -11298.417336267621
Iteration 800: Loss = -11298.382337476405
Iteration 900: Loss = -11298.35775339831
Iteration 1000: Loss = -11298.336457624453
Iteration 1100: Loss = -11298.317042300987
Iteration 1200: Loss = -11298.30013894588
Iteration 1300: Loss = -11298.285483233858
Iteration 1400: Loss = -11298.29363603592
1
Iteration 1500: Loss = -11298.261831521935
Iteration 1600: Loss = -11298.249897378448
Iteration 1700: Loss = -11298.228819779526
Iteration 1800: Loss = -11298.146709246279
Iteration 1900: Loss = -11298.137875978373
Iteration 2000: Loss = -11298.133739227973
Iteration 2100: Loss = -11298.133138693389
Iteration 2200: Loss = -11298.126471046824
Iteration 2300: Loss = -11298.121287325923
Iteration 2400: Loss = -11298.182181291704
1
Iteration 2500: Loss = -11298.104518760148
Iteration 2600: Loss = -11298.095304719227
Iteration 2700: Loss = -11298.087375230052
Iteration 2800: Loss = -11298.085022006031
Iteration 2900: Loss = -11298.083452684192
Iteration 3000: Loss = -11298.081995895553
Iteration 3100: Loss = -11298.08799523363
1
Iteration 3200: Loss = -11298.078679147618
Iteration 3300: Loss = -11298.075297447793
Iteration 3400: Loss = -11298.573094196001
1
Iteration 3500: Loss = -11298.063781012945
Iteration 3600: Loss = -11298.059517505118
Iteration 3700: Loss = -11298.053087691285
Iteration 3800: Loss = -11298.037533837503
Iteration 3900: Loss = -11298.007485356144
Iteration 4000: Loss = -11298.006272176783
Iteration 4100: Loss = -11298.006487570963
1
Iteration 4200: Loss = -11298.00253345318
Iteration 4300: Loss = -11297.996380679413
Iteration 4400: Loss = -11297.989822991749
Iteration 4500: Loss = -11297.988427455679
Iteration 4600: Loss = -11297.985694209052
Iteration 4700: Loss = -11297.983241930484
Iteration 4800: Loss = -11297.987362198695
1
Iteration 4900: Loss = -11297.96794283626
Iteration 5000: Loss = -11297.951134833073
Iteration 5100: Loss = -11297.958686410995
1
Iteration 5200: Loss = -11297.943019571025
Iteration 5300: Loss = -11297.938393634025
Iteration 5400: Loss = -11297.930656372246
Iteration 5500: Loss = -11297.93684935242
1
Iteration 5600: Loss = -11297.862599702785
Iteration 5700: Loss = -11297.904169313684
1
Iteration 5800: Loss = -11297.807306758092
Iteration 5900: Loss = -11297.808379106154
1
Iteration 6000: Loss = -11297.797281972504
Iteration 6100: Loss = -11297.794701905254
Iteration 6200: Loss = -11297.792528146063
Iteration 6300: Loss = -11297.785229340341
Iteration 6400: Loss = -11297.98412791458
1
Iteration 6500: Loss = -11297.783811533627
Iteration 6600: Loss = -11297.78323620968
Iteration 6700: Loss = -11297.806370458831
1
Iteration 6800: Loss = -11297.782338350587
Iteration 6900: Loss = -11297.781784714409
Iteration 7000: Loss = -11297.7931052184
1
Iteration 7100: Loss = -11297.781286001344
Iteration 7200: Loss = -11297.781062017757
Iteration 7300: Loss = -11297.797171474725
1
Iteration 7400: Loss = -11297.780756410311
Iteration 7500: Loss = -11297.780585182978
Iteration 7600: Loss = -11297.78207371256
1
Iteration 7700: Loss = -11297.77679867131
Iteration 7800: Loss = -11297.776557049907
Iteration 7900: Loss = -11297.774862767596
Iteration 8000: Loss = -11297.77420834283
Iteration 8100: Loss = -11297.776070611324
1
Iteration 8200: Loss = -11297.774084713572
Iteration 8300: Loss = -11297.796141301316
1
Iteration 8400: Loss = -11297.774587375177
2
Iteration 8500: Loss = -11297.778261963804
3
Iteration 8600: Loss = -11297.774015972875
Iteration 8700: Loss = -11297.840480286435
1
Iteration 8800: Loss = -11297.773981419374
Iteration 8900: Loss = -11297.775653586097
1
Iteration 9000: Loss = -11297.774050157761
2
Iteration 9100: Loss = -11297.77394600558
Iteration 9200: Loss = -11297.792798322964
1
Iteration 9300: Loss = -11297.774258925132
2
Iteration 9400: Loss = -11297.773968725354
3
Iteration 9500: Loss = -11297.775598134964
4
Iteration 9600: Loss = -11297.850764452598
5
Iteration 9700: Loss = -11297.773919446417
Iteration 9800: Loss = -11297.774308695494
1
Iteration 9900: Loss = -11297.775788662344
2
Iteration 10000: Loss = -11297.773922841909
3
Iteration 10100: Loss = -11297.78327427564
4
Iteration 10200: Loss = -11297.773919786327
5
Iteration 10300: Loss = -11297.774333446425
6
Iteration 10400: Loss = -11297.773875302684
Iteration 10500: Loss = -11297.77387284487
Iteration 10600: Loss = -11297.778184238185
1
Iteration 10700: Loss = -11297.77387561582
2
Iteration 10800: Loss = -11297.785726182223
3
Iteration 10900: Loss = -11297.773891254965
4
Iteration 11000: Loss = -11297.901879324188
5
Iteration 11100: Loss = -11297.774049670967
6
Iteration 11200: Loss = -11297.773927525997
7
Iteration 11300: Loss = -11297.801112394813
8
Iteration 11400: Loss = -11297.77387404822
9
Iteration 11500: Loss = -11297.781131693868
10
Stopping early at iteration 11500 due to no improvement.
tensor([[-1.9236,  0.5337],
        [-1.9455,  0.4894],
        [-1.7769, -0.5762],
        [-2.0189, -0.1630],
        [-1.4058, -0.0762],
        [-1.4562,  0.0541],
        [-1.6699,  0.1600],
        [-1.7661,  0.1539],
        [-1.8251,  0.1323],
        [-1.9932, -0.1753],
        [-2.1894, -0.5777],
        [-2.0014, -0.6421],
        [-2.8710, -1.6501],
        [-1.5383,  0.1404],
        [-2.2802, -0.0124],
        [-0.9531, -2.1172],
        [-1.3637, -0.0868],
        [-0.2708, -1.2511],
        [-1.3805, -0.4021],
        [-2.0531, -0.0760],
        [-1.3318, -2.1689],
        [-1.8712,  0.1303],
        [-2.5730, -2.0422],
        [-2.5331, -0.8477],
        [-1.3359, -0.0752],
        [-2.5499, -0.9031],
        [-1.6851,  0.0974],
        [-1.9892, -0.0736],
        [-1.7170, -0.0690],
        [-1.3344, -0.2279],
        [-1.6941, -0.4657],
        [-1.6963,  0.3085],
        [-2.2712, -0.7283],
        [-1.7881,  0.4018],
        [-1.5537,  0.0810],
        [-1.9825,  0.2084],
        [-1.7070,  0.1253],
        [-2.2091, -0.3032],
        [-1.7579,  0.0878],
        [-1.6131,  0.1816],
        [-1.5069,  0.1199],
        [-2.2704, -1.0284],
        [-1.9768, -0.0987],
        [-1.5488, -0.2705],
        [-1.4938, -0.4926],
        [-0.8852, -0.5605],
        [-1.5731,  0.0960],
        [-1.7077,  0.1622],
        [-1.6408, -0.5338],
        [-1.6287,  0.0748],
        [-2.1275,  0.0975],
        [-1.6323,  0.1739],
        [-1.8235,  0.4370],
        [-1.7162, -0.1906],
        [-1.5915,  0.0823],
        [-1.5308,  0.1328],
        [-2.0902, -0.4106],
        [-2.7322, -1.4075],
        [-2.0787, -0.0647],
        [-1.8812,  0.4647],
        [-1.4833,  0.0373],
        [-1.3440, -0.0744],
        [-1.6392, -0.2877],
        [-2.2739, -0.4923],
        [-1.6025,  0.1751],
        [-2.5431, -1.7298],
        [-1.5291,  0.1364],
        [-1.4183, -0.2608],
        [-1.7857,  0.0510],
        [-0.8354, -0.5726],
        [-1.2912, -0.1048],
        [-1.4687,  0.0347],
        [-2.2925, -0.3734],
        [-2.2477, -0.2148],
        [-1.7119,  0.1800],
        [-1.7180,  0.1660],
        [-0.7679, -0.6300],
        [-1.5200, -0.2292],
        [-1.6913, -0.0147],
        [-1.4054,  0.0191],
        [-1.2781, -0.6204],
        [-0.5990, -1.2751],
        [-1.4979, -0.0592],
        [-1.3639, -0.1524],
        [-1.9459, -0.2277],
        [-1.8051,  0.0371],
        [-1.9670, -0.2431],
        [-1.9452, -0.0538],
        [-1.5338, -0.3818],
        [-1.4810, -0.1138],
        [-1.6857, -0.1744],
        [-1.6379,  0.1316],
        [-2.7946, -0.9533],
        [-1.4117, -0.0153],
        [-1.4513, -0.4164],
        [-1.4553, -0.1343],
        [-1.8953, -0.1425],
        [-1.8483,  0.2495],
        [-1.8646,  0.2949],
        [-1.1344, -0.5890]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.3074, 0.6926],
        [0.0072, 0.9928]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2025, 0.7975], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1464, 0.1616],
         [0.8162, 0.1729]],

        [[0.3894, 0.0982],
         [0.6193, 0.0325]],

        [[0.6713, 0.1949],
         [0.5504, 0.4428]],

        [[0.8460, 0.1718],
         [0.8850, 0.6840]],

        [[0.6453, 0.2668],
         [0.2967, 0.1120]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: -0.021712907117008445
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.009696969696969697
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00512830579860434
Average Adjusted Rand Index: -0.006281975362795627
11073.925390049195
new:  [0.023223897416870986, -0.00512830579860434, -0.004358509047233172, -0.00512830579860434] [0.05716920852713964, -0.006281975362795627, 0.0008888888888888889, -0.006281975362795627] [11289.117034279447, 11297.775679867165, 11300.566362425207, 11297.781131693868]
prior:  [0.0, 0.0009285052269176978, -0.0006117518216548744, 0.9061114589584036] [0.0, 0.0008888888888888889, -0.00040404040404040404, 0.905767827130212] [11301.655337655246, 11300.770948901278, 11298.75818691989, 11043.00697282774]
-----------------------------------------------------------------------------------------
This iteration is 7
True Objective function: Loss = -11142.298662686346
Iteration 0: Loss = -17531.7198804588
Iteration 10: Loss = -11389.647990184727
Iteration 20: Loss = -11252.080102984131
Iteration 30: Loss = -11122.494189104995
Iteration 40: Loss = -11122.50747072758
1
Iteration 50: Loss = -11122.50756951943
2
Iteration 60: Loss = -11122.507551960785
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7425, 0.2575],
        [0.2777, 0.7223]], dtype=torch.float64)
alpha: tensor([0.5317, 0.4683])
beta: tensor([[[0.1914, 0.0977],
         [0.2868, 0.3024]],

        [[0.1743, 0.0968],
         [0.5412, 0.0475]],

        [[0.2280, 0.1026],
         [0.0703, 0.6555]],

        [[0.0792, 0.1141],
         [0.4454, 0.1769]],

        [[0.2880, 0.0977],
         [0.1472, 0.3987]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 1
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208093606567974
time is 2
tensor([1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 3
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208
time is 4
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080890789891884
Global Adjusted Rand Index: 0.9061159818913771
Average Adjusted Rand Index: 0.9060985920554934
tensor([[-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17547.677144626556
Iteration 100: Loss = -11403.385663925745
Iteration 200: Loss = -11395.408747774705
Iteration 300: Loss = -11391.769368999261
Iteration 400: Loss = -11192.009732731427
Iteration 500: Loss = -11119.143960489699
Iteration 600: Loss = -11118.765104606666
Iteration 700: Loss = -11118.605866566793
Iteration 800: Loss = -11118.553935494452
Iteration 900: Loss = -11118.516718991097
Iteration 1000: Loss = -11118.491751736005
Iteration 1100: Loss = -11118.480362649958
Iteration 1200: Loss = -11118.4724645083
Iteration 1300: Loss = -11118.466540617279
Iteration 1400: Loss = -11118.461853462639
Iteration 1500: Loss = -11118.45792302691
Iteration 1600: Loss = -11118.454467083724
Iteration 1700: Loss = -11118.451132693119
Iteration 1800: Loss = -11118.447886595495
Iteration 1900: Loss = -11118.445254769393
Iteration 2000: Loss = -11118.443735011779
Iteration 2100: Loss = -11118.44208045122
Iteration 2200: Loss = -11118.44080269517
Iteration 2300: Loss = -11118.438446912305
Iteration 2400: Loss = -11118.435841825003
Iteration 2500: Loss = -11118.434986578744
Iteration 2600: Loss = -11118.436453737433
1
Iteration 2700: Loss = -11118.434189316027
Iteration 2800: Loss = -11118.43319594447
Iteration 2900: Loss = -11118.43891534727
1
Iteration 3000: Loss = -11118.432392268587
Iteration 3100: Loss = -11118.432068993377
Iteration 3200: Loss = -11118.433302502128
1
Iteration 3300: Loss = -11118.431484769642
Iteration 3400: Loss = -11118.43120340594
Iteration 3500: Loss = -11118.431104047133
Iteration 3600: Loss = -11118.430781393243
Iteration 3700: Loss = -11118.439140974007
1
Iteration 3800: Loss = -11118.42896121707
Iteration 3900: Loss = -11118.42894433277
Iteration 4000: Loss = -11118.42883804239
Iteration 4100: Loss = -11118.42843031593
Iteration 4200: Loss = -11118.42874517164
1
Iteration 4300: Loss = -11118.436291429844
2
Iteration 4400: Loss = -11118.42820986914
Iteration 4500: Loss = -11118.430335590547
1
Iteration 4600: Loss = -11118.428316134181
2
Iteration 4700: Loss = -11118.428350837416
3
Iteration 4800: Loss = -11118.430908766159
4
Iteration 4900: Loss = -11118.42761153211
Iteration 5000: Loss = -11118.429256657353
1
Iteration 5100: Loss = -11118.428032928245
2
Iteration 5200: Loss = -11118.427480994915
Iteration 5300: Loss = -11118.428393360407
1
Iteration 5400: Loss = -11118.455825839696
2
Iteration 5500: Loss = -11118.426744837188
Iteration 5600: Loss = -11118.430126479125
1
Iteration 5700: Loss = -11118.42675691416
2
Iteration 5800: Loss = -11118.430090928674
3
Iteration 5900: Loss = -11118.427107649206
4
Iteration 6000: Loss = -11118.442608184623
5
Iteration 6100: Loss = -11118.42640638795
Iteration 6200: Loss = -11118.431490709012
1
Iteration 6300: Loss = -11118.432316698263
2
Iteration 6400: Loss = -11118.42691896945
3
Iteration 6500: Loss = -11118.430066214014
4
Iteration 6600: Loss = -11118.426239549664
Iteration 6700: Loss = -11118.430056706842
1
Iteration 6800: Loss = -11118.426536005154
2
Iteration 6900: Loss = -11118.42683244016
3
Iteration 7000: Loss = -11118.431638982402
4
Iteration 7100: Loss = -11118.442793351765
5
Iteration 7200: Loss = -11118.427078742709
6
Iteration 7300: Loss = -11118.426120007689
Iteration 7400: Loss = -11118.42540193935
Iteration 7500: Loss = -11118.42557731954
1
Iteration 7600: Loss = -11118.426530372466
2
Iteration 7700: Loss = -11118.42536554114
Iteration 7800: Loss = -11118.426934017074
1
Iteration 7900: Loss = -11118.42533186441
Iteration 8000: Loss = -11118.425270166435
Iteration 8100: Loss = -11118.4257225638
1
Iteration 8200: Loss = -11118.425238148542
Iteration 8300: Loss = -11118.42527240133
1
Iteration 8400: Loss = -11118.425186627228
Iteration 8500: Loss = -11118.42518946393
1
Iteration 8600: Loss = -11118.426384560322
2
Iteration 8700: Loss = -11118.425150324236
Iteration 8800: Loss = -11118.458102105926
1
Iteration 8900: Loss = -11118.42510432986
Iteration 9000: Loss = -11118.425110516939
1
Iteration 9100: Loss = -11118.83437066051
2
Iteration 9200: Loss = -11118.425073689788
Iteration 9300: Loss = -11118.424502222377
Iteration 9400: Loss = -11118.623710165113
1
Iteration 9500: Loss = -11118.424439804177
Iteration 9600: Loss = -11118.424397340117
Iteration 9700: Loss = -11118.477239303753
1
Iteration 9800: Loss = -11118.424401679646
2
Iteration 9900: Loss = -11118.424378205433
Iteration 10000: Loss = -11118.424732017183
1
Iteration 10100: Loss = -11118.424410972231
2
Iteration 10200: Loss = -11118.595900516582
3
Iteration 10300: Loss = -11118.424598802843
4
Iteration 10400: Loss = -11118.424482331453
5
Iteration 10500: Loss = -11118.428035390089
6
Iteration 10600: Loss = -11118.426046274139
7
Iteration 10700: Loss = -11118.424480411724
8
Iteration 10800: Loss = -11118.485475160749
9
Iteration 10900: Loss = -11118.424342477321
Iteration 11000: Loss = -11118.489393066982
1
Iteration 11100: Loss = -11118.424303332184
Iteration 11200: Loss = -11118.424294294404
Iteration 11300: Loss = -11118.423537908402
Iteration 11400: Loss = -11118.423451788769
Iteration 11500: Loss = -11118.78755166047
1
Iteration 11600: Loss = -11118.423458326224
2
Iteration 11700: Loss = -11118.423454326727
3
Iteration 11800: Loss = -11118.91748669955
4
Iteration 11900: Loss = -11118.42348472401
5
Iteration 12000: Loss = -11118.4234401916
Iteration 12100: Loss = -11118.550653266282
1
Iteration 12200: Loss = -11118.423481354002
2
Iteration 12300: Loss = -11118.426457973055
3
Iteration 12400: Loss = -11118.425675408589
4
Iteration 12500: Loss = -11118.42777955916
5
Iteration 12600: Loss = -11118.425510661888
6
Iteration 12700: Loss = -11118.486137952517
7
Iteration 12800: Loss = -11118.423598887566
8
Iteration 12900: Loss = -11118.425900762451
9
Iteration 13000: Loss = -11118.42383763288
10
Stopping early at iteration 13000 due to no improvement.
tensor([[-10.5789,   5.9636],
        [  2.8755,  -7.4907],
        [  3.4746,  -8.0898],
        [ -7.0451,   2.4299],
        [-10.1564,   5.5412],
        [ -9.6518,   5.0366],
        [  6.3614, -10.9766],
        [ -8.9878,   4.3726],
        [  2.6482,  -7.2635],
        [  1.5991,  -6.2144],
        [ -1.3833,  -3.2319],
        [ -7.2977,   2.6824],
        [ -3.5485,  -1.0667],
        [ -7.2324,   2.6171],
        [ -4.0870,  -0.5282],
        [ -5.7852,   1.1700],
        [  4.2354,  -8.8506],
        [  0.2641,  -4.8794],
        [  1.8960,  -6.5112],
        [ -6.2082,   1.5930],
        [  2.6098,  -7.2250],
        [ -0.7705,  -3.8447],
        [  3.8058,  -8.4210],
        [  2.2562,  -6.8714],
        [  4.3718,  -8.9870],
        [ -8.6995,   4.0843],
        [ -7.0351,   2.4199],
        [  0.5597,  -5.1749],
        [ -6.0461,   1.4309],
        [  3.5916,  -8.2068],
        [ -4.6705,   0.0553],
        [ -0.1172,  -4.4980],
        [  2.6910,  -7.3062],
        [  3.3040,  -7.9192],
        [ -5.4756,   0.8604],
        [  3.3971,  -8.0124],
        [ -6.8769,   2.2617],
        [ -9.7419,   5.1267],
        [  2.0926,  -6.7078],
        [  1.2258,  -5.8410],
        [ -8.3244,   3.7092],
        [ -6.9867,   2.3714],
        [ -6.7146,   2.0994],
        [ -8.3116,   3.6964],
        [  3.7574,  -8.3727],
        [ -7.6924,   3.0772],
        [ -5.8139,   1.1987],
        [  3.8966,  -8.5119],
        [  2.7850,  -7.4002],
        [ -1.4860,  -3.1292],
        [ -4.9897,   0.3745],
        [ -9.7515,   5.1362],
        [  2.9503,  -7.5655],
        [  1.8882,  -6.5034],
        [  1.9415,  -6.5567],
        [ -6.4204,   1.8052],
        [  0.3041,  -4.9193],
        [  4.1765,  -8.7917],
        [  5.1746,  -9.7899],
        [ -5.6446,   1.0294],
        [  2.6219,  -7.2371],
        [  5.0221,  -9.6373],
        [ -2.3087,  -2.3065],
        [ -9.4490,   4.8337],
        [ -6.5370,   1.9218],
        [ -0.0470,  -4.5683],
        [  3.4193,  -8.0345],
        [ -7.3534,   2.7382],
        [  1.5506,  -6.1658],
        [ -4.3124,  -0.3028],
        [  0.2528,  -4.8680],
        [ -9.0640,   4.4487],
        [  4.7073,  -9.3225],
        [ -8.0956,   3.4804],
        [  0.2977,  -4.9130],
        [  0.2148,  -4.8300],
        [  2.4646,  -7.0798],
        [-10.0530,   5.4377],
        [  5.3435,  -9.9588],
        [ -6.6352,   2.0199],
        [  0.5752,  -5.1904],
        [  2.8255,  -7.4407],
        [  3.3515,  -7.9667],
        [  1.9217,  -6.5369],
        [  3.4078,  -8.0230],
        [ -8.9945,   4.3793],
        [ -6.0755,   1.4603],
        [  2.1930,  -6.8082],
        [  1.8187,  -6.4339],
        [  0.7554,  -5.3707],
        [  0.2058,  -4.8210],
        [  0.0961,  -4.7113],
        [ -7.0064,   2.3912],
        [ -6.6634,   2.0482],
        [ -6.6138,   1.9986],
        [  4.1169,  -8.7321],
        [ -5.7636,   1.1483],
        [  0.2482,  -4.8635],
        [  3.5369,  -8.1521],
        [  4.2232,  -8.8385]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7583, 0.2417],
        [0.2614, 0.7386]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5719, 0.4281], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1947, 0.0979],
         [0.2868, 0.3101]],

        [[0.1743, 0.0963],
         [0.5412, 0.0475]],

        [[0.2280, 0.1037],
         [0.0703, 0.6555]],

        [[0.0792, 0.1143],
         [0.4454, 0.1769]],

        [[0.2880, 0.0977],
         [0.1472, 0.3987]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 1
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824127248460211
time is 3
tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080890789891884
Global Adjusted Rand Index: 0.921442740070811
Average Adjusted Rand Index: 0.9222604874860281
Iteration 0: Loss = -17850.462782658356
Iteration 10: Loss = -11234.63523908051
Iteration 20: Loss = -11122.493991903091
Iteration 30: Loss = -11122.507374666155
1
Iteration 40: Loss = -11122.507569494763
2
Iteration 50: Loss = -11122.507570653288
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7425, 0.2575],
        [0.2777, 0.7223]], dtype=torch.float64)
alpha: tensor([0.5317, 0.4683])
beta: tensor([[[0.1914, 0.0977],
         [0.5139, 0.3024]],

        [[0.1119, 0.0968],
         [0.2447, 0.1701]],

        [[0.6271, 0.1026],
         [0.8927, 0.2088]],

        [[0.8276, 0.1141],
         [0.9337, 0.3048]],

        [[0.7337, 0.0977],
         [0.0693, 0.2013]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 1
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208093606567974
time is 2
tensor([1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 3
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208
time is 4
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080890789891884
Global Adjusted Rand Index: 0.9061159818913771
Average Adjusted Rand Index: 0.9060985920554934
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17850.195001742693
Iteration 100: Loss = -11470.632679291817
Iteration 200: Loss = -11422.37942062144
Iteration 300: Loss = -11408.437505546282
Iteration 400: Loss = -11401.225739499892
Iteration 500: Loss = -11399.742907193395
Iteration 600: Loss = -11399.069147493092
Iteration 700: Loss = -11398.526844375343
Iteration 800: Loss = -11398.377071417739
Iteration 900: Loss = -11398.242922035453
Iteration 1000: Loss = -11398.101193219845
Iteration 1100: Loss = -11397.990390487204
Iteration 1200: Loss = -11397.895266163865
Iteration 1300: Loss = -11397.826762490866
Iteration 1400: Loss = -11397.781133979859
Iteration 1500: Loss = -11397.759018196784
Iteration 1600: Loss = -11397.740916073215
Iteration 1700: Loss = -11397.718987984555
Iteration 1800: Loss = -11397.696692397765
Iteration 1900: Loss = -11397.673105645175
Iteration 2000: Loss = -11397.644914499906
Iteration 2100: Loss = -11397.632613804271
Iteration 2200: Loss = -11397.625200645618
Iteration 2300: Loss = -11397.618503884292
Iteration 2400: Loss = -11397.610749887855
Iteration 2500: Loss = -11397.599165332444
Iteration 2600: Loss = -11397.58618355193
Iteration 2700: Loss = -11397.575220858336
Iteration 2800: Loss = -11397.562375784079
Iteration 2900: Loss = -11397.547119951325
Iteration 3000: Loss = -11397.526191690926
Iteration 3100: Loss = -11397.505232836364
Iteration 3200: Loss = -11397.483089822783
Iteration 3300: Loss = -11397.4588558985
Iteration 3400: Loss = -11397.428779868469
Iteration 3500: Loss = -11397.381995553282
Iteration 3600: Loss = -11397.308277968377
Iteration 3700: Loss = -11397.220913900937
Iteration 3800: Loss = -11397.165467779947
Iteration 3900: Loss = -11397.139437844196
Iteration 4000: Loss = -11397.118276361187
Iteration 4100: Loss = -11397.104201215408
Iteration 4200: Loss = -11397.097148841965
Iteration 4300: Loss = -11397.094127504988
Iteration 4400: Loss = -11397.094427729517
1
Iteration 4500: Loss = -11397.0907894562
Iteration 4600: Loss = -11397.130118369596
1
Iteration 4700: Loss = -11397.089665882495
Iteration 4800: Loss = -11397.093837940729
1
Iteration 4900: Loss = -11397.089258199598
Iteration 5000: Loss = -11397.089190227809
Iteration 5100: Loss = -11397.089210138865
1
Iteration 5200: Loss = -11397.093247355513
2
Iteration 5300: Loss = -11397.088973261447
Iteration 5400: Loss = -11397.13579762278
1
Iteration 5500: Loss = -11397.088748781229
Iteration 5600: Loss = -11397.08859995472
Iteration 5700: Loss = -11397.088286842436
Iteration 5800: Loss = -11397.087610270955
Iteration 5900: Loss = -11397.086873530772
Iteration 6000: Loss = -11397.085307101086
Iteration 6100: Loss = -11397.084241000159
Iteration 6200: Loss = -11397.083958542178
Iteration 6300: Loss = -11397.083540332447
Iteration 6400: Loss = -11397.083788057833
1
Iteration 6500: Loss = -11397.080182802172
Iteration 6600: Loss = -11397.069988607933
Iteration 6700: Loss = -11397.062842381803
Iteration 6800: Loss = -11397.062222512539
Iteration 6900: Loss = -11397.06142181681
Iteration 7000: Loss = -11397.06140456948
Iteration 7100: Loss = -11397.061247923399
Iteration 7200: Loss = -11397.0537162385
Iteration 7300: Loss = -11397.090619053144
1
Iteration 7400: Loss = -11397.053600799285
Iteration 7500: Loss = -11397.053530133458
Iteration 7600: Loss = -11397.055996988818
1
Iteration 7700: Loss = -11397.054898202967
2
Iteration 7800: Loss = -11397.127112862956
3
Iteration 7900: Loss = -11397.053297379962
Iteration 8000: Loss = -11397.06959646966
1
Iteration 8100: Loss = -11397.053267806685
Iteration 8200: Loss = -11397.079408759631
1
Iteration 8300: Loss = -11397.053298226489
2
Iteration 8400: Loss = -11397.05331827401
3
Iteration 8500: Loss = -11397.053416076975
4
Iteration 8600: Loss = -11397.055554401679
5
Iteration 8700: Loss = -11397.053369723415
6
Iteration 8800: Loss = -11397.055069856746
7
Iteration 8900: Loss = -11397.055170117517
8
Iteration 9000: Loss = -11397.053283467723
9
Iteration 9100: Loss = -11397.063894849129
10
Stopping early at iteration 9100 due to no improvement.
tensor([[-0.1544, -3.9879],
        [ 1.4926, -3.4265],
        [ 1.3049, -2.8762],
        [ 1.0190, -3.6242],
        [-0.1781, -1.8952],
        [ 1.3125, -3.0787],
        [ 1.9253, -3.4284],
        [-0.3431, -2.7619],
        [ 2.8404, -4.2841],
        [ 1.6669, -3.1194],
        [ 1.6892, -5.0023],
        [ 0.7050, -2.0929],
        [ 1.7524, -3.1387],
        [ 0.6780, -2.5028],
        [ 2.2157, -3.9219],
        [ 1.3431, -4.4232],
        [ 1.1957, -2.5820],
        [ 2.1051, -3.4983],
        [ 3.2219, -4.7025],
        [ 1.3215, -3.9867],
        [ 1.6432, -4.3237],
        [ 2.7991, -4.2393],
        [ 2.7978, -4.1992],
        [ 0.8588, -2.8598],
        [ 1.6651, -3.0771],
        [ 1.4322, -3.2380],
        [ 2.2555, -4.7977],
        [ 2.4264, -4.5317],
        [ 2.1822, -3.8138],
        [ 1.5964, -4.7492],
        [ 1.3969, -3.6166],
        [ 2.1096, -3.6714],
        [ 2.3111, -3.7070],
        [ 1.5401, -3.1953],
        [ 1.8463, -3.5026],
        [ 1.0869, -2.5120],
        [ 1.3873, -3.4574],
        [ 0.6734, -2.5389],
        [ 0.7146, -2.9300],
        [ 1.4972, -3.4447],
        [ 0.0636, -2.0729],
        [ 1.9948, -3.4669],
        [ 0.6057, -3.1169],
        [ 1.9312, -3.3244],
        [ 0.9528, -2.3613],
        [ 2.0154, -3.5219],
        [ 1.6854, -3.0776],
        [ 2.5165, -3.9901],
        [ 1.6246, -4.4067],
        [ 1.9483, -3.3604],
        [ 1.4532, -2.8829],
        [ 0.7813, -2.3840],
        [ 1.7796, -3.1776],
        [ 2.1502, -5.5660],
        [ 1.7371, -3.1238],
        [ 0.9946, -2.3976],
        [ 1.9381, -3.3381],
        [ 1.6854, -3.0775],
        [ 0.0913, -3.2207],
        [ 0.7346, -2.4207],
        [ 0.4373, -4.1181],
        [ 1.5068, -3.1703],
        [ 2.4941, -4.0115],
        [ 1.7908, -3.1771],
        [ 0.8141, -2.3728],
        [ 1.1307, -3.7316],
        [ 0.8206, -3.0433],
        [ 1.5901, -2.9789],
        [ 0.8686, -2.5052],
        [ 1.2625, -2.9666],
        [ 1.7821, -3.4551],
        [-0.1720, -1.2156],
        [ 1.4722, -2.9711],
        [-0.3948, -2.5229],
        [ 1.9350, -3.4732],
        [ 1.3851, -3.0253],
        [ 2.1714, -4.3607],
        [ 0.3870, -1.7830],
        [ 1.3046, -3.3239],
        [ 1.3237, -3.0050],
        [ 2.2250, -3.6585],
        [ 1.2447, -2.6381],
        [ 1.3253, -3.1934],
        [ 3.1664, -4.5557],
        [ 2.9062, -4.4959],
        [ 1.3891, -2.9873],
        [ 0.1925, -2.7956],
        [ 1.0359, -2.4225],
        [ 2.1037, -3.7080],
        [-0.2943, -1.7419],
        [ 0.4057, -2.3885],
        [ 1.6686, -3.6438],
        [ 2.2142, -3.7090],
        [ 1.1789, -2.9851],
        [ 0.8214, -3.7940],
        [ 3.1059, -4.5444],
        [ 1.6340, -3.0240],
        [ 1.6923, -3.3207],
        [ 1.8715, -3.3533],
        [-0.3943, -2.6727]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.2146, 0.7854],
        [0.0436, 0.9564]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9779, 0.0221], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1702, 0.2266],
         [0.5139, 0.1693]],

        [[0.1119, 0.1733],
         [0.2447, 0.1701]],

        [[0.6271, 0.2303],
         [0.8927, 0.2088]],

        [[0.8276, 0.2698],
         [0.9337, 0.3048]],

        [[0.7337, 0.1029],
         [0.0693, 0.2013]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.008096346523631212
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
Global Adjusted Rand Index: -0.002080424150059911
Average Adjusted Rand Index: 0.0014813205719862925
Iteration 0: Loss = -24852.473581134684
Iteration 10: Loss = -11387.790496208117
Iteration 20: Loss = -11153.673294606844
Iteration 30: Loss = -11122.503093840076
Iteration 40: Loss = -11122.507513450795
1
Iteration 50: Loss = -11122.507559139334
2
Iteration 60: Loss = -11122.507563854393
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7425, 0.2575],
        [0.2777, 0.7223]], dtype=torch.float64)
alpha: tensor([0.5317, 0.4683])
beta: tensor([[[0.1914, 0.0977],
         [0.5434, 0.3024]],

        [[0.3173, 0.0968],
         [0.9974, 0.4781]],

        [[0.2059, 0.1026],
         [0.7211, 0.7033]],

        [[0.2401, 0.1141],
         [0.9049, 0.8142]],

        [[0.7271, 0.0977],
         [0.8754, 0.7462]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 1
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208093606567974
time is 2
tensor([1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 3
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208
time is 4
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080890789891884
Global Adjusted Rand Index: 0.9061159818913771
Average Adjusted Rand Index: 0.9060985920554934
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24852.030952600784
Iteration 100: Loss = -11508.371014061715
Iteration 200: Loss = -11454.572203424741
Iteration 300: Loss = -11428.5733047528
Iteration 400: Loss = -11417.712435646761
Iteration 500: Loss = -11411.531932531612
Iteration 600: Loss = -11410.089831954043
Iteration 700: Loss = -11407.91792450164
Iteration 800: Loss = -11407.356895031118
Iteration 900: Loss = -11406.746702824832
Iteration 1000: Loss = -11406.429151664095
Iteration 1100: Loss = -11406.309187690831
Iteration 1200: Loss = -11406.225256630503
Iteration 1300: Loss = -11406.165390981536
Iteration 1400: Loss = -11406.117229319367
Iteration 1500: Loss = -11406.077267003484
Iteration 1600: Loss = -11406.04374636536
Iteration 1700: Loss = -11406.01516523845
Iteration 1800: Loss = -11405.990585022162
Iteration 1900: Loss = -11405.969298018075
Iteration 2000: Loss = -11405.950684110228
Iteration 2100: Loss = -11405.934301375744
Iteration 2200: Loss = -11405.919770617033
Iteration 2300: Loss = -11405.906602224795
Iteration 2400: Loss = -11405.891921466331
Iteration 2500: Loss = -11405.875743990953
Iteration 2600: Loss = -11405.86641485814
Iteration 2700: Loss = -11405.599083306583
Iteration 2800: Loss = -11405.58245433766
Iteration 2900: Loss = -11405.575107014285
Iteration 3000: Loss = -11405.568454588962
Iteration 3100: Loss = -11405.562418820093
Iteration 3200: Loss = -11405.556913960743
Iteration 3300: Loss = -11405.551819635188
Iteration 3400: Loss = -11405.547172015542
Iteration 3500: Loss = -11405.542658878783
Iteration 3600: Loss = -11405.538459768843
Iteration 3700: Loss = -11405.534822415015
Iteration 3800: Loss = -11405.531683000316
Iteration 3900: Loss = -11405.52879786386
Iteration 4000: Loss = -11405.52601445524
Iteration 4100: Loss = -11405.523441623345
Iteration 4200: Loss = -11405.521062425867
Iteration 4300: Loss = -11405.518842431007
Iteration 4400: Loss = -11405.516813911254
Iteration 4500: Loss = -11405.51489045658
Iteration 4600: Loss = -11405.512928868471
Iteration 4700: Loss = -11405.511209800421
Iteration 4800: Loss = -11405.510330010633
Iteration 4900: Loss = -11405.508528490378
Iteration 5000: Loss = -11405.506091181402
Iteration 5100: Loss = -11405.503894555079
Iteration 5200: Loss = -11405.424914282907
Iteration 5300: Loss = -11402.708018522004
Iteration 5400: Loss = -11402.704140021828
Iteration 5500: Loss = -11402.098257504098
Iteration 5600: Loss = -11400.886055158904
Iteration 5700: Loss = -11400.676414194688
Iteration 5800: Loss = -11400.671434913766
Iteration 5900: Loss = -11399.797670869762
Iteration 6000: Loss = -11398.343646710573
Iteration 6100: Loss = -11398.31054356964
Iteration 6200: Loss = -11398.306381762772
Iteration 6300: Loss = -11397.518841001822
Iteration 6400: Loss = -11397.497861163698
Iteration 6500: Loss = -11397.309743590147
Iteration 6600: Loss = -11396.789329082792
Iteration 6700: Loss = -11396.585805975577
Iteration 6800: Loss = -11396.549380799555
Iteration 6900: Loss = -11396.283133356228
Iteration 7000: Loss = -11396.171062759197
Iteration 7100: Loss = -11395.91713384899
Iteration 7200: Loss = -11395.68006336066
Iteration 7300: Loss = -11395.378361149966
Iteration 7400: Loss = -11395.046533483015
Iteration 7500: Loss = -11394.677331823448
Iteration 7600: Loss = -11394.627907746224
Iteration 7700: Loss = -11394.364627923334
Iteration 7800: Loss = -11394.281646620198
Iteration 7900: Loss = -11393.848110042887
Iteration 8000: Loss = -11393.4296381332
Iteration 8100: Loss = -11393.206470472325
Iteration 8200: Loss = -11392.9051362533
Iteration 8300: Loss = -11392.705021264665
Iteration 8400: Loss = -11391.50578662677
Iteration 8500: Loss = -11389.695820128765
Iteration 8600: Loss = -11389.333828011922
Iteration 8700: Loss = -11389.02097718369
Iteration 8800: Loss = -11387.524942745058
Iteration 8900: Loss = -11386.828328324003
Iteration 9000: Loss = -11386.806305056627
Iteration 9100: Loss = -11386.765965783576
Iteration 9200: Loss = -11386.831293123092
1
Iteration 9300: Loss = -11386.451773832949
Iteration 9400: Loss = -11386.412501943098
Iteration 9500: Loss = -11386.314217332161
Iteration 9600: Loss = -11386.308868630818
Iteration 9700: Loss = -11386.256271460834
Iteration 9800: Loss = -11385.569601896717
Iteration 9900: Loss = -11384.46056775057
Iteration 10000: Loss = -11384.417761024764
Iteration 10100: Loss = -11384.297155711101
Iteration 10200: Loss = -11384.248801841837
Iteration 10300: Loss = -11333.501397550652
Iteration 10400: Loss = -11303.342721566829
Iteration 10500: Loss = -11297.452194451607
Iteration 10600: Loss = -11275.08844117603
Iteration 10700: Loss = -11274.31846304503
Iteration 10800: Loss = -11270.968405260834
Iteration 10900: Loss = -11257.467443841395
Iteration 11000: Loss = -11250.712382465643
Iteration 11100: Loss = -11225.929475580755
Iteration 11200: Loss = -11222.345806901738
Iteration 11300: Loss = -11215.892590553205
Iteration 11400: Loss = -11209.84998008156
Iteration 11500: Loss = -11209.76927928922
Iteration 11600: Loss = -11206.035573562782
Iteration 11700: Loss = -11202.483628726166
Iteration 11800: Loss = -11202.454751655796
Iteration 11900: Loss = -11195.334742862167
Iteration 12000: Loss = -11195.33314194909
Iteration 12100: Loss = -11195.346447251239
1
Iteration 12200: Loss = -11189.967048657403
Iteration 12300: Loss = -11189.95597908094
Iteration 12400: Loss = -11187.467438588095
Iteration 12500: Loss = -11187.50870454306
1
Iteration 12600: Loss = -11187.458013420795
Iteration 12700: Loss = -11185.272503691962
Iteration 12800: Loss = -11171.479145824309
Iteration 12900: Loss = -11171.436861827617
Iteration 13000: Loss = -11171.429878825958
Iteration 13100: Loss = -11167.510280721952
Iteration 13200: Loss = -11167.514638467535
1
Iteration 13300: Loss = -11167.510354616892
2
Iteration 13400: Loss = -11167.514278333727
3
Iteration 13500: Loss = -11167.546926472041
4
Iteration 13600: Loss = -11167.510767014635
5
Iteration 13700: Loss = -11164.458435066808
Iteration 13800: Loss = -11164.43819026678
Iteration 13900: Loss = -11164.358503233429
Iteration 14000: Loss = -11164.358507198502
1
Iteration 14100: Loss = -11164.38201041273
2
Iteration 14200: Loss = -11164.40187933721
3
Iteration 14300: Loss = -11164.343679358333
Iteration 14400: Loss = -11160.549586821619
Iteration 14500: Loss = -11160.552825381754
1
Iteration 14600: Loss = -11160.577035517425
2
Iteration 14700: Loss = -11160.319883016598
Iteration 14800: Loss = -11160.19694538726
Iteration 14900: Loss = -11160.187995859374
Iteration 15000: Loss = -11160.182042137578
Iteration 15100: Loss = -11160.189412768135
1
Iteration 15200: Loss = -11160.186373714523
2
Iteration 15300: Loss = -11160.178718654031
Iteration 15400: Loss = -11154.587590806736
Iteration 15500: Loss = -11154.623448081242
1
Iteration 15600: Loss = -11154.582563381917
Iteration 15700: Loss = -11151.032696791239
Iteration 15800: Loss = -11151.035670758803
1
Iteration 15900: Loss = -11151.029755078169
Iteration 16000: Loss = -11151.024658671215
Iteration 16100: Loss = -11151.028323191853
1
Iteration 16200: Loss = -11151.114007608385
2
Iteration 16300: Loss = -11151.036469666335
3
Iteration 16400: Loss = -11151.02486862944
4
Iteration 16500: Loss = -11151.024485062057
Iteration 16600: Loss = -11151.020647503374
Iteration 16700: Loss = -11151.015495917143
Iteration 16800: Loss = -11151.014077621054
Iteration 16900: Loss = -11151.01454275071
1
Iteration 17000: Loss = -11151.01518977128
2
Iteration 17100: Loss = -11151.015957714893
3
Iteration 17200: Loss = -11151.190801185538
4
Iteration 17300: Loss = -11151.010976778245
Iteration 17400: Loss = -11151.011120751435
1
Iteration 17500: Loss = -11151.065820501308
2
Iteration 17600: Loss = -11151.006099879409
Iteration 17700: Loss = -11151.004947953214
Iteration 17800: Loss = -11151.007665972553
1
Iteration 17900: Loss = -11151.086320722867
2
Iteration 18000: Loss = -11150.988442211717
Iteration 18100: Loss = -11150.23119245904
Iteration 18200: Loss = -11150.239365172538
1
Iteration 18300: Loss = -11149.743033245697
Iteration 18400: Loss = -11149.061693817392
Iteration 18500: Loss = -11149.031220338586
Iteration 18600: Loss = -11149.030235282487
Iteration 18700: Loss = -11149.031904110472
1
Iteration 18800: Loss = -11149.030262064389
2
Iteration 18900: Loss = -11149.026571963308
Iteration 19000: Loss = -11147.38363156841
Iteration 19100: Loss = -11147.382153267785
Iteration 19200: Loss = -11147.38216769453
1
Iteration 19300: Loss = -11147.38209856352
Iteration 19400: Loss = -11147.406759751968
1
Iteration 19500: Loss = -11147.398924455078
2
Iteration 19600: Loss = -11147.376505246188
Iteration 19700: Loss = -11147.37409191852
Iteration 19800: Loss = -11147.37474686642
1
Iteration 19900: Loss = -11147.373904526741
tensor([[-9.7842e+00,  6.2417e+00],
        [ 3.6834e+00, -5.4498e+00],
        [ 2.7180e+00, -5.4648e+00],
        [-5.3752e+00,  3.9887e+00],
        [-9.0622e+00,  7.4945e+00],
        [-8.1804e+00,  5.9635e+00],
        [ 6.4570e+00, -7.9937e+00],
        [-7.3538e+00,  5.4284e+00],
        [ 6.7825e+00, -8.1785e+00],
        [-5.7217e+00,  4.1982e+00],
        [-4.0491e-01, -1.5748e+00],
        [-6.7082e+00,  5.2265e+00],
        [-2.1228e+00,  6.7649e-01],
        [-4.6738e+00,  3.0380e+00],
        [-1.7816e+00,  3.7281e-01],
        [-3.5507e+00,  1.5968e+00],
        [ 5.3606e+00, -9.9758e+00],
        [ 2.0834e-01, -1.6095e+00],
        [ 3.8891e+00, -5.3314e+00],
        [-3.5079e+00,  2.1209e+00],
        [ 3.6741e+00, -5.1979e+00],
        [ 5.3524e-01, -1.9246e+00],
        [ 5.6094e+00, -7.2981e+00],
        [ 6.4291e+00, -9.6987e+00],
        [ 3.8485e+00, -5.7848e+00],
        [-6.8582e+00,  3.4119e+00],
        [-4.3166e+00,  2.8656e+00],
        [-7.8349e+00,  6.4472e+00],
        [-4.6288e+00,  2.5053e+00],
        [ 8.1159e+00, -9.5029e+00],
        [-2.9843e+00, -1.1902e-01],
        [ 1.8980e-02, -1.4181e+00],
        [ 3.8218e+00, -7.1406e+00],
        [ 3.0065e+00, -5.0660e+00],
        [-3.3452e+00,  1.3185e+00],
        [ 3.1314e+00, -4.5760e+00],
        [-5.5084e+00,  3.2721e+00],
        [-6.5275e+00,  5.0035e+00],
        [-7.5087e+00,  6.1198e+00],
        [ 1.3138e+00, -4.5005e+00],
        [-7.6178e+00,  3.9851e+00],
        [-4.8658e+00,  2.7648e+00],
        [-5.0183e+00,  3.6041e+00],
        [-5.7365e+00,  4.3325e+00],
        [ 2.6237e+00, -4.0885e+00],
        [-1.0176e+01,  8.2967e+00],
        [-1.0368e+01,  7.8446e+00],
        [ 4.5939e+00, -6.3594e+00],
        [ 1.1564e+00, -5.7716e+00],
        [-9.3949e-01, -3.6757e+00],
        [-3.7206e+00,  1.9083e+00],
        [-7.4146e+00,  5.9741e+00],
        [ 3.8243e+00, -5.3679e+00],
        [ 1.8896e+00, -5.4346e+00],
        [ 7.3570e-01, -2.1678e+00],
        [-4.7587e+00,  3.2783e+00],
        [ 1.8588e+00, -4.5788e+00],
        [ 5.9028e+00, -7.4907e+00],
        [ 5.4964e+00, -6.8977e+00],
        [-4.2532e+00,  2.8475e+00],
        [ 5.2958e+00, -9.6185e+00],
        [ 7.2607e+00, -1.0635e+01],
        [-9.4841e-01, -4.8494e-01],
        [-7.1337e+00,  4.4029e+00],
        [-5.1019e+00,  1.4276e+00],
        [-1.2015e+01,  9.5600e+00],
        [ 3.2790e+00, -4.6960e+00],
        [-4.9158e+00,  3.0744e+00],
        [ 3.6512e+00, -5.0391e+00],
        [-7.3654e+00,  4.5940e+00],
        [ 1.4135e+00, -2.8199e+00],
        [-8.2275e+00,  6.4104e+00],
        [ 5.3930e+00, -6.9288e+00],
        [-5.6134e+00,  3.7991e+00],
        [ 2.2822e+00, -3.8770e+00],
        [ 9.9379e+00, -1.2564e+01],
        [ 3.3598e+00, -6.8813e+00],
        [-9.3961e+00,  7.9936e+00],
        [ 6.4359e+00, -8.4563e+00],
        [-6.2534e+00,  4.4438e+00],
        [ 2.7935e+00, -4.1801e+00],
        [ 3.7187e+00, -5.1366e+00],
        [ 5.1698e+00, -6.5997e+00],
        [ 3.8092e+00, -5.5457e+00],
        [ 4.3625e+00, -5.7509e+00],
        [-6.3133e+00,  4.2938e+00],
        [-4.6097e+00,  3.1751e+00],
        [-7.1600e+00,  5.0569e+00],
        [ 5.2434e-01, -1.9179e+00],
        [-7.5275e-01, -3.2532e+00],
        [ 9.2410e-03, -3.3736e+00],
        [ 1.9735e+00, -3.5863e+00],
        [-4.9153e+00,  2.3290e+00],
        [-7.2273e+00,  3.9583e+00],
        [-5.1126e+00,  2.9910e+00],
        [ 7.5381e+00, -1.0240e+01],
        [-3.2290e+00,  1.5415e+00],
        [ 2.0187e+00, -4.0236e+00],
        [ 4.0509e+00, -6.1458e+00],
        [ 5.5990e+00, -7.5775e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7527, 0.2473],
        [0.2751, 0.7249]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5166, 0.4834], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1962, 0.1043],
         [0.5434, 0.3006]],

        [[0.3173, 0.0962],
         [0.9974, 0.4781]],

        [[0.2059, 0.1019],
         [0.7211, 0.7033]],

        [[0.2401, 0.1136],
         [0.9049, 0.8142]],

        [[0.7271, 0.0975],
         [0.8754, 0.7462]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 93
Adjusted Rand Index: 0.7369913366172994
time is 1
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 3
tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080890789891884
Global Adjusted Rand Index: 0.8909178406058668
Average Adjusted Rand Index: 0.8931762098402839
Iteration 0: Loss = -20957.97275000535
Iteration 10: Loss = -11395.478225133807
Iteration 20: Loss = -11384.71091558976
Iteration 30: Loss = -11139.072764706298
Iteration 40: Loss = -11122.50552667317
Iteration 50: Loss = -11122.507579399015
1
Iteration 60: Loss = -11122.507558446123
2
Iteration 70: Loss = -11122.507572252493
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[0.7223, 0.2777],
        [0.2575, 0.7425]], dtype=torch.float64)
alpha: tensor([0.4683, 0.5317])
beta: tensor([[[0.3024, 0.0977],
         [0.6582, 0.1914]],

        [[0.0947, 0.0968],
         [0.2142, 0.4560]],

        [[0.1036, 0.1026],
         [0.7499, 0.8825]],

        [[0.1922, 0.1141],
         [0.9675, 0.5234]],

        [[0.3871, 0.0977],
         [0.2833, 0.2357]]], dtype=torch.float64)
time is 0
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
time is 1
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208093606567974
time is 2
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 3
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208
time is 4
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 5
Adjusted Rand Index: 0.8080890789891884
Global Adjusted Rand Index: 0.9061159818913771
Average Adjusted Rand Index: 0.9060985920554934
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20957.35913016453
Iteration 100: Loss = -11462.707284914615
Iteration 200: Loss = -11434.7007009627
Iteration 300: Loss = -11418.383513459446
Iteration 400: Loss = -11405.244067600357
Iteration 500: Loss = -11403.958292009578
Iteration 600: Loss = -11403.49010327394
Iteration 700: Loss = -11403.22445819172
Iteration 800: Loss = -11403.043903349791
Iteration 900: Loss = -11402.897773317749
Iteration 1000: Loss = -11402.763779680552
Iteration 1100: Loss = -11402.597896880821
Iteration 1200: Loss = -11401.934800801959
Iteration 1300: Loss = -11401.09032565735
Iteration 1400: Loss = -11400.458545502706
Iteration 1500: Loss = -11399.536857751016
Iteration 1600: Loss = -11398.004136314647
Iteration 1700: Loss = -11397.040495163454
Iteration 1800: Loss = -11395.93138372361
Iteration 1900: Loss = -11371.463675899387
Iteration 2000: Loss = -11327.53354391178
Iteration 2100: Loss = -11324.101371723013
Iteration 2200: Loss = -11322.971161584428
Iteration 2300: Loss = -11298.667441728005
Iteration 2400: Loss = -11276.44586774147
Iteration 2500: Loss = -11250.837314496926
Iteration 2600: Loss = -11242.377363661692
Iteration 2700: Loss = -11242.307224926026
Iteration 2800: Loss = -11242.266988195794
Iteration 2900: Loss = -11242.237545794069
Iteration 3000: Loss = -11242.210522246736
Iteration 3100: Loss = -11242.187070057396
Iteration 3200: Loss = -11242.171597644357
Iteration 3300: Loss = -11242.16097507943
Iteration 3400: Loss = -11242.15231062143
Iteration 3500: Loss = -11242.14443511562
Iteration 3600: Loss = -11242.134281233555
Iteration 3700: Loss = -11242.00622048727
Iteration 3800: Loss = -11241.997225889312
Iteration 3900: Loss = -11241.984840118781
Iteration 4000: Loss = -11241.954192674202
Iteration 4100: Loss = -11241.096217946582
Iteration 4200: Loss = -11238.210510712031
Iteration 4300: Loss = -11238.188343917987
Iteration 4400: Loss = -11238.17518149468
Iteration 4500: Loss = -11238.163909261084
Iteration 4600: Loss = -11238.141355990363
Iteration 4700: Loss = -11238.137469949066
Iteration 4800: Loss = -11238.136175168664
Iteration 4900: Loss = -11238.132536975583
Iteration 5000: Loss = -11238.127935725444
Iteration 5100: Loss = -11238.104342675073
Iteration 5200: Loss = -11238.103133744531
Iteration 5300: Loss = -11238.102571412306
Iteration 5400: Loss = -11238.106088008346
1
Iteration 5500: Loss = -11238.100827000355
Iteration 5600: Loss = -11238.218626054044
1
Iteration 5700: Loss = -11238.09971728373
Iteration 5800: Loss = -11238.100218896487
1
Iteration 5900: Loss = -11238.098732742475
Iteration 6000: Loss = -11238.098278213072
Iteration 6100: Loss = -11238.100826781834
1
Iteration 6200: Loss = -11238.097418637539
Iteration 6300: Loss = -11238.09705621492
Iteration 6400: Loss = -11238.096641846732
Iteration 6500: Loss = -11238.09059090897
Iteration 6600: Loss = -11238.030377587635
Iteration 6700: Loss = -11237.956432099872
Iteration 6800: Loss = -11238.051278244984
1
Iteration 6900: Loss = -11237.955543786924
Iteration 7000: Loss = -11237.9550803545
Iteration 7100: Loss = -11237.955012338733
Iteration 7200: Loss = -11237.954327524203
Iteration 7300: Loss = -11237.964221050992
1
Iteration 7400: Loss = -11237.953947165286
Iteration 7500: Loss = -11237.976994339482
1
Iteration 7600: Loss = -11237.953843958736
Iteration 7700: Loss = -11237.953517746952
Iteration 7800: Loss = -11237.95351626786
Iteration 7900: Loss = -11237.95317308912
Iteration 8000: Loss = -11237.95901116016
1
Iteration 8100: Loss = -11237.978736490948
2
Iteration 8200: Loss = -11237.951546769396
Iteration 8300: Loss = -11237.950679642901
Iteration 8400: Loss = -11237.951823640224
1
Iteration 8500: Loss = -11237.947132289959
Iteration 8600: Loss = -11237.888862598697
Iteration 8700: Loss = -11237.888154798478
Iteration 8800: Loss = -11237.88774817813
Iteration 8900: Loss = -11237.89414343655
1
Iteration 9000: Loss = -11237.886916499734
Iteration 9100: Loss = -11237.905383401278
1
Iteration 9200: Loss = -11237.868812323695
Iteration 9300: Loss = -11237.870286328895
1
Iteration 9400: Loss = -11237.833191762418
Iteration 9500: Loss = -11237.832690244095
Iteration 9600: Loss = -11237.815907559778
Iteration 9700: Loss = -11235.04166070438
Iteration 9800: Loss = -11235.05576786705
1
Iteration 9900: Loss = -11235.040937905495
Iteration 10000: Loss = -11235.095496893498
1
Iteration 10100: Loss = -11235.031711205187
Iteration 10200: Loss = -11235.022245354869
Iteration 10300: Loss = -11235.01225503414
Iteration 10400: Loss = -11235.012252487519
Iteration 10500: Loss = -11235.012128905952
Iteration 10600: Loss = -11235.03627004197
1
Iteration 10700: Loss = -11235.012055544474
Iteration 10800: Loss = -11235.012039120162
Iteration 10900: Loss = -11235.01224826397
1
Iteration 11000: Loss = -11235.011709008842
Iteration 11100: Loss = -11235.021524514374
1
Iteration 11200: Loss = -11235.011353534117
Iteration 11300: Loss = -11235.11191663942
1
Iteration 11400: Loss = -11235.01711379346
2
Iteration 11500: Loss = -11235.01370856765
3
Iteration 11600: Loss = -11235.010481313697
Iteration 11700: Loss = -11235.015191085746
1
Iteration 11800: Loss = -11235.01238425049
2
Iteration 11900: Loss = -11235.01983045192
3
Iteration 12000: Loss = -11235.008389275523
Iteration 12100: Loss = -11235.047620013658
1
Iteration 12200: Loss = -11235.008391848241
2
Iteration 12300: Loss = -11235.067334359648
3
Iteration 12400: Loss = -11235.010235443371
4
Iteration 12500: Loss = -11235.009057521318
5
Iteration 12600: Loss = -11235.008318418777
Iteration 12700: Loss = -11234.856984867785
Iteration 12800: Loss = -11234.87718794066
1
Iteration 12900: Loss = -11234.856183027745
Iteration 13000: Loss = -11234.859532350009
1
Iteration 13100: Loss = -11234.855824064525
Iteration 13200: Loss = -11234.85707986798
1
Iteration 13300: Loss = -11234.85163119717
Iteration 13400: Loss = -11234.832780982431
Iteration 13500: Loss = -11234.833897021854
1
Iteration 13600: Loss = -11234.834833682951
2
Iteration 13700: Loss = -11234.832567961526
Iteration 13800: Loss = -11234.826465005928
Iteration 13900: Loss = -11234.826513878983
1
Iteration 14000: Loss = -11234.825426181937
Iteration 14100: Loss = -11234.825394395224
Iteration 14200: Loss = -11234.825450887583
1
Iteration 14300: Loss = -11234.825444230493
2
Iteration 14400: Loss = -11234.828014270946
3
Iteration 14500: Loss = -11234.828953096025
4
Iteration 14600: Loss = -11234.825317073792
Iteration 14700: Loss = -11234.825299973347
Iteration 14800: Loss = -11234.807095713364
Iteration 14900: Loss = -11234.799917995158
Iteration 15000: Loss = -11234.799975035889
1
Iteration 15100: Loss = -11234.800565626192
2
Iteration 15200: Loss = -11234.800187747345
3
Iteration 15300: Loss = -11234.813881073984
4
Iteration 15400: Loss = -11234.800530769387
5
Iteration 15500: Loss = -11234.799870684892
Iteration 15600: Loss = -11234.828945862924
1
Iteration 15700: Loss = -11234.800424592404
2
Iteration 15800: Loss = -11234.901701503142
3
Iteration 15900: Loss = -11234.800246105648
4
Iteration 16000: Loss = -11234.812906903668
5
Iteration 16100: Loss = -11234.799638166736
Iteration 16200: Loss = -11234.847790448885
1
Iteration 16300: Loss = -11234.808398298914
2
Iteration 16400: Loss = -11234.797965860615
Iteration 16500: Loss = -11234.79778255982
Iteration 16600: Loss = -11234.816581595409
1
Iteration 16700: Loss = -11234.797465176604
Iteration 16800: Loss = -11234.80105820152
1
Iteration 16900: Loss = -11234.798553728157
2
Iteration 17000: Loss = -11234.798965495562
3
Iteration 17100: Loss = -11234.79712493775
Iteration 17200: Loss = -11234.799027771633
1
Iteration 17300: Loss = -11234.807798329348
2
Iteration 17400: Loss = -11234.802581601483
3
Iteration 17500: Loss = -11234.796917246264
Iteration 17600: Loss = -11234.79854811771
1
Iteration 17700: Loss = -11234.820921587792
2
Iteration 17800: Loss = -11234.79818747838
3
Iteration 17900: Loss = -11234.795407503501
Iteration 18000: Loss = -11234.914568818805
1
Iteration 18100: Loss = -11234.797729155362
2
Iteration 18200: Loss = -11234.842400392808
3
Iteration 18300: Loss = -11234.7953600369
Iteration 18400: Loss = -11234.796021885726
1
Iteration 18500: Loss = -11234.802864241325
2
Iteration 18600: Loss = -11234.796327513019
3
Iteration 18700: Loss = -11234.796408820725
4
Iteration 18800: Loss = -11234.795264925211
Iteration 18900: Loss = -11234.795459679584
1
Iteration 19000: Loss = -11234.79527752401
2
Iteration 19100: Loss = -11234.795438910229
3
Iteration 19200: Loss = -11234.795625780207
4
Iteration 19300: Loss = -11234.795665875607
5
Iteration 19400: Loss = -11234.795315584724
6
Iteration 19500: Loss = -11234.794990385022
Iteration 19600: Loss = -11234.87894280031
1
Iteration 19700: Loss = -11234.794715775708
Iteration 19800: Loss = -11234.84546010061
1
Iteration 19900: Loss = -11234.794791916722
2
tensor([[ -6.1304,   4.7212],
        [ -4.5936,   3.2037],
        [ -4.4759,   3.0638],
        [ -4.7543,   3.3333],
        [ -9.2755,   7.0934],
        [ -5.3383,   3.4533],
        [ -3.3354,   1.7830],
        [ -6.0654,   4.6611],
        [  0.3033,  -3.5593],
        [ -5.1155,   3.6579],
        [ -2.7164,   0.9794],
        [ -7.5332,   5.8778],
        [ -5.3914,   2.2586],
        [ -6.1657,   4.6277],
        [ -2.4871,   0.9234],
        [ -2.2100,   0.8036],
        [ -3.9338,   2.4922],
        [ -3.9331,   2.4838],
        [  0.0746,  -1.4857],
        [ -4.0442,   2.3896],
        [ -2.4452,   0.5244],
        [ -0.8341,  -0.7949],
        [  0.9792,  -2.7394],
        [ -6.1600,   4.6430],
        [ -3.6362,   2.0913],
        [ -4.7946,   3.4053],
        [ -1.9223,   0.5022],
        [ -2.2398,   0.4679],
        [ -3.2664,   1.1294],
        [ -2.7510,   1.2869],
        [ -5.3480,   3.8930],
        [ -3.3166,   1.8850],
        [ -3.6721,  -0.5239],
        [ -4.5610,   3.1693],
        [ -2.1865,   0.7775],
        [ -5.3762,   3.2473],
        [ -5.5725,   3.3030],
        [ -6.7966,   5.4024],
        [ -6.5471,   5.0916],
        [ -3.1580,  -0.8895],
        [ -8.6362,   6.4817],
        [ -5.5820,   0.9668],
        [ -6.7072,   4.1709],
        [ -4.4536,   2.9969],
        [ -6.0880,   4.6813],
        [ -4.5604,   2.7294],
        [ -5.2262,   2.7673],
        [ -4.3144,   0.1273],
        [ -2.8507,   1.2145],
        [ -3.7790,   2.1463],
        [ -5.6910,   4.2692],
        [ -7.0456,   5.5153],
        [ -4.5404,   2.9728],
        [ -1.1890,  -0.3629],
        [ -4.6589,   3.2719],
        [-10.8448,   9.1044],
        [ -3.8510,   2.3744],
        [ -5.3996,   3.5849],
        [ -6.9082,   5.1545],
        [ -7.7640,   5.6389],
        [ -4.8444,   3.4479],
        [ -3.1631,   1.6756],
        [ -2.4522,   1.0430],
        [ -4.6317,   2.7210],
        [ -9.6845,   7.3684],
        [ -4.4809,   3.0880],
        [ -3.9361,   2.5450],
        [ -6.2140,   3.9223],
        [ -5.3311,   3.7745],
        [ -4.9991,   3.3612],
        [ -4.2468,   2.8547],
        [ -9.4638,   7.9959],
        [ -4.3744,   2.8250],
        [ -8.6668,   6.5163],
        [ -2.7175,   1.2215],
        [ -3.8410,   2.4425],
        [ -2.0481,  -0.4615],
        [ -9.3724,   5.3099],
        [ -4.7584,   3.2636],
        [ -6.0034,   3.1218],
        [ -1.0274,  -0.7513],
        [ -3.8719,   2.4764],
        [ -3.9405,   2.5339],
        [  2.3899,  -4.6096],
        [ -1.6182,   0.1467],
        [ -5.7448,   4.3275],
        [ -6.9180,   5.3767],
        [ -6.0463,   4.4195],
        [ -3.6043,   1.6355],
        [ -7.9759,   4.5674],
        [ -8.0384,   6.5233],
        [ -4.9490,   2.1516],
        [ -2.9083,   1.4644],
        [ -5.8810,   4.0749],
        [ -4.9688,   3.4668],
        [  0.1757,  -1.6250],
        [ -4.9411,   2.3709],
        [ -3.6397,   2.2133],
        [ -3.7431,   1.0406],
        [-12.1840,   7.5687]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.5565, 0.4435],
        [0.3466, 0.6534]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0685, 0.9315], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3090, 0.0864],
         [0.6582, 0.1922]],

        [[0.0947, 0.0978],
         [0.2142, 0.4560]],

        [[0.1036, 0.1030],
         [0.7499, 0.8825]],

        [[0.1922, 0.1137],
         [0.9675, 0.5234]],

        [[0.3871, 0.0913],
         [0.2833, 0.2357]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: -0.016449682236070833
time is 1
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208093606567974
time is 2
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 3
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 75
Adjusted Rand Index: 0.24405617404826213
Global Adjusted Rand Index: 0.23267500784258285
Average Adjusted Rand Index: 0.598003297212784
Iteration 0: Loss = -33989.0766039455
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.6070,    nan]],

        [[0.2698,    nan],
         [0.8481, 0.3271]],

        [[0.4090,    nan],
         [0.1012, 0.8702]],

        [[0.8655,    nan],
         [0.5853, 0.7270]],

        [[0.6085,    nan],
         [0.2603, 0.8633]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -33988.22265889139
Iteration 100: Loss = -11412.347765773859
Iteration 200: Loss = -11409.470813809046
Iteration 300: Loss = -11408.184565405194
Iteration 400: Loss = -11407.388780892099
Iteration 500: Loss = -11406.852341629768
Iteration 600: Loss = -11406.463847055757
Iteration 700: Loss = -11406.149550459877
Iteration 800: Loss = -11405.815857306854
Iteration 900: Loss = -11405.039087854457
Iteration 1000: Loss = -11404.306782652566
Iteration 1100: Loss = -11404.051715698692
Iteration 1200: Loss = -11403.881447683587
Iteration 1300: Loss = -11403.74798860708
Iteration 1400: Loss = -11403.629183548395
Iteration 1500: Loss = -11403.509836258067
Iteration 1600: Loss = -11403.373819827895
Iteration 1700: Loss = -11403.198361379285
Iteration 1800: Loss = -11402.952646706864
Iteration 1900: Loss = -11402.603567541442
Iteration 2000: Loss = -11402.190682397802
Iteration 2100: Loss = -11401.746557368482
Iteration 2200: Loss = -11401.160743895649
Iteration 2300: Loss = -11400.232334904274
Iteration 2400: Loss = -11398.28054830044
Iteration 2500: Loss = -11390.620855324396
Iteration 2600: Loss = -11377.025370837138
Iteration 2700: Loss = -11350.857842365058
Iteration 2800: Loss = -11304.382336900999
Iteration 2900: Loss = -11284.794374866884
Iteration 3000: Loss = -11278.52720961963
Iteration 3100: Loss = -11268.293490339958
Iteration 3200: Loss = -11266.226090377826
Iteration 3300: Loss = -11259.011044245515
Iteration 3400: Loss = -11257.477066281144
Iteration 3500: Loss = -11257.26833030362
Iteration 3600: Loss = -11257.211856780505
Iteration 3700: Loss = -11257.156043211568
Iteration 3800: Loss = -11257.11497528286
Iteration 3900: Loss = -11257.072766906003
Iteration 4000: Loss = -11256.79589336655
Iteration 4100: Loss = -11256.705172736165
Iteration 4200: Loss = -11254.456213849824
Iteration 4300: Loss = -11254.423171868826
Iteration 4400: Loss = -11253.10661501679
Iteration 4500: Loss = -11252.760200407287
Iteration 4600: Loss = -11252.305568621481
Iteration 4700: Loss = -11251.340624083765
Iteration 4800: Loss = -11251.281975991891
Iteration 4900: Loss = -11251.263551927012
Iteration 5000: Loss = -11251.250925142122
Iteration 5100: Loss = -11251.235106464625
Iteration 5200: Loss = -11251.170061246728
Iteration 5300: Loss = -11251.156573379103
Iteration 5400: Loss = -11251.150603832226
Iteration 5500: Loss = -11251.139859795974
Iteration 5600: Loss = -11251.133226231932
Iteration 5700: Loss = -11251.129648159553
Iteration 5800: Loss = -11251.12682594447
Iteration 5900: Loss = -11251.124096329575
Iteration 6000: Loss = -11251.121179453381
Iteration 6100: Loss = -11251.118138140911
Iteration 6200: Loss = -11251.114132466426
Iteration 6300: Loss = -11251.12848503732
1
Iteration 6400: Loss = -11251.110425043855
Iteration 6500: Loss = -11251.109123373957
Iteration 6600: Loss = -11251.105717348251
Iteration 6700: Loss = -11251.097796725582
Iteration 6800: Loss = -11251.068832389026
Iteration 6900: Loss = -11251.06715830953
Iteration 7000: Loss = -11251.065660003283
Iteration 7100: Loss = -11251.06345322978
Iteration 7200: Loss = -11249.825741737199
Iteration 7300: Loss = -11247.33618341447
Iteration 7400: Loss = -11247.334781664063
Iteration 7500: Loss = -11247.329628267697
Iteration 7600: Loss = -11247.32616552967
Iteration 7700: Loss = -11247.291972028544
Iteration 7800: Loss = -11240.89668777908
Iteration 7900: Loss = -11240.738828223857
Iteration 8000: Loss = -11240.733803124509
Iteration 8100: Loss = -11240.73354055595
Iteration 8200: Loss = -11240.731334674156
Iteration 8300: Loss = -11240.723286212462
Iteration 8400: Loss = -11240.262097271745
Iteration 8500: Loss = -11239.710019845592
Iteration 8600: Loss = -11239.709012354388
Iteration 8700: Loss = -11239.711040921355
1
Iteration 8800: Loss = -11239.723677569044
2
Iteration 8900: Loss = -11239.706749562898
Iteration 9000: Loss = -11239.707468947816
1
Iteration 9100: Loss = -11239.72977533344
2
Iteration 9200: Loss = -11239.701255644384
Iteration 9300: Loss = -11239.700049277339
Iteration 9400: Loss = -11239.699304550157
Iteration 9500: Loss = -11239.696609704682
Iteration 9600: Loss = -11239.697038306418
1
Iteration 9700: Loss = -11239.696064809928
Iteration 9800: Loss = -11239.695501764227
Iteration 9900: Loss = -11239.730612201065
1
Iteration 10000: Loss = -11235.120187950524
Iteration 10100: Loss = -11235.091732723744
Iteration 10200: Loss = -11235.083314306507
Iteration 10300: Loss = -11235.082344352093
Iteration 10400: Loss = -11235.090662682645
1
Iteration 10500: Loss = -11235.068537675048
Iteration 10600: Loss = -11235.068889939483
1
Iteration 10700: Loss = -11235.067765456162
Iteration 10800: Loss = -11235.067289919978
Iteration 10900: Loss = -11235.226100380722
1
Iteration 11000: Loss = -11235.047715462635
Iteration 11100: Loss = -11235.044776090908
Iteration 11200: Loss = -11235.055719911756
1
Iteration 11300: Loss = -11235.043896073355
Iteration 11400: Loss = -11235.043755220502
Iteration 11500: Loss = -11235.040058211995
Iteration 11600: Loss = -11235.0106364792
Iteration 11700: Loss = -11234.95133337281
Iteration 11800: Loss = -11234.950707813296
Iteration 11900: Loss = -11234.949381393832
Iteration 12000: Loss = -11234.869486943237
Iteration 12100: Loss = -11234.860807498075
Iteration 12200: Loss = -11234.861418102057
1
Iteration 12300: Loss = -11234.86088290761
2
Iteration 12400: Loss = -11234.884005651858
3
Iteration 12500: Loss = -11234.86029523596
Iteration 12600: Loss = -11234.860463595367
1
Iteration 12700: Loss = -11234.861993881716
2
Iteration 12800: Loss = -11234.861530331678
3
Iteration 12900: Loss = -11234.850494829778
Iteration 13000: Loss = -11234.850476127893
Iteration 13100: Loss = -11234.858058283868
1
Iteration 13200: Loss = -11234.854543868818
2
Iteration 13300: Loss = -11234.845223884466
Iteration 13400: Loss = -11234.8432393634
Iteration 13500: Loss = -11234.903304322976
1
Iteration 13600: Loss = -11234.84268278258
Iteration 13700: Loss = -11234.842849993982
1
Iteration 13800: Loss = -11234.842770142366
2
Iteration 13900: Loss = -11234.842114270956
Iteration 14000: Loss = -11234.83146681297
Iteration 14100: Loss = -11235.446100019824
1
Iteration 14200: Loss = -11234.83037519737
Iteration 14300: Loss = -11234.830526054884
1
Iteration 14400: Loss = -11234.831788859048
2
Iteration 14500: Loss = -11234.830238511939
Iteration 14600: Loss = -11234.843528097379
1
Iteration 14700: Loss = -11234.83094218145
2
Iteration 14800: Loss = -11234.831709524855
3
Iteration 14900: Loss = -11234.835027266026
4
Iteration 15000: Loss = -11234.829785249101
Iteration 15100: Loss = -11234.831286807297
1
Iteration 15200: Loss = -11234.931551292128
2
Iteration 15300: Loss = -11234.828995479194
Iteration 15400: Loss = -11234.833066297588
1
Iteration 15500: Loss = -11234.836279483914
2
Iteration 15600: Loss = -11234.82837740627
Iteration 15700: Loss = -11234.815683027551
Iteration 15800: Loss = -11234.912631067735
1
Iteration 15900: Loss = -11234.821581879543
2
Iteration 16000: Loss = -11234.83841115425
3
Iteration 16100: Loss = -11234.815304641239
Iteration 16200: Loss = -11234.844488247827
1
Iteration 16300: Loss = -11234.8152814875
Iteration 16400: Loss = -11234.815022956798
Iteration 16500: Loss = -11234.818016277546
1
Iteration 16600: Loss = -11234.828092555697
2
Iteration 16700: Loss = -11234.814926782232
Iteration 16800: Loss = -11234.820453745211
1
Iteration 16900: Loss = -11234.814905637102
Iteration 17000: Loss = -11234.81574740074
1
Iteration 17100: Loss = -11234.814949324655
2
Iteration 17200: Loss = -11234.814958299567
3
Iteration 17300: Loss = -11234.818763561216
4
Iteration 17400: Loss = -11234.814850337007
Iteration 17500: Loss = -11234.814959130728
1
Iteration 17600: Loss = -11234.820274437343
2
Iteration 17700: Loss = -11234.815029797312
3
Iteration 17800: Loss = -11234.909559287282
4
Iteration 17900: Loss = -11234.815039196135
5
Iteration 18000: Loss = -11234.85139631604
6
Iteration 18100: Loss = -11234.811535383473
Iteration 18200: Loss = -11234.821923685147
1
Iteration 18300: Loss = -11234.825039026204
2
Iteration 18400: Loss = -11234.811205078571
Iteration 18500: Loss = -11234.812351383933
1
Iteration 18600: Loss = -11234.811211036205
2
Iteration 18700: Loss = -11234.813038637922
3
Iteration 18800: Loss = -11234.8162436591
4
Iteration 18900: Loss = -11234.813569202814
5
Iteration 19000: Loss = -11234.811162842407
Iteration 19100: Loss = -11234.811587561928
1
Iteration 19200: Loss = -11234.811056178994
Iteration 19300: Loss = -11234.811876113996
1
Iteration 19400: Loss = -11234.811165217321
2
Iteration 19500: Loss = -11234.811379711171
3
Iteration 19600: Loss = -11234.811134989173
4
Iteration 19700: Loss = -11234.812049232954
5
Iteration 19800: Loss = -11234.833336980006
6
Iteration 19900: Loss = -11234.835637042175
7
tensor([[-8.9335,  6.0318],
        [-4.7522,  3.0717],
        [-4.5000,  3.0734],
        [-5.0006,  3.1181],
        [-9.1003,  7.5680],
        [-5.3136,  3.5140],
        [-3.6171,  1.5144],
        [-6.1018,  4.6625],
        [ 1.0233, -2.8051],
        [-5.7003,  3.1086],
        [-2.5616,  1.1443],
        [-7.5594,  5.9431],
        [-4.5474,  3.1338],
        [-6.3918,  4.4410],
        [-9.4464,  6.5701],
        [-2.2401,  0.7904],
        [-3.9419,  2.5062],
        [-3.9597,  2.4860],
        [-0.1125, -1.6876],
        [-4.0982,  2.3649],
        [-2.2366,  0.7457],
        [-1.0418, -0.9138],
        [ 1.1028, -2.6298],
        [-6.1219,  4.7219],
        [-3.6391,  2.1099],
        [-5.2990,  3.0115],
        [-2.1998,  0.2373],
        [-2.0508,  0.6642],
        [-2.9929,  1.4243],
        [-2.9076,  1.1509],
        [-5.4092,  3.8622],
        [-3.3913,  1.9194],
        [-2.5888,  0.5654],
        [-4.7576,  3.0775],
        [-3.3520, -0.3717],
        [-5.0420,  3.6204],
        [-5.4751,  3.5121],
        [-7.9325,  4.3110],
        [-6.5952,  5.0935],
        [-1.8333,  0.4470],
        [-9.9141,  5.2988],
        [-4.1631,  2.4120],
        [-6.1769,  4.7448],
        [-4.7265,  2.7563],
        [-6.1248,  4.6780],
        [-4.4576,  2.8555],
        [-4.7025,  3.3139],
        [-2.9380,  1.5230],
        [-2.8675,  1.2091],
        [-3.8499,  2.1514],
        [-5.7099,  4.2924],
        [-7.4002,  5.2985],
        [-4.4861,  3.0565],
        [-2.7183, -1.8969],
        [-5.1406,  2.8710],
        [-5.5208,  4.1234],
        [-4.0511,  2.2004],
        [-5.4060,  3.6109],
        [-6.8601,  5.2544],
        [-8.2135,  5.2429],
        [-5.7462,  2.5694],
        [-3.5947,  1.2645],
        [-2.5260,  0.9870],
        [-4.8220,  2.5598],
        [-7.0609,  4.9312],
        [-5.1913,  2.4076],
        [-8.4545,  6.4017],
        [-6.7049,  3.4717],
        [-9.4759,  7.6948],
        [-5.5858,  2.9437],
        [-4.2550,  2.8685],
        [-9.3834,  7.5676],
        [-4.4619,  2.7639],
        [-9.1969,  6.0450],
        [-2.9581,  1.0782],
        [-3.8513,  2.4621],
        [-1.4943,  0.0989],
        [-8.5574,  6.2888],
        [-4.7237,  3.3213],
        [-5.2783,  3.8825],
        [-1.1687, -0.8929],
        [-3.9369,  2.4279],
        [-4.0582,  2.4452],
        [ 1.5463, -5.4851],
        [-1.6047,  0.1657],
        [-6.0128,  4.1785],
        [-6.9117,  5.4981],
        [-5.9834,  4.5745],
        [-3.3495,  1.9614],
        [-7.0934,  5.5042],
        [-8.3937,  6.2224],
        [-4.5221,  2.6040],
        [-2.9212,  1.5251],
        [-8.3822,  6.9347],
        [-9.8491,  7.4377],
        [ 0.1153, -1.7007],
        [-4.5706,  2.7664],
        [-3.6447,  2.2319],
        [-3.2830,  1.5220],
        [-6.0931,  4.5042]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.5605, 0.4395],
        [0.3480, 0.6520]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0683, 0.9317], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3082, 0.0863],
         [0.6070, 0.1926]],

        [[0.2698, 0.0975],
         [0.8481, 0.3271]],

        [[0.4090, 0.1026],
         [0.1012, 0.8702]],

        [[0.8655, 0.1133],
         [0.5853, 0.7270]],

        [[0.6085, 0.0914],
         [0.2603, 0.8633]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: -0.016449682236070833
time is 1
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208093606567974
time is 2
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 3
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 75
Adjusted Rand Index: 0.24405617404826213
Global Adjusted Rand Index: 0.23267500784258285
Average Adjusted Rand Index: 0.598003297212784
11142.298662686346
new:  [-0.002080424150059911, 0.8909178406058668, 0.23267500784258285, 0.23267500784258285] [0.0014813205719862925, 0.8931762098402839, 0.598003297212784, 0.598003297212784] [11397.063894849129, 11147.373745399085, 11234.794757854059, 11234.831181181022]
prior:  [0.9061159818913771, 0.9061159818913771, 0.9061159818913771, 0.0] [0.9060985920554934, 0.9060985920554934, 0.9060985920554934, 0.0] [11122.507570653288, 11122.507563854393, 11122.507572252493, nan]
-----------------------------------------------------------------------------------------
This iteration is 8
True Objective function: Loss = -11293.149371766836
Iteration 0: Loss = -23695.155915846655
Iteration 10: Loss = -11267.974387523169
Iteration 20: Loss = -11257.501955747884
Iteration 30: Loss = -11257.502951163322
1
Iteration 40: Loss = -11257.502962998677
2
Iteration 50: Loss = -11257.502960961217
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7285, 0.2715],
        [0.2089, 0.7911]], dtype=torch.float64)
alpha: tensor([0.5006, 0.4994])
beta: tensor([[[0.2886, 0.1033],
         [0.3475, 0.2019]],

        [[0.2307, 0.1048],
         [0.0446, 0.2327]],

        [[0.2702, 0.1150],
         [0.3987, 0.3445]],

        [[0.1496, 0.0917],
         [0.9479, 0.5926]],

        [[0.1193, 0.0974],
         [0.6002, 0.6830]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
time is 1
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 2
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.844814436176263
Global Adjusted Rand Index: 0.9061158614302716
Average Adjusted Rand Index: 0.9059289878530082
tensor([[-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23620.825915128422
Iteration 100: Loss = -11543.138638297301
Iteration 200: Loss = -11540.799298214473
Iteration 300: Loss = -11539.778887676235
Iteration 400: Loss = -11538.942016671228
Iteration 500: Loss = -11536.826165901455
Iteration 600: Loss = -11534.23017352213
Iteration 700: Loss = -11531.764290146166
Iteration 800: Loss = -11528.629649378003
Iteration 900: Loss = -11526.852090156759
Iteration 1000: Loss = -11525.453548281983
Iteration 1100: Loss = -11517.786877946692
Iteration 1200: Loss = -11487.159707421753
Iteration 1300: Loss = -11470.420508972582
Iteration 1400: Loss = -11468.145340444718
Iteration 1500: Loss = -11467.608708200009
Iteration 1600: Loss = -11466.498567680934
Iteration 1700: Loss = -11463.219312700025
Iteration 1800: Loss = -11452.347482018618
Iteration 1900: Loss = -11427.265341362801
Iteration 2000: Loss = -11418.290720705325
Iteration 2100: Loss = -11416.781453631906
Iteration 2200: Loss = -11414.45040044332
Iteration 2300: Loss = -11414.05656519565
Iteration 2400: Loss = -11413.908725785595
Iteration 2500: Loss = -11413.828354292395
Iteration 2600: Loss = -11413.462630197451
Iteration 2700: Loss = -11409.621539795125
Iteration 2800: Loss = -11409.392060527296
Iteration 2900: Loss = -11409.31988289738
Iteration 3000: Loss = -11409.27629046874
Iteration 3100: Loss = -11409.246056025651
Iteration 3200: Loss = -11409.2224851923
Iteration 3300: Loss = -11409.20376028596
Iteration 3400: Loss = -11409.18785563334
Iteration 3500: Loss = -11409.173215255263
Iteration 3600: Loss = -11409.156707212902
Iteration 3700: Loss = -11409.11773210565
Iteration 3800: Loss = -11408.96654472775
Iteration 3900: Loss = -11408.950672462604
Iteration 4000: Loss = -11408.942996181055
Iteration 4100: Loss = -11408.936979578768
Iteration 4200: Loss = -11408.929275973756
Iteration 4300: Loss = -11408.914364183202
Iteration 4400: Loss = -11408.910588831703
Iteration 4500: Loss = -11408.90720872405
Iteration 4600: Loss = -11408.903967596407
Iteration 4700: Loss = -11408.88037444482
Iteration 4800: Loss = -11408.877710580935
Iteration 4900: Loss = -11408.875614583489
Iteration 5000: Loss = -11408.873628615362
Iteration 5100: Loss = -11408.871563087207
Iteration 5200: Loss = -11408.869901923274
Iteration 5300: Loss = -11408.868314068739
Iteration 5400: Loss = -11408.880974448535
1
Iteration 5500: Loss = -11408.865955697695
Iteration 5600: Loss = -11408.864831950668
Iteration 5700: Loss = -11408.865816536541
1
Iteration 5800: Loss = -11408.86406499506
Iteration 5900: Loss = -11408.861466559125
Iteration 6000: Loss = -11408.860001439505
Iteration 6100: Loss = -11408.846835530647
Iteration 6200: Loss = -11408.84388847698
Iteration 6300: Loss = -11408.842578495642
Iteration 6400: Loss = -11408.84206940754
Iteration 6500: Loss = -11408.86638334615
1
Iteration 6600: Loss = -11408.84066142539
Iteration 6700: Loss = -11408.846489322965
1
Iteration 6800: Loss = -11408.839606490885
Iteration 6900: Loss = -11408.843058159073
1
Iteration 7000: Loss = -11408.838473866886
Iteration 7100: Loss = -11408.839499984331
1
Iteration 7200: Loss = -11408.837617118428
Iteration 7300: Loss = -11408.857617233745
1
Iteration 7400: Loss = -11408.921319299197
2
Iteration 7500: Loss = -11408.836665946901
Iteration 7600: Loss = -11408.836319587668
Iteration 7700: Loss = -11408.835892037052
Iteration 7800: Loss = -11408.83606931591
1
Iteration 7900: Loss = -11408.836076081709
2
Iteration 8000: Loss = -11408.835991825588
3
Iteration 8100: Loss = -11408.8348197908
Iteration 8200: Loss = -11408.834652412748
Iteration 8300: Loss = -11408.88243733383
1
Iteration 8400: Loss = -11408.834154541819
Iteration 8500: Loss = -11408.837492532883
1
Iteration 8600: Loss = -11408.833749669513
Iteration 8700: Loss = -11408.833992212585
1
Iteration 8800: Loss = -11408.833447301953
Iteration 8900: Loss = -11408.833529611446
1
Iteration 9000: Loss = -11408.83315724394
Iteration 9100: Loss = -11408.83336211223
1
Iteration 9200: Loss = -11408.832888415565
Iteration 9300: Loss = -11408.833919680648
1
Iteration 9400: Loss = -11408.848441817925
2
Iteration 9500: Loss = -11408.819330376135
Iteration 9600: Loss = -11408.81961873369
1
Iteration 9700: Loss = -11408.820832818947
2
Iteration 9800: Loss = -11408.819055806009
Iteration 9900: Loss = -11408.819097412363
1
Iteration 10000: Loss = -11408.82081882357
2
Iteration 10100: Loss = -11408.81884740072
Iteration 10200: Loss = -11408.81873702536
Iteration 10300: Loss = -11408.819848437597
1
Iteration 10400: Loss = -11408.81207641364
Iteration 10500: Loss = -11408.81214022843
1
Iteration 10600: Loss = -11408.834424911685
2
Iteration 10700: Loss = -11408.811785301707
Iteration 10800: Loss = -11408.818509314782
1
Iteration 10900: Loss = -11408.81814062574
2
Iteration 11000: Loss = -11408.814992490039
3
Iteration 11100: Loss = -11408.812956499249
4
Iteration 11200: Loss = -11408.815078189644
5
Iteration 11300: Loss = -11408.847395765146
6
Iteration 11400: Loss = -11408.810676198429
Iteration 11500: Loss = -11408.812698078413
1
Iteration 11600: Loss = -11408.800682855124
Iteration 11700: Loss = -11408.849680475883
1
Iteration 11800: Loss = -11408.905215991688
2
Iteration 11900: Loss = -11408.805679926645
3
Iteration 12000: Loss = -11408.780439956621
Iteration 12100: Loss = -11408.780453273685
1
Iteration 12200: Loss = -11408.780488881835
2
Iteration 12300: Loss = -11408.781793135153
3
Iteration 12400: Loss = -11408.911306177684
4
Iteration 12500: Loss = -11408.780285332843
Iteration 12600: Loss = -11408.905593066796
1
Iteration 12700: Loss = -11408.780301660652
2
Iteration 12800: Loss = -11408.780274627654
Iteration 12900: Loss = -11408.780362431566
1
Iteration 13000: Loss = -11408.780228229723
Iteration 13100: Loss = -11408.785405149052
1
Iteration 13200: Loss = -11408.78018455813
Iteration 13300: Loss = -11408.904907021426
1
Iteration 13400: Loss = -11408.780136258874
Iteration 13500: Loss = -11408.784518345023
1
Iteration 13600: Loss = -11408.780001080444
Iteration 13700: Loss = -11408.790833654904
1
Iteration 13800: Loss = -11408.779918867389
Iteration 13900: Loss = -11408.779864199532
Iteration 14000: Loss = -11408.779901781363
1
Iteration 14100: Loss = -11408.779700123023
Iteration 14200: Loss = -11408.781022566718
1
Iteration 14300: Loss = -11408.778582240766
Iteration 14400: Loss = -11408.885500610542
1
Iteration 14500: Loss = -11408.776181871435
Iteration 14600: Loss = -11408.77805466136
1
Iteration 14700: Loss = -11408.776177223988
Iteration 14800: Loss = -11408.77603272866
Iteration 14900: Loss = -11408.777410559354
1
Iteration 15000: Loss = -11408.775771702472
Iteration 15100: Loss = -11408.837544066451
1
Iteration 15200: Loss = -11408.7758275556
2
Iteration 15300: Loss = -11408.77651068879
3
Iteration 15400: Loss = -11408.778064637958
4
Iteration 15500: Loss = -11408.799141189596
5
Iteration 15600: Loss = -11408.776036982385
6
Iteration 15700: Loss = -11408.775070884192
Iteration 15800: Loss = -11408.74957873525
Iteration 15900: Loss = -11408.700242928437
Iteration 16000: Loss = -11408.701146498905
1
Iteration 16100: Loss = -11408.700263302371
2
Iteration 16200: Loss = -11408.700459151933
3
Iteration 16300: Loss = -11408.723709497866
4
Iteration 16400: Loss = -11408.700241892644
Iteration 16500: Loss = -11408.701072076019
1
Iteration 16600: Loss = -11408.700302298219
2
Iteration 16700: Loss = -11408.700901529359
3
Iteration 16800: Loss = -11408.700119979976
Iteration 16900: Loss = -11408.699616893467
Iteration 17000: Loss = -11408.699556532978
Iteration 17100: Loss = -11408.699598902425
1
Iteration 17200: Loss = -11408.699477899314
Iteration 17300: Loss = -11408.70005967762
1
Iteration 17400: Loss = -11408.699087692396
Iteration 17500: Loss = -11408.700531145947
1
Iteration 17600: Loss = -11408.698994977409
Iteration 17700: Loss = -11408.700791058372
1
Iteration 17800: Loss = -11408.790911242104
2
Iteration 17900: Loss = -11408.698939829119
Iteration 18000: Loss = -11408.705503500734
1
Iteration 18100: Loss = -11408.698931405059
Iteration 18200: Loss = -11408.702788560815
1
Iteration 18300: Loss = -11408.69918010958
2
Iteration 18400: Loss = -11408.698940157918
3
Iteration 18500: Loss = -11408.69892909269
Iteration 18600: Loss = -11408.698946305953
1
Iteration 18700: Loss = -11408.69891438787
Iteration 18800: Loss = -11408.699507214362
1
Iteration 18900: Loss = -11408.698908392207
Iteration 19000: Loss = -11408.705778035976
1
Iteration 19100: Loss = -11408.698890583717
Iteration 19200: Loss = -11408.755508087459
1
Iteration 19300: Loss = -11408.698826624357
Iteration 19400: Loss = -11408.698767369055
Iteration 19500: Loss = -11408.715314296716
1
Iteration 19600: Loss = -11408.698750375272
Iteration 19700: Loss = -11408.698794593836
1
Iteration 19800: Loss = -11408.700909903253
2
Iteration 19900: Loss = -11408.690940252822
tensor([[  2.5678,  -7.1830],
        [ -7.5354,   2.9202],
        [-10.7539,   6.1387],
        [ -8.0443,   3.4291],
        [  1.0887,  -5.7039],
        [  1.6576,  -6.2728],
        [ -6.6779,   2.0627],
        [-11.2838,   6.6686],
        [-11.6235,   7.0083],
        [ -6.2493,   1.6341],
        [ -7.5176,   2.9024],
        [  1.7274,  -6.3426],
        [  2.9964,  -7.6116],
        [-11.0407,   6.4254],
        [  3.8490,  -8.4642],
        [-11.4523,   6.8371],
        [ -6.5293,   1.9141],
        [ -5.0194,   0.4041],
        [ -9.5848,   4.9695],
        [ -7.2808,   2.6656],
        [ -8.5267,   3.9115],
        [ -6.6906,   2.0753],
        [ -0.3774,  -4.2378],
        [ -8.5959,   3.9807],
        [  0.5491,  -5.1644],
        [ -6.1763,   1.5610],
        [ -6.9374,   2.3222],
        [  1.8282,  -6.4434],
        [ -4.7218,   0.1066],
        [  6.5875, -11.2027],
        [  2.3650,  -6.9802],
        [-11.1624,   6.5472],
        [ -5.1057,   0.4905],
        [  3.7498,  -8.3651],
        [ -9.2120,   4.5968],
        [-12.6866,   8.0714],
        [  6.5411, -11.1563],
        [ -5.0024,   0.3872],
        [ -7.1686,   2.5534],
        [  1.4633,  -6.0786],
        [ -6.1574,   1.5421],
        [  4.7226,  -9.3379],
        [-11.3717,   6.7565],
        [ -7.5123,   2.8970],
        [-11.5838,   6.9685],
        [  3.8394,  -8.4546],
        [  2.7378,  -7.3530],
        [ -7.5335,   2.9183],
        [  1.5000,  -6.1152],
        [  1.7097,  -6.3250],
        [ -6.9689,   2.3537],
        [ -1.1667,  -3.4485],
        [  1.1813,  -5.7966],
        [ -1.5662,  -3.0490],
        [ -6.3918,   1.7766],
        [-11.3032,   6.6880],
        [  2.9825,  -7.5977],
        [  5.8744, -10.4896],
        [-10.0945,   5.4793],
        [ -8.7695,   4.1543],
        [ -7.5295,   2.9143],
        [  4.6532,  -9.2684],
        [ -4.0867,  -0.5285],
        [  1.1780,  -5.7932],
        [  1.4883,  -6.1035],
        [ -8.7882,   4.1730],
        [ -1.4010,  -3.2142],
        [  3.0617,  -7.6769],
        [  2.9497,  -7.5649],
        [  1.4875,  -6.1027],
        [  3.0574,  -7.6726],
        [-11.5999,   6.9847],
        [ -8.5265,   3.9113],
        [ -7.0148,   2.3996],
        [ -7.5744,   2.9592],
        [ -0.3828,  -4.2324],
        [-10.4603,   5.8451],
        [  5.4160, -10.0312],
        [ -4.7882,   0.1729],
        [-12.5006,   7.8854],
        [-10.9665,   6.3513],
        [-11.7377,   7.1224],
        [  5.1135,  -9.7287],
        [ -1.5096,  -3.1056],
        [  4.3021,  -8.9173],
        [ -6.4575,   1.8422],
        [ -9.6474,   5.0322],
        [ -6.9939,   2.3787],
        [ -3.5618,  -1.0534],
        [ -0.1841,  -4.4312],
        [  0.4445,  -5.0597],
        [ -5.8543,   1.2390],
        [  0.9013,  -5.5165],
        [  3.7945,  -8.4097],
        [ -5.8373,   1.2221],
        [  2.4840,  -7.0992],
        [  3.5177,  -8.1329],
        [ -5.8420,   1.2268],
        [ -7.4595,   2.8442],
        [ -5.1184,   0.5032]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.9103, 0.0897],
        [0.4854, 0.5146]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4252, 0.5748], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1728, 0.1044],
         [0.3475, 0.3166]],

        [[0.2307, 0.1084],
         [0.0446, 0.2327]],

        [[0.2702, 0.1258],
         [0.3987, 0.3445]],

        [[0.1496, 0.6503],
         [0.9479, 0.5926]],

        [[0.1193, 0.1590],
         [0.6002, 0.6830]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
time is 2
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448373142749671
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 59
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.37839635344463945
Average Adjusted Rand Index: 0.5454503764170275
Iteration 0: Loss = -30028.738951558986
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.6727,    nan]],

        [[0.7680,    nan],
         [0.8336, 0.3748]],

        [[0.3860,    nan],
         [0.4069, 0.2952]],

        [[0.2154,    nan],
         [0.1195, 0.9957]],

        [[0.4347,    nan],
         [0.2613, 0.2903]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 59
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -30029.16638104871
Iteration 100: Loss = -11551.89768596715
Iteration 200: Loss = -11543.328952364236
Iteration 300: Loss = -11541.657611904644
Iteration 400: Loss = -11540.445550973565
Iteration 500: Loss = -11538.461944416698
Iteration 600: Loss = -11536.122011392697
Iteration 700: Loss = -11534.234336140818
Iteration 800: Loss = -11532.216826077862
Iteration 900: Loss = -11531.076717035701
Iteration 1000: Loss = -11530.253196777734
Iteration 1100: Loss = -11529.519073795716
Iteration 1200: Loss = -11528.653115270505
Iteration 1300: Loss = -11527.544479476743
Iteration 1400: Loss = -11526.122491909055
Iteration 1500: Loss = -11524.36284772545
Iteration 1600: Loss = -11522.96496427741
Iteration 1700: Loss = -11521.502690642246
Iteration 1800: Loss = -11519.455399151344
Iteration 1900: Loss = -11515.366585737243
Iteration 2000: Loss = -11505.78992158032
Iteration 2100: Loss = -11473.017754693326
Iteration 2200: Loss = -11386.452950746145
Iteration 2300: Loss = -11345.967864506567
Iteration 2400: Loss = -11319.134706905921
Iteration 2500: Loss = -11312.9207903332
Iteration 2600: Loss = -11306.877088600115
Iteration 2700: Loss = -11293.409515484604
Iteration 2800: Loss = -11282.106341131048
Iteration 2900: Loss = -11281.625159574827
Iteration 3000: Loss = -11280.555951096434
Iteration 3100: Loss = -11280.462636870127
Iteration 3200: Loss = -11280.40204625016
Iteration 3300: Loss = -11280.324848331895
Iteration 3400: Loss = -11280.263659761025
Iteration 3500: Loss = -11274.145483333195
Iteration 3600: Loss = -11273.968812124422
Iteration 3700: Loss = -11273.695361685417
Iteration 3800: Loss = -11273.672096353008
Iteration 3900: Loss = -11273.653231415747
Iteration 4000: Loss = -11273.63241844455
Iteration 4100: Loss = -11273.590030323943
Iteration 4200: Loss = -11273.44393111111
Iteration 4300: Loss = -11269.917695111031
Iteration 4400: Loss = -11267.905365893828
Iteration 4500: Loss = -11267.833151780198
Iteration 4600: Loss = -11267.812353052139
Iteration 4700: Loss = -11267.803091764537
Iteration 4800: Loss = -11267.797705384091
Iteration 4900: Loss = -11267.793000761254
Iteration 5000: Loss = -11267.788778117416
Iteration 5100: Loss = -11267.784871491769
Iteration 5200: Loss = -11267.781118846277
Iteration 5300: Loss = -11267.777346021834
Iteration 5400: Loss = -11267.773203138133
Iteration 5500: Loss = -11267.769855903742
Iteration 5600: Loss = -11267.767039508246
Iteration 5700: Loss = -11267.764927989036
Iteration 5800: Loss = -11267.76254338226
Iteration 5900: Loss = -11267.76254045299
Iteration 6000: Loss = -11267.759746717229
Iteration 6100: Loss = -11267.757514226943
Iteration 6200: Loss = -11267.756584619938
Iteration 6300: Loss = -11267.75649396575
Iteration 6400: Loss = -11267.754096108356
Iteration 6500: Loss = -11267.753528244972
Iteration 6600: Loss = -11267.76205681609
1
Iteration 6700: Loss = -11267.749236118894
Iteration 6800: Loss = -11267.747953174252
Iteration 6900: Loss = -11266.226966795895
Iteration 7000: Loss = -11251.348385512209
Iteration 7100: Loss = -11251.336022842628
Iteration 7200: Loss = -11251.338990770173
1
Iteration 7300: Loss = -11251.329628019103
Iteration 7400: Loss = -11251.330183360127
1
Iteration 7500: Loss = -11251.35225557756
2
Iteration 7600: Loss = -11251.328194839043
Iteration 7700: Loss = -11251.326077381418
Iteration 7800: Loss = -11251.325985311023
Iteration 7900: Loss = -11251.344312486179
1
Iteration 8000: Loss = -11251.323117267282
Iteration 8100: Loss = -11251.32275914272
Iteration 8200: Loss = -11251.321366269533
Iteration 8300: Loss = -11251.32077003526
Iteration 8400: Loss = -11251.329222214752
1
Iteration 8500: Loss = -11251.338497161272
2
Iteration 8600: Loss = -11251.321615352013
3
Iteration 8700: Loss = -11251.334906972083
4
Iteration 8800: Loss = -11251.328172939691
5
Iteration 8900: Loss = -11251.313470654435
Iteration 9000: Loss = -11251.308906576625
Iteration 9100: Loss = -11251.368553409791
1
Iteration 9200: Loss = -11251.310647302233
2
Iteration 9300: Loss = -11251.306875339898
Iteration 9400: Loss = -11251.307106060447
1
Iteration 9500: Loss = -11251.500351650093
2
Iteration 9600: Loss = -11251.304934767937
Iteration 9700: Loss = -11251.360534948893
1
Iteration 9800: Loss = -11251.304520530797
Iteration 9900: Loss = -11251.304342304527
Iteration 10000: Loss = -11251.306337102678
1
Iteration 10100: Loss = -11251.304440633528
2
Iteration 10200: Loss = -11251.307266299702
3
Iteration 10300: Loss = -11251.304240830335
Iteration 10400: Loss = -11251.302500248832
Iteration 10500: Loss = -11251.295516512588
Iteration 10600: Loss = -11251.28144008578
Iteration 10700: Loss = -11251.282158103299
1
Iteration 10800: Loss = -11251.281582457093
2
Iteration 10900: Loss = -11251.281208637756
Iteration 11000: Loss = -11251.31377571877
1
Iteration 11100: Loss = -11251.280408816796
Iteration 11200: Loss = -11251.2864302495
1
Iteration 11300: Loss = -11251.277335807239
Iteration 11400: Loss = -11251.277622546611
1
Iteration 11500: Loss = -11251.274395963024
Iteration 11600: Loss = -11251.273424149922
Iteration 11700: Loss = -11251.27310952906
Iteration 11800: Loss = -11251.273912909122
1
Iteration 11900: Loss = -11251.293937748354
2
Iteration 12000: Loss = -11251.26491187401
Iteration 12100: Loss = -11251.290200346899
1
Iteration 12200: Loss = -11251.273248427751
2
Iteration 12300: Loss = -11251.266978465814
3
Iteration 12400: Loss = -11251.264704146695
Iteration 12500: Loss = -11251.268632131536
1
Iteration 12600: Loss = -11251.352928299455
2
Iteration 12700: Loss = -11251.26333402902
Iteration 12800: Loss = -11251.30173363711
1
Iteration 12900: Loss = -11251.263809681444
2
Iteration 13000: Loss = -11251.390717233062
3
Iteration 13100: Loss = -11251.269102698088
4
Iteration 13200: Loss = -11251.347147082279
5
Iteration 13300: Loss = -11251.262847433516
Iteration 13400: Loss = -11251.263249802092
1
Iteration 13500: Loss = -11251.28614286335
2
Iteration 13600: Loss = -11251.26279636667
Iteration 13700: Loss = -11251.271246972168
1
Iteration 13800: Loss = -11251.262722558598
Iteration 13900: Loss = -11251.267902465546
1
Iteration 14000: Loss = -11251.2624014644
Iteration 14100: Loss = -11251.264384449485
1
Iteration 14200: Loss = -11251.262212962254
Iteration 14300: Loss = -11251.268657015777
1
Iteration 14400: Loss = -11251.26216674025
Iteration 14500: Loss = -11251.262165295138
Iteration 14600: Loss = -11251.263872732161
1
Iteration 14700: Loss = -11251.262145897661
Iteration 14800: Loss = -11251.263937243984
1
Iteration 14900: Loss = -11251.342533390782
2
Iteration 15000: Loss = -11251.261937378029
Iteration 15100: Loss = -11251.262018991938
1
Iteration 15200: Loss = -11251.261960549036
2
Iteration 15300: Loss = -11251.261911472311
Iteration 15400: Loss = -11251.286613819831
1
Iteration 15500: Loss = -11251.26165129764
Iteration 15600: Loss = -11251.262055549189
1
Iteration 15700: Loss = -11251.284875304911
2
Iteration 15800: Loss = -11251.261700271332
3
Iteration 15900: Loss = -11251.261598402752
Iteration 16000: Loss = -11251.309117756506
1
Iteration 16100: Loss = -11251.261549573876
Iteration 16200: Loss = -11251.265007444452
1
Iteration 16300: Loss = -11251.261524076785
Iteration 16400: Loss = -11251.261693225573
1
Iteration 16500: Loss = -11251.261467879633
Iteration 16600: Loss = -11251.262679907955
1
Iteration 16700: Loss = -11251.26144275627
Iteration 16800: Loss = -11251.263172213597
1
Iteration 16900: Loss = -11251.273869480861
2
Iteration 17000: Loss = -11251.261495129818
3
Iteration 17100: Loss = -11251.261422569916
Iteration 17200: Loss = -11251.277934667061
1
Iteration 17300: Loss = -11251.263915490457
2
Iteration 17400: Loss = -11251.26427282607
3
Iteration 17500: Loss = -11251.251625370405
Iteration 17600: Loss = -11251.252003290678
1
Iteration 17700: Loss = -11251.251481274485
Iteration 17800: Loss = -11251.251564437984
1
Iteration 17900: Loss = -11251.25409714533
2
Iteration 18000: Loss = -11251.251501844386
3
Iteration 18100: Loss = -11251.252402358152
4
Iteration 18200: Loss = -11251.445847167042
5
Iteration 18300: Loss = -11251.251494088741
6
Iteration 18400: Loss = -11251.253782404861
7
Iteration 18500: Loss = -11251.263202635237
8
Iteration 18600: Loss = -11251.262769522253
9
Iteration 18700: Loss = -11251.239828694852
Iteration 18800: Loss = -11251.239493418381
Iteration 18900: Loss = -11251.243188035882
1
Iteration 19000: Loss = -11251.239480818062
Iteration 19100: Loss = -11251.239633449168
1
Iteration 19200: Loss = -11251.23892278217
Iteration 19300: Loss = -11251.23837531756
Iteration 19400: Loss = -11251.239569434198
1
Iteration 19500: Loss = -11251.242711721496
2
Iteration 19600: Loss = -11251.23822767737
Iteration 19700: Loss = -11251.242987607406
1
Iteration 19800: Loss = -11251.238232412705
2
Iteration 19900: Loss = -11251.24078357932
3
tensor([[  2.3520,  -6.7529],
        [ -7.8539,   6.1353],
        [ -8.9363,   7.4199],
        [ -9.8983,   8.3443],
        [  2.2682,  -3.6736],
        [  2.5930,  -4.6570],
        [ -5.0527,   3.6577],
        [ -9.8733,   8.1927],
        [ -8.0392,   6.4114],
        [ -5.6286,   2.6876],
        [ -6.9325,   4.2410],
        [  2.4606,  -4.2874],
        [  4.8411,  -6.2498],
        [ -9.6712,   8.2510],
        [  4.1542,  -6.6908],
        [ -6.4630,   4.4496],
        [ -7.1507,   5.7635],
        [ -3.8159,   1.2264],
        [ -9.0138,   6.0121],
        [ -8.3396,   6.2464],
        [ -7.3919,   6.0048],
        [ -5.4053,   3.8431],
        [ -0.2729,  -4.3423],
        [ -8.1990,   4.9915],
        [  2.3863,  -3.8694],
        [ -5.4515,   3.4770],
        [ -5.9363,   3.6678],
        [  3.2959,  -4.6849],
        [ -3.5603,   2.0311],
        [  5.1898,  -6.7714],
        [  3.6048,  -5.0437],
        [ -9.0370,   7.6036],
        [ -3.6254,   2.1906],
        [  4.7748,  -6.9857],
        [ -7.7029,   6.0472],
        [-11.1443,   9.2163],
        [  4.7775,  -7.5434],
        [ -4.0653,   2.6565],
        [ -6.4226,   5.0283],
        [  1.7295,  -5.1365],
        [ -5.0652,   3.4333],
        [  6.3904,  -7.8958],
        [-14.3255,   9.7102],
        [-10.3294,   7.9538],
        [ -9.2952,   7.8864],
        [  4.9904,  -6.9913],
        [  4.3182,  -5.7798],
        [ -6.1409,   4.7064],
        [  2.9439,  -4.4001],
        [  3.2161,  -4.8272],
        [ -5.7019,   3.9007],
        [  0.3168,  -1.9463],
        [  1.9356,  -5.3578],
        [ -0.6241,  -0.7884],
        [ -5.1733,   3.1064],
        [ -5.6354,   4.2484],
        [  3.6147,  -6.0051],
        [  6.5631, -10.0431],
        [ -6.5697,   4.3229],
        [ -7.1377,   5.6405],
        [ -6.3058,   4.8732],
        [  5.7865,  -7.2132],
        [ -3.5426,   1.0339],
        [  2.0639,  -4.4248],
        [  2.6020,  -4.7400],
        [ -7.3676,   5.8459],
        [  0.4910,  -1.9353],
        [  4.1329,  -5.8239],
        [  3.1377,  -6.4386],
        [  2.8619,  -4.4701],
        [  4.2158,  -6.2230],
        [-10.3671,   8.2860],
        [ -7.2391,   5.2060],
        [ -5.6445,   4.2365],
        [ -6.0688,   4.6815],
        [  0.7013,  -2.0928],
        [ -5.8425,   4.4541],
        [  6.2115,  -9.4889],
        [ -3.8734,   2.1999],
        [ -9.3510,   7.9575],
        [ -9.4459,   8.0515],
        [-10.6190,   8.8287],
        [  6.6036,  -7.9916],
        [ -0.4959,  -1.3092],
        [  5.2804,  -8.1594],
        [ -5.2030,   3.1417],
        [ -8.1501,   6.2960],
        [ -6.2900,   4.8801],
        [ -2.2678,   0.7494],
        [  0.5731,  -3.3764],
        [  1.4443,  -2.8400],
        [ -6.6025,   5.0013],
        [  2.1568,  -3.5485],
        [  5.6424,  -7.0305],
        [ -5.6174,   3.0961],
        [  3.4545,  -5.4665],
        [  3.2249,  -7.8401],
        [ -4.1696,   2.7550],
        [ -8.6437,   7.1655],
        [ -3.7518,   2.0959]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.8114, 0.1886],
        [0.2545, 0.7455]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4201, 0.5799], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2045, 0.1037],
         [0.6727, 0.2966]],

        [[0.7680, 0.1053],
         [0.8336, 0.3748]],

        [[0.3860, 0.1150],
         [0.4069, 0.2952]],

        [[0.2154, 0.0928],
         [0.1195, 0.9957]],

        [[0.4347, 0.0973],
         [0.2613, 0.2903]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 2
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208023791611811
time is 3
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 4
tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.844814436176263
Global Adjusted Rand Index: 0.9214425753087997
Average Adjusted Rand Index: 0.9214446172556053
Iteration 0: Loss = -19867.198095457443
Iteration 10: Loss = -11538.910065815855
Iteration 20: Loss = -11537.532440947421
Iteration 30: Loss = -11524.974963808241
Iteration 40: Loss = -11520.150739311375
Iteration 50: Loss = -11516.747443316204
Iteration 60: Loss = -11501.914378631012
Iteration 70: Loss = -11258.303468618444
Iteration 80: Loss = -11257.50260810464
Iteration 90: Loss = -11257.502954344258
1
Iteration 100: Loss = -11257.502949029304
2
Iteration 110: Loss = -11257.502950189686
3
Stopping early at iteration 109 due to no improvement.
pi: tensor([[0.7285, 0.2715],
        [0.2089, 0.7911]], dtype=torch.float64)
alpha: tensor([0.5006, 0.4994])
beta: tensor([[[0.2886, 0.1033],
         [0.1400, 0.2019]],

        [[0.8749, 0.1048],
         [0.9240, 0.5933]],

        [[0.4252, 0.1150],
         [0.5762, 0.5026]],

        [[0.4592, 0.0917],
         [0.4757, 0.2935]],

        [[0.6636, 0.0974],
         [0.8652, 0.5696]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
time is 1
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 2
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.844814436176263
Global Adjusted Rand Index: 0.9061158614302716
Average Adjusted Rand Index: 0.9059289878530082
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19867.013228558735
Iteration 100: Loss = -11539.471951529094
Iteration 200: Loss = -11535.233971906926
Iteration 300: Loss = -11523.422897989105
Iteration 400: Loss = -11514.154733246747
Iteration 500: Loss = -11442.838810424384
Iteration 600: Loss = -11273.701842522567
Iteration 700: Loss = -11256.632013597613
Iteration 800: Loss = -11255.528324391435
Iteration 900: Loss = -11252.877937002731
Iteration 1000: Loss = -11252.650220501228
Iteration 1100: Loss = -11252.542840688358
Iteration 1200: Loss = -11252.471483798736
Iteration 1300: Loss = -11252.42019286063
Iteration 1400: Loss = -11252.382038335983
Iteration 1500: Loss = -11252.352969697404
Iteration 1600: Loss = -11252.32975150899
Iteration 1700: Loss = -11252.098960975865
Iteration 1800: Loss = -11252.082981756002
Iteration 1900: Loss = -11252.069822727824
Iteration 2000: Loss = -11252.058231996629
Iteration 2100: Loss = -11252.047153588039
Iteration 2200: Loss = -11252.029806011666
Iteration 2300: Loss = -11251.957377584966
Iteration 2400: Loss = -11251.950534978072
Iteration 2500: Loss = -11251.944274820497
Iteration 2600: Loss = -11251.938308795668
Iteration 2700: Loss = -11251.93224507437
Iteration 2800: Loss = -11251.924668349215
Iteration 2900: Loss = -11251.906448111928
Iteration 3000: Loss = -11251.347635425325
Iteration 3100: Loss = -11251.337815270623
Iteration 3200: Loss = -11251.333826486714
Iteration 3300: Loss = -11251.330451890248
Iteration 3400: Loss = -11251.326623702884
Iteration 3500: Loss = -11251.315762396433
Iteration 3600: Loss = -11251.30733831126
Iteration 3700: Loss = -11251.299283300154
Iteration 3800: Loss = -11251.297045386465
Iteration 3900: Loss = -11251.294810528572
Iteration 4000: Loss = -11251.293193705804
Iteration 4100: Loss = -11251.292094645634
Iteration 4200: Loss = -11251.29067172201
Iteration 4300: Loss = -11251.28970076193
Iteration 4400: Loss = -11251.289525068436
Iteration 4500: Loss = -11251.287713633097
Iteration 4600: Loss = -11251.288990990957
1
Iteration 4700: Loss = -11251.299394509006
2
Iteration 4800: Loss = -11251.284785716896
Iteration 4900: Loss = -11251.289438044676
1
Iteration 5000: Loss = -11251.28298841644
Iteration 5100: Loss = -11251.291489279143
1
Iteration 5200: Loss = -11251.281574350762
Iteration 5300: Loss = -11251.284695580422
1
Iteration 5400: Loss = -11251.280332247157
Iteration 5500: Loss = -11251.27975108489
Iteration 5600: Loss = -11251.277252605403
Iteration 5700: Loss = -11251.267924922673
Iteration 5800: Loss = -11251.26869350704
1
Iteration 5900: Loss = -11251.267160190611
Iteration 6000: Loss = -11251.27423447563
1
Iteration 6100: Loss = -11251.266699453014
Iteration 6200: Loss = -11251.284985572103
1
Iteration 6300: Loss = -11251.266158704884
Iteration 6400: Loss = -11251.265773488565
Iteration 6500: Loss = -11251.268873296502
1
Iteration 6600: Loss = -11251.265305582598
Iteration 6700: Loss = -11251.265371787911
1
Iteration 6800: Loss = -11251.265037520347
Iteration 6900: Loss = -11251.265634503985
1
Iteration 7000: Loss = -11251.265169475906
2
Iteration 7100: Loss = -11251.278693835771
3
Iteration 7200: Loss = -11251.290120635964
4
Iteration 7300: Loss = -11251.27000367869
5
Iteration 7400: Loss = -11251.264266667953
Iteration 7500: Loss = -11251.26356927998
Iteration 7600: Loss = -11251.264942201786
1
Iteration 7700: Loss = -11251.266525818188
2
Iteration 7800: Loss = -11251.28274833622
3
Iteration 7900: Loss = -11251.263180668182
Iteration 8000: Loss = -11251.263341753554
1
Iteration 8100: Loss = -11251.263155840732
Iteration 8200: Loss = -11251.262879235363
Iteration 8300: Loss = -11251.26320213411
1
Iteration 8400: Loss = -11251.26307280376
2
Iteration 8500: Loss = -11251.304103341821
3
Iteration 8600: Loss = -11251.243451090502
Iteration 8700: Loss = -11251.249507834851
1
Iteration 8800: Loss = -11251.243043654635
Iteration 8900: Loss = -11251.243533266428
1
Iteration 9000: Loss = -11251.303958578805
2
Iteration 9100: Loss = -11251.241427220244
Iteration 9200: Loss = -11251.243003250256
1
Iteration 9300: Loss = -11251.241973796019
2
Iteration 9400: Loss = -11251.249004266243
3
Iteration 9500: Loss = -11251.324167513038
4
Iteration 9600: Loss = -11251.241226232507
Iteration 9700: Loss = -11251.24332907146
1
Iteration 9800: Loss = -11251.246915308078
2
Iteration 9900: Loss = -11251.241099244746
Iteration 10000: Loss = -11251.241132974428
1
Iteration 10100: Loss = -11251.322471594798
2
Iteration 10200: Loss = -11251.240890323223
Iteration 10300: Loss = -11251.29666709426
1
Iteration 10400: Loss = -11251.240853731897
Iteration 10500: Loss = -11251.257359997411
1
Iteration 10600: Loss = -11251.249242459298
2
Iteration 10700: Loss = -11251.397998190987
3
Iteration 10800: Loss = -11251.244473464694
4
Iteration 10900: Loss = -11251.241016543068
5
Iteration 11000: Loss = -11251.240884368473
6
Iteration 11100: Loss = -11251.245208492157
7
Iteration 11200: Loss = -11251.252950085096
8
Iteration 11300: Loss = -11251.250186866155
9
Iteration 11400: Loss = -11251.240555609873
Iteration 11500: Loss = -11251.240776417333
1
Iteration 11600: Loss = -11251.24007727632
Iteration 11700: Loss = -11251.251908507942
1
Iteration 11800: Loss = -11251.229694667314
Iteration 11900: Loss = -11251.231429052375
1
Iteration 12000: Loss = -11251.229650564705
Iteration 12100: Loss = -11251.230013216598
1
Iteration 12200: Loss = -11251.22999487177
2
Iteration 12300: Loss = -11251.229580176807
Iteration 12400: Loss = -11251.232811191729
1
Iteration 12500: Loss = -11251.229685312297
2
Iteration 12600: Loss = -11251.234420409155
3
Iteration 12700: Loss = -11251.229225213558
Iteration 12800: Loss = -11251.23078452693
1
Iteration 12900: Loss = -11251.273216784824
2
Iteration 13000: Loss = -11251.230743305901
3
Iteration 13100: Loss = -11251.233755688481
4
Iteration 13200: Loss = -11251.229521966568
5
Iteration 13300: Loss = -11251.22916094154
Iteration 13400: Loss = -11251.229131955806
Iteration 13500: Loss = -11251.228782389702
Iteration 13600: Loss = -11251.236126168806
1
Iteration 13700: Loss = -11251.229547378718
2
Iteration 13800: Loss = -11251.260114167253
3
Iteration 13900: Loss = -11251.236836669756
4
Iteration 14000: Loss = -11251.228667774092
Iteration 14100: Loss = -11251.22922388624
1
Iteration 14200: Loss = -11251.315267732378
2
Iteration 14300: Loss = -11251.22855363788
Iteration 14400: Loss = -11251.229197711677
1
Iteration 14500: Loss = -11251.226769328798
Iteration 14600: Loss = -11251.226768809134
Iteration 14700: Loss = -11251.237580307947
1
Iteration 14800: Loss = -11251.226757487233
Iteration 14900: Loss = -11251.229122169845
1
Iteration 15000: Loss = -11251.239463834096
2
Iteration 15100: Loss = -11251.229158717002
3
Iteration 15200: Loss = -11251.252573517702
4
Iteration 15300: Loss = -11251.22674073261
Iteration 15400: Loss = -11251.227179362879
1
Iteration 15500: Loss = -11251.226901405118
2
Iteration 15600: Loss = -11251.232969578252
3
Iteration 15700: Loss = -11251.493876946846
4
Iteration 15800: Loss = -11251.226731430746
Iteration 15900: Loss = -11251.277917234927
1
Iteration 16000: Loss = -11251.2267314067
Iteration 16100: Loss = -11251.2268044202
1
Iteration 16200: Loss = -11251.231406943472
2
Iteration 16300: Loss = -11251.226881249428
3
Iteration 16400: Loss = -11251.227094554737
4
Iteration 16500: Loss = -11251.23779957706
5
Iteration 16600: Loss = -11251.22700653405
6
Iteration 16700: Loss = -11251.227087785595
7
Iteration 16800: Loss = -11251.237883480144
8
Iteration 16900: Loss = -11251.226706951753
Iteration 17000: Loss = -11251.227026001108
1
Iteration 17100: Loss = -11251.226875471313
2
Iteration 17200: Loss = -11251.246019295015
3
Iteration 17300: Loss = -11251.226157733608
Iteration 17400: Loss = -11251.226163546711
1
Iteration 17500: Loss = -11251.22801228997
2
Iteration 17600: Loss = -11251.238696678556
3
Iteration 17700: Loss = -11251.22614135238
Iteration 17800: Loss = -11251.226328957702
1
Iteration 17900: Loss = -11251.238827319476
2
Iteration 18000: Loss = -11251.395899441806
3
Iteration 18100: Loss = -11251.22605928832
Iteration 18200: Loss = -11251.266728838802
1
Iteration 18300: Loss = -11251.226258504888
2
Iteration 18400: Loss = -11251.22588801723
Iteration 18500: Loss = -11251.226032717575
1
Iteration 18600: Loss = -11251.225918947957
2
Iteration 18700: Loss = -11251.225767468502
Iteration 18800: Loss = -11251.233838954271
1
Iteration 18900: Loss = -11251.22556676721
Iteration 19000: Loss = -11251.226258828405
1
Iteration 19100: Loss = -11251.300095601377
2
Iteration 19200: Loss = -11251.225679041629
3
Iteration 19300: Loss = -11251.226058981463
4
Iteration 19400: Loss = -11251.270194549863
5
Iteration 19500: Loss = -11251.225839184466
6
Iteration 19600: Loss = -11251.225549029947
Iteration 19700: Loss = -11251.232186269104
1
Iteration 19800: Loss = -11251.35583956946
2
Iteration 19900: Loss = -11251.225488878923
tensor([[ -5.3756,   3.7284],
        [  4.7937,  -6.1801],
        [  8.7160, -10.4792],
        [  5.6025,  -7.0795],
        [ -3.6843,   2.2536],
        [ -4.4444,   2.8025],
        [  3.4051,  -5.3070],
        [  7.7762,  -9.8706],
        [  6.5155,  -7.9802],
        [  3.3871,  -4.9324],
        [  4.7423,  -6.4339],
        [ -4.0684,   2.6753],
        [ -6.3303,   4.7570],
        [  6.9348,  -9.1025],
        [ -6.2570,   4.5859],
        [  4.0577,  -6.8550],
        [  3.4226,  -5.6879],
        [  1.3423,  -3.7159],
        [  6.1944,  -8.9244],
        [  4.7525,  -6.6093],
        [  6.0088,  -7.4001],
        [  3.9016,  -5.3492],
        [ -3.7167,   0.3497],
        [  4.9849,  -8.2067],
        [ -3.8327,   2.4319],
        [  3.6941,  -5.2412],
        [  4.1091,  -5.4956],
        [ -5.1685,   2.8131],
        [  2.0969,  -3.5017],
        [ -6.7243,   5.2341],
        [ -6.6312,   2.0160],
        [  9.2960, -10.7092],
        [  2.1802,  -3.6413],
        [ -6.6163,   5.1386],
        [  5.9307,  -7.8006],
        [  9.2572, -10.8601],
        [-10.8997,   9.5097],
        [  2.4389,  -4.2865],
        [  4.8584,  -6.5931],
        [ -4.1481,   2.7164],
        [  3.5392,  -5.0649],
        [ -7.8989,   6.5119],
        [  5.4809,  -7.7741],
        [  4.4499,  -6.7352],
        [  8.5763, -10.2457],
        [ -6.6824,   5.2886],
        [ -5.9669,   4.1299],
        [  4.6614,  -6.1858],
        [ -4.7177,   2.6230],
        [ -5.2695,   2.7696],
        [  3.9802,  -5.6235],
        [ -1.8236,   0.4358],
        [ -4.3435,   2.9571],
        [ -0.9181,  -0.7585],
        [  3.3001,  -4.9827],
        [  8.8523, -11.5989],
        [ -5.7738,   3.8451],
        [ -6.8881,   5.2875],
        [  8.9627, -10.7511],
        [  5.6794,  -7.0932],
        [  4.8198,  -6.3543],
        [ -7.8911,   5.1080],
        [  0.5652,  -4.0171],
        [ -4.0229,   2.4638],
        [ -4.5705,   2.7701],
        [  5.5204,  -7.6692],
        [ -1.9755,   0.4491],
        [ -6.0810,   3.8740],
        [ -5.6416,   3.9319],
        [ -4.3775,   2.9522],
        [ -6.0404,   4.3974],
        [  9.2778, -10.7465],
        [  5.5139,  -6.9364],
        [  3.6841,  -6.1981],
        [  3.5333,  -7.2199],
        [ -2.2873,   0.5028],
        [  4.3290,  -5.9668],
        [ -9.0500,   6.6507],
        [  2.2053,  -3.8723],
        [  7.8682,  -9.2658],
        [  7.0971,  -8.9717],
        [  4.7925,  -6.7441],
        [ -8.2503,   6.4203],
        [ -1.3689,  -0.5609],
        [ -7.5387,   5.8982],
        [  3.1908,  -5.1576],
        [  6.5177,  -7.9788],
        [  4.5036,  -6.5282],
        [  0.7723,  -2.2521],
        [ -2.7186,   1.2262],
        [ -2.8571,   1.4235],
        [  3.2168,  -4.6931],
        [ -3.5454,   2.1576],
        [ -7.5978,   5.0733],
        [  3.5930,  -5.1226],
        [ -5.1991,   3.7205],
        [ -6.4570,   4.5992],
        [  2.5974,  -4.3337],
        [  7.1767,  -8.5723],
        [  1.6124,  -4.2411]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7454, 0.2546],
        [0.1881, 0.8119]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5797, 0.4203], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2970, 0.1041],
         [0.1400, 0.2042]],

        [[0.8749, 0.1054],
         [0.9240, 0.5933]],

        [[0.4252, 0.1151],
         [0.5762, 0.5026]],

        [[0.4592, 0.0931],
         [0.4757, 0.2935]],

        [[0.6636, 0.0976],
         [0.8652, 0.5696]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 2
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
time is 3
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 4
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.844814436176263
Global Adjusted Rand Index: 0.9214425753087997
Average Adjusted Rand Index: 0.9214446172556053
Iteration 0: Loss = -19186.60213597257
Iteration 10: Loss = -11538.911896848445
Iteration 20: Loss = -11538.911691865946
Iteration 30: Loss = -11538.156688608282
Iteration 40: Loss = -11534.81449804238
Iteration 50: Loss = -11522.456401889736
Iteration 60: Loss = -11518.894469587896
Iteration 70: Loss = -11514.80122977123
Iteration 80: Loss = -11450.221189019272
Iteration 90: Loss = -11257.46826373007
Iteration 100: Loss = -11257.502970003397
1
Iteration 110: Loss = -11257.502952126342
2
Iteration 120: Loss = -11257.502950189999
3
Stopping early at iteration 119 due to no improvement.
pi: tensor([[0.7285, 0.2715],
        [0.2089, 0.7911]], dtype=torch.float64)
alpha: tensor([0.5006, 0.4994])
beta: tensor([[[0.2886, 0.1033],
         [0.8472, 0.2019]],

        [[0.9212, 0.1048],
         [0.4391, 0.6795]],

        [[0.3762, 0.1150],
         [0.8238, 0.5644]],

        [[0.4610, 0.0917],
         [0.3720, 0.4504]],

        [[0.8270, 0.0974],
         [0.5459, 0.9814]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
time is 1
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 2
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.844814436176263
Global Adjusted Rand Index: 0.9061158614302716
Average Adjusted Rand Index: 0.9059289878530082
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19186.53660744349
Iteration 100: Loss = -11541.01575576269
Iteration 200: Loss = -11537.789871396919
Iteration 300: Loss = -11528.709908231283
Iteration 400: Loss = -11521.838838550786
Iteration 500: Loss = -11517.22917238566
Iteration 600: Loss = -11493.931528268651
Iteration 700: Loss = -11404.635931318582
Iteration 800: Loss = -11263.789855248331
Iteration 900: Loss = -11256.516087543696
Iteration 1000: Loss = -11255.662657108616
Iteration 1100: Loss = -11255.289069809109
Iteration 1200: Loss = -11255.074449222453
Iteration 1300: Loss = -11254.93889171222
Iteration 1400: Loss = -11254.84482825891
Iteration 1500: Loss = -11254.77525648791
Iteration 1600: Loss = -11254.721754109845
Iteration 1700: Loss = -11254.6791046547
Iteration 1800: Loss = -11254.644371424078
Iteration 1900: Loss = -11254.615428492092
Iteration 2000: Loss = -11254.590713523516
Iteration 2100: Loss = -11254.568500160509
Iteration 2200: Loss = -11254.544211225642
Iteration 2300: Loss = -11254.525301884249
Iteration 2400: Loss = -11254.673107365888
1
Iteration 2500: Loss = -11254.499642063643
Iteration 2600: Loss = -11254.48741300388
Iteration 2700: Loss = -11254.472735918898
Iteration 2800: Loss = -11254.441457829322
Iteration 2900: Loss = -11252.2999282015
Iteration 3000: Loss = -11252.250841745603
Iteration 3100: Loss = -11252.261265898107
1
Iteration 3200: Loss = -11252.236885489347
Iteration 3300: Loss = -11252.229999753028
Iteration 3400: Loss = -11252.222714132062
Iteration 3500: Loss = -11252.215294797914
Iteration 3600: Loss = -11252.206393991491
Iteration 3700: Loss = -11252.197997580199
Iteration 3800: Loss = -11252.185115732878
Iteration 3900: Loss = -11252.177706062363
Iteration 4000: Loss = -11252.174778163077
Iteration 4100: Loss = -11252.256203126155
1
Iteration 4200: Loss = -11252.170339304075
Iteration 4300: Loss = -11252.168449174033
Iteration 4400: Loss = -11252.169326038365
1
Iteration 4500: Loss = -11252.166801536641
Iteration 4600: Loss = -11252.177735629542
1
Iteration 4700: Loss = -11252.165841103233
Iteration 4800: Loss = -11252.161320726615
Iteration 4900: Loss = -11252.159508640048
Iteration 5000: Loss = -11252.157553952404
Iteration 5100: Loss = -11252.156250272497
Iteration 5200: Loss = -11252.161469451374
1
Iteration 5300: Loss = -11252.15371647235
Iteration 5400: Loss = -11252.156753063948
1
Iteration 5500: Loss = -11252.149874442954
Iteration 5600: Loss = -11252.146715906345
Iteration 5700: Loss = -11252.143043178163
Iteration 5800: Loss = -11252.15160780126
1
Iteration 5900: Loss = -11252.141156660828
Iteration 6000: Loss = -11252.152523314708
1
Iteration 6100: Loss = -11252.139992202203
Iteration 6200: Loss = -11252.141824538383
1
Iteration 6300: Loss = -11252.15389972445
2
Iteration 6400: Loss = -11252.139696967775
Iteration 6500: Loss = -11252.140451271187
1
Iteration 6600: Loss = -11252.138340658754
Iteration 6700: Loss = -11252.138854486853
1
Iteration 6800: Loss = -11252.136725632667
Iteration 6900: Loss = -11252.136156646737
Iteration 7000: Loss = -11252.140053693176
1
Iteration 7100: Loss = -11252.196081067143
2
Iteration 7200: Loss = -11252.13863830147
3
Iteration 7300: Loss = -11252.131920339307
Iteration 7400: Loss = -11252.131031194402
Iteration 7500: Loss = -11252.128071381154
Iteration 7600: Loss = -11252.128019406218
Iteration 7700: Loss = -11252.142689381577
1
Iteration 7800: Loss = -11252.174398446496
2
Iteration 7900: Loss = -11252.13850328722
3
Iteration 8000: Loss = -11252.13505198047
4
Iteration 8100: Loss = -11252.128345375217
5
Iteration 8200: Loss = -11252.122001890046
Iteration 8300: Loss = -11252.123384815734
1
Iteration 8400: Loss = -11252.237286921882
2
Iteration 8500: Loss = -11252.122299419992
3
Iteration 8600: Loss = -11252.119274843937
Iteration 8700: Loss = -11252.121766226139
1
Iteration 8800: Loss = -11252.12297659365
2
Iteration 8900: Loss = -11252.119135174764
Iteration 9000: Loss = -11252.146902850393
1
Iteration 9100: Loss = -11252.12073303547
2
Iteration 9200: Loss = -11252.143041582876
3
Iteration 9300: Loss = -11252.117838158001
Iteration 9400: Loss = -11252.152328239983
1
Iteration 9500: Loss = -11252.117619315404
Iteration 9600: Loss = -11252.1175229493
Iteration 9700: Loss = -11252.118779751781
1
Iteration 9800: Loss = -11252.117208483749
Iteration 9900: Loss = -11252.128142201293
1
Iteration 10000: Loss = -11252.116904835515
Iteration 10100: Loss = -11252.298305788549
1
Iteration 10200: Loss = -11252.116817919006
Iteration 10300: Loss = -11252.15410651411
1
Iteration 10400: Loss = -11252.116745824282
Iteration 10500: Loss = -11252.11771127951
1
Iteration 10600: Loss = -11252.116650422317
Iteration 10700: Loss = -11252.339436553853
1
Iteration 10800: Loss = -11252.116488612604
Iteration 10900: Loss = -11252.156804220369
1
Iteration 11000: Loss = -11252.11269252037
Iteration 11100: Loss = -11252.190396757524
1
Iteration 11200: Loss = -11252.114265544738
2
Iteration 11300: Loss = -11252.109726558556
Iteration 11400: Loss = -11252.10980151983
1
Iteration 11500: Loss = -11252.110353882974
2
Iteration 11600: Loss = -11252.34254259089
3
Iteration 11700: Loss = -11252.10950015256
Iteration 11800: Loss = -11252.114141275992
1
Iteration 11900: Loss = -11252.109361143926
Iteration 12000: Loss = -11252.11054502315
1
Iteration 12100: Loss = -11252.109333906115
Iteration 12200: Loss = -11252.109483746402
1
Iteration 12300: Loss = -11252.193522382127
2
Iteration 12400: Loss = -11252.109430432687
3
Iteration 12500: Loss = -11252.109601394468
4
Iteration 12600: Loss = -11252.111488838662
5
Iteration 12700: Loss = -11252.109146017832
Iteration 12800: Loss = -11252.15473353905
1
Iteration 12900: Loss = -11252.109183404937
2
Iteration 13000: Loss = -11252.298707313179
3
Iteration 13100: Loss = -11252.109077810639
Iteration 13200: Loss = -11252.22732155304
1
Iteration 13300: Loss = -11252.109707659665
2
Iteration 13400: Loss = -11252.10908662004
3
Iteration 13500: Loss = -11252.142600108184
4
Iteration 13600: Loss = -11252.108994606453
Iteration 13700: Loss = -11252.139400599746
1
Iteration 13800: Loss = -11252.11134091487
2
Iteration 13900: Loss = -11251.547484963057
Iteration 14000: Loss = -11251.543430372285
Iteration 14100: Loss = -11251.533555689059
Iteration 14200: Loss = -11251.53451524904
1
Iteration 14300: Loss = -11251.54023939814
2
Iteration 14400: Loss = -11251.713834302773
3
Iteration 14500: Loss = -11251.533532490392
Iteration 14600: Loss = -11251.54600044486
1
Iteration 14700: Loss = -11251.533510813297
Iteration 14800: Loss = -11251.69938084778
1
Iteration 14900: Loss = -11251.533836199618
2
Iteration 15000: Loss = -11251.53472020674
3
Iteration 15100: Loss = -11251.533406953413
Iteration 15200: Loss = -11251.533564219886
1
Iteration 15300: Loss = -11251.53335933855
Iteration 15400: Loss = -11251.533780671763
1
Iteration 15500: Loss = -11251.533412491795
2
Iteration 15600: Loss = -11251.533998514746
3
Iteration 15700: Loss = -11251.534229943687
4
Iteration 15800: Loss = -11251.534089116927
5
Iteration 15900: Loss = -11251.53335342928
Iteration 16000: Loss = -11251.533479794753
1
Iteration 16100: Loss = -11251.533355343838
2
Iteration 16200: Loss = -11251.533379602482
3
Iteration 16300: Loss = -11251.53330882486
Iteration 16400: Loss = -11251.5350171109
1
Iteration 16500: Loss = -11251.53325358802
Iteration 16600: Loss = -11251.62980755066
1
Iteration 16700: Loss = -11251.53325041046
Iteration 16800: Loss = -11251.53325871171
1
Iteration 16900: Loss = -11251.53422444316
2
Iteration 17000: Loss = -11251.533259867569
3
Iteration 17100: Loss = -11251.533352953435
4
Iteration 17200: Loss = -11251.533345994274
5
Iteration 17300: Loss = -11251.71037219415
6
Iteration 17400: Loss = -11251.3149247182
Iteration 17500: Loss = -11251.436743736775
1
Iteration 17600: Loss = -11251.229966339695
Iteration 17700: Loss = -11251.230388516273
1
Iteration 17800: Loss = -11251.2299533447
Iteration 17900: Loss = -11251.230300761203
1
Iteration 18000: Loss = -11251.231380506062
2
Iteration 18100: Loss = -11251.250383021787
3
Iteration 18200: Loss = -11251.229139152874
Iteration 18300: Loss = -11251.229842202782
1
Iteration 18400: Loss = -11251.236434435066
2
Iteration 18500: Loss = -11251.229762330164
3
Iteration 18600: Loss = -11251.228765682788
Iteration 18700: Loss = -11251.239498468913
1
Iteration 18800: Loss = -11251.269238022998
2
Iteration 18900: Loss = -11251.227652491409
Iteration 19000: Loss = -11251.228758088379
1
Iteration 19100: Loss = -11251.227655367418
2
Iteration 19200: Loss = -11251.228504689703
3
Iteration 19300: Loss = -11251.236521390547
4
Iteration 19400: Loss = -11251.227670828108
5
Iteration 19500: Loss = -11251.235197775575
6
Iteration 19600: Loss = -11251.22763270635
Iteration 19700: Loss = -11251.235887379942
1
Iteration 19800: Loss = -11251.226843978184
Iteration 19900: Loss = -11251.236991350828
1
tensor([[ -5.8283,   3.2822],
        [  4.6033,  -6.3700],
        [  7.7800,  -9.1748],
        [  5.4373,  -7.2500],
        [ -3.6793,   2.2732],
        [ -4.4300,   2.8276],
        [  3.6541,  -5.0512],
        [  8.0941,  -9.4871],
        [  6.4229,  -8.0494],
        [  2.9396,  -5.3703],
        [  4.8938,  -6.2824],
        [ -4.2587,   2.4974],
        [ -6.2821,   4.8073],
        [  7.3342,  -9.1275],
        [ -6.3961,   4.4468],
        [  4.6385,  -6.2744],
        [  3.7216,  -5.3830],
        [  1.8185,  -3.2253],
        [  6.7358,  -8.3749],
        [  4.9589,  -6.4023],
        [  4.3910,  -9.0062],
        [  3.9265,  -5.3202],
        [ -3.1896,   0.8903],
        [  5.4868,  -7.7136],
        [ -3.8317,   2.4244],
        [  3.7459,  -5.1813],
        [  3.1894,  -6.4075],
        [ -4.7034,   3.2779],
        [  1.6701,  -3.9129],
        [ -7.3990,   4.5869],
        [ -5.0191,   3.6324],
        [  8.5699, -10.1101],
        [  2.1958,  -3.6122],
        [ -6.5812,   5.1736],
        [  5.7329,  -8.0036],
        [  9.6072, -11.0207],
        [-12.4111,   7.9013],
        [  1.8342,  -4.8793],
        [  5.8617,  -8.0830],
        [ -4.5542,   2.3199],
        [  3.5222,  -5.0767],
        [ -7.9353,   6.4764],
        [  5.9328,  -7.3191],
        [  4.8935,  -6.2806],
        [  8.1658,  -9.5842],
        [ -7.2874,   4.6839],
        [ -5.7492,   4.3477],
        [  4.7260,  -6.1202],
        [ -4.4544,   2.8968],
        [ -6.3342,   1.7190],
        [  3.4189,  -6.1819],
        [ -1.8334,   0.4405],
        [ -4.4600,   2.8517],
        [ -1.9527,  -1.7789],
        [  3.4350,  -4.8369],
        [  4.1541,  -5.7288],
        [ -5.5133,   4.1073],
        [ -6.7899,   5.3872],
        [  4.5875,  -6.3040],
        [  5.7163,  -7.1084],
        [  8.0908, -12.7060],
        [ -7.3747,   5.6244],
        [  1.5632,  -3.0067],
        [ -3.9510,   2.5446],
        [ -4.3741,   2.9759],
        [  5.9007,  -7.2870],
        [ -3.3218,  -0.8778],
        [ -5.8033,   4.1533],
        [ -5.4921,   4.0831],
        [ -4.3665,   2.9684],
        [ -6.1486,   4.2896],
        [  8.9492, -11.1789],
        [  5.0024,  -7.4454],
        [  3.8440,  -6.0366],
        [  4.0037,  -6.7435],
        [ -3.7111,  -0.9042],
        [  2.8368,  -7.4521],
        [ -8.6653,   7.0836],
        [  2.0098,  -4.0550],
        [  8.6464, -11.0555],
        [  8.4172, -10.1323],
        [  4.9670,  -6.5698],
        [ -8.3479,   6.3698],
        [ -1.9942,  -1.1702],
        [ -7.5660,   5.8867],
        [  3.4747,  -4.8637],
        [  6.3895,  -8.1125],
        [  4.0284,  -7.0013],
        [  0.5893,  -2.4209],
        [ -2.7529,   1.2086],
        [ -4.1839,   0.1120],
        [  3.1112,  -4.7880],
        [ -3.5655,   2.1504],
        [ -7.2733,   5.3989],
        [  3.6577,  -5.0453],
        [ -5.1562,   3.7650],
        [ -6.2712,   4.7883],
        [  2.7548,  -4.1614],
        [  6.8023,  -9.2029],
        [  2.0188,  -3.8210]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7462, 0.2538],
        [0.1902, 0.8098]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5823, 0.4177], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2956, 0.1032],
         [0.8472, 0.2053]],

        [[0.9212, 0.1047],
         [0.4391, 0.6795]],

        [[0.3762, 0.1145],
         [0.8238, 0.5644]],

        [[0.4610, 0.0922],
         [0.3720, 0.4504]],

        [[0.8270, 0.0967],
         [0.5459, 0.9814]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 2
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
time is 3
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 4
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.844814436176263
Global Adjusted Rand Index: 0.9214425753087997
Average Adjusted Rand Index: 0.9214446172556053
Iteration 0: Loss = -23123.79311735819
Iteration 10: Loss = -11420.24168489942
Iteration 20: Loss = -11257.488497004564
Iteration 30: Loss = -11257.502952143699
1
Iteration 40: Loss = -11257.502971394893
2
Iteration 50: Loss = -11257.502967946753
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.7285, 0.2715],
        [0.2089, 0.7911]], dtype=torch.float64)
alpha: tensor([0.5006, 0.4994])
beta: tensor([[[0.2886, 0.1033],
         [0.4886, 0.2019]],

        [[0.4887, 0.1048],
         [0.7544, 0.9138]],

        [[0.4073, 0.1150],
         [0.4241, 0.8827]],

        [[0.4363, 0.0917],
         [0.6100, 0.8296]],

        [[0.7492, 0.0974],
         [0.3300, 0.4954]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
time is 1
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 2
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.844814436176263
Global Adjusted Rand Index: 0.9061158614302716
Average Adjusted Rand Index: 0.9059289878530082
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23123.496318376318
Iteration 100: Loss = -11595.004703372817
Iteration 200: Loss = -11565.490752264715
Iteration 300: Loss = -11546.71067868703
Iteration 400: Loss = -11542.836318700804
Iteration 500: Loss = -11539.485261627158
Iteration 600: Loss = -11537.922296063814
Iteration 700: Loss = -11537.309811154319
Iteration 800: Loss = -11536.881546799612
Iteration 900: Loss = -11536.483298281228
Iteration 1000: Loss = -11536.254370131672
Iteration 1100: Loss = -11536.08097359766
Iteration 1200: Loss = -11535.94512744664
Iteration 1300: Loss = -11535.851614660773
Iteration 1400: Loss = -11535.76956995933
Iteration 1500: Loss = -11535.710297219475
Iteration 1600: Loss = -11535.664519637
Iteration 1700: Loss = -11535.62672566574
Iteration 1800: Loss = -11535.594857986305
Iteration 1900: Loss = -11535.56742164548
Iteration 2000: Loss = -11535.5425752234
Iteration 2100: Loss = -11535.511831489894
Iteration 2200: Loss = -11535.468842359465
Iteration 2300: Loss = -11534.929490129676
Iteration 2400: Loss = -11534.349156781109
Iteration 2500: Loss = -11533.964284440444
Iteration 2600: Loss = -11531.043215917089
Iteration 2700: Loss = -11530.86867580238
Iteration 2800: Loss = -11530.765930647545
Iteration 2900: Loss = -11530.732601838257
Iteration 3000: Loss = -11530.710397739424
Iteration 3100: Loss = -11530.691370836272
Iteration 3200: Loss = -11530.676804681902
Iteration 3300: Loss = -11530.667788236924
Iteration 3400: Loss = -11530.6606680708
Iteration 3500: Loss = -11530.654550671816
Iteration 3600: Loss = -11530.648957434647
Iteration 3700: Loss = -11530.643813376646
Iteration 3800: Loss = -11530.63810080729
Iteration 3900: Loss = -11530.614989229647
Iteration 4000: Loss = -11528.532313355821
Iteration 4100: Loss = -11528.198281330266
Iteration 4200: Loss = -11528.09271678994
Iteration 4300: Loss = -11528.031334827881
Iteration 4400: Loss = -11527.956652899535
Iteration 4500: Loss = -11527.90880592329
Iteration 4600: Loss = -11527.842021356653
Iteration 4700: Loss = -11527.719841100496
Iteration 4800: Loss = -11527.68061654704
Iteration 4900: Loss = -11527.59313949385
Iteration 5000: Loss = -11527.216598756064
Iteration 5100: Loss = -11527.073821320655
Iteration 5200: Loss = -11527.043725084084
Iteration 5300: Loss = -11527.012324105615
Iteration 5400: Loss = -11527.036957778379
1
Iteration 5500: Loss = -11526.945642369692
Iteration 5600: Loss = -11526.907320258799
Iteration 5700: Loss = -11526.880776592408
Iteration 5800: Loss = -11526.862713124847
Iteration 5900: Loss = -11527.039379421334
1
Iteration 6000: Loss = -11526.827034578833
Iteration 6100: Loss = -11526.819083239976
Iteration 6200: Loss = -11526.850028173345
1
Iteration 6300: Loss = -11526.803875924486
Iteration 6400: Loss = -11526.748297230994
Iteration 6500: Loss = -11526.47608956309
Iteration 6600: Loss = -11526.50934329277
1
Iteration 6700: Loss = -11526.464046226882
Iteration 6800: Loss = -11526.475651474719
1
Iteration 6900: Loss = -11526.456861533612
Iteration 7000: Loss = -11526.455912339186
Iteration 7100: Loss = -11526.45353211114
Iteration 7200: Loss = -11526.451395059566
Iteration 7300: Loss = -11526.45458027989
1
Iteration 7400: Loss = -11526.450419544626
Iteration 7500: Loss = -11526.447978334512
Iteration 7600: Loss = -11526.447699296723
Iteration 7700: Loss = -11526.446260208304
Iteration 7800: Loss = -11526.445808197905
Iteration 7900: Loss = -11526.44531141634
Iteration 8000: Loss = -11526.467059852293
1
Iteration 8100: Loss = -11526.44633683593
2
Iteration 8200: Loss = -11526.444469468388
Iteration 8300: Loss = -11526.448803141015
1
Iteration 8400: Loss = -11526.443982100765
Iteration 8500: Loss = -11526.443783898229
Iteration 8600: Loss = -11526.444063071514
1
Iteration 8700: Loss = -11526.443407916924
Iteration 8800: Loss = -11526.443190889953
Iteration 8900: Loss = -11526.44287997839
Iteration 9000: Loss = -11526.485348429438
1
Iteration 9100: Loss = -11526.442398603507
Iteration 9200: Loss = -11526.44167204592
Iteration 9300: Loss = -11526.441006359431
Iteration 9400: Loss = -11526.440450650789
Iteration 9500: Loss = -11526.439313165401
Iteration 9600: Loss = -11526.435267180856
Iteration 9700: Loss = -11526.38960415099
Iteration 9800: Loss = -11526.371731968498
Iteration 9900: Loss = -11526.368035167709
Iteration 10000: Loss = -11526.366311835343
Iteration 10100: Loss = -11526.366175004327
Iteration 10200: Loss = -11526.366400892113
1
Iteration 10300: Loss = -11526.422792167516
2
Iteration 10400: Loss = -11526.3683528259
3
Iteration 10500: Loss = -11526.367546262523
4
Iteration 10600: Loss = -11526.36602467874
Iteration 10700: Loss = -11526.434294698289
1
Iteration 10800: Loss = -11526.365532952606
Iteration 10900: Loss = -11526.456464269557
1
Iteration 11000: Loss = -11526.36545847797
Iteration 11100: Loss = -11526.365475222887
1
Iteration 11200: Loss = -11526.36646666109
2
Iteration 11300: Loss = -11526.369522916933
3
Iteration 11400: Loss = -11526.73604523151
4
Iteration 11500: Loss = -11526.377392696419
5
Iteration 11600: Loss = -11526.40609569326
6
Iteration 11700: Loss = -11526.36535458006
Iteration 11800: Loss = -11526.531037105691
1
Iteration 11900: Loss = -11526.365435793809
2
Iteration 12000: Loss = -11526.365233161425
Iteration 12100: Loss = -11526.370489998752
1
Iteration 12200: Loss = -11526.36568196874
2
Iteration 12300: Loss = -11526.365417590287
3
Iteration 12400: Loss = -11526.370662661122
4
Iteration 12500: Loss = -11526.415781846228
5
Iteration 12600: Loss = -11526.366653392386
6
Iteration 12700: Loss = -11526.365353884858
7
Iteration 12800: Loss = -11526.408574683872
8
Iteration 12900: Loss = -11526.3650492142
Iteration 13000: Loss = -11526.368445771444
1
Iteration 13100: Loss = -11526.401156714066
2
Iteration 13200: Loss = -11526.36742103655
3
Iteration 13300: Loss = -11526.365485408915
4
Iteration 13400: Loss = -11526.371952349731
5
Iteration 13500: Loss = -11526.39413854279
6
Iteration 13600: Loss = -11526.366124676053
7
Iteration 13700: Loss = -11526.365123183286
8
Iteration 13800: Loss = -11526.373029687385
9
Iteration 13900: Loss = -11526.370395391024
10
Stopping early at iteration 13900 due to no improvement.
tensor([[-4.8457,  3.4068],
        [-3.3056,  1.9010],
        [-2.9445,  1.1005],
        [-4.3491,  2.9021],
        [-4.1807,  2.6160],
        [-5.3613,  3.3235],
        [-2.9763,  1.4417],
        [-2.5490,  0.2489],
        [-3.0589,  1.6330],
        [-3.0415,  1.6082],
        [-3.9109,  2.1025],
        [-6.2293,  4.7045],
        [-2.3527,  0.9359],
        [-1.0274, -0.4088],
        [-5.6257,  3.4563],
        [-2.4621,  0.9782],
        [-5.4440,  0.8288],
        [-3.1335,  1.1943],
        [-2.7547,  1.3682],
        [-4.5245,  2.3514],
        [-3.4839,  1.5531],
        [-4.1745,  1.6338],
        [-4.1571,  2.7689],
        [-2.7048,  0.8875],
        [-2.7345,  1.3028],
        [-3.3538,  1.7504],
        [-3.2873,  1.2555],
        [-4.5158,  3.1292],
        [-5.8249,  3.3851],
        [-2.2940,  0.9072],
        [-3.8757,  2.3499],
        [-1.4714, -0.3352],
        [-4.2231,  1.6411],
        [-4.6298,  3.1714],
        [-1.2204, -0.5581],
        [-1.0828, -1.2382],
        [-4.1137,  2.7253],
        [-5.2142,  3.5414],
        [-4.3336,  2.9345],
        [-7.8593,  6.3095],
        [-4.7210,  3.1063],
        [-5.1328,  0.5176],
        [-3.4126,  2.0135],
        [-3.9604,  1.8615],
        [-3.9541,  2.0243],
        [-5.1349,  3.7445],
        [-4.6884,  2.4867],
        [-3.2563,  0.5156],
        [-4.8168,  3.0376],
        [-3.5933,  2.2012],
        [-3.2585,  1.5729],
        [-2.6081,  1.0664],
        [-3.2934,  1.4591],
        [-5.4209,  2.7072],
        [-2.7881,  1.2981],
        [-5.0030,  1.4654],
        [-5.8334,  3.5961],
        [-5.1185,  3.6145],
        [-2.3713,  0.8482],
        [-2.7211,  0.9411],
        [-4.0869,  2.6034],
        [-7.3233,  2.7081],
        [-3.4347,  2.0415],
        [-4.2143,  2.6466],
        [-4.0816,  2.6878],
        [-2.3514,  0.1802],
        [-1.8324,  0.4457],
        [-6.5231,  3.8020],
        [-6.0833,  4.6887],
        [-4.5717,  2.8942],
        [-4.6423,  2.1630],
        [ 0.6044, -2.1132],
        [ 1.6068, -3.0859],
        [-3.0327,  1.2544],
        [-3.5084,  0.9898],
        [-4.9420,  3.5382],
        [-4.3957,  1.0389],
        [-4.1831,  2.0724],
        [-2.0531, -1.0514],
        [-2.8525,  1.2732],
        [-3.0008,  1.5088],
        [ 1.1595, -3.3387],
        [-3.9889,  2.1926],
        [-5.5055,  3.7591],
        [-4.2725,  2.4061],
        [-3.3064,  1.8380],
        [-3.1380,  1.4834],
        [-3.1510,  1.6933],
        [-5.1790,  2.9222],
        [-3.8799,  2.4925],
        [-6.3572,  4.0189],
        [-4.1470,  2.6751],
        [-5.8022,  3.9273],
        [-3.4653,  1.8235],
        [-3.7117,  1.3716],
        [-4.8903,  3.4605],
        [-4.8012,  3.2060],
        [-2.7089,  0.8251],
        [-4.3244,  2.7592],
        [-3.5070,  1.7690]], dtype=torch.float64, requires_grad=True)
pi: tensor([[6.4677e-01, 3.5323e-01],
        [1.0000e+00, 7.7139e-07]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0544, 0.9456], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1733, 0.2793],
         [0.4886, 0.1768]],

        [[0.4887, 0.3045],
         [0.7544, 0.9138]],

        [[0.4073, 0.1859],
         [0.4241, 0.8827]],

        [[0.4363, 0.1609],
         [0.6100, 0.8296]],

        [[0.7492, 0.1610],
         [0.3300, 0.4954]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.014778186472389411
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 59
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0014367305347718008
Average Adjusted Rand Index: -0.0031078623973487527
11293.149371766836
new:  [0.9214425753087997, 0.9214425753087997, 0.9214425753087997, 0.0014367305347718008] [0.9214446172556053, 0.9214446172556053, 0.9214446172556053, -0.0031078623973487527] [11251.237743250094, 11251.235016736597, 11251.286209355267, 11526.370395391024]
prior:  [0.0, 0.9061158614302716, 0.9061158614302716, 0.9061158614302716] [0.0, 0.9059289878530082, 0.9059289878530082, 0.9059289878530082] [nan, 11257.502950189686, 11257.502950189999, 11257.502967946753]
-----------------------------------------------------------------------------------------
This iteration is 9
True Objective function: Loss = -10922.335915728168
Iteration 0: Loss = -18867.47528578713
Iteration 10: Loss = -11120.669882041893
Iteration 20: Loss = -11120.648864756186
Iteration 30: Loss = -11120.648068308345
Iteration 40: Loss = -11120.648278894536
1
Iteration 50: Loss = -11120.64850078668
2
Iteration 60: Loss = -11120.648587706664
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.0075, 0.9925],
        [0.0176, 0.9824]], dtype=torch.float64)
alpha: tensor([0.0176, 0.9824])
beta: tensor([[[0.1210, 0.1232],
         [0.8856, 0.1640]],

        [[0.9377, 0.1628],
         [0.3655, 0.3329]],

        [[0.0562, 0.3104],
         [0.1815, 0.2761]],

        [[0.4059, 0.1088],
         [0.3389, 0.8775]],

        [[0.4125, 0.1971],
         [0.0539, 0.1644]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 37
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 40
Adjusted Rand Index: 0.00877236457352431
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0019004397557010203
Average Adjusted Rand Index: 0.0017544729147048617
tensor([[-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19352.872692026536
Iteration 100: Loss = -11131.928573878096
Iteration 200: Loss = -11127.48058183174
Iteration 300: Loss = -11125.957106199596
Iteration 400: Loss = -11125.014670124625
Iteration 500: Loss = -11122.77890543305
Iteration 600: Loss = -11122.523658318054
Iteration 700: Loss = -11122.358640360382
Iteration 800: Loss = -11122.200961223662
Iteration 900: Loss = -11121.988388618185
Iteration 1000: Loss = -11121.812186925257
Iteration 1100: Loss = -11121.724601472437
Iteration 1200: Loss = -11121.635002402856
Iteration 1300: Loss = -11121.534866254371
Iteration 1400: Loss = -11121.436935697424
Iteration 1500: Loss = -11121.359922979374
Iteration 1600: Loss = -11121.306876863786
Iteration 1700: Loss = -11121.266820312108
Iteration 1800: Loss = -11121.23236839962
Iteration 1900: Loss = -11121.201500563853
Iteration 2000: Loss = -11121.173874017279
Iteration 2100: Loss = -11121.14907834544
Iteration 2200: Loss = -11121.12637322528
Iteration 2300: Loss = -11121.104872140382
Iteration 2400: Loss = -11121.083813879486
Iteration 2500: Loss = -11121.062581029886
Iteration 2600: Loss = -11121.04050947169
Iteration 2700: Loss = -11121.017030217323
Iteration 2800: Loss = -11120.991407549809
Iteration 2900: Loss = -11120.962646471828
Iteration 3000: Loss = -11120.929981150191
Iteration 3100: Loss = -11120.892688836155
Iteration 3200: Loss = -11120.850600953918
Iteration 3300: Loss = -11120.801001651258
Iteration 3400: Loss = -11120.722317239331
Iteration 3500: Loss = -11120.628385808699
Iteration 3600: Loss = -11120.57413019442
Iteration 3700: Loss = -11120.547270582056
Iteration 3800: Loss = -11120.528492339645
Iteration 3900: Loss = -11120.515387898526
Iteration 4000: Loss = -11120.506400030003
Iteration 4100: Loss = -11120.500128728689
Iteration 4200: Loss = -11120.495589575143
Iteration 4300: Loss = -11120.492161710728
Iteration 4400: Loss = -11120.48937653687
Iteration 4500: Loss = -11120.487109035557
Iteration 4600: Loss = -11120.48518924618
Iteration 4700: Loss = -11120.483480670608
Iteration 4800: Loss = -11120.482013202638
Iteration 4900: Loss = -11120.480641320974
Iteration 5000: Loss = -11120.479402817076
Iteration 5100: Loss = -11120.484087461464
1
Iteration 5200: Loss = -11120.496121039612
2
Iteration 5300: Loss = -11120.607722684794
3
Iteration 5400: Loss = -11120.475434006703
Iteration 5500: Loss = -11120.546347231144
1
Iteration 5600: Loss = -11120.473888793507
Iteration 5700: Loss = -11120.473228611243
Iteration 5800: Loss = -11120.472622149902
Iteration 5900: Loss = -11120.47191198478
Iteration 6000: Loss = -11120.519263623184
1
Iteration 6100: Loss = -11120.47081351237
Iteration 6200: Loss = -11120.470280554044
Iteration 6300: Loss = -11120.469808905198
Iteration 6400: Loss = -11120.469502979775
Iteration 6500: Loss = -11120.468969162503
Iteration 6600: Loss = -11120.468554172281
Iteration 6700: Loss = -11120.469679870574
1
Iteration 6800: Loss = -11120.467834588339
Iteration 6900: Loss = -11120.467472892879
Iteration 7000: Loss = -11120.54133332423
1
Iteration 7100: Loss = -11120.466823821593
Iteration 7200: Loss = -11120.46648654428
Iteration 7300: Loss = -11120.466212879774
Iteration 7400: Loss = -11120.465790490949
Iteration 7500: Loss = -11120.465135390159
Iteration 7600: Loss = -11120.463549859804
Iteration 7700: Loss = -11120.376898326216
Iteration 7800: Loss = -11119.43903349369
Iteration 7900: Loss = -11119.378822445537
Iteration 8000: Loss = -11118.85287201453
Iteration 8100: Loss = -11118.76196562597
Iteration 8200: Loss = -11118.740672000109
Iteration 8300: Loss = -11118.782390048127
1
Iteration 8400: Loss = -11118.725201557743
Iteration 8500: Loss = -11118.721453860626
Iteration 8600: Loss = -11118.718834014368
Iteration 8700: Loss = -11118.716854838554
Iteration 8800: Loss = -11118.715284484395
Iteration 8900: Loss = -11118.714061356917
Iteration 9000: Loss = -11118.846521921376
1
Iteration 9100: Loss = -11118.712186306248
Iteration 9200: Loss = -11118.7114821212
Iteration 9300: Loss = -11118.71084818008
Iteration 9400: Loss = -11118.710407807781
Iteration 9500: Loss = -11118.709881355227
Iteration 9600: Loss = -11118.709416420841
Iteration 9700: Loss = -11118.747139807407
1
Iteration 9800: Loss = -11118.70878178937
Iteration 9900: Loss = -11118.708457118946
Iteration 10000: Loss = -11118.708217014211
Iteration 10100: Loss = -11118.70880879651
1
Iteration 10200: Loss = -11118.707742651393
Iteration 10300: Loss = -11118.707569049855
Iteration 10400: Loss = -11118.83492876809
1
Iteration 10500: Loss = -11118.707219511216
Iteration 10600: Loss = -11118.707033614486
Iteration 10700: Loss = -11118.706907746973
Iteration 10800: Loss = -11118.707061765956
1
Iteration 10900: Loss = -11118.7066265097
Iteration 11000: Loss = -11118.706504495156
Iteration 11100: Loss = -11118.720642527904
1
Iteration 11200: Loss = -11118.706384211619
Iteration 11300: Loss = -11118.706267574235
Iteration 11400: Loss = -11118.706209404794
Iteration 11500: Loss = -11118.754110919019
1
Iteration 11600: Loss = -11118.706046503761
Iteration 11700: Loss = -11118.706001742483
Iteration 11800: Loss = -11118.705895123914
Iteration 11900: Loss = -11118.70886158994
1
Iteration 12000: Loss = -11118.705794367823
Iteration 12100: Loss = -11118.705750801008
Iteration 12200: Loss = -11118.873965081462
1
Iteration 12300: Loss = -11118.705583442415
Iteration 12400: Loss = -11118.70551995582
Iteration 12500: Loss = -11118.705505145266
Iteration 12600: Loss = -11118.706541293486
1
Iteration 12700: Loss = -11118.705456023587
Iteration 12800: Loss = -11118.70542830168
Iteration 12900: Loss = -11118.70587889382
1
Iteration 13000: Loss = -11118.705404217038
Iteration 13100: Loss = -11118.705387112546
Iteration 13200: Loss = -11118.70538373562
Iteration 13300: Loss = -11118.705361248809
Iteration 13400: Loss = -11118.705342820793
Iteration 13500: Loss = -11118.707192610018
1
Iteration 13600: Loss = -11118.752935256056
2
Iteration 13700: Loss = -11118.705330721374
Iteration 13800: Loss = -11118.774315923774
1
Iteration 13900: Loss = -11118.705328143118
Iteration 14000: Loss = -11118.705329916662
1
Iteration 14100: Loss = -11118.705362994455
2
Iteration 14200: Loss = -11118.705588541727
3
Iteration 14300: Loss = -11118.705413689668
4
Iteration 14400: Loss = -11118.705255306415
Iteration 14500: Loss = -11118.705253593944
Iteration 14600: Loss = -11118.70579508978
1
Iteration 14700: Loss = -11118.705492595005
2
Iteration 14800: Loss = -11118.70525455347
3
Iteration 14900: Loss = -11118.705222408465
Iteration 15000: Loss = -11118.705236732258
1
Iteration 15100: Loss = -11118.705988427972
2
Iteration 15200: Loss = -11118.70520159209
Iteration 15300: Loss = -11118.705187460195
Iteration 15400: Loss = -11118.70687343407
1
Iteration 15500: Loss = -11118.705146674354
Iteration 15600: Loss = -11118.705139935448
Iteration 15700: Loss = -11118.705138004727
Iteration 15800: Loss = -11118.705177802984
1
Iteration 15900: Loss = -11118.705084448735
Iteration 16000: Loss = -11118.705742695587
1
Iteration 16100: Loss = -11118.70556439235
2
Iteration 16200: Loss = -11118.70505976678
Iteration 16300: Loss = -11118.705058439282
Iteration 16400: Loss = -11118.708994329374
1
Iteration 16500: Loss = -11118.705035107374
Iteration 16600: Loss = -11118.70504548945
1
Iteration 16700: Loss = -11119.322601240438
2
Iteration 16800: Loss = -11118.705016366674
Iteration 16900: Loss = -11118.705006603486
Iteration 17000: Loss = -11118.704982593183
Iteration 17100: Loss = -11118.720875032644
1
Iteration 17200: Loss = -11118.704979575461
Iteration 17300: Loss = -11118.704979165841
Iteration 17400: Loss = -11118.706678086766
1
Iteration 17500: Loss = -11118.705017463242
2
Iteration 17600: Loss = -11118.704962944783
Iteration 17700: Loss = -11119.035836784837
1
Iteration 17800: Loss = -11118.70495914242
Iteration 17900: Loss = -11118.704934230176
Iteration 18000: Loss = -11118.704934273148
1
Iteration 18100: Loss = -11118.705136823304
2
Iteration 18200: Loss = -11118.704934642808
3
Iteration 18300: Loss = -11118.70493507871
4
Iteration 18400: Loss = -11118.712015644733
5
Iteration 18500: Loss = -11118.704934505071
6
Iteration 18600: Loss = -11118.70493095068
Iteration 18700: Loss = -11118.707561387044
1
Iteration 18800: Loss = -11118.70590740533
2
Iteration 18900: Loss = -11118.704921072356
Iteration 19000: Loss = -11118.70491955353
Iteration 19100: Loss = -11118.982153225907
1
Iteration 19200: Loss = -11118.704931608552
2
Iteration 19300: Loss = -11118.704912134835
Iteration 19400: Loss = -11118.704968441216
1
Iteration 19500: Loss = -11118.705811605168
2
Iteration 19600: Loss = -11118.70489933065
Iteration 19700: Loss = -11118.7049043513
1
Iteration 19800: Loss = -11118.705307175698
2
Iteration 19900: Loss = -11118.70490370662
3
tensor([[-11.6559,   7.0407],
        [-11.2465,   6.6313],
        [-11.5307,   6.9155],
        [-12.6058,   7.9906],
        [-12.5640,   7.9488],
        [-10.5532,   5.9379],
        [-12.5010,   7.8858],
        [-12.4899,   7.8747],
        [-12.5507,   7.9355],
        [-12.6177,   8.0025],
        [-12.4595,   7.8443],
        [-12.4519,   7.8367],
        [-10.9956,   6.3804],
        [-12.0818,   7.4666],
        [-12.7617,   8.1465],
        [-11.4312,   6.8160],
        [-12.6165,   8.0013],
        [-11.5477,   6.9325],
        [-12.3517,   7.7365],
        [-12.5408,   7.9256],
        [-12.6903,   8.0751],
        [-12.3930,   7.7778],
        [-12.4060,   7.7908],
        [-12.5289,   7.9137],
        [-11.4619,   6.8466],
        [-12.5258,   7.9105],
        [-12.6246,   8.0094],
        [-12.1459,   7.5306],
        [ -2.9076,  -1.7077],
        [-11.3202,   6.7050],
        [-12.4873,   7.8721],
        [-12.7265,   8.1112],
        [-12.6247,   8.0095],
        [-12.6327,   8.0175],
        [-11.0627,   6.4475],
        [-12.7751,   8.1599],
        [-11.4907,   6.8755],
        [-12.6296,   8.0144],
        [-12.6594,   8.0442],
        [-12.6256,   8.0104],
        [-11.6714,   7.0562],
        [-12.5476,   7.9324],
        [-12.6210,   8.0058],
        [-11.3601,   6.7449],
        [-11.4719,   6.8567],
        [  2.7795,  -7.3947],
        [-11.4886,   6.8734],
        [-12.6127,   7.9974],
        [-11.1019,   6.4867],
        [-11.1336,   6.5184],
        [-10.7390,   6.1238],
        [-11.4179,   6.8027],
        [-10.7635,   6.1483],
        [-12.3341,   7.7189],
        [-11.8575,   7.2422],
        [-11.2818,   6.6665],
        [-12.5749,   7.9597],
        [-11.5074,   6.8921],
        [-12.5282,   7.9129],
        [-11.1204,   6.5052],
        [-10.3509,   5.7357],
        [-12.5248,   7.9096],
        [-12.6409,   8.0257],
        [-12.5338,   7.9186],
        [-12.6698,   8.0546],
        [-12.5775,   7.9623],
        [-11.4974,   6.8822],
        [-12.4225,   7.8073],
        [-12.7198,   8.1046],
        [-11.3747,   6.7594],
        [-11.1676,   6.5523],
        [-12.1914,   7.5761],
        [-12.4635,   7.8483],
        [-12.5661,   7.9509],
        [ -9.5805,   4.9653],
        [-11.4224,   6.8072],
        [-12.1428,   7.5276],
        [-11.5585,   6.9433],
        [-10.8658,   6.2506],
        [-10.4867,   5.8714],
        [ -1.0942,  -3.5210],
        [-12.6211,   8.0059],
        [-12.6745,   8.0592],
        [-12.4403,   7.8251],
        [-12.4197,   7.8045],
        [-12.5454,   7.9302],
        [-12.3834,   7.7682],
        [-12.5308,   7.9156],
        [-10.9630,   6.3478],
        [-11.3430,   6.7278],
        [-11.5601,   6.9448],
        [-12.1947,   7.5795],
        [-12.5339,   7.9187],
        [-11.4559,   6.8406],
        [-10.5308,   5.9156],
        [-10.0227,   5.4075],
        [-11.4519,   6.8366],
        [-12.5633,   7.9481],
        [-12.2948,   7.6796],
        [-12.4444,   7.8292]], dtype=torch.float64, requires_grad=True)
pi: tensor([[7.7274e-01, 2.2726e-01],
        [3.1027e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0215, 0.9785], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[1.0000, 0.1409],
         [0.8856, 0.1657]],

        [[0.9377, 0.0981],
         [0.3655, 0.3329]],

        [[0.0562, 0.3326],
         [0.1815, 0.2761]],

        [[0.4059, 0.2020],
         [0.3389, 0.8775]],

        [[0.4125, 0.2121],
         [0.0539, 0.1644]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 39
Adjusted Rand Index: -0.015623423336712405
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.001829943009345947
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 40
Adjusted Rand Index: 0.00877236457352431
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 40
Adjusted Rand Index: 0.00877236457352431
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.007614171548597778
Global Adjusted Rand Index: 0.0036069163954011887
Average Adjusted Rand Index: 0.0022730840736559874
Iteration 0: Loss = -15850.827052808436
Iteration 10: Loss = -11120.942733426435
Iteration 20: Loss = -11120.506747976917
Iteration 30: Loss = -11120.507745376606
1
Iteration 40: Loss = -11120.510193673597
2
Iteration 50: Loss = -11120.510719735541
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.9820, 0.0180],
        [0.9939, 0.0061]], dtype=torch.float64)
alpha: tensor([0.9827, 0.0173])
beta: tensor([[[0.1630, 0.2634],
         [0.7202, 0.1341]],

        [[0.0747, 0.1704],
         [0.3526, 0.7934]],

        [[0.5054, 0.3086],
         [0.3864, 0.0837]],

        [[0.1228, 0.1100],
         [0.6339, 0.3478]],

        [[0.9323, 0.1999],
         [0.6787, 0.5097]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 64
Adjusted Rand Index: 0.014019411492836234
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 59
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 60
Adjusted Rand Index: 0.00877236457352431
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 59
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0038318294750973054
Average Adjusted Rand Index: 0.004558355213272108
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -15850.286319913319
Iteration 100: Loss = -11124.863992612794
Iteration 200: Loss = -11122.64407946518
Iteration 300: Loss = -11121.823197754997
Iteration 400: Loss = -11121.442930697438
Iteration 500: Loss = -11121.181048117753
Iteration 600: Loss = -11120.984624526776
Iteration 700: Loss = -11120.841906268026
Iteration 800: Loss = -11120.739850038619
Iteration 900: Loss = -11120.666247704434
Iteration 1000: Loss = -11120.612318833939
Iteration 1100: Loss = -11120.571467973574
Iteration 1200: Loss = -11120.539581069408
Iteration 1300: Loss = -11120.51371873406
Iteration 1400: Loss = -11120.4923462428
Iteration 1500: Loss = -11120.474178074775
Iteration 1600: Loss = -11120.458478880635
Iteration 1700: Loss = -11120.444706703562
Iteration 1800: Loss = -11120.43240260535
Iteration 1900: Loss = -11120.421260698711
Iteration 2000: Loss = -11120.411035076826
Iteration 2100: Loss = -11120.401551132956
Iteration 2200: Loss = -11120.392536996433
Iteration 2300: Loss = -11120.383996516168
Iteration 2400: Loss = -11120.375736875641
Iteration 2500: Loss = -11120.367960900543
Iteration 2600: Loss = -11120.360823606326
Iteration 2700: Loss = -11120.785126000424
1
Iteration 2800: Loss = -11120.349307769995
Iteration 2900: Loss = -11120.344934021636
Iteration 3000: Loss = -11120.341172622864
Iteration 3100: Loss = -11120.337886405203
Iteration 3200: Loss = -11120.334893332629
Iteration 3300: Loss = -11120.33231271331
Iteration 3400: Loss = -11120.330515903963
Iteration 3500: Loss = -11120.327887923198
Iteration 3600: Loss = -11120.326132995215
Iteration 3700: Loss = -11120.324540895599
Iteration 3800: Loss = -11120.324203302082
Iteration 3900: Loss = -11120.321878485038
Iteration 4000: Loss = -11120.32077079054
Iteration 4100: Loss = -11120.327408014124
1
Iteration 4200: Loss = -11120.318912090379
Iteration 4300: Loss = -11120.318126223276
Iteration 4400: Loss = -11120.317423157621
Iteration 4500: Loss = -11120.316731162546
Iteration 4600: Loss = -11120.31615065284
Iteration 4700: Loss = -11120.315601568807
Iteration 4800: Loss = -11120.39544651018
1
Iteration 4900: Loss = -11120.314627039445
Iteration 5000: Loss = -11120.314171703014
Iteration 5100: Loss = -11120.313763755765
Iteration 5200: Loss = -11120.32071067809
1
Iteration 5300: Loss = -11120.313011661612
Iteration 5400: Loss = -11120.312671820424
Iteration 5500: Loss = -11120.312905845365
1
Iteration 5600: Loss = -11120.312042901
Iteration 5700: Loss = -11120.311763556638
Iteration 5800: Loss = -11120.31142841879
Iteration 5900: Loss = -11120.311231153668
Iteration 6000: Loss = -11120.310963592434
Iteration 6100: Loss = -11120.31076000064
Iteration 6200: Loss = -11120.32338206795
1
Iteration 6300: Loss = -11120.310309277535
Iteration 6400: Loss = -11120.310097416406
Iteration 6500: Loss = -11120.338955942936
1
Iteration 6600: Loss = -11120.309765064549
Iteration 6700: Loss = -11120.309569084418
Iteration 6800: Loss = -11120.309397911926
Iteration 6900: Loss = -11120.309448930919
1
Iteration 7000: Loss = -11120.309100347575
Iteration 7100: Loss = -11120.308968659705
Iteration 7200: Loss = -11120.30939140577
1
Iteration 7300: Loss = -11120.308700087198
Iteration 7400: Loss = -11120.308613456684
Iteration 7500: Loss = -11120.355874206998
1
Iteration 7600: Loss = -11120.308397043338
Iteration 7700: Loss = -11120.308251412727
Iteration 7800: Loss = -11120.308156837023
Iteration 7900: Loss = -11120.309099196118
1
Iteration 8000: Loss = -11120.307964040538
Iteration 8100: Loss = -11120.307923567845
Iteration 8200: Loss = -11120.313773262573
1
Iteration 8300: Loss = -11120.307804065582
Iteration 8400: Loss = -11120.307689624191
Iteration 8500: Loss = -11120.308472209328
1
Iteration 8600: Loss = -11120.307579281776
Iteration 8700: Loss = -11120.307620777756
1
Iteration 8800: Loss = -11120.307453390049
Iteration 8900: Loss = -11120.308451397399
1
Iteration 9000: Loss = -11120.30735164935
Iteration 9100: Loss = -11120.31257897174
1
Iteration 9200: Loss = -11120.30930367042
2
Iteration 9300: Loss = -11120.314893716533
3
Iteration 9400: Loss = -11120.317332421773
4
Iteration 9500: Loss = -11120.307185500042
Iteration 9600: Loss = -11120.311161882615
1
Iteration 9700: Loss = -11120.307096284232
Iteration 9800: Loss = -11120.307077710631
Iteration 9900: Loss = -11120.313770485209
1
Iteration 10000: Loss = -11120.306927934305
Iteration 10100: Loss = -11120.373292138822
1
Iteration 10200: Loss = -11120.314653082354
2
Iteration 10300: Loss = -11120.306885567938
Iteration 10400: Loss = -11120.307178915426
1
Iteration 10500: Loss = -11120.402047919024
2
Iteration 10600: Loss = -11120.306806370929
Iteration 10700: Loss = -11120.322854844586
1
Iteration 10800: Loss = -11120.306722246982
Iteration 10900: Loss = -11120.307196060237
1
Iteration 11000: Loss = -11120.309848225683
2
Iteration 11100: Loss = -11120.306686084019
Iteration 11200: Loss = -11120.307331373495
1
Iteration 11300: Loss = -11120.30696765732
2
Iteration 11400: Loss = -11120.354308308453
3
Iteration 11500: Loss = -11120.306607703717
Iteration 11600: Loss = -11120.310790353138
1
Iteration 11700: Loss = -11120.306545504722
Iteration 11800: Loss = -11120.306531099131
Iteration 11900: Loss = -11120.30747426653
1
Iteration 12000: Loss = -11120.30661331537
2
Iteration 12100: Loss = -11120.310907629522
3
Iteration 12200: Loss = -11120.306484477069
Iteration 12300: Loss = -11120.306686458121
1
Iteration 12400: Loss = -11120.309288299382
2
Iteration 12500: Loss = -11120.307081349523
3
Iteration 12600: Loss = -11120.311067171166
4
Iteration 12700: Loss = -11120.306461078559
Iteration 12800: Loss = -11120.307331266424
1
Iteration 12900: Loss = -11120.306473930086
2
Iteration 13000: Loss = -11120.307007228903
3
Iteration 13100: Loss = -11120.307297515132
4
Iteration 13200: Loss = -11120.319526238625
5
Iteration 13300: Loss = -11120.316416950107
6
Iteration 13400: Loss = -11120.306375851147
Iteration 13500: Loss = -11120.307354129864
1
Iteration 13600: Loss = -11120.314820541444
2
Iteration 13700: Loss = -11120.30686726607
3
Iteration 13800: Loss = -11120.306489543444
4
Iteration 13900: Loss = -11120.425742508727
5
Iteration 14000: Loss = -11120.306370686623
Iteration 14100: Loss = -11120.306369712527
Iteration 14200: Loss = -11120.306482514558
1
Iteration 14300: Loss = -11120.307004513941
2
Iteration 14400: Loss = -11120.323138303927
3
Iteration 14500: Loss = -11120.306387297873
4
Iteration 14600: Loss = -11120.3071118047
5
Iteration 14700: Loss = -11120.306470302177
6
Iteration 14800: Loss = -11120.30655751455
7
Iteration 14900: Loss = -11120.318896671726
8
Iteration 15000: Loss = -11120.307843277094
9
Iteration 15100: Loss = -11120.307394314561
10
Stopping early at iteration 15100 due to no improvement.
tensor([[-5.1372e-02, -1.4142e+00],
        [-6.3171e-01, -2.3281e+00],
        [ 3.7062e-02, -1.4933e+00],
        [-4.3927e-01, -2.3029e+00],
        [ 3.2102e-01, -1.7092e+00],
        [-1.1451e-01, -2.0606e+00],
        [-2.7197e-01, -1.9681e+00],
        [ 3.7830e-02, -1.4931e+00],
        [ 3.0868e-01, -1.8880e+00],
        [-5.7167e-01, -2.4350e+00],
        [-9.1541e-01, -2.3626e+00],
        [-2.9023e-02, -1.4766e+00],
        [ 8.5411e-02, -1.6955e+00],
        [-1.4657e-01, -1.2597e+00],
        [-9.1099e-02, -1.8715e+00],
        [ 5.7296e-02, -1.8058e+00],
        [-8.4235e-01, -2.5402e+00],
        [-9.5431e-03, -1.5399e+00],
        [ 6.5459e-02, -1.4643e+00],
        [-4.9962e-02, -1.5807e+00],
        [ 2.5221e-02, -1.9211e+00],
        [-6.9131e-02, -2.0160e+00],
        [ 1.6658e-01, -1.6135e+00],
        [-1.4724e-01, -2.0108e+00],
        [-1.4296e-01, -2.2562e+00],
        [-4.9160e-01, -2.0219e+00],
        [ 3.1180e-01, -1.8019e+00],
        [-6.5038e-01, -1.7640e+00],
        [-1.4253e-01, -1.8397e+00],
        [ 2.1304e-01, -2.0671e+00],
        [ 1.5295e-01, -1.5440e+00],
        [ 1.1383e-01, -1.5006e+00],
        [-8.3082e-01, -2.5276e+00],
        [ 2.8742e-01, -1.7426e+00],
        [-6.9823e-02, -1.5163e+00],
        [ 3.1549e-02, -1.7487e+00],
        [-2.3603e-01, -1.5993e+00],
        [ 2.3596e-01, -1.6267e+00],
        [-1.2550e+00, -2.8687e+00],
        [ 2.4899e-02, -1.9222e+00],
        [ 1.4097e-01, -1.6391e+00],
        [ 3.6151e-01, -1.7520e+00],
        [ 1.6766e-01, -1.6127e+00],
        [ 1.6599e-01, -2.2810e+00],
        [ 1.3593e-01, -1.5612e+00],
        [ 8.6413e-02, -1.6752e+00],
        [ 1.3853e-01, -1.8080e+00],
        [-8.6795e-02, -1.7838e+00],
        [ 2.7398e-01, -1.6725e+00],
        [ 2.3544e-01, -1.6286e+00],
        [-2.9282e-01, -2.8194e+00],
        [-3.0368e-02, -1.3945e+00],
        [ 2.5769e-01, -2.0225e+00],
        [-5.9627e-02, -1.3403e+00],
        [ 4.7515e-01, -1.8874e+00],
        [-6.7850e-02, -1.6811e+00],
        [-1.9688e-01, -1.8936e+00],
        [ 2.6866e-01, -1.6785e+00],
        [ 1.7594e-01, -1.7708e+00],
        [ 2.3680e-01, -1.7933e+00],
        [-1.3688e-01, -1.2506e+00],
        [ 1.0314e-01, -1.6777e+00],
        [-8.1694e-02, -1.7790e+00],
        [ 1.5206e-01, -1.5451e+00],
        [-3.8681e-01, -2.1671e+00],
        [-5.7849e-01, -2.4417e+00],
        [ 2.3043e-01, -1.6336e+00],
        [ 6.3514e-01, -2.2289e+00],
        [-1.3482e-01, -1.9147e+00],
        [ 5.5351e-01, -1.9762e+00],
        [-7.6168e-01, -2.2915e+00],
        [ 2.0361e-02, -1.4262e+00],
        [-3.0715e-03, -2.0327e+00],
        [ 2.2990e-01, -1.7167e+00],
        [ 8.8638e-02, -1.7746e+00],
        [-1.3309e-01, -2.1632e+00],
        [-1.4775e-01, -1.6774e+00],
        [ 2.7126e-01, -1.7589e+00],
        [ 1.0547e-01, -1.5084e+00],
        [ 2.0636e-01, -1.9071e+00],
        [-1.5285e-02, -1.6282e+00],
        [-1.0331e+00, -2.8973e+00],
        [ 2.2800e-01, -1.6354e+00],
        [-9.5302e-02, -1.6251e+00],
        [-9.5673e-03, -1.4572e+00],
        [-7.2085e-02, -1.6856e+00],
        [ 1.7145e-02, -2.0128e+00],
        [ 6.3444e-02, -1.4635e+00],
        [ 4.0015e-02, -1.6564e+00],
        [-1.8271e-02, -2.0490e+00],
        [-5.1644e-01, -2.3807e+00],
        [-1.1123e-01, -1.3078e+00],
        [ 2.7919e-01, -1.6683e+00],
        [-1.1676e+00, -3.4476e+00],
        [ 2.0738e-01, -1.6560e+00],
        [-1.2320e-01, -2.3204e+00],
        [ 1.1221e-01, -1.8341e+00],
        [-1.9818e-01, -1.8947e+00],
        [-1.5032e+00, -2.7833e+00],
        [ 2.9549e-02, -1.4171e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.8109e-01, 1.8912e-02],
        [9.9998e-01, 2.4317e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8527, 0.1473], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1661, 0.1548],
         [0.7202, 0.1443]],

        [[0.0747, 0.1673],
         [0.3526, 0.7934]],

        [[0.5054, 0.3092],
         [0.3864, 0.0837]],

        [[0.1228, 0.1102],
         [0.6339, 0.3478]],

        [[0.9323, 0.1984],
         [0.6787, 0.5097]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 63
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 59
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 60
Adjusted Rand Index: 0.00877236457352431
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 59
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0019004397557010203
Average Adjusted Rand Index: 0.0017544729147048617
Iteration 0: Loss = -17893.10403859876
Iteration 10: Loss = -11121.126315935286
Iteration 20: Loss = -11120.50671351503
Iteration 30: Loss = -11120.507258329677
1
Iteration 40: Loss = -11120.510112583142
2
Iteration 50: Loss = -11120.51069044901
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.0061, 0.9939],
        [0.0180, 0.9820]], dtype=torch.float64)
alpha: tensor([0.0173, 0.9827])
beta: tensor([[[0.1340, 0.2634],
         [0.1882, 0.1630]],

        [[0.4359, 0.1704],
         [0.2076, 0.3838]],

        [[0.4345, 0.3086],
         [0.0719, 0.9326]],

        [[0.5913, 0.1100],
         [0.5147, 0.4772]],

        [[0.5393, 0.1999],
         [0.2632, 0.3035]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 36
Adjusted Rand Index: 0.014019411492836234
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 40
Adjusted Rand Index: 0.00877236457352431
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0038318294750973054
Average Adjusted Rand Index: 0.004558355213272108
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17892.848146464512
Iteration 100: Loss = -11133.987402905701
Iteration 200: Loss = -11123.749238493492
Iteration 300: Loss = -11122.439709372544
Iteration 400: Loss = -11121.918924149482
Iteration 500: Loss = -11121.613897561292
Iteration 600: Loss = -11121.401458383025
Iteration 700: Loss = -11121.23824935822
Iteration 800: Loss = -11121.102601164745
Iteration 900: Loss = -11120.985589011796
Iteration 1000: Loss = -11120.885950288206
Iteration 1100: Loss = -11120.804825726234
Iteration 1200: Loss = -11120.738801797197
Iteration 1300: Loss = -11120.680826674654
Iteration 1400: Loss = -11120.641086130372
Iteration 1500: Loss = -11120.608366663548
Iteration 1600: Loss = -11120.582233066438
Iteration 1700: Loss = -11120.595200422085
1
Iteration 1800: Loss = -11120.540714909841
Iteration 1900: Loss = -11120.523555211996
Iteration 2000: Loss = -11120.508072552797
Iteration 2100: Loss = -11120.635720202175
1
Iteration 2200: Loss = -11120.48124082879
Iteration 2300: Loss = -11120.46979921836
Iteration 2400: Loss = -11120.459633870589
Iteration 2500: Loss = -11120.451395833443
Iteration 2600: Loss = -11120.442847668177
Iteration 2700: Loss = -11120.4359521708
Iteration 2800: Loss = -11120.439562858543
1
Iteration 2900: Loss = -11120.42422525819
Iteration 3000: Loss = -11120.419157754202
Iteration 3100: Loss = -11120.414390420126
Iteration 3200: Loss = -11120.415949454431
1
Iteration 3300: Loss = -11120.405370663366
Iteration 3400: Loss = -11120.400907187752
Iteration 3500: Loss = -11120.396276030297
Iteration 3600: Loss = -11120.394413702106
Iteration 3700: Loss = -11120.385999200942
Iteration 3800: Loss = -11120.380317428495
Iteration 3900: Loss = -11120.374156691947
Iteration 4000: Loss = -11120.386263943352
1
Iteration 4100: Loss = -11120.361950850673
Iteration 4200: Loss = -11120.356638411264
Iteration 4300: Loss = -11120.352022582665
Iteration 4400: Loss = -11120.34809654663
Iteration 4500: Loss = -11120.344648261644
Iteration 4600: Loss = -11120.34192230547
Iteration 4700: Loss = -11120.339662026861
Iteration 4800: Loss = -11120.337844760094
Iteration 4900: Loss = -11120.336273537709
Iteration 5000: Loss = -11120.335008229253
Iteration 5100: Loss = -11120.410186712348
1
Iteration 5200: Loss = -11120.333124541878
Iteration 5300: Loss = -11120.33236400812
Iteration 5400: Loss = -11120.331726738574
Iteration 5500: Loss = -11120.331195617897
Iteration 5600: Loss = -11120.330673167164
Iteration 5700: Loss = -11120.330242226795
Iteration 5800: Loss = -11120.333986147341
1
Iteration 5900: Loss = -11120.329525821462
Iteration 6000: Loss = -11120.32915331498
Iteration 6100: Loss = -11120.343531077198
1
Iteration 6200: Loss = -11120.328522908903
Iteration 6300: Loss = -11120.3282749987
Iteration 6400: Loss = -11120.355291365388
1
Iteration 6500: Loss = -11120.327770181704
Iteration 6600: Loss = -11120.327526925883
Iteration 6700: Loss = -11120.327277226641
Iteration 6800: Loss = -11120.329862834113
1
Iteration 6900: Loss = -11120.32689548354
Iteration 7000: Loss = -11120.326711253767
Iteration 7100: Loss = -11120.383719465532
1
Iteration 7200: Loss = -11120.326360743322
Iteration 7300: Loss = -11120.326191375967
Iteration 7400: Loss = -11120.326063863362
Iteration 7500: Loss = -11120.326214397508
1
Iteration 7600: Loss = -11120.325775654086
Iteration 7700: Loss = -11120.325630208137
Iteration 7800: Loss = -11120.325712120344
1
Iteration 7900: Loss = -11120.331987929438
2
Iteration 8000: Loss = -11120.325278167324
Iteration 8100: Loss = -11120.329443628314
1
Iteration 8200: Loss = -11120.325416021615
2
Iteration 8300: Loss = -11120.325450172282
3
Iteration 8400: Loss = -11120.348104058727
4
Iteration 8500: Loss = -11120.325742448395
5
Iteration 8600: Loss = -11120.457772835081
6
Iteration 8700: Loss = -11120.324691290623
Iteration 8800: Loss = -11120.32454538791
Iteration 8900: Loss = -11120.32527455845
1
Iteration 9000: Loss = -11120.324421601536
Iteration 9100: Loss = -11120.350954929598
1
Iteration 9200: Loss = -11120.32431050978
Iteration 9300: Loss = -11120.33227368652
1
Iteration 9400: Loss = -11120.324205806919
Iteration 9500: Loss = -11120.324113563995
Iteration 9600: Loss = -11120.326283028944
1
Iteration 9700: Loss = -11120.323986549647
Iteration 9800: Loss = -11120.324889614792
1
Iteration 9900: Loss = -11120.32391665622
Iteration 10000: Loss = -11120.341206322406
1
Iteration 10100: Loss = -11120.323835095694
Iteration 10200: Loss = -11120.323803679377
Iteration 10300: Loss = -11120.323892995013
1
Iteration 10400: Loss = -11120.327088429895
2
Iteration 10500: Loss = -11120.324299048703
3
Iteration 10600: Loss = -11120.323652367213
Iteration 10700: Loss = -11120.323826131456
1
Iteration 10800: Loss = -11120.323801798648
2
Iteration 10900: Loss = -11120.324118467777
3
Iteration 11000: Loss = -11120.325074407658
4
Iteration 11100: Loss = -11120.365839480672
5
Iteration 11200: Loss = -11120.323496516214
Iteration 11300: Loss = -11120.324289998425
1
Iteration 11400: Loss = -11120.323692763508
2
Iteration 11500: Loss = -11120.323449314104
Iteration 11600: Loss = -11120.356768983866
1
Iteration 11700: Loss = -11120.323387141558
Iteration 11800: Loss = -11120.326207824744
1
Iteration 11900: Loss = -11120.325064520248
2
Iteration 12000: Loss = -11120.323318795214
Iteration 12100: Loss = -11120.326492968676
1
Iteration 12200: Loss = -11120.384371523383
2
Iteration 12300: Loss = -11120.328733099745
3
Iteration 12400: Loss = -11120.323338524346
4
Iteration 12500: Loss = -11120.325012000589
5
Iteration 12600: Loss = -11120.324357305297
6
Iteration 12700: Loss = -11120.323300536998
Iteration 12800: Loss = -11120.323563465023
1
Iteration 12900: Loss = -11120.32326352746
Iteration 13000: Loss = -11120.323312236489
1
Iteration 13100: Loss = -11120.340845474686
2
Iteration 13200: Loss = -11120.323200022664
Iteration 13300: Loss = -11120.434221596948
1
Iteration 13400: Loss = -11120.387401585855
2
Iteration 13500: Loss = -11120.324309813122
3
Iteration 13600: Loss = -11120.324688027584
4
Iteration 13700: Loss = -11120.323318298317
5
Iteration 13800: Loss = -11120.333387715398
6
Iteration 13900: Loss = -11120.32314040819
Iteration 14000: Loss = -11120.33709148099
1
Iteration 14100: Loss = -11120.323106322357
Iteration 14200: Loss = -11120.323132694835
1
Iteration 14300: Loss = -11120.323752866989
2
Iteration 14400: Loss = -11120.323255110767
3
Iteration 14500: Loss = -11120.420091858512
4
Iteration 14600: Loss = -11120.32309728909
Iteration 14700: Loss = -11120.323185648367
1
Iteration 14800: Loss = -11120.323462635357
2
Iteration 14900: Loss = -11120.323113569842
3
Iteration 15000: Loss = -11120.323236068578
4
Iteration 15100: Loss = -11120.323074414322
Iteration 15200: Loss = -11120.324207398487
1
Iteration 15300: Loss = -11120.323081885246
2
Iteration 15400: Loss = -11120.325398909492
3
Iteration 15500: Loss = -11120.323082599325
4
Iteration 15600: Loss = -11120.354344849138
5
Iteration 15700: Loss = -11120.323072498071
Iteration 15800: Loss = -11120.323143661286
1
Iteration 15900: Loss = -11120.538755906324
2
Iteration 16000: Loss = -11120.323066743433
Iteration 16100: Loss = -11120.321604778765
Iteration 16200: Loss = -11120.306347291473
Iteration 16300: Loss = -11120.306656931823
1
Iteration 16400: Loss = -11120.309659683116
2
Iteration 16500: Loss = -11120.306417200673
3
Iteration 16600: Loss = -11120.30642620901
4
Iteration 16700: Loss = -11120.483911643098
5
Iteration 16800: Loss = -11120.306392954451
6
Iteration 16900: Loss = -11120.307393316065
7
Iteration 17000: Loss = -11120.306370342849
8
Iteration 17100: Loss = -11120.306915943527
9
Iteration 17200: Loss = -11120.313544223514
10
Stopping early at iteration 17200 due to no improvement.
tensor([[-1.3863e+00, -3.0890e-02],
        [-1.8537e+00, -1.6588e-01],
        [-1.5185e+00,  3.6992e-03],
        [-2.0589e+00, -1.9893e-01],
        [-1.9087e+00,  1.1932e-01],
        [-2.1303e+00, -1.8709e-01],
        [-2.0477e+00, -3.6044e-01],
        [-1.6136e+00, -9.0930e-02],
        [-1.8290e+00,  3.6561e-01],
        [-1.7318e+00,  1.2762e-01],
        [-1.7484e+00, -3.0890e-01],
        [-1.5057e+00, -6.5812e-02],
        [-1.6914e+00,  8.2523e-02],
        [-1.2729e+00, -1.6631e-01],
        [-1.6641e+00,  1.0907e-01],
        [-1.6317e+00,  2.2826e-01],
        [-1.8969e+00, -2.0763e-01],
        [-1.4717e+00,  5.0336e-02],
        [-1.4594e+00,  6.2098e-02],
        [-1.4697e+00,  5.2887e-02],
        [-2.2506e+00, -3.0726e-01],
        [-2.1842e+00, -2.4009e-01],
        [-2.4530e+00, -6.7963e-01],
        [-1.6997e+00,  1.6028e-01],
        [-1.8781e+00,  2.3307e-01],
        [-2.1645e+00, -6.4242e-01],
        [-2.1267e+00, -1.5036e-02],
        [-1.4223e+00, -3.1512e-01],
        [-1.5810e+00,  1.0787e-01],
        [-1.8490e+00,  4.2902e-01],
        [-1.6109e+00,  7.7318e-02],
        [-1.8347e+00, -2.2892e-01],
        [-1.5590e+00,  1.2916e-01],
        [-1.8867e+00,  1.4106e-01],
        [-1.4488e+00, -1.0145e-02],
        [-1.5844e+00,  1.8998e-01],
        [-1.5463e+00, -1.9043e-01],
        [-2.2620e+00, -4.0329e-01],
        [-1.5028e+00,  1.0235e-01],
        [-2.4348e+00, -4.9058e-01],
        [-1.5822e+00,  1.9110e-01],
        [-3.3633e+00, -1.2519e+00],
        [-1.5827e+00,  1.9087e-01],
        [-2.0014e+00,  4.4306e-01],
        [-1.7444e+00, -5.5936e-02],
        [-2.2713e+00, -5.1656e-01],
        [-2.0075e+00, -6.3618e-02],
        [-1.5494e+00,  1.3891e-01],
        [-1.9221e+00,  2.1661e-02],
        [-1.8338e+00,  2.5081e-02],
        [-1.9595e+00,  5.6440e-01],
        [-1.4299e+00, -7.3217e-02],
        [-2.1917e+00,  8.6269e-02],
        [-1.3443e+00, -7.0738e-02],
        [-2.1708e+00,  1.8946e-01],
        [-1.5567e+00,  4.8102e-02],
        [-2.2199e+00, -5.3167e-01],
        [-3.2798e+00, -1.3354e+00],
        [-2.1177e+00, -1.7361e-01],
        [-2.0438e+00, -1.5993e-02],
        [-1.5188e+00, -4.1149e-01],
        [-2.3187e+00, -5.4471e-01],
        [-2.1230e+00, -4.3453e-01],
        [-1.5594e+00,  1.2936e-01],
        [-1.6395e+00,  1.3463e-01],
        [-2.2193e+00, -3.5940e-01],
        [-1.6241e+00,  2.3615e-01],
        [-2.1289e+00,  7.3166e-01],
        [-1.5812e+00,  1.9255e-01],
        [-2.0072e+00,  5.1992e-01],
        [-1.5841e+00, -6.2394e-02],
        [-1.7791e+00, -3.4046e-01],
        [-2.5713e+00, -5.4387e-01],
        [-2.6789e+00, -7.3504e-01],
        [-1.6317e+00,  2.2766e-01],
        [-2.3649e+00, -3.3697e-01],
        [-1.4708e+00,  5.0493e-02],
        [-2.2778e+00, -2.4981e-01],
        [-1.5015e+00,  1.0376e-01],
        [-1.8047e+00,  3.0669e-01],
        [-1.7920e+00, -1.8779e-01],
        [-1.7488e+00,  1.1087e-01],
        [-1.8753e+00, -1.5873e-02],
        [-1.6315e+00, -1.0997e-01],
        [-1.4391e+00,  7.5691e-04],
        [-1.6302e+00, -2.5393e-02],
        [-1.7373e+00,  2.9043e-01],
        [-1.6375e+00, -1.1897e-01],
        [-1.5389e+00,  1.4865e-01],
        [-2.5850e+00, -5.5646e-01],
        [-2.4295e+00, -5.6930e-01],
        [-1.3142e+00, -1.2437e-01],
        [-1.7826e+00,  1.6227e-01],
        [-2.2400e+00,  3.7897e-02],
        [-1.6421e+00,  2.1751e-01],
        [-2.0440e+00,  1.5104e-01],
        [-1.7963e+00,  1.4707e-01],
        [-2.0272e+00, -3.3930e-01],
        [-2.3829e+00, -1.1099e+00],
        [-1.5695e+00, -1.3067e-01]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.6072e-06, 9.9999e-01],
        [1.8322e-02, 9.8168e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1472, 0.8528], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1446, 0.1551],
         [0.1882, 0.1658]],

        [[0.4359, 0.1676],
         [0.2076, 0.3838]],

        [[0.4345, 0.3096],
         [0.0719, 0.9326]],

        [[0.5913, 0.1094],
         [0.5147, 0.4772]],

        [[0.5393, 0.1993],
         [0.2632, 0.3035]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 37
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 40
Adjusted Rand Index: 0.00877236457352431
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0019004397557010203
Average Adjusted Rand Index: 0.0017544729147048617
Iteration 0: Loss = -22226.531253497866
Iteration 10: Loss = -11121.166813941545
Iteration 20: Loss = -11120.552670139949
Iteration 30: Loss = -11120.515352099934
Iteration 40: Loss = -11120.511429748754
Iteration 50: Loss = -11120.510896392918
Iteration 60: Loss = -11120.510809283687
Iteration 70: Loss = -11120.510832685077
1
Iteration 80: Loss = -11120.510823642067
2
Iteration 90: Loss = -11120.510784636266
Iteration 100: Loss = -11120.510791199049
1
Iteration 110: Loss = -11120.510792305598
2
Iteration 120: Loss = -11120.510778523301
Iteration 130: Loss = -11120.510785385366
1
Iteration 140: Loss = -11120.510796897315
2
Iteration 150: Loss = -11120.510796358516
3
Stopping early at iteration 149 due to no improvement.
pi: tensor([[0.0061, 0.9939],
        [0.0180, 0.9820]], dtype=torch.float64)
alpha: tensor([0.0173, 0.9827])
beta: tensor([[[0.1341, 0.2634],
         [0.6888, 0.1630]],

        [[0.3592, 0.1704],
         [0.8820, 0.7978]],

        [[0.3208, 0.3086],
         [0.4283, 0.7130]],

        [[0.8539, 0.1100],
         [0.8961, 0.7115]],

        [[0.2174, 0.1999],
         [0.5539, 0.1376]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 36
Adjusted Rand Index: 0.014019411492836234
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 40
Adjusted Rand Index: 0.00877236457352431
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0038318294750973054
Average Adjusted Rand Index: 0.004558355213272108
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22226.28527081461
Iteration 100: Loss = -11173.264170399892
Iteration 200: Loss = -11126.79385519925
Iteration 300: Loss = -11122.067741258335
Iteration 400: Loss = -11121.30696132985
Iteration 500: Loss = -11121.048022861803
Iteration 600: Loss = -11120.911813831612
Iteration 700: Loss = -11120.814318591536
Iteration 800: Loss = -11120.743639791344
Iteration 900: Loss = -11120.695032336213
Iteration 1000: Loss = -11120.657850240279
Iteration 1100: Loss = -11120.62761561016
Iteration 1200: Loss = -11120.602707846387
Iteration 1300: Loss = -11120.581503988986
Iteration 1400: Loss = -11120.562459918661
Iteration 1500: Loss = -11120.54523466435
Iteration 1600: Loss = -11120.530982085322
Iteration 1700: Loss = -11120.519230421653
Iteration 1800: Loss = -11120.50923771035
Iteration 1900: Loss = -11120.500683442648
Iteration 2000: Loss = -11120.493327551172
Iteration 2100: Loss = -11120.486925358431
Iteration 2200: Loss = -11120.481342174624
Iteration 2300: Loss = -11120.476396034514
Iteration 2400: Loss = -11120.47210068416
Iteration 2500: Loss = -11120.468244828344
Iteration 2600: Loss = -11120.464782901365
Iteration 2700: Loss = -11120.461772937257
Iteration 2800: Loss = -11120.459010764664
Iteration 2900: Loss = -11120.456563155107
Iteration 3000: Loss = -11120.454301784295
Iteration 3100: Loss = -11120.452261198934
Iteration 3200: Loss = -11120.450289013124
Iteration 3300: Loss = -11120.448355781122
Iteration 3400: Loss = -11120.44632696352
Iteration 3500: Loss = -11120.444250803534
Iteration 3600: Loss = -11120.442366482
Iteration 3700: Loss = -11120.440942140796
Iteration 3800: Loss = -11120.439793297632
Iteration 3900: Loss = -11120.438730159607
Iteration 4000: Loss = -11120.43780873813
Iteration 4100: Loss = -11120.436955151608
Iteration 4200: Loss = -11120.43612676802
Iteration 4300: Loss = -11120.43540611832
Iteration 4400: Loss = -11120.43471831643
Iteration 4500: Loss = -11120.434052796236
Iteration 4600: Loss = -11120.43341612796
Iteration 4700: Loss = -11120.43283377008
Iteration 4800: Loss = -11120.432252834342
Iteration 4900: Loss = -11120.431675912176
Iteration 5000: Loss = -11120.431146747855
Iteration 5100: Loss = -11120.430660149694
Iteration 5200: Loss = -11120.430135564073
Iteration 5300: Loss = -11120.429703449328
Iteration 5400: Loss = -11120.42927262044
Iteration 5500: Loss = -11120.428863272326
Iteration 5600: Loss = -11120.42844713617
Iteration 5700: Loss = -11120.428106978196
Iteration 5800: Loss = -11120.427622616302
Iteration 5900: Loss = -11120.426835919252
Iteration 6000: Loss = -11120.426561084667
Iteration 6100: Loss = -11120.426329039616
Iteration 6200: Loss = -11120.426140302641
Iteration 6300: Loss = -11120.425940787403
Iteration 6400: Loss = -11120.425745097487
Iteration 6500: Loss = -11120.425611201726
Iteration 6600: Loss = -11120.425389182134
Iteration 6700: Loss = -11120.42525222252
Iteration 6800: Loss = -11120.425112126199
Iteration 6900: Loss = -11120.42496454983
Iteration 7000: Loss = -11120.424807007357
Iteration 7100: Loss = -11120.428750520365
1
Iteration 7200: Loss = -11120.491117327028
2
Iteration 7300: Loss = -11120.423979841582
Iteration 7400: Loss = -11120.411856522003
Iteration 7500: Loss = -11120.430507903537
1
Iteration 7600: Loss = -11120.398594304752
Iteration 7700: Loss = -11120.398351651384
Iteration 7800: Loss = -11120.397486130498
Iteration 7900: Loss = -11120.397253622572
Iteration 8000: Loss = -11120.397164394013
Iteration 8100: Loss = -11120.397697571747
1
Iteration 8200: Loss = -11120.396916315554
Iteration 8300: Loss = -11120.537066164328
1
Iteration 8400: Loss = -11120.396758350982
Iteration 8500: Loss = -11120.396687693546
Iteration 8600: Loss = -11120.418257681647
1
Iteration 8700: Loss = -11120.396531719889
Iteration 8800: Loss = -11120.396515719121
Iteration 8900: Loss = -11120.589312752894
1
Iteration 9000: Loss = -11120.396328779696
Iteration 9100: Loss = -11120.396317816889
Iteration 9200: Loss = -11120.39626420255
Iteration 9300: Loss = -11120.428841660912
1
Iteration 9400: Loss = -11120.396193315359
Iteration 9500: Loss = -11120.39612667114
Iteration 9600: Loss = -11120.693222945923
1
Iteration 9700: Loss = -11120.396053715405
Iteration 9800: Loss = -11120.39599670857
Iteration 9900: Loss = -11120.39769352788
1
Iteration 10000: Loss = -11120.395973043911
Iteration 10100: Loss = -11120.395868507618
Iteration 10200: Loss = -11120.395847092554
Iteration 10300: Loss = -11120.396507616184
1
Iteration 10400: Loss = -11120.395784441067
Iteration 10500: Loss = -11120.395794788126
1
Iteration 10600: Loss = -11120.514237421809
2
Iteration 10700: Loss = -11120.395724584032
Iteration 10800: Loss = -11120.395697425563
Iteration 10900: Loss = -11120.395667866253
Iteration 11000: Loss = -11120.395866748517
1
Iteration 11100: Loss = -11120.395647118625
Iteration 11200: Loss = -11120.395698087257
1
Iteration 11300: Loss = -11120.396501935187
2
Iteration 11400: Loss = -11120.395658250516
3
Iteration 11500: Loss = -11120.39558762224
Iteration 11600: Loss = -11120.405480187386
1
Iteration 11700: Loss = -11120.395687795695
2
Iteration 11800: Loss = -11120.397158338126
3
Iteration 11900: Loss = -11120.395538851097
Iteration 12000: Loss = -11120.401590497535
1
Iteration 12100: Loss = -11120.395511499155
Iteration 12200: Loss = -11120.397441647192
1
Iteration 12300: Loss = -11120.395508861078
Iteration 12400: Loss = -11120.395470118936
Iteration 12500: Loss = -11120.401818127448
1
Iteration 12600: Loss = -11120.395382214394
Iteration 12700: Loss = -11120.500193441181
1
Iteration 12800: Loss = -11120.395344479066
Iteration 12900: Loss = -11120.402276909725
1
Iteration 13000: Loss = -11120.38369817202
Iteration 13100: Loss = -11120.380297873977
Iteration 13200: Loss = -11120.380574822237
1
Iteration 13300: Loss = -11120.38026518149
Iteration 13400: Loss = -11120.381299379913
1
Iteration 13500: Loss = -11120.380630776524
2
Iteration 13600: Loss = -11120.403047931928
3
Iteration 13700: Loss = -11120.38127339063
4
Iteration 13800: Loss = -11120.385977147906
5
Iteration 13900: Loss = -11120.380253074936
Iteration 14000: Loss = -11120.380217424756
Iteration 14100: Loss = -11120.38028472128
1
Iteration 14200: Loss = -11120.380205216285
Iteration 14300: Loss = -11120.416305371942
1
Iteration 14400: Loss = -11120.380156552443
Iteration 14500: Loss = -11120.380158843545
1
Iteration 14600: Loss = -11120.389421553891
2
Iteration 14700: Loss = -11120.368145999024
Iteration 14800: Loss = -11120.36999986235
1
Iteration 14900: Loss = -11120.368136819874
Iteration 15000: Loss = -11120.368234252708
1
Iteration 15100: Loss = -11120.368180222813
2
Iteration 15200: Loss = -11120.34399485072
Iteration 15300: Loss = -11120.310596345607
Iteration 15400: Loss = -11120.309394144158
Iteration 15500: Loss = -11120.322453951097
1
Iteration 15600: Loss = -11120.306416722327
Iteration 15700: Loss = -11120.306663897085
1
Iteration 15800: Loss = -11120.307241490314
2
Iteration 15900: Loss = -11120.306368791491
Iteration 16000: Loss = -11120.311021077478
1
Iteration 16100: Loss = -11120.306314932286
Iteration 16200: Loss = -11120.3125740199
1
Iteration 16300: Loss = -11120.33122265054
2
Iteration 16400: Loss = -11120.306350073632
3
Iteration 16500: Loss = -11120.30668999724
4
Iteration 16600: Loss = -11120.477211123734
5
Iteration 16700: Loss = -11120.306339802635
6
Iteration 16800: Loss = -11120.42340874693
7
Iteration 16900: Loss = -11120.30634525053
8
Iteration 17000: Loss = -11120.385148059839
9
Iteration 17100: Loss = -11120.30647444314
10
Stopping early at iteration 17100 due to no improvement.
tensor([[-1.9697e+00, -6.0099e-01],
        [-1.5662e+00,  1.3896e-01],
        [-1.5037e+00,  3.3808e-02],
        [-2.9747e+00, -1.1000e+00],
        [-1.8371e+00,  2.0577e-01],
        [-2.0797e+00, -1.2176e-01],
        [-2.0082e+00, -3.0349e-01],
        [-1.7966e+00, -2.5851e-01],
        [-2.1997e+00,  1.1097e-02],
        [-1.8770e+00, -2.7055e-03],
        [-1.4780e+00, -2.4382e-02],
        [-1.9888e+00, -5.3475e-01],
        [-1.6853e+00,  1.0536e-01],
        [-1.6869e+00, -5.6979e-01],
        [-1.5950e+00,  1.9512e-01],
        [-2.6954e+00, -8.2112e-01],
        [-2.1106e+00, -4.0409e-01],
        [-1.6847e+00, -1.4722e-01],
        [-1.8896e+00, -3.5264e-01],
        [-1.5091e+00,  2.8846e-02],
        [-1.8561e+00,  1.0202e-01],
        [-1.9998e+00, -4.1039e-02],
        [-1.6195e+00,  1.7047e-01],
        [-1.9763e+00, -1.0161e-01],
        [-2.6458e+00, -5.1923e-01],
        [-1.4837e+00,  5.3717e-02],
        [-1.7627e+00,  3.6435e-01],
        [-1.2667e+00, -1.4904e-01],
        [-1.5897e+00,  1.1632e-01],
        [-1.8416e+00,  4.5320e-01],
        [-1.5460e+00,  1.5958e-01],
        [-1.6018e+00,  2.0469e-02],
        [-1.8531e+00, -1.4759e-01],
        [-2.1359e+00, -9.3235e-02],
        [-1.6377e+00, -1.8470e-01],
        [-1.7957e+00, -5.4674e-03],
        [-1.6154e+00, -2.4625e-01],
        [-1.6858e+00,  1.8793e-01],
        [-1.8484e+00, -2.2690e-01],
        [-2.2541e+00, -2.9519e-01],
        [-1.5887e+00,  2.0122e-01],
        [-1.7768e+00,  3.5009e-01],
        [-1.7699e+00,  2.0235e-02],
        [-1.9949e+00,  4.6805e-01],
        [-1.8105e+00, -1.0470e-01],
        [-1.7590e+00,  1.2268e-02],
        [-1.8697e+00,  8.8761e-02],
        [-1.9002e+00, -1.9460e-01],
        [-1.6771e+00,  2.8124e-01],
        [-1.9831e+00, -1.0833e-01],
        [-3.5792e+00, -1.0360e+00],
        [-2.7660e+00, -1.3960e+00],
        [-1.8418e+00,  4.5302e-01],
        [-2.9506e+00, -1.6646e+00],
        [-1.9096e+00,  4.6820e-01],
        [-1.5932e+00,  2.7849e-02],
        [-1.7021e+00,  3.3220e-03],
        [-2.0336e+00, -7.4516e-02],
        [-1.6886e+00,  2.7009e-01],
        [-2.0936e+00, -5.0896e-02],
        [-1.7832e+00, -6.6535e-01],
        [-1.5982e+00,  1.9251e-01],
        [-1.5596e+00,  1.4616e-01],
        [-1.5465e+00,  1.5943e-01],
        [-1.9768e+00, -1.8655e-01],
        [-1.6589e+00,  2.1544e-01],
        [-1.6502e+00,  2.2488e-01],
        [-2.5410e+00,  3.4215e-01],
        [-1.8497e+00, -5.9926e-02],
        [-2.1730e+00,  3.7329e-01],
        [-1.4832e+00,  5.3714e-02],
        [-2.5224e+00, -1.0694e+00],
        [-1.7148e+00,  3.2742e-01],
        [-2.1285e+00, -1.6998e-01],
        [-3.2447e+00, -1.3705e+00],
        [-1.7448e+00,  2.9803e-01],
        [-1.5326e+00,  4.2349e-03],
        [-1.7316e+00,  3.1133e-01],
        [-1.9106e+00, -2.8886e-01],
        [-1.8604e+00,  2.6645e-01],
        [-1.7096e+00, -8.8817e-02],
        [-1.7309e+00,  1.4411e-01],
        [-1.8566e+00,  1.7709e-02],
        [-1.5170e+00,  1.9930e-02],
        [-1.5805e+00, -1.2631e-01],
        [-1.5249e+00,  9.6415e-02],
        [-2.0798e+00, -3.7142e-02],
        [-1.4948e+00,  3.9185e-02],
        [-2.4153e+00, -7.1038e-01],
        [-2.2039e+00, -1.6048e-01],
        [-1.6449e+00,  2.3027e-01],
        [-1.3354e+00, -1.3416e-01],
        [-1.8757e+00,  8.3750e-02],
        [-1.8801e+00,  4.1457e-01],
        [-1.8719e+00,  2.5833e-03],
        [-1.9207e+00,  2.9047e-01],
        [-1.8746e+00,  8.3524e-02],
        [-2.8254e+00, -1.1203e+00],
        [-1.3905e+00, -1.0508e-01],
        [-1.5649e+00, -1.1172e-01]], dtype=torch.float64, requires_grad=True)
pi: tensor([[6.4114e-06, 9.9999e-01],
        [1.8677e-02, 9.8132e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1460, 0.8540], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1442, 0.1548],
         [0.6888, 0.1660]],

        [[0.3592, 0.1674],
         [0.8820, 0.7978]],

        [[0.3208, 0.3094],
         [0.4283, 0.7130]],

        [[0.8539, 0.1098],
         [0.8961, 0.7115]],

        [[0.2174, 0.1987],
         [0.5539, 0.1376]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 37
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 40
Adjusted Rand Index: 0.00877236457352431
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0019004397557010203
Average Adjusted Rand Index: 0.0017544729147048617
Iteration 0: Loss = -22551.16276525299
Iteration 10: Loss = -11122.048260975991
Iteration 20: Loss = -11120.984442202282
Iteration 30: Loss = -11120.67284873444
Iteration 40: Loss = -11120.65152456587
Iteration 50: Loss = -11120.648123496683
Iteration 60: Loss = -11120.64813439604
1
Iteration 70: Loss = -11120.648407383158
2
Iteration 80: Loss = -11120.648534170274
3
Stopping early at iteration 79 due to no improvement.
pi: tensor([[0.0075, 0.9925],
        [0.0176, 0.9824]], dtype=torch.float64)
alpha: tensor([0.0176, 0.9824])
beta: tensor([[[0.1211, 0.1232],
         [0.2450, 0.1640]],

        [[0.7146, 0.1628],
         [0.3890, 0.9333]],

        [[0.1440, 0.3104],
         [0.3686, 0.6003]],

        [[0.7686, 0.1088],
         [0.1259, 0.6659]],

        [[0.4464, 0.1971],
         [0.1642, 0.4229]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 37
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 40
Adjusted Rand Index: 0.00877236457352431
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.0
time is 4
 10%|█         | 10/100 [7:55:20<71:22:15, 2854.84s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 11%|█         | 11/100 [8:37:23<68:03:43, 2753.07s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 12%|█▏        | 12/100 [9:17:56<64:55:09, 2655.79s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 13%|█▎        | 13/100 [10:00:40<63:30:28, 2627.91s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 14%|█▍        | 14/100 [10:49:06<64:47:18, 2712.08s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 15%|█▌        | 15/100 [11:37:38<65:27:26, 2772.32s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 16%|█▌        | 16/100 [12:28:46<66:45:55, 2861.38s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 17%|█▋        | 17/100 [13:19:41<67:18:41, 2919.54s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 18%|█▊        | 18/100 [14:09:24<66:55:56, 2938.49s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 19%|█▉        | 19/100 [14:50:44<63:01:18, 2800.96s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 20%|██        | 20/100 [15:37:09<62:08:01, 2796.02s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0019004397557010203
Average Adjusted Rand Index: 0.0017544729147048617
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22551.04717023398
Iteration 100: Loss = -11207.600303559484
Iteration 200: Loss = -11159.26350370345
Iteration 300: Loss = -11137.253440878303
Iteration 400: Loss = -11124.969659622308
Iteration 500: Loss = -11123.118004965105
Iteration 600: Loss = -11122.466324780677
Iteration 700: Loss = -11122.028482616968
Iteration 800: Loss = -11121.69692662103
Iteration 900: Loss = -11121.514400893906
Iteration 1000: Loss = -11121.387070007198
Iteration 1100: Loss = -11121.290130525753
Iteration 1200: Loss = -11121.204180578769
Iteration 1300: Loss = -11121.130960659908
Iteration 1400: Loss = -11121.071350439182
Iteration 1500: Loss = -11121.02325261382
Iteration 1600: Loss = -11120.984748053716
Iteration 1700: Loss = -11120.953244919197
Iteration 1800: Loss = -11120.92629358379
Iteration 1900: Loss = -11120.90163814616
Iteration 2000: Loss = -11120.874838947564
Iteration 2100: Loss = -11120.830985141785
Iteration 2200: Loss = -11120.766276544784
Iteration 2300: Loss = -11120.648373632088
Iteration 2400: Loss = -11120.555552237309
Iteration 2500: Loss = -11120.456315650195
Iteration 2600: Loss = -11120.417536590421
Iteration 2700: Loss = -11120.397660234752
Iteration 2800: Loss = -11120.365484477717
Iteration 2900: Loss = -11120.313687428157
Iteration 3000: Loss = -11120.260554323197
Iteration 3100: Loss = -11120.200396664566
Iteration 3200: Loss = -11120.16277107165
Iteration 3300: Loss = -11120.125137338182
Iteration 3400: Loss = -11120.08132706576
Iteration 3500: Loss = -11120.03536564983
Iteration 3600: Loss = -11119.991510637363
Iteration 3700: Loss = -11119.955797992076
Iteration 3800: Loss = -11119.929767560845
Iteration 3900: Loss = -11119.902935702654
Iteration 4000: Loss = -11119.876494016387
Iteration 4100: Loss = -11119.852960953081
Iteration 4200: Loss = -11119.835457619069
Iteration 4300: Loss = -11119.825864431714
Iteration 4400: Loss = -11119.806579540813
Iteration 4500: Loss = -11119.791272009119
Iteration 4600: Loss = -11119.778258780676
Iteration 4700: Loss = -11119.765873752694
Iteration 4800: Loss = -11119.75437787639
Iteration 4900: Loss = -11119.74203293304
Iteration 5000: Loss = -11119.732149126457
Iteration 5100: Loss = -11119.72132141132
Iteration 5200: Loss = -11119.713039322602
Iteration 5300: Loss = -11119.90227225204
1
Iteration 5400: Loss = -11119.700402153947
Iteration 5500: Loss = -11119.695885995297
Iteration 5600: Loss = -11119.691943257487
Iteration 5700: Loss = -11119.689070825343
Iteration 5800: Loss = -11119.687147221739
Iteration 5900: Loss = -11119.685823625441
Iteration 6000: Loss = -11119.692857000642
1
Iteration 6100: Loss = -11119.684066760974
Iteration 6200: Loss = -11119.683627513228
Iteration 6300: Loss = -11119.839684132305
1
Iteration 6400: Loss = -11119.68306285578
Iteration 6500: Loss = -11119.682847783528
Iteration 6600: Loss = -11119.682698852235
Iteration 6700: Loss = -11119.682581750616
Iteration 6800: Loss = -11119.682427671156
Iteration 6900: Loss = -11119.682351395693
Iteration 7000: Loss = -11119.682549354045
1
Iteration 7100: Loss = -11119.68219680012
Iteration 7200: Loss = -11119.682185993724
Iteration 7300: Loss = -11119.753795292327
1
Iteration 7400: Loss = -11119.682095705868
Iteration 7500: Loss = -11119.682030400381
Iteration 7600: Loss = -11119.682018533169
Iteration 7700: Loss = -11119.682160451683
1
Iteration 7800: Loss = -11119.681896487113
Iteration 7900: Loss = -11119.842406321304
1
Iteration 8000: Loss = -11119.682573352662
2
Iteration 8100: Loss = -11119.685168231188
3
Iteration 8200: Loss = -11119.682461979128
4
Iteration 8300: Loss = -11119.681851926562
Iteration 8400: Loss = -11119.715100981222
1
Iteration 8500: Loss = -11119.684444923874
2
Iteration 8600: Loss = -11119.681789715878
Iteration 8700: Loss = -11119.683993174467
1
Iteration 8800: Loss = -11119.681761564396
Iteration 8900: Loss = -11119.681677974702
Iteration 9000: Loss = -11119.681938765918
1
Iteration 9100: Loss = -11119.926070365334
2
Iteration 9200: Loss = -11119.681581873569
Iteration 9300: Loss = -11119.681969878744
1
Iteration 9400: Loss = -11119.699696146066
2
Iteration 9500: Loss = -11119.682870569191
3
Iteration 9600: Loss = -11119.681885367756
4
Iteration 9700: Loss = -11119.697796064655
5
Iteration 9800: Loss = -11119.861195753501
6
Iteration 9900: Loss = -11119.702134507972
7
Iteration 10000: Loss = -11119.68139597614
Iteration 10100: Loss = -11119.681731870394
1
Iteration 10200: Loss = -11119.681358824173
Iteration 10300: Loss = -11119.681737912348
1
Iteration 10400: Loss = -11119.684963298057
2
Iteration 10500: Loss = -11119.68235542144
3
Iteration 10600: Loss = -11119.681811441025
4
Iteration 10700: Loss = -11119.682668838323
5
Iteration 10800: Loss = -11119.681248401233
Iteration 10900: Loss = -11119.683711357799
1
Iteration 11000: Loss = -11119.707440632907
2
Iteration 11100: Loss = -11119.70538021599
3
Iteration 11200: Loss = -11119.681172633314
Iteration 11300: Loss = -11119.681254622556
1
Iteration 11400: Loss = -11119.681117425014
Iteration 11500: Loss = -11119.742661459424
1
Iteration 11600: Loss = -11119.726084645064
2
Iteration 11700: Loss = -11119.68642360713
3
Iteration 11800: Loss = -11119.683442121546
4
Iteration 11900: Loss = -11119.737662323903
5
Iteration 12000: Loss = -11119.681019892181
Iteration 12100: Loss = -11119.682331409189
1
Iteration 12200: Loss = -11119.681531323642
2
Iteration 12300: Loss = -11119.699169989062
3
Iteration 12400: Loss = -11119.699054936116
4
Iteration 12500: Loss = -11119.681007509089
Iteration 12600: Loss = -11119.681444866295
1
Iteration 12700: Loss = -11119.682050625288
2
Iteration 12800: Loss = -11119.68098696901
Iteration 12900: Loss = -11119.68286639232
1
Iteration 13000: Loss = -11119.680949789214
Iteration 13100: Loss = -11120.00578056758
1
Iteration 13200: Loss = -11119.68095560843
2
Iteration 13300: Loss = -11119.692925677551
3
Iteration 13400: Loss = -11119.679949900561
Iteration 13500: Loss = -11119.686898903941
1
Iteration 13600: Loss = -11119.695483124608
2
Iteration 13700: Loss = -11119.751611008634
3
Iteration 13800: Loss = -11119.680057132979
4
Iteration 13900: Loss = -11119.677251831336
Iteration 14000: Loss = -11119.692860868816
1
Iteration 14100: Loss = -11119.67691193167
Iteration 14200: Loss = -11119.680867893076
1
Iteration 14300: Loss = -11119.704183883421
2
Iteration 14400: Loss = -11119.677000413825
3
Iteration 14500: Loss = -11119.677406043354
4
Iteration 14600: Loss = -11119.713351081047
5
Iteration 14700: Loss = -11119.677154261728
6
Iteration 14800: Loss = -11119.676947126169
7
Iteration 14900: Loss = -11119.687676712618
8
Iteration 15000: Loss = -11119.676954292281
9
Iteration 15100: Loss = -11119.677025401605
10
Stopping early at iteration 15100 due to no improvement.
tensor([[ 3.9658, -5.3523],
        [ 2.5648, -4.6296],
        [ 3.3105, -4.8582],
        [ 2.5881, -4.0511],
        [ 1.7481, -3.5977],
        [ 1.8685, -3.7475],
        [ 2.4897, -4.4768],
        [ 3.3984, -4.7926],
        [ 1.6626, -3.1308],
        [ 2.1605, -3.5552],
        [ 3.0756, -5.6232],
        [ 2.8280, -6.2572],
        [ 2.2270, -4.3529],
        [ 4.7625, -6.8219],
        [ 0.8274, -5.4404],
        [ 2.3849, -3.9042],
        [ 2.4148, -4.8103],
        [ 3.3068, -4.7707],
        [ 3.6758, -5.4873],
        [ 3.3651, -4.7672],
        [ 1.3707, -3.6687],
        [ 1.3296, -4.0697],
        [ 2.3798, -4.2434],
        [ 2.2119, -3.6074],
        [ 1.3543, -2.7647],
        [ 3.5651, -5.4650],
        [ 1.5687, -3.6252],
        [ 4.8746, -6.3768],
        [ 3.3555, -4.8320],
        [ 0.7309, -2.1335],
        [ 2.7138, -4.1686],
        [ 3.5852, -4.9789],
        [ 2.5101, -4.3704],
        [ 2.1307, -3.5282],
        [ 3.5528, -6.1288],
        [ 2.6352, -5.0702],
        [ 3.9935, -5.6352],
        [ 2.1274, -3.5141],
        [ 3.0183, -4.4292],
        [ 2.3286, -3.7480],
        [ 2.3641, -3.8555],
        [ 1.0172, -2.9806],
        [ 2.9011, -4.2897],
        [ 0.0555, -1.9415],
        [ 2.4456, -4.8069],
        [ 2.6438, -4.1515],
        [ 1.3970, -3.9509],
        [ 2.7315, -4.1188],
        [ 2.0751, -3.5458],
        [ 1.8260, -3.7575],
        [-0.4838, -1.4767],
        [ 3.6823, -5.6101],
        [ 0.8950, -2.5760],
        [ 4.8055, -6.4125],
        [ 0.2046, -2.1086],
        [ 3.0234, -4.4198],
        [ 2.8733, -4.3384],
        [ 1.8513, -3.2847],
        [ 2.0568, -4.0265],
        [ 1.4621, -3.0738],
        [ 4.4931, -6.7908],
        [ 2.5549, -3.9446],
        [ 1.8206, -5.0551],
        [ 3.4303, -4.8169],
        [ 2.7363, -4.9652],
        [ 2.8136, -4.2179],
        [ 2.4845, -4.2785],
        [-2.1291, -0.5710],
        [ 1.3187, -5.9070],
        [-0.2344, -1.2436],
        [ 3.0421, -5.0072],
        [ 4.3633, -5.7496],
        [ 1.0824, -3.5042],
        [ 1.8104, -3.2426],
        [ 2.6356, -4.0226],
        [ 1.0721, -3.7959],
        [ 3.4107, -5.2850],
        [ 2.1081, -3.7151],
        [ 3.3308, -4.7628],
        [ 1.7769, -3.1821],
        [ 2.9011, -4.9180],
        [ 2.3117, -3.7658],
        [ 2.3463, -3.7543],
        [ 3.2850, -5.2835],
        [ 3.4634, -5.7063],
        [ 2.8346, -5.1623],
        [ 0.7074, -3.7929],
        [ 3.3089, -5.1067],
        [ 2.4581, -4.3692],
        [ 1.9312, -3.6259],
        [ 2.1999, -3.8788],
        [ 4.3862, -6.1449],
        [ 2.5218, -3.9925],
        [ 1.0733, -2.4800],
        [ 2.1373, -3.5516],
        [ 0.6858, -2.5078],
        [ 1.8237, -3.2530],
        [ 3.3961, -4.7908],
        [ 4.4488, -5.8688],
        [ 4.1430, -5.5360]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.9696e-05, 9.9998e-01],
        [1.7926e-02, 9.8207e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9806, 0.0194], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1590, 0.2596],
         [0.2450, 0.1661]],

        [[0.7146, 0.1650],
         [0.3890, 0.9333]],

        [[0.1440, 0.3113],
         [0.3686, 0.6003]],

        [[0.7686, 0.1097],
         [0.1259, 0.6659]],

        [[0.4464, 0.1998],
         [0.1642, 0.4229]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 64
Adjusted Rand Index: 0.014019411492836234
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 40
Adjusted Rand Index: 0.00877236457352431
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.006861865092023175
Average Adjusted Rand Index: 0.004558355213272108
10922.335915728168
new:  [0.0019004397557010203, 0.0019004397557010203, 0.0019004397557010203, -0.006861865092023175] [0.0017544729147048617, 0.0017544729147048617, 0.0017544729147048617, 0.004558355213272108] [11120.307394314561, 11120.313544223514, 11120.30647444314, 11119.677025401605]
prior:  [0.0038318294750973054, 0.0038318294750973054, 0.0038318294750973054, 0.0019004397557010203] [0.004558355213272108, 0.004558355213272108, 0.004558355213272108, 0.0017544729147048617] [11120.510719735541, 11120.51069044901, 11120.510796358516, 11120.648534170274]
-----------------------------------------------------------------------------------------
This iteration is 10
True Objective function: Loss = -11368.183325692185
Iteration 0: Loss = -24005.0622533545
Iteration 10: Loss = -11354.95577128154
Iteration 20: Loss = -11354.972627752404
1
Iteration 30: Loss = -11354.97868954116
2
Iteration 40: Loss = -11354.97914653869
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[0.7423, 0.2577],
        [0.2686, 0.7314]], dtype=torch.float64)
alpha: tensor([0.5281, 0.4719])
beta: tensor([[[0.2985, 0.0953],
         [0.6552, 0.2021]],

        [[0.3495, 0.1009],
         [0.6471, 0.8089]],

        [[0.1439, 0.0975],
         [0.9902, 0.2871]],

        [[0.4804, 0.1119],
         [0.5112, 0.4376]],

        [[0.4459, 0.0992],
         [0.9793, 0.5961]]], dtype=torch.float64)
time is 0
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 2
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 3
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 4
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9446733348050895
Average Adjusted Rand Index: 0.9446438845001811
tensor([[-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24196.10529709609
Iteration 100: Loss = -11708.781216865982
Iteration 200: Loss = -11705.669390877412
Iteration 300: Loss = -11704.639529712984
Iteration 400: Loss = -11704.13261725902
Iteration 500: Loss = -11703.841831394177
Iteration 600: Loss = -11703.656134492847
Iteration 700: Loss = -11703.51169909782
Iteration 800: Loss = -11703.186979255173
Iteration 900: Loss = -11694.152764033188
Iteration 1000: Loss = -11691.129333355118
Iteration 1100: Loss = -11680.840414900005
Iteration 1200: Loss = -11675.551756912579
Iteration 1300: Loss = -11675.373272850136
Iteration 1400: Loss = -11675.289556571546
Iteration 1500: Loss = -11675.208029694273
Iteration 1600: Loss = -11675.156989609673
Iteration 1700: Loss = -11675.123469092427
Iteration 1800: Loss = -11675.101775956531
Iteration 1900: Loss = -11675.086611598674
Iteration 2000: Loss = -11675.074788000173
Iteration 2100: Loss = -11675.06517456074
Iteration 2200: Loss = -11675.057087719131
Iteration 2300: Loss = -11675.050176998924
Iteration 2400: Loss = -11675.044117187024
Iteration 2500: Loss = -11675.037281671312
Iteration 2600: Loss = -11675.030892440529
Iteration 2700: Loss = -11675.025506729611
Iteration 2800: Loss = -11675.01975190213
Iteration 2900: Loss = -11675.015474710592
Iteration 3000: Loss = -11675.011558271965
Iteration 3100: Loss = -11675.008972858192
Iteration 3200: Loss = -11675.006634569118
Iteration 3300: Loss = -11675.003929865805
Iteration 3400: Loss = -11675.001924565966
Iteration 3500: Loss = -11675.00016297359
Iteration 3600: Loss = -11674.998614215641
Iteration 3700: Loss = -11674.997121841778
Iteration 3800: Loss = -11674.995941638677
Iteration 3900: Loss = -11674.99478069921
Iteration 4000: Loss = -11674.993526060849
Iteration 4100: Loss = -11674.99241123474
Iteration 4200: Loss = -11674.99147139883
Iteration 4300: Loss = -11674.99053889833
Iteration 4400: Loss = -11674.9897483143
Iteration 4500: Loss = -11674.989070746149
Iteration 4600: Loss = -11674.988318080985
Iteration 4700: Loss = -11674.98763138447
Iteration 4800: Loss = -11674.987088322654
Iteration 4900: Loss = -11674.98646234186
Iteration 5000: Loss = -11674.985869137061
Iteration 5100: Loss = -11674.985548065395
Iteration 5200: Loss = -11674.984970587811
Iteration 5300: Loss = -11674.984460739413
Iteration 5400: Loss = -11674.984063752347
Iteration 5500: Loss = -11674.983689629344
Iteration 5600: Loss = -11674.983369689691
Iteration 5700: Loss = -11674.983008479801
Iteration 5800: Loss = -11674.983223214
1
Iteration 5900: Loss = -11674.982720991504
Iteration 6000: Loss = -11674.982096633308
Iteration 6100: Loss = -11674.981877445685
Iteration 6200: Loss = -11674.981619852138
Iteration 6300: Loss = -11674.981369941363
Iteration 6400: Loss = -11674.981142390041
Iteration 6500: Loss = -11674.980925225816
Iteration 6600: Loss = -11674.98073729641
Iteration 6700: Loss = -11674.980504085253
Iteration 6800: Loss = -11674.980998466172
1
Iteration 6900: Loss = -11674.980122089884
Iteration 7000: Loss = -11674.979972380524
Iteration 7100: Loss = -11674.98110293161
1
Iteration 7200: Loss = -11674.981597630413
2
Iteration 7300: Loss = -11674.980894919388
3
Iteration 7400: Loss = -11674.979457231706
Iteration 7500: Loss = -11674.9849912255
1
Iteration 7600: Loss = -11674.979180668595
Iteration 7700: Loss = -11674.97968233845
1
Iteration 7800: Loss = -11674.978961805822
Iteration 7900: Loss = -11674.979812933647
1
Iteration 8000: Loss = -11674.978757392812
Iteration 8100: Loss = -11674.978661603735
Iteration 8200: Loss = -11674.988325589047
1
Iteration 8300: Loss = -11674.978483573474
Iteration 8400: Loss = -11674.980921761626
1
Iteration 8500: Loss = -11674.987326480092
2
Iteration 8600: Loss = -11674.978227503223
Iteration 8700: Loss = -11674.97910902906
1
Iteration 8800: Loss = -11674.978110543641
Iteration 8900: Loss = -11675.24845760985
1
Iteration 9000: Loss = -11674.978026882673
Iteration 9100: Loss = -11674.988795761246
1
Iteration 9200: Loss = -11674.977937196503
Iteration 9300: Loss = -11674.977894078304
Iteration 9400: Loss = -11674.977928011787
1
Iteration 9500: Loss = -11674.977820511016
Iteration 9600: Loss = -11674.991083957762
1
Iteration 9700: Loss = -11674.9778025122
Iteration 9800: Loss = -11674.980780787324
1
Iteration 9900: Loss = -11674.977767099832
Iteration 10000: Loss = -11674.978167547453
1
Iteration 10100: Loss = -11674.980586365382
2
Iteration 10200: Loss = -11675.027960067831
3
Iteration 10300: Loss = -11674.978458420297
4
Iteration 10400: Loss = -11674.981585191763
5
Iteration 10500: Loss = -11675.046565075507
6
Iteration 10600: Loss = -11674.977536453402
Iteration 10700: Loss = -11674.977619914664
1
Iteration 10800: Loss = -11674.983376810595
2
Iteration 10900: Loss = -11674.977388462568
Iteration 11000: Loss = -11674.977589367489
1
Iteration 11100: Loss = -11674.977350977004
Iteration 11200: Loss = -11674.977289763932
Iteration 11300: Loss = -11674.977216145537
Iteration 11400: Loss = -11674.97736914268
1
Iteration 11500: Loss = -11674.977196520267
Iteration 11600: Loss = -11674.991714028087
1
Iteration 11700: Loss = -11674.977183316654
Iteration 11800: Loss = -11674.97866933318
1
Iteration 11900: Loss = -11674.977372528894
2
Iteration 12000: Loss = -11674.98180378204
3
Iteration 12100: Loss = -11674.977701571852
4
Iteration 12200: Loss = -11674.978592931515
5
Iteration 12300: Loss = -11674.977250397802
6
Iteration 12400: Loss = -11674.981564174373
7
Iteration 12500: Loss = -11674.977062310596
Iteration 12600: Loss = -11674.97790261403
1
Iteration 12700: Loss = -11674.977174233103
2
Iteration 12800: Loss = -11674.97720279046
3
Iteration 12900: Loss = -11674.990060696875
4
Iteration 13000: Loss = -11674.981408323765
5
Iteration 13100: Loss = -11674.986803909067
6
Iteration 13200: Loss = -11674.982440303475
7
Iteration 13300: Loss = -11674.977125522975
8
Iteration 13400: Loss = -11674.978712410268
9
Iteration 13500: Loss = -11674.987555409238
10
Stopping early at iteration 13500 due to no improvement.
tensor([[-3.3370e-03, -4.6119e+00],
        [-3.8507e+00, -7.6455e-01],
        [-1.0870e+01,  6.2551e+00],
        [-7.1100e+00,  2.4948e+00],
        [-2.0486e+00, -2.5666e+00],
        [ 7.7377e-01, -5.3890e+00],
        [-2.4207e+00, -2.1945e+00],
        [-5.4272e-01, -4.0725e+00],
        [-1.9680e-01, -4.4184e+00],
        [ 2.7089e+00, -7.3242e+00],
        [-1.0854e+00, -3.5298e+00],
        [-3.3815e+00, -1.2338e+00],
        [-8.5534e+00,  3.9382e+00],
        [-6.5816e+00,  1.9664e+00],
        [-7.2107e+00,  2.5955e+00],
        [-7.8925e+00,  3.2773e+00],
        [-1.4603e+00, -3.1549e+00],
        [-8.8384e+00,  4.2232e+00],
        [-2.3248e+00, -2.2904e+00],
        [-5.7471e+00,  1.1319e+00],
        [ 9.1727e-01, -5.5325e+00],
        [-6.5099e+00,  1.8947e+00],
        [-8.4219e+00,  3.8067e+00],
        [-3.7963e+00, -8.1887e-01],
        [-8.7799e+00,  4.1647e+00],
        [-2.7646e+00, -1.8506e+00],
        [-8.8770e-01, -3.7275e+00],
        [ 4.3117e-01, -5.0464e+00],
        [-7.1605e+00,  2.5453e+00],
        [-4.7535e+00,  1.3830e-01],
        [-3.6049e+00, -1.0103e+00],
        [-8.3458e+00,  3.7306e+00],
        [-3.5474e+00, -1.0678e+00],
        [-7.4598e+00,  2.8446e+00],
        [-8.0884e+00,  3.4732e+00],
        [-5.9622e+00,  1.3470e+00],
        [-7.1959e+00,  2.5807e+00],
        [-8.8352e+00,  4.2200e+00],
        [-8.1605e+00,  3.5453e+00],
        [-8.3141e+00,  3.6989e+00],
        [-8.4093e-04, -4.6144e+00],
        [ 1.5738e+00, -6.1890e+00],
        [-8.4339e+00,  3.8186e+00],
        [-1.1560e+00, -3.4592e+00],
        [-9.2074e+00,  4.5922e+00],
        [-9.6512e+00,  5.0360e+00],
        [-8.3625e+00,  3.7472e+00],
        [-6.5686e+00,  1.9534e+00],
        [-5.0116e-01, -4.1141e+00],
        [-2.2240e+00, -2.3913e+00],
        [-8.8789e+00,  4.2637e+00],
        [-9.6330e+00,  5.0178e+00],
        [-5.6033e+00,  9.8809e-01],
        [ 2.6721e+00, -7.2873e+00],
        [-9.1357e+00,  4.5205e+00],
        [-5.6057e+00,  9.9043e-01],
        [ 1.9844e+00, -6.5996e+00],
        [ 1.9710e-01, -4.8123e+00],
        [-5.0884e+00,  4.7321e-01],
        [-6.9876e+00,  2.3724e+00],
        [-7.9338e+00,  3.3186e+00],
        [-7.1995e+00,  2.5843e+00],
        [-7.9135e+00,  3.2982e+00],
        [-9.5680e+00,  4.9528e+00],
        [-9.6996e+00,  5.0844e+00],
        [-6.0561e+00,  1.4409e+00],
        [-1.0065e+00, -3.6088e+00],
        [-7.6302e+00,  3.0150e+00],
        [-8.3843e+00,  3.7691e+00],
        [-1.0276e+00, -3.5876e+00],
        [-6.1519e+00,  1.5367e+00],
        [-6.6522e+00,  2.0370e+00],
        [-6.7718e+00,  2.1565e+00],
        [-9.1497e+00,  4.5345e+00],
        [-3.9684e+00, -6.4678e-01],
        [ 3.9692e-01, -5.0121e+00],
        [-9.4984e+00,  4.8831e+00],
        [-8.5714e+00,  3.9562e+00],
        [-2.0321e-01, -4.4120e+00],
        [-1.0830e+01,  6.2144e+00],
        [-2.7698e+00, -1.8454e+00],
        [ 4.7704e-01, -5.0923e+00],
        [-3.6730e+00, -9.4224e-01],
        [ 1.4399e-02, -4.6296e+00],
        [-7.7117e+00,  3.0965e+00],
        [-7.9503e+00,  3.3351e+00],
        [-6.6367e+00,  2.0215e+00],
        [-7.7335e+00,  3.1183e+00],
        [ 7.7425e-01, -5.3895e+00],
        [ 4.6520e-01, -5.0804e+00],
        [-4.1828e+00, -4.3244e-01],
        [ 1.3346e+00, -5.9498e+00],
        [-6.8698e+00,  2.2546e+00],
        [-9.7333e+00,  5.1181e+00],
        [-6.4109e+00,  1.7956e+00],
        [-7.1723e+00,  2.5571e+00],
        [ 1.5631e+00, -6.1783e+00],
        [-8.0262e+00,  3.4109e+00],
        [-7.6436e+00,  3.0284e+00],
        [-1.0100e+01,  5.4848e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[3.9860e-01, 6.0140e-01],
        [4.5492e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2934, 0.7066], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2253, 0.0906],
         [0.6552, 0.1926]],

        [[0.3495, 0.1366],
         [0.6471, 0.8089]],

        [[0.1439, 0.1247],
         [0.9902, 0.2871]],

        [[0.4804, 0.6886],
         [0.5112, 0.4376]],

        [[0.4459, 0.2080],
         [0.9793, 0.5961]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 87
Adjusted Rand Index: 0.5418167836748644
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 70
Adjusted Rand Index: 0.15151515151515152
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.008484848484848486
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.05527247709643121
Average Adjusted Rand Index: 0.13696941734103346
Iteration 0: Loss = -31922.1196319506
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.8202,    nan]],

        [[0.6045,    nan],
         [0.5140, 0.1117]],

        [[0.5763,    nan],
         [0.9922, 0.8698]],

        [[0.1468,    nan],
         [0.6467, 0.8084]],

        [[0.8438,    nan],
         [0.5518, 0.5002]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -31925.281032777315
Iteration 100: Loss = -11715.998579414985
Iteration 200: Loss = -11708.116833492817
Iteration 300: Loss = -11706.632147471695
Iteration 400: Loss = -11705.763590371589
Iteration 500: Loss = -11705.193632016999
Iteration 600: Loss = -11704.799685383501
Iteration 700: Loss = -11704.514735256214
Iteration 800: Loss = -11704.301498629067
Iteration 900: Loss = -11704.137225392897
Iteration 1000: Loss = -11704.00716111907
Iteration 1100: Loss = -11703.901217009496
Iteration 1200: Loss = -11703.813895342279
Iteration 1300: Loss = -11703.742271935645
Iteration 1400: Loss = -11703.68097078468
Iteration 1500: Loss = -11703.6271919559
Iteration 1600: Loss = -11703.578390165794
Iteration 1700: Loss = -11703.531897069088
Iteration 1800: Loss = -11703.483930537408
Iteration 1900: Loss = -11703.426615148524
Iteration 2000: Loss = -11703.336509321865
Iteration 2100: Loss = -11703.104661814961
Iteration 2200: Loss = -11702.556877633559
Iteration 2300: Loss = -11701.266851086486
Iteration 2400: Loss = -11700.517622840329
Iteration 2500: Loss = -11699.531245529939
Iteration 2600: Loss = -11697.98059670493
Iteration 2700: Loss = -11695.724327648533
Iteration 2800: Loss = -11693.440311014323
Iteration 2900: Loss = -11691.12092730555
Iteration 3000: Loss = -11667.101452026314
Iteration 3100: Loss = -11635.990448294731
Iteration 3200: Loss = -11629.974547450696
Iteration 3300: Loss = -11622.453249851429
Iteration 3400: Loss = -11618.434520346218
Iteration 3500: Loss = -11618.304991102163
Iteration 3600: Loss = -11618.253304238604
Iteration 3700: Loss = -11618.215658905929
Iteration 3800: Loss = -11618.188800394775
Iteration 3900: Loss = -11618.170266727147
Iteration 4000: Loss = -11618.15600415143
Iteration 4100: Loss = -11618.144707840855
Iteration 4200: Loss = -11618.135174428857
Iteration 4300: Loss = -11618.126807820146
Iteration 4400: Loss = -11618.119591215935
Iteration 4500: Loss = -11618.113344428788
Iteration 4600: Loss = -11618.107897435611
Iteration 4700: Loss = -11618.103346528169
Iteration 4800: Loss = -11618.099634098957
Iteration 4900: Loss = -11618.096541592446
Iteration 5000: Loss = -11618.09380168933
Iteration 5100: Loss = -11618.091262165402
Iteration 5200: Loss = -11618.089022324164
Iteration 5300: Loss = -11618.096548062133
1
Iteration 5400: Loss = -11618.085247384184
Iteration 5500: Loss = -11618.08314093496
Iteration 5600: Loss = -11618.121716496582
1
Iteration 5700: Loss = -11618.079871006892
Iteration 5800: Loss = -11618.291169944798
1
Iteration 5900: Loss = -11618.077432601309
Iteration 6000: Loss = -11618.076346324166
Iteration 6100: Loss = -11618.078013275657
1
Iteration 6200: Loss = -11618.075976089067
Iteration 6300: Loss = -11618.073573670492
Iteration 6400: Loss = -11618.129960550916
1
Iteration 6500: Loss = -11618.07211506361
Iteration 6600: Loss = -11618.071461912936
Iteration 6700: Loss = -11618.18727035865
1
Iteration 6800: Loss = -11618.070243483811
Iteration 6900: Loss = -11618.069583894556
Iteration 7000: Loss = -11618.069965308678
1
Iteration 7100: Loss = -11618.067284627685
Iteration 7200: Loss = -11618.059321838098
Iteration 7300: Loss = -11618.052523553071
Iteration 7400: Loss = -11618.051479963131
Iteration 7500: Loss = -11618.083309476267
1
Iteration 7600: Loss = -11618.051030500808
Iteration 7700: Loss = -11618.051332860172
1
Iteration 7800: Loss = -11618.050398107158
Iteration 7900: Loss = -11618.050056060112
Iteration 8000: Loss = -11618.277118750968
1
Iteration 8100: Loss = -11618.049625146543
Iteration 8200: Loss = -11618.04942274842
Iteration 8300: Loss = -11618.049195942653
Iteration 8400: Loss = -11618.049043465768
Iteration 8500: Loss = -11618.048889084192
Iteration 8600: Loss = -11618.048713592783
Iteration 8700: Loss = -11618.049298109567
1
Iteration 8800: Loss = -11618.048490331817
Iteration 8900: Loss = -11618.048390314965
Iteration 9000: Loss = -11618.048343574656
Iteration 9100: Loss = -11618.048213225644
Iteration 9200: Loss = -11618.123993431112
1
Iteration 9300: Loss = -11618.048075666467
Iteration 9400: Loss = -11618.04802854913
Iteration 9500: Loss = -11618.050267455843
1
Iteration 9600: Loss = -11618.047859205075
Iteration 9700: Loss = -11618.048012025934
1
Iteration 9800: Loss = -11618.243171282995
2
Iteration 9900: Loss = -11618.047678522327
Iteration 10000: Loss = -11618.05230417886
1
Iteration 10100: Loss = -11618.04785418344
2
Iteration 10200: Loss = -11618.047574326756
Iteration 10300: Loss = -11618.047932592026
1
Iteration 10400: Loss = -11618.047510824055
Iteration 10500: Loss = -11618.047989260906
1
Iteration 10600: Loss = -11618.051344644236
2
Iteration 10700: Loss = -11618.049984266898
3
Iteration 10800: Loss = -11618.133464994875
4
Iteration 10900: Loss = -11618.047966246113
5
Iteration 11000: Loss = -11618.048859137773
6
Iteration 11100: Loss = -11618.047250295522
Iteration 11200: Loss = -11618.056254859552
1
Iteration 11300: Loss = -11618.047682997927
2
Iteration 11400: Loss = -11618.047594692589
3
Iteration 11500: Loss = -11618.047587859062
4
Iteration 11600: Loss = -11618.05279828414
5
Iteration 11700: Loss = -11618.132560400762
6
Iteration 11800: Loss = -11618.048241259881
7
Iteration 11900: Loss = -11618.066672612731
8
Iteration 12000: Loss = -11618.047775230987
9
Iteration 12100: Loss = -11618.047185678903
Iteration 12200: Loss = -11618.047893960163
1
Iteration 12300: Loss = -11618.089948825107
2
Iteration 12400: Loss = -11618.053361172284
3
Iteration 12500: Loss = -11618.152882768933
4
Iteration 12600: Loss = -11618.047067114254
Iteration 12700: Loss = -11618.047392662771
1
Iteration 12800: Loss = -11618.050232573716
2
Iteration 12900: Loss = -11618.141355148062
3
Iteration 13000: Loss = -11618.047014905516
Iteration 13100: Loss = -11618.047277921687
1
Iteration 13200: Loss = -11618.076811899107
2
Iteration 13300: Loss = -11618.130746143073
3
Iteration 13400: Loss = -11618.047021991399
4
Iteration 13500: Loss = -11618.04702966766
5
Iteration 13600: Loss = -11618.103240709743
6
Iteration 13700: Loss = -11618.047147671927
7
Iteration 13800: Loss = -11618.074818172881
8
Iteration 13900: Loss = -11618.04694778632
Iteration 14000: Loss = -11618.049389010135
1
Iteration 14100: Loss = -11618.047071273897
2
Iteration 14200: Loss = -11618.048604312879
3
Iteration 14300: Loss = -11618.080046397203
4
Iteration 14400: Loss = -11618.054602188215
5
Iteration 14500: Loss = -11618.06442926396
6
Iteration 14600: Loss = -11618.202753931584
7
Iteration 14700: Loss = -11618.047127363838
8
Iteration 14800: Loss = -11618.047159038355
9
Iteration 14900: Loss = -11618.054000707421
10
Stopping early at iteration 14900 due to no improvement.
tensor([[  4.8210,  -6.6827],
        [ -0.4702,  -1.8464],
        [ -8.1091,   6.4510],
        [ -5.1093,   3.6891],
        [  4.0624,  -6.0214],
        [  1.6755,  -3.6022],
        [  3.5760,  -5.0262],
        [  0.3628,  -2.0762],
        [  3.5444,  -5.1857],
        [  5.9638,  -7.5444],
        [  2.8538,  -6.0002],
        [  0.5937,  -2.4597],
        [ -7.3178,   5.9262],
        [ -4.8679,   2.5619],
        [ -6.4696,   5.0820],
        [ -5.6313,   4.1452],
        [  2.6696,  -4.5871],
        [ -7.2285,   5.8406],
        [  2.8251,  -5.2854],
        [ -2.8898,   1.4950],
        [  5.4835,  -7.4398],
        [ -3.6551,   2.1952],
        [ -4.5078,   3.0956],
        [ -0.4267,  -3.8243],
        [ -7.6868,   5.4923],
        [  2.9989,  -4.4577],
        [  3.7635,  -5.1656],
        [  4.6232,  -6.0306],
        [ -4.8642,   3.0651],
        [  3.0306,  -4.6495],
        [  1.3858,  -4.4835],
        [ -6.1172,   4.7084],
        [  2.3267,  -4.0696],
        [ -7.0295,   5.2008],
        [ -6.8766,   5.1285],
        [ -2.9062,   1.4992],
        [ -4.0146,   2.1213],
        [ -8.1363,   6.4758],
        [ -5.2898,   3.2673],
        [ -6.2149,   4.1261],
        [  3.9760,  -5.6797],
        [  4.8081,  -7.4638],
        [ -8.4247,   3.8135],
        [  4.2698,  -5.6593],
        [ -8.0691,   6.1511],
        [ -7.8081,   6.2341],
        [-10.5276,   5.9124],
        [ -6.6044,   4.3273],
        [  3.3832,  -5.4846],
        [  3.4794,  -8.0946],
        [ -6.4021,   4.9630],
        [ -9.4844,   7.4585],
        [ -3.9373,   1.2487],
        [  2.8168,  -5.4034],
        [ -7.8909,   5.6781],
        [  1.4300,  -3.4476],
        [  4.0969,  -5.5346],
        [  3.8367,  -5.3683],
        [ -2.7201,  -0.7465],
        [ -5.1563,   3.6429],
        [ -6.2984,   4.6639],
        [ -5.4162,   3.8346],
        [ -6.3634,   4.9188],
        [ -8.0956,   6.5241],
        [ -9.2584,   6.5221],
        [ -4.9919,   3.5622],
        [  2.7032,  -5.3503],
        [ -7.2597,   5.0106],
        [ -6.5031,   4.1098],
        [  2.9461,  -4.8489],
        [ -2.9753,   1.5843],
        [ -4.6697,   3.2559],
        [ -6.5650,   5.1435],
        [ -7.8390,   6.3936],
        [  1.0165,  -2.4562],
        [  4.2522,  -6.6610],
        [ -8.1540,   6.7326],
        [ -7.4111,   5.9097],
        [  4.7143,  -7.0687],
        [ -7.0261,   5.5586],
        [ -0.1167,  -2.4950],
        [  4.3866,  -6.0865],
        [  4.3668,  -5.8548],
        [  4.7566,  -6.3520],
        [ -4.7173,   2.8954],
        [ -4.3242,   2.9363],
        [ -3.9211,   2.2933],
        [ -5.9842,   3.4227],
        [  5.0273,  -6.4872],
        [  4.1101,  -5.5742],
        [  1.2356,  -3.3929],
        [  6.2307,  -7.6181],
        [ -4.8341,   2.7629],
        [ -7.8295,   6.4050],
        [ -5.1695,   2.1544],
        [ -5.7590,   3.9820],
        [  4.0139,  -5.6593],
        [ -6.9698,   5.1819],
        [ -4.3933,   2.8724],
        [ -7.4828,   6.0803]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.9662, 0.0338],
        [0.8505, 0.1495]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4258, 0.5742], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1768, 0.0958],
         [0.8202, 0.3071]],

        [[0.6045, 0.2199],
         [0.5140, 0.1117]],

        [[0.5763, 0.0885],
         [0.9922, 0.8698]],

        [[0.1468, 0.2742],
         [0.6467, 0.8084]],

        [[0.8438, 0.2412],
         [0.5518, 0.5002]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.008484848484848486
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.002427096100870114
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
Global Adjusted Rand Index: 0.034569591662986024
Average Adjusted Rand Index: 0.1882013683446557
Iteration 0: Loss = -19769.93116904024
Iteration 10: Loss = -11698.688512718927
Iteration 20: Loss = -11694.548064534978
Iteration 30: Loss = -11689.151894852912
Iteration 40: Loss = -11549.258577769855
Iteration 50: Loss = -11354.911013446901
Iteration 60: Loss = -11354.974310739446
1
Iteration 70: Loss = -11354.978847967375
2
Iteration 80: Loss = -11354.979167593749
3
Stopping early at iteration 79 due to no improvement.
pi: tensor([[0.7314, 0.2686],
        [0.2577, 0.7423]], dtype=torch.float64)
alpha: tensor([0.4719, 0.5281])
beta: tensor([[[0.2021, 0.0953],
         [0.5041, 0.2985]],

        [[0.7092, 0.1009],
         [0.7006, 0.1217]],

        [[0.0971, 0.0975],
         [0.7626, 0.0155]],

        [[0.2630, 0.1119],
         [0.2338, 0.7483]],

        [[0.1103, 0.0992],
         [0.8431, 0.7657]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 2
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 4
tensor([0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9446733348050895
Average Adjusted Rand Index: 0.9446438845001811
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19770.342856308456
Iteration 100: Loss = -11705.525157985112
Iteration 200: Loss = -11697.685248624171
Iteration 300: Loss = -11691.097593852179
Iteration 400: Loss = -11645.55545651659
Iteration 500: Loss = -11419.191312924553
Iteration 600: Loss = -11386.075509440287
Iteration 700: Loss = -11363.942917757879
Iteration 800: Loss = -11359.94849734081
Iteration 900: Loss = -11358.954110748477
Iteration 1000: Loss = -11353.278303198253
Iteration 1100: Loss = -11353.051876365846
Iteration 1200: Loss = -11352.941595570424
Iteration 1300: Loss = -11352.858004409987
Iteration 1400: Loss = -11352.788842154998
Iteration 1500: Loss = -11351.615604402981
Iteration 1600: Loss = -11351.41018838581
Iteration 1700: Loss = -11351.370682203165
Iteration 1800: Loss = -11351.344427966998
Iteration 1900: Loss = -11351.345596373
1
Iteration 2000: Loss = -11351.298975479043
Iteration 2100: Loss = -11351.28007162246
Iteration 2200: Loss = -11351.265949172586
Iteration 2300: Loss = -11351.251097526234
Iteration 2400: Loss = -11351.220390951597
Iteration 2500: Loss = -11351.186766203398
Iteration 2600: Loss = -11351.177853803994
Iteration 2700: Loss = -11351.169971457011
Iteration 2800: Loss = -11351.161357399002
Iteration 2900: Loss = -11351.148450323026
Iteration 3000: Loss = -11351.13976778565
Iteration 3100: Loss = -11351.134822910575
Iteration 3200: Loss = -11351.130990879117
Iteration 3300: Loss = -11351.127870380158
Iteration 3400: Loss = -11351.124800013846
Iteration 3500: Loss = -11351.1219893046
Iteration 3600: Loss = -11351.119452737183
Iteration 3700: Loss = -11351.116797248227
Iteration 3800: Loss = -11351.114381392028
Iteration 3900: Loss = -11351.111863813749
Iteration 4000: Loss = -11351.10351378732
Iteration 4100: Loss = -11351.100767131054
Iteration 4200: Loss = -11351.09938982738
Iteration 4300: Loss = -11351.097686174651
Iteration 4400: Loss = -11351.096260334802
Iteration 4500: Loss = -11351.094983397554
Iteration 4600: Loss = -11351.093852981334
Iteration 4700: Loss = -11351.099607745055
1
Iteration 4800: Loss = -11351.091976883617
Iteration 4900: Loss = -11351.09116176531
Iteration 5000: Loss = -11351.095531557921
1
Iteration 5100: Loss = -11351.08968267427
Iteration 5200: Loss = -11351.089120608647
Iteration 5300: Loss = -11351.088458654425
Iteration 5400: Loss = -11351.08780428179
Iteration 5500: Loss = -11351.087689776117
Iteration 5600: Loss = -11351.088834223836
1
Iteration 5700: Loss = -11351.089774868753
2
Iteration 5800: Loss = -11351.085629963285
Iteration 5900: Loss = -11351.085335092968
Iteration 6000: Loss = -11351.084799068303
Iteration 6100: Loss = -11351.084471572725
Iteration 6200: Loss = -11351.084106573158
Iteration 6300: Loss = -11351.086080064199
1
Iteration 6400: Loss = -11351.092772530505
2
Iteration 6500: Loss = -11351.083383876257
Iteration 6600: Loss = -11351.082962864883
Iteration 6700: Loss = -11351.082693050348
Iteration 6800: Loss = -11351.093414921359
1
Iteration 6900: Loss = -11351.082473827208
Iteration 7000: Loss = -11351.082505606688
1
Iteration 7100: Loss = -11351.081950343001
Iteration 7200: Loss = -11351.081729321457
Iteration 7300: Loss = -11351.085951772497
1
Iteration 7400: Loss = -11351.081884715488
2
Iteration 7500: Loss = -11351.086980914206
3
Iteration 7600: Loss = -11351.081688616465
Iteration 7700: Loss = -11351.087694129375
1
Iteration 7800: Loss = -11351.089328082891
2
Iteration 7900: Loss = -11351.085317535568
3
Iteration 8000: Loss = -11351.081072741099
Iteration 8100: Loss = -11351.085807349644
1
Iteration 8200: Loss = -11351.100780082601
2
Iteration 8300: Loss = -11351.079693598347
Iteration 8400: Loss = -11351.079124416141
Iteration 8500: Loss = -11351.150689733637
1
Iteration 8600: Loss = -11351.07915388818
2
Iteration 8700: Loss = -11351.07436211271
Iteration 8800: Loss = -11351.070384500139
Iteration 8900: Loss = -11351.279793084092
1
Iteration 9000: Loss = -11351.069395526714
Iteration 9100: Loss = -11351.073238051475
1
Iteration 9200: Loss = -11351.068727665652
Iteration 9300: Loss = -11351.07473752869
1
Iteration 9400: Loss = -11351.067749418806
Iteration 9500: Loss = -11351.037995864714
Iteration 9600: Loss = -11351.042252480715
1
Iteration 9700: Loss = -11351.06029199352
2
Iteration 9800: Loss = -11351.036920507791
Iteration 9900: Loss = -11351.036780706741
Iteration 10000: Loss = -11351.119321994387
1
Iteration 10100: Loss = -11351.039701393087
2
Iteration 10200: Loss = -11351.036463781646
Iteration 10300: Loss = -11351.042466051415
1
Iteration 10400: Loss = -11351.036345537617
Iteration 10500: Loss = -11351.036406516714
1
Iteration 10600: Loss = -11351.047539348501
2
Iteration 10700: Loss = -11351.036207329491
Iteration 10800: Loss = -11351.038418667178
1
Iteration 10900: Loss = -11351.036145044076
Iteration 11000: Loss = -11351.036104754312
Iteration 11100: Loss = -11351.036452528811
1
Iteration 11200: Loss = -11351.035960427851
Iteration 11300: Loss = -11351.041993058168
1
Iteration 11400: Loss = -11351.062341350063
2
Iteration 11500: Loss = -11351.035996674293
3
Iteration 11600: Loss = -11351.02706096719
Iteration 11700: Loss = -11351.028256379654
1
Iteration 11800: Loss = -11351.05801019442
2
Iteration 11900: Loss = -11351.026030228004
Iteration 12000: Loss = -11351.026412384115
1
Iteration 12100: Loss = -11351.027631851497
2
Iteration 12200: Loss = -11351.025961546788
Iteration 12300: Loss = -11351.026848929198
1
Iteration 12400: Loss = -11351.025910262048
Iteration 12500: Loss = -11351.026144286849
1
Iteration 12600: Loss = -11351.025820415847
Iteration 12700: Loss = -11351.024708314899
Iteration 12800: Loss = -11351.034400855058
1
Iteration 12900: Loss = -11351.024392519434
Iteration 13000: Loss = -11351.024383750064
Iteration 13100: Loss = -11351.02440189379
1
Iteration 13200: Loss = -11351.024415558573
2
Iteration 13300: Loss = -11351.024386391116
3
Iteration 13400: Loss = -11351.029442157345
4
Iteration 13500: Loss = -11351.024340481119
Iteration 13600: Loss = -11351.042802210079
1
Iteration 13700: Loss = -11351.024316479383
Iteration 13800: Loss = -11351.024330570712
1
Iteration 13900: Loss = -11351.025254466515
2
Iteration 14000: Loss = -11351.024292319931
Iteration 14100: Loss = -11351.025025240286
1
Iteration 14200: Loss = -11351.025362983186
2
Iteration 14300: Loss = -11351.025853914605
3
Iteration 14400: Loss = -11351.024285406798
Iteration 14500: Loss = -11351.024531859077
1
Iteration 14600: Loss = -11351.024262278888
Iteration 14700: Loss = -11351.058765540845
1
Iteration 14800: Loss = -11351.024252418872
Iteration 14900: Loss = -11351.032787630897
1
Iteration 15000: Loss = -11351.045926997314
2
Iteration 15100: Loss = -11351.034186283525
3
Iteration 15200: Loss = -11351.06280020564
4
Iteration 15300: Loss = -11351.024261343311
5
Iteration 15400: Loss = -11351.034179220433
6
Iteration 15500: Loss = -11351.024240259318
Iteration 15600: Loss = -11351.045510157375
1
Iteration 15700: Loss = -11351.024263083196
2
Iteration 15800: Loss = -11351.034137921522
3
Iteration 15900: Loss = -11351.024239766017
Iteration 16000: Loss = -11351.024214078243
Iteration 16100: Loss = -11351.025539723332
1
Iteration 16200: Loss = -11351.024231154464
2
Iteration 16300: Loss = -11351.026541550076
3
Iteration 16400: Loss = -11351.024260884871
4
Iteration 16500: Loss = -11351.024669399638
5
Iteration 16600: Loss = -11351.046065204602
6
Iteration 16700: Loss = -11351.024260867121
7
Iteration 16800: Loss = -11351.024378718193
8
Iteration 16900: Loss = -11351.133105874333
9
Iteration 17000: Loss = -11351.024308735514
10
Stopping early at iteration 17000 due to no improvement.
tensor([[  4.2016,  -6.7843],
        [ -0.4529,  -1.0150],
        [ -9.7815,   7.8434],
        [ -9.9999,   8.4075],
        [  7.2822, -11.7214],
        [  2.6843,  -4.5036],
        [  3.0834,  -4.4789],
        [  0.3716,  -2.7377],
        [  3.4855,  -6.2223],
        [  6.8077,  -8.1956],
        [  1.2772,  -5.8924],
        [  0.8344,  -2.3346],
        [ -8.6478,   6.9391],
        [ -5.1708,   3.6791],
        [ -6.2617,   4.8249],
        [ -6.5721,   5.1542],
        [  2.9718,  -5.0193],
        [ -7.2649,   5.8626],
        [  2.8314,  -4.2179],
        [ -4.4964,   1.9390],
        [  5.5162,  -6.9040],
        [ -4.6583,   2.8003],
        [ -5.4203,   3.9711],
        [  1.1863,  -2.5748],
        [ -7.5297,   6.1296],
        [  0.8144,  -4.5571],
        [  3.9630,  -5.3507],
        [  4.1245,  -6.7233],
        [ -4.7149,   2.7259],
        [  2.2645,  -3.6866],
        [  2.4320,  -4.0161],
        [ -8.9910,   7.3084],
        [  7.3514,  -9.1387],
        [ -7.8583,   5.2887],
        [ -7.0038,   5.3490],
        [ -4.1114,   2.6410],
        [ -6.1075,   2.2566],
        [ -9.2379,   7.8121],
        [ -4.6819,   2.7839],
        [ -7.9827,   3.9034],
        [  4.0086,  -6.6159],
        [  5.7177,  -8.4266],
        [ -9.1830,   7.7697],
        [  4.4095,  -5.9018],
        [ -9.7385,   7.1365],
        [ -9.0233,   7.3222],
        [ -9.3559,   7.3835],
        [ -7.7390,   3.6317],
        [  4.0938,  -5.5493],
        [  5.1949,  -7.2321],
        [ -9.5000,   8.0311],
        [-10.3616,   8.9708],
        [ -3.1465,   1.3606],
        [  4.0141,  -5.5477],
        [ -8.9954,   7.6078],
        [  0.9965,  -2.7021],
        [  3.9018,  -6.7278],
        [  4.4831,  -5.9479],
        [ -1.6239,   0.2375],
        [ -6.1328,   4.6318],
        [-10.3291,   7.9409],
        [ -5.4115,   4.0152],
        [ -8.8100,   6.7740],
        [-10.3153,   8.8520],
        [ -9.0282,   7.6257],
        [ -5.9444,   3.7710],
        [  2.1172,  -4.3715],
        [ -8.7975,   6.7306],
        [ -6.8629,   5.4757],
        [  2.6085,  -4.0822],
        [ -4.7605,   1.3162],
        [ -5.5234,   4.1008],
        [ -7.3159,   5.4907],
        [-10.2835,   8.0157],
        [ -0.8561,  -3.1717],
        [  5.4039,  -6.9121],
        [ -9.4311,   8.0292],
        [ -7.9439,   6.4529],
        [  4.3993,  -5.8378],
        [ -9.0523,   7.6658],
        [  0.8217,  -2.2163],
        [  4.1980,  -6.8033],
        [  4.7946,  -6.3351],
        [  4.6079,  -6.7211],
        [ -4.4158,   2.5972],
        [ -4.2085,   2.2294],
        [ -6.4543,   1.8391],
        [ -6.4821,   5.0671],
        [  5.8491,  -7.3775],
        [  4.4415,  -5.8492],
        [  2.1567,  -3.6027],
        [  6.6904,  -8.3252],
        [ -6.4024,   3.7841],
        [ -9.0122,   7.2840],
        [ -4.4066,   2.4619],
        [ -6.0574,   4.6704],
        [  4.1289,  -6.4990],
        [ -8.0558,   6.0573],
        [ -8.7922,   7.1022],
        [-10.9109,   7.7261]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7414, 0.2586],
        [0.2514, 0.7486]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4252, 0.5748], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2056, 0.0955],
         [0.5041, 0.3058]],

        [[0.7092, 0.1016],
         [0.7006, 0.1217]],

        [[0.0971, 0.0974],
         [0.7626, 0.0155]],

        [[0.2630, 0.1120],
         [0.2338, 0.7483]],

        [[0.1103, 0.0993],
         [0.8431, 0.7657]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 4
tensor([0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9524809640734552
Average Adjusted Rand Index: 0.9523206521769488
Iteration 0: Loss = -19608.64541475275
Iteration 10: Loss = -11679.26606396851
Iteration 20: Loss = -11355.075932548529
Iteration 30: Loss = -11354.96168047241
Iteration 40: Loss = -11354.977983998935
1
Iteration 50: Loss = -11354.979107558318
2
Iteration 60: Loss = -11354.979203132023
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7314, 0.2686],
        [0.2577, 0.7423]], dtype=torch.float64)
alpha: tensor([0.4719, 0.5281])
beta: tensor([[[0.2021, 0.0953],
         [0.8529, 0.2985]],

        [[0.9260, 0.1009],
         [0.7727, 0.7505]],

        [[0.2164, 0.0975],
         [0.9526, 0.0248]],

        [[0.5116, 0.1119],
         [0.6887, 0.0128]],

        [[0.4797, 0.0992],
         [0.7321, 0.8411]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824242424242424
time is 2
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 4
tensor([0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9446733348050895
Average Adjusted Rand Index: 0.9446438845001811
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19608.27697837352
Iteration 100: Loss = -11701.855071955732
Iteration 200: Loss = -11699.00668019041
Iteration 300: Loss = -11691.947939432777
Iteration 400: Loss = -11389.723679297249
Iteration 500: Loss = -11357.905246302367
Iteration 600: Loss = -11357.490842850786
Iteration 700: Loss = -11357.390645293764
Iteration 800: Loss = -11357.112309485363
Iteration 900: Loss = -11351.143937222407
Iteration 1000: Loss = -11351.121728762622
Iteration 1100: Loss = -11351.10475824431
Iteration 1200: Loss = -11351.09038362801
Iteration 1300: Loss = -11351.081318611468
Iteration 1400: Loss = -11351.074872975645
Iteration 1500: Loss = -11351.069894349452
Iteration 1600: Loss = -11351.065997855767
Iteration 1700: Loss = -11351.062783626145
Iteration 1800: Loss = -11351.060091004902
Iteration 1900: Loss = -11351.057684836092
Iteration 2000: Loss = -11351.05502835867
Iteration 2100: Loss = -11351.04619778142
Iteration 2200: Loss = -11351.042601436102
Iteration 2300: Loss = -11351.036873970188
Iteration 2400: Loss = -11351.03372513808
Iteration 2500: Loss = -11351.032564783536
Iteration 2600: Loss = -11351.031588171061
Iteration 2700: Loss = -11351.030734860373
Iteration 2800: Loss = -11351.029962903243
Iteration 2900: Loss = -11351.029301853961
Iteration 3000: Loss = -11351.034775969829
1
Iteration 3100: Loss = -11351.028222549585
Iteration 3200: Loss = -11351.02769198628
Iteration 3300: Loss = -11351.027408129807
Iteration 3400: Loss = -11351.026759670285
Iteration 3500: Loss = -11351.026436603062
Iteration 3600: Loss = -11351.026163558436
Iteration 3700: Loss = -11351.025828292466
Iteration 3800: Loss = -11351.025544457285
Iteration 3900: Loss = -11351.025246574227
Iteration 4000: Loss = -11351.027283887981
1
Iteration 4100: Loss = -11351.024481040387
Iteration 4200: Loss = -11351.024696740842
1
Iteration 4300: Loss = -11351.024033779964
Iteration 4400: Loss = -11351.029348155613
1
Iteration 4500: Loss = -11351.023703704024
Iteration 4600: Loss = -11351.023812904874
1
Iteration 4700: Loss = -11351.02870313857
2
Iteration 4800: Loss = -11351.023332331937
Iteration 4900: Loss = -11351.024705137936
1
Iteration 5000: Loss = -11351.023149855597
Iteration 5100: Loss = -11351.02302780803
Iteration 5200: Loss = -11351.022876342045
Iteration 5300: Loss = -11351.024094798031
1
Iteration 5400: Loss = -11351.025901926172
2
Iteration 5500: Loss = -11351.022691736407
Iteration 5600: Loss = -11351.026761974284
1
Iteration 5700: Loss = -11351.021958875152
Iteration 5800: Loss = -11351.021915255358
Iteration 5900: Loss = -11351.024279369964
1
Iteration 6000: Loss = -11351.025480653438
2
Iteration 6100: Loss = -11351.021747326811
Iteration 6200: Loss = -11351.02190461462
1
Iteration 6300: Loss = -11351.030825641728
2
Iteration 6400: Loss = -11351.021812013247
3
Iteration 6500: Loss = -11351.021616839702
Iteration 6600: Loss = -11351.02806325657
1
Iteration 6700: Loss = -11351.02245927605
2
Iteration 6800: Loss = -11351.022905499894
3
Iteration 6900: Loss = -11351.023109292018
4
Iteration 7000: Loss = -11351.023554090127
5
Iteration 7100: Loss = -11351.02197372253
6
Iteration 7200: Loss = -11351.066668067344
7
Iteration 7300: Loss = -11351.02010221879
Iteration 7400: Loss = -11351.02008150275
Iteration 7500: Loss = -11351.026443349107
1
Iteration 7600: Loss = -11351.019964597657
Iteration 7700: Loss = -11351.02088951445
1
Iteration 7800: Loss = -11351.019963340836
Iteration 7900: Loss = -11351.020584010423
1
Iteration 8000: Loss = -11351.019926337389
Iteration 8100: Loss = -11351.027006216229
1
Iteration 8200: Loss = -11351.019887015667
Iteration 8300: Loss = -11351.026110057815
1
Iteration 8400: Loss = -11351.019853404952
Iteration 8500: Loss = -11351.0635344516
1
Iteration 8600: Loss = -11351.01983501405
Iteration 8700: Loss = -11351.019825887044
Iteration 8800: Loss = -11351.019954073136
1
Iteration 8900: Loss = -11351.019780392913
Iteration 9000: Loss = -11351.043244694114
1
Iteration 9100: Loss = -11351.019798192709
2
Iteration 9200: Loss = -11351.019778199208
Iteration 9300: Loss = -11351.022108215639
1
Iteration 9400: Loss = -11351.019787434157
2
Iteration 9500: Loss = -11351.0937532953
3
Iteration 9600: Loss = -11351.019758759148
Iteration 9700: Loss = -11351.019749980522
Iteration 9800: Loss = -11351.019926858102
1
Iteration 9900: Loss = -11351.019774599214
2
Iteration 10000: Loss = -11351.019961330032
3
Iteration 10100: Loss = -11351.019980273335
4
Iteration 10200: Loss = -11351.019884787513
5
Iteration 10300: Loss = -11351.019726799967
Iteration 10400: Loss = -11351.022467623492
1
Iteration 10500: Loss = -11351.075091888728
2
Iteration 10600: Loss = -11351.019703028514
Iteration 10700: Loss = -11351.032459241518
1
Iteration 10800: Loss = -11351.019690678739
Iteration 10900: Loss = -11351.022408215627
1
Iteration 11000: Loss = -11351.019675393223
Iteration 11100: Loss = -11351.025409651236
1
Iteration 11200: Loss = -11351.019710829045
2
Iteration 11300: Loss = -11351.445093461367
3
Iteration 11400: Loss = -11351.019686053512
4
Iteration 11500: Loss = -11351.019719537822
5
Iteration 11600: Loss = -11351.020287370786
6
Iteration 11700: Loss = -11351.019911762027
7
Iteration 11800: Loss = -11351.052041623234
8
Iteration 11900: Loss = -11351.04096151383
9
Iteration 12000: Loss = -11351.020555754876
10
Stopping early at iteration 12000 due to no improvement.
tensor([[  4.7792,  -6.2071],
        [ -0.4288,  -0.9899],
        [ -9.6029,   8.2098],
        [ -5.6705,   3.9759],
        [  3.7846,  -5.6335],
        [  2.8462,  -4.3434],
        [  2.6278,  -4.9220],
        [ -0.2459,  -3.3548],
        [  4.1593,  -5.5510],
        [  5.3810,  -8.7366],
        [  2.7695,  -4.4007],
        [  0.8847,  -2.2827],
        [ -9.5217,   7.4150],
        [ -5.4474,   3.4028],
        [ -6.4214,   4.6649],
        [ -6.8369,   4.9012],
        [  3.1159,  -4.8769],
        [ -7.2982,   5.7937],
        [  2.8212,  -4.2283],
        [ -5.3159,   1.1183],
        [  5.3714,  -7.0353],
        [ -4.4233,   3.0343],
        [ -6.1251,   3.2699],
        [  0.4862,  -3.2750],
        [ -7.4927,   6.0942],
        [  1.9710,  -3.4001],
        [  3.9576,  -5.3562],
        [  4.6699,  -6.1883],
        [ -4.7843,   2.6669],
        [  2.1650,  -3.7862],
        [  2.5129,  -3.9350],
        [ -7.9655,   6.3327],
        [  0.9827,  -4.2209],
        [ -7.3325,   5.7908],
        [ -6.9005,   5.4354],
        [ -4.1660,   2.5856],
        [ -6.4887,   1.8735],
        [-10.3383,   6.8110],
        [ -4.4611,   3.0023],
        [ -6.6537,   5.2367],
        [  4.2800,  -6.3456],
        [  4.7431,  -9.3584],
        [ -9.0207,   7.3038],
        [  3.2666,  -7.0559],
        [ -8.6498,   7.0041],
        [ -8.7734,   7.3694],
        [ -8.5204,   7.1338],
        [ -7.0835,   4.3756],
        [  3.5561,  -6.0862],
        [  5.3659,  -7.0501],
        [ -7.3984,   5.7352],
        [ -9.2066,   7.5243],
        [ -3.2244,   1.2838],
        [  2.4751,  -7.0903],
        [-10.5081,   6.7694],
        [  1.1258,  -2.5597],
        [  3.9747,  -6.6632],
        [  3.5293,  -6.9027],
        [ -1.9758,  -0.1144],
        [ -6.1813,   4.5855],
        [ -7.2329,   5.4780],
        [ -5.4898,   3.9355],
        [ -8.1182,   6.5535],
        [ -9.4411,   7.9840],
        [ -8.6662,   7.1093],
        [ -5.9084,   3.8048],
        [  3.5633,  -6.7894],
        [ -7.9577,   6.4683],
        [ -8.1415,   4.2021],
        [  2.3962,  -4.2818],
        [ -3.8126,   2.2601],
        [ -5.8358,   3.8000],
        [ -7.1126,   5.6878],
        [ -9.1108,   7.6325],
        [  0.2542,  -2.0637],
        [  5.3344,  -6.9959],
        [ -8.4492,   6.8405],
        [ -9.6707,   5.0554],
        [  3.5531,  -6.6849],
        [ -7.9153,   6.0559],
        [  0.8250,  -2.2124],
        [  4.6686,  -6.3362],
        [  4.7772,  -6.3404],
        [  3.9671,  -7.3653],
        [ -4.2709,   2.7412],
        [ -3.9292,   2.5070],
        [ -4.8953,   3.3961],
        [ -6.6858,   4.8591],
        [  5.6426,  -7.4318],
        [  4.4497,  -5.8453],
        [  1.7178,  -4.0423],
        [  6.5126,  -8.1433],
        [ -5.7885,   4.3926],
        [ -8.4559,   6.9840],
        [ -5.2973,   1.5706],
        [ -6.1046,   4.6367],
        [  4.0161,  -6.6314],
        [ -8.1681,   6.6995],
        [ -5.3231,   3.6242],
        [ -8.9218,   6.0245]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7416, 0.2584],
        [0.2517, 0.7483]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4256, 0.5744], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2055, 0.0956],
         [0.8529, 0.3059]],

        [[0.9260, 0.1016],
         [0.7727, 0.7505]],

        [[0.2164, 0.0974],
         [0.9526, 0.0248]],

        [[0.5116, 0.1119],
         [0.6887, 0.0128]],

        [[0.4797, 0.0993],
         [0.7321, 0.8411]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 4
tensor([0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9524809640734552
Average Adjusted Rand Index: 0.9523206521769488
Iteration 0: Loss = -22709.536174045443
Iteration 10: Loss = -11703.577276299346
Iteration 20: Loss = -11703.576786806085
Iteration 30: Loss = -11698.911703309099
Iteration 40: Loss = -11695.359306864448
Iteration 50: Loss = -11694.925063322165
Iteration 60: Loss = -11694.175446967989
Iteration 70: Loss = -11693.252972733417
Iteration 80: Loss = -11693.098049280372
Iteration 90: Loss = -11693.038386072605
Iteration 100: Loss = -11693.0141226584
Iteration 110: Loss = -11693.0041411739
Iteration 120: Loss = -11692.999901567855
Iteration 130: Loss = -11692.998178897195
Iteration 140: Loss = -11692.997472616164
Iteration 150: Loss = -11692.997194181846
Iteration 160: Loss = -11692.996996012309
Iteration 170: Loss = -11692.99697561527
Iteration 180: Loss = -11692.996950539567
Iteration 190: Loss = -11692.996917654938
Iteration 200: Loss = -11692.996893704267
Iteration 210: Loss = -11692.99691226616
1
Iteration 220: Loss = -11692.996921082362
2
Iteration 230: Loss = -11692.996912743734
3
Stopping early at iteration 229 due to no improvement.
pi: tensor([[0.9436, 0.0564],
        [0.8902, 0.1098]], dtype=torch.float64)
alpha: tensor([0.9357, 0.0643])
beta: tensor([[[0.1723, 0.2473],
         [0.6453, 0.3515]],

        [[0.8256, 0.2287],
         [0.7978, 0.1023]],

        [[0.7890, 0.0915],
         [0.3127, 0.3697]],

        [[0.5750, 0.2501],
         [0.6654, 0.0307]],

        [[0.6747, 0.2307],
         [0.8166, 0.1956]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.01717781179455718
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.008484848484848486
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.002427096100870114
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
Global Adjusted Rand Index: -0.0020619729413603
Average Adjusted Rand Index: -0.006667314855909131
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22708.978045060983
Iteration 100: Loss = -11707.339970885969
Iteration 200: Loss = -11704.131330094713
Iteration 300: Loss = -11701.069236461524
Iteration 400: Loss = -11697.789957918752
Iteration 500: Loss = -11694.691074677146
Iteration 600: Loss = -11692.192488772833
Iteration 700: Loss = -11686.521289389937
Iteration 800: Loss = -11601.955530829715
Iteration 900: Loss = -11540.154771614605
Iteration 1000: Loss = -11505.506900152826
Iteration 1100: Loss = -11475.532022614541
Iteration 1200: Loss = -11455.53548559092
Iteration 1300: Loss = -11421.586740894167
Iteration 1400: Loss = -11417.398050943324
Iteration 1500: Loss = -11408.564290472677
Iteration 1600: Loss = -11393.88324086759
Iteration 1700: Loss = -11386.463221131879
Iteration 1800: Loss = -11377.469557758477
Iteration 1900: Loss = -11377.25749184356
Iteration 2000: Loss = -11371.460305222581
Iteration 2100: Loss = -11371.364126133982
Iteration 2200: Loss = -11371.179406765317
Iteration 2300: Loss = -11366.051162391019
Iteration 2400: Loss = -11366.008974276654
Iteration 2500: Loss = -11365.982738211627
Iteration 2600: Loss = -11365.962488137615
Iteration 2700: Loss = -11365.945749591438
Iteration 2800: Loss = -11365.930433737165
Iteration 2900: Loss = -11365.912842526852
Iteration 3000: Loss = -11365.882900335904
Iteration 3100: Loss = -11354.926754877604
Iteration 3200: Loss = -11354.904944871567
Iteration 3300: Loss = -11354.894219097712
Iteration 3400: Loss = -11354.88390316194
Iteration 3500: Loss = -11354.869371284114
Iteration 3600: Loss = -11354.820721482665
Iteration 3700: Loss = -11354.813504616146
Iteration 3800: Loss = -11354.721037166606
Iteration 3900: Loss = -11354.700123740322
Iteration 4000: Loss = -11354.697155815835
Iteration 4100: Loss = -11354.692537251252
Iteration 4200: Loss = -11354.690538059482
Iteration 4300: Loss = -11354.685496807613
Iteration 4400: Loss = -11354.682734977276
Iteration 4500: Loss = -11354.679673673574
Iteration 4600: Loss = -11354.678637849041
Iteration 4700: Loss = -11354.664227812247
Iteration 4800: Loss = -11351.327432896509
Iteration 4900: Loss = -11351.282370207995
Iteration 5000: Loss = -11351.279870947907
Iteration 5100: Loss = -11351.278320803007
Iteration 5200: Loss = -11351.282655195457
1
Iteration 5300: Loss = -11351.275663189941
Iteration 5400: Loss = -11351.274692764546
Iteration 5500: Loss = -11351.273455014001
Iteration 5600: Loss = -11351.27484064171
1
Iteration 5700: Loss = -11351.27195150349
Iteration 5800: Loss = -11351.272173203643
1
Iteration 5900: Loss = -11351.270268276101
Iteration 6000: Loss = -11351.270102556893
Iteration 6100: Loss = -11351.273485146532
1
Iteration 6200: Loss = -11351.270771194499
2
Iteration 6300: Loss = -11351.267814016144
Iteration 6400: Loss = -11351.266913788133
Iteration 6500: Loss = -11351.268214892882
1
Iteration 6600: Loss = -11351.265682047746
Iteration 6700: Loss = -11351.26559716519
Iteration 6800: Loss = -11351.26427009931
Iteration 6900: Loss = -11351.271825581749
1
Iteration 7000: Loss = -11351.270103065224
2
Iteration 7100: Loss = -11351.269603395343
3
Iteration 7200: Loss = -11351.26390959546
Iteration 7300: Loss = -11351.264285664314
1
Iteration 7400: Loss = -11351.264205464675
2
Iteration 7500: Loss = -11351.266101836687
3
Iteration 7600: Loss = -11351.264709746818
4
Iteration 7700: Loss = -11351.24172924443
Iteration 7800: Loss = -11351.239012723956
Iteration 7900: Loss = -11351.242400667741
1
Iteration 8000: Loss = -11351.356959821549
2
Iteration 8100: Loss = -11351.237017154239
Iteration 8200: Loss = -11351.23824124493
1
Iteration 8300: Loss = -11351.234830259375
Iteration 8400: Loss = -11351.233468805629
Iteration 8500: Loss = -11351.231897178839
Iteration 8600: Loss = -11351.231632139474
Iteration 8700: Loss = -11351.231598577215
Iteration 8800: Loss = -11351.223293339355
Iteration 8900: Loss = -11351.222939526157
Iteration 9000: Loss = -11351.222948677027
1
Iteration 9100: Loss = -11351.222694933196
Iteration 9200: Loss = -11351.43788178987
1
Iteration 9300: Loss = -11351.222515664906
Iteration 9400: Loss = -11351.223666839469
1
Iteration 9500: Loss = -11351.229330507156
2
Iteration 9600: Loss = -11351.224172663031
3
Iteration 9700: Loss = -11351.222256994988
Iteration 9800: Loss = -11351.275340969132
1
Iteration 9900: Loss = -11351.22209143836
Iteration 10000: Loss = -11351.222903504166
1
Iteration 10100: Loss = -11351.221992982453
Iteration 10200: Loss = -11351.22192452742
Iteration 10300: Loss = -11351.222834021051
1
Iteration 10400: Loss = -11351.22185242918
Iteration 10500: Loss = -11351.564301299673
1
Iteration 10600: Loss = -11351.221728409102
Iteration 10700: Loss = -11351.221659614175
Iteration 10800: Loss = -11351.232345805294
1
Iteration 10900: Loss = -11351.221389396527
Iteration 11000: Loss = -11351.221247961057
Iteration 11100: Loss = -11351.23015159956
1
Iteration 11200: Loss = -11351.221157341583
Iteration 11300: Loss = -11351.221510067428
1
Iteration 11400: Loss = -11351.22115951208
2
Iteration 11500: Loss = -11351.221614691536
3
Iteration 11600: Loss = -11351.241308662678
4
Iteration 11700: Loss = -11351.221123432724
Iteration 11800: Loss = -11351.221181568604
1
Iteration 11900: Loss = -11351.305549920313
2
Iteration 12000: Loss = -11351.221021887497
Iteration 12100: Loss = -11351.227431780237
1
Iteration 12200: Loss = -11351.22091184494
Iteration 12300: Loss = -11351.223850728089
1
Iteration 12400: Loss = -11351.22081811276
Iteration 12500: Loss = -11351.223393203925
1
Iteration 12600: Loss = -11351.220718958648
Iteration 12700: Loss = -11351.650557886838
1
Iteration 12800: Loss = -11351.220717141554
Iteration 12900: Loss = -11351.220683537693
Iteration 13000: Loss = -11351.223405355486
1
Iteration 13100: Loss = -11351.220691693765
2
Iteration 13200: Loss = -11351.246933645578
3
Iteration 13300: Loss = -11351.220669033939
Iteration 13400: Loss = -11351.220707215523
1
Iteration 13500: Loss = -11351.220861910513
2
Iteration 13600: Loss = -11351.222468763513
3
Iteration 13700: Loss = -11351.223661778013
4
Iteration 13800: Loss = -11351.220630351048
Iteration 13900: Loss = -11351.221083140577
1
Iteration 14000: Loss = -11351.251646561737
2
Iteration 14100: Loss = -11351.220589238088
Iteration 14200: Loss = -11351.220636498227
1
Iteration 14300: Loss = -11351.229750460969
2
Iteration 14400: Loss = -11351.220471051707
Iteration 14500: Loss = -11351.223971596753
1
Iteration 14600: Loss = -11351.231095343639
2
Iteration 14700: Loss = -11351.220139117924
Iteration 14800: Loss = -11351.228398777586
1
Iteration 14900: Loss = -11351.21982738146
Iteration 15000: Loss = -11351.553057060659
1
Iteration 15100: Loss = -11351.219830039012
2
Iteration 15200: Loss = -11351.02277640743
Iteration 15300: Loss = -11351.022790137748
1
Iteration 15400: Loss = -11351.023436944926
2
Iteration 15500: Loss = -11351.02243208024
Iteration 15600: Loss = -11351.023155346275
1
Iteration 15700: Loss = -11351.022326786564
Iteration 15800: Loss = -11351.050578589087
1
Iteration 15900: Loss = -11351.051760193925
2
Iteration 16000: Loss = -11351.041701766591
3
Iteration 16100: Loss = -11351.133928117904
4
Iteration 16200: Loss = -11351.024185588996
5
Iteration 16300: Loss = -11351.021340598048
Iteration 16400: Loss = -11351.042820438868
1
Iteration 16500: Loss = -11351.021170930248
Iteration 16600: Loss = -11351.02761860395
1
Iteration 16700: Loss = -11351.020319959143
Iteration 16800: Loss = -11351.020289129587
Iteration 16900: Loss = -11351.0206083763
1
Iteration 17000: Loss = -11351.02027900734
Iteration 17100: Loss = -11351.043425446598
1
Iteration 17200: Loss = -11351.020293286378
2
Iteration 17300: Loss = -11351.020403899865
3
Iteration 17400: Loss = -11351.06311710339
4
Iteration 17500: Loss = -11351.064225587665
5
Iteration 17600: Loss = -11351.1300914327
6
Iteration 17700: Loss = -11351.020285194865
7
Iteration 17800: Loss = -11351.020299910058
8
Iteration 17900: Loss = -11351.022932933714
9
Iteration 18000: Loss = -11351.042106745934
10
Stopping early at iteration 18000 due to no improvement.
tensor([[  4.0597,  -6.9272],
        [ -0.4134,  -0.9759],
        [ -9.8215,   8.4149],
        [ -6.1781,   3.4639],
        [  3.0665,  -6.3498],
        [  2.8716,  -4.3187],
        [  2.9857,  -4.5646],
        [  0.4821,  -2.6320],
        [  2.9551,  -6.7536],
        [  6.9571,  -8.5701],
        [  2.6994,  -4.4703],
        [  0.5459,  -2.6314],
        [ -8.8767,   7.2452],
        [ -5.2665,   3.5810],
        [ -6.2337,   4.8474],
        [ -6.6047,   5.1178],
        [  2.6885,  -5.3047],
        [ -7.3579,   5.7342],
        [  2.5094,  -4.5402],
        [ -8.1679,   6.7245],
        [  5.4686,  -6.9782],
        [ -4.4636,   2.9920],
        [ -5.4841,   3.9048],
        [  1.1807,  -2.5849],
        [ -7.9271,   5.7146],
        [  1.8095,  -3.5645],
        [  3.8083,  -5.5062],
        [  4.6616,  -6.1883],
        [ -4.4414,   3.0089],
        [  1.8206,  -4.1314],
        [  2.0200,  -4.4282],
        [ -7.4767,   5.2017],
        [  0.8590,  -4.3475],
        [ -7.2697,   5.8538],
        [ -6.9742,   5.3517],
        [ -4.2000,   2.5491],
        [ -5.7381,   2.6240],
        [ -8.9408,   6.9633],
        [ -4.6298,   2.8365],
        [ -6.8509,   5.0283],
        [  4.5586,  -6.0684],
        [  6.3430,  -7.7315],
        [ -7.9537,   6.5473],
        [  4.1635,  -6.1502],
        [-10.7721,   7.1442],
        [-10.0670,   8.1582],
        [ -8.5348,   7.1484],
        [ -6.7477,   4.6203],
        [  2.5609,  -7.0797],
        [  4.6025,  -7.8256],
        [ -7.3408,   5.9545],
        [-10.3244,   8.3865],
        [ -3.2611,   1.2404],
        [  3.9789,  -5.5850],
        [ -8.3301,   6.9407],
        [  1.0345,  -2.6478],
        [  4.5586,  -6.0746],
        [  4.3135,  -6.1205],
        [ -1.9545,  -0.0995],
        [ -6.1618,   4.6049],
        [ -7.0601,   5.6723],
        [ -5.4197,   4.0037],
        [ -8.3184,   5.1004],
        [-11.5400,   8.7975],
        [ -9.9063,   7.3054],
        [ -5.9911,   3.7196],
        [  2.4437,  -4.0485],
        [ -8.1127,   6.2946],
        [ -7.7500,   4.5842],
        [  2.2260,  -4.4524],
        [ -3.7944,   2.2800],
        [ -5.5160,   4.1190],
        [ -7.0911,   5.6971],
        [ -9.0298,   7.3077],
        [  0.4652,  -1.8532],
        [  5.4676,  -6.8569],
        [ -9.6214,   7.2149],
        [ -8.0597,   6.4410],
        [  4.3222,  -5.9168],
        [-10.1119,   8.3275],
        [  0.1587,  -2.8850],
        [  4.3624,  -6.6401],
        [  4.7964,  -6.3223],
        [  4.5889,  -6.7401],
        [ -4.3683,   2.6433],
        [ -4.7189,   1.7116],
        [ -7.8260,   6.4155],
        [ -7.0200,   4.5182],
        [  5.8897,  -7.3129],
        [  4.3695,  -5.9225],
        [  2.1410,  -3.6204],
        [  6.6064,  -8.0518],
        [ -5.8209,   4.3635],
        [ -9.0490,   7.3927],
        [ -4.1531,   2.7093],
        [ -6.2416,   4.4808],
        [  3.9702,  -6.6601],
        [ -7.7388,   6.3515],
        [ -5.3068,   3.6387],
        [ -8.3376,   6.9468]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7410, 0.2590],
        [0.2524, 0.7476]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4246, 0.5754], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2055, 0.0951],
         [0.6453, 0.3059]],

        [[0.8256, 0.1015],
         [0.7978, 0.1023]],

        [[0.7890, 0.0970],
         [0.3127, 0.3697]],

        [[0.5750, 0.1112],
         [0.6654, 0.0307]],

        [[0.6747, 0.0993],
         [0.8166, 0.1956]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 3
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 4
tensor([0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9524809640734552
Average Adjusted Rand Index: 0.9523206521769488
11368.183325692185
new:  [0.034569591662986024, 0.9524809640734552, 0.9524809640734552, 0.9524809640734552] [0.1882013683446557, 0.9523206521769488, 0.9523206521769488, 0.9523206521769488] [11618.054000707421, 11351.024308735514, 11351.020555754876, 11351.042106745934]
prior:  [0.0, 0.9446733348050895, 0.9446733348050895, -0.0020619729413603] [0.0, 0.9446438845001811, 0.9446438845001811, -0.006667314855909131] [nan, 11354.979167593749, 11354.979203132023, 11692.996912743734]
-----------------------------------------------------------------------------------------
This iteration is 11
True Objective function: Loss = -11297.408809010527
Iteration 0: Loss = -27350.953773654546
Iteration 10: Loss = -11626.21815751785
Iteration 20: Loss = -11626.1569427914
Iteration 30: Loss = -11618.985508599997
Iteration 40: Loss = -11619.17719504949
1
Iteration 50: Loss = -11619.162038665438
2
Iteration 60: Loss = -11618.803564074327
Iteration 70: Loss = -11615.45413696904
Iteration 80: Loss = -11611.619666204584
Iteration 90: Loss = -11609.130274823463
Iteration 100: Loss = -11520.989243734519
Iteration 110: Loss = -11284.339857006184
Iteration 120: Loss = -11284.336668413618
Iteration 130: Loss = -11284.336666598361
Iteration 140: Loss = -11284.336666598361
1
Iteration 150: Loss = -11284.336666598361
2
Iteration 160: Loss = -11284.336666598361
3
Stopping early at iteration 159 due to no improvement.
pi: tensor([[0.7190, 0.2810],
        [0.2798, 0.7202]], dtype=torch.float64)
alpha: tensor([0.5339, 0.4661])
beta: tensor([[[0.2966, 0.1032],
         [0.3996, 0.1956]],

        [[0.8804, 0.0981],
         [0.3880, 0.6432]],

        [[0.9373, 0.0984],
         [0.4950, 0.6333]],

        [[0.9713, 0.1000],
         [0.8882, 0.4988]],

        [[0.6249, 0.0973],
         [0.1154, 0.7774]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
time is 3
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
Global Adjusted Rand Index: 0.9524807616186577
Average Adjusted Rand Index: 0.9523203153929535
tensor([[-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26987.483740055635
Iteration 100: Loss = -11623.80351548654
Iteration 200: Loss = -11616.921827461945
Iteration 300: Loss = -11615.350012495299
Iteration 400: Loss = -11613.448597534465
Iteration 500: Loss = -11611.223734292798
Iteration 600: Loss = -11610.208850739746
Iteration 700: Loss = -11608.774516673817
Iteration 800: Loss = -11569.774081091275
Iteration 900: Loss = -11458.999849730837
Iteration 1000: Loss = -11415.59498469319
Iteration 1100: Loss = -11377.685617767878
Iteration 1200: Loss = -11372.859186991345
Iteration 1300: Loss = -11372.250405581788
Iteration 1400: Loss = -11372.021490304882
Iteration 1500: Loss = -11371.85343316567
Iteration 1600: Loss = -11371.795976487227
Iteration 1700: Loss = -11371.755956137451
Iteration 1800: Loss = -11371.723607357886
Iteration 1900: Loss = -11371.636304463864
Iteration 2000: Loss = -11371.592258202372
Iteration 2100: Loss = -11371.476701524505
Iteration 2200: Loss = -11371.3840581717
Iteration 2300: Loss = -11371.323852460046
Iteration 2400: Loss = -11369.785180723939
Iteration 2500: Loss = -11368.586138103952
Iteration 2600: Loss = -11368.547217247107
Iteration 2700: Loss = -11368.502044729576
Iteration 2800: Loss = -11368.49116666885
Iteration 2900: Loss = -11368.483221558889
Iteration 3000: Loss = -11368.476627242413
Iteration 3100: Loss = -11368.466694509621
Iteration 3200: Loss = -11368.44760043067
Iteration 3300: Loss = -11368.443497356999
Iteration 3400: Loss = -11368.439117352482
Iteration 3500: Loss = -11368.434626371984
Iteration 3600: Loss = -11368.431407780292
Iteration 3700: Loss = -11368.428815620766
Iteration 3800: Loss = -11368.42690781018
Iteration 3900: Loss = -11368.425285743611
Iteration 4000: Loss = -11368.4239138266
Iteration 4100: Loss = -11368.422655180486
Iteration 4200: Loss = -11368.421632004063
Iteration 4300: Loss = -11368.420393252924
Iteration 4400: Loss = -11368.419327431508
Iteration 4500: Loss = -11368.418256300367
Iteration 4600: Loss = -11368.416674747814
Iteration 4700: Loss = -11368.413399186353
Iteration 4800: Loss = -11368.39909573311
Iteration 4900: Loss = -11368.398548931194
Iteration 5000: Loss = -11368.396691970105
Iteration 5100: Loss = -11368.396423171096
Iteration 5200: Loss = -11368.396171255385
Iteration 5300: Loss = -11368.395046640973
Iteration 5400: Loss = -11368.39498393677
Iteration 5500: Loss = -11368.394239065427
Iteration 5600: Loss = -11368.397321150585
1
Iteration 5700: Loss = -11368.393472933372
Iteration 5800: Loss = -11368.398689391732
1
Iteration 5900: Loss = -11368.393094217136
Iteration 6000: Loss = -11368.398864982928
1
Iteration 6100: Loss = -11368.39258825144
Iteration 6200: Loss = -11368.41008953004
1
Iteration 6300: Loss = -11368.391740116622
Iteration 6400: Loss = -11368.394028757226
1
Iteration 6500: Loss = -11368.39121802187
Iteration 6600: Loss = -11368.392374662042
1
Iteration 6700: Loss = -11368.390894937682
Iteration 6800: Loss = -11368.390981260647
1
Iteration 6900: Loss = -11368.390766573733
Iteration 7000: Loss = -11368.390505590089
Iteration 7100: Loss = -11368.390962529367
1
Iteration 7200: Loss = -11368.390298508519
Iteration 7300: Loss = -11368.39427777903
1
Iteration 7400: Loss = -11368.396513965228
2
Iteration 7500: Loss = -11368.39036005063
3
Iteration 7600: Loss = -11368.390721573336
4
Iteration 7700: Loss = -11368.388198666089
Iteration 7800: Loss = -11368.405213940561
1
Iteration 7900: Loss = -11368.388054519142
Iteration 8000: Loss = -11368.398701164673
1
Iteration 8100: Loss = -11368.387928040791
Iteration 8200: Loss = -11368.387823970408
Iteration 8300: Loss = -11368.38778455551
Iteration 8400: Loss = -11368.391365205523
1
Iteration 8500: Loss = -11368.392009349838
2
Iteration 8600: Loss = -11368.446907118143
3
Iteration 8700: Loss = -11368.388999971972
4
Iteration 8800: Loss = -11368.387657294625
Iteration 8900: Loss = -11368.4037783824
1
Iteration 9000: Loss = -11368.387355543993
Iteration 9100: Loss = -11368.477414118173
1
Iteration 9200: Loss = -11368.387260401627
Iteration 9300: Loss = -11368.390555960943
1
Iteration 9400: Loss = -11368.389090538254
2
Iteration 9500: Loss = -11368.391439842382
3
Iteration 9600: Loss = -11368.395424555167
4
Iteration 9700: Loss = -11368.39694727027
5
Iteration 9800: Loss = -11368.387348469698
6
Iteration 9900: Loss = -11368.38885456771
7
Iteration 10000: Loss = -11368.444044195645
8
Iteration 10100: Loss = -11368.409690569793
9
Iteration 10200: Loss = -11368.390250165978
10
Stopping early at iteration 10200 due to no improvement.
tensor([[ -8.3094,   3.6942],
        [  4.3122,  -8.9274],
        [  4.5569,  -9.1721],
        [ -7.5475,   2.9323],
        [ -2.0061,  -2.6091],
        [  3.5519,  -8.1671],
        [  4.0470,  -8.6622],
        [  2.8304,  -7.4456],
        [  4.1464,  -8.7616],
        [  4.2061,  -8.8213],
        [ -6.4880,   1.8728],
        [ -4.8298,   0.2146],
        [  3.7370,  -8.3522],
        [  4.7756,  -9.3908],
        [ -5.3306,   0.7154],
        [ -3.4707,  -1.1445],
        [  4.3959,  -9.0112],
        [ -7.2022,   2.5870],
        [  2.5178,  -7.1330],
        [ -5.6418,   1.0266],
        [ -5.5417,   0.9265],
        [ -3.0023,  -1.6130],
        [  6.0077, -10.6230],
        [ -4.7762,   0.1610],
        [  4.7524,  -9.3676],
        [  4.1632,  -8.7784],
        [ -4.1314,  -0.4838],
        [  4.3493,  -8.9645],
        [ -5.5992,   0.9840],
        [  4.3472,  -8.9624],
        [  4.4700,  -9.0852],
        [ -3.0751,  -1.5402],
        [  4.5963,  -9.2115],
        [  3.5338,  -8.1490],
        [  5.1949,  -9.8102],
        [  3.3833,  -7.9985],
        [ -6.3266,   1.7114],
        [  5.5425, -10.1577],
        [ -7.0477,   2.4325],
        [  0.0701,  -4.6853],
        [  4.3522,  -8.9674],
        [ -7.0040,   2.3887],
        [  1.7020,  -6.3172],
        [  4.6121,  -9.2273],
        [  2.2450,  -6.8602],
        [  3.6647,  -8.2800],
        [ -6.8398,   2.2246],
        [  5.7548, -10.3700],
        [  1.0804,  -5.6956],
        [  2.0434,  -6.6586],
        [ -6.0797,   1.4645],
        [  4.6133,  -9.2285],
        [ -1.6817,  -2.9335],
        [  4.2604,  -8.8756],
        [ -0.3075,  -4.3077],
        [  3.9597,  -8.5749],
        [  0.8903,  -5.5055],
        [ -7.5478,   2.9326],
        [ -6.6566,   2.0414],
        [ -4.7313,   0.1161],
        [  5.3103,  -9.9255],
        [  3.4130,  -8.0282],
        [ -3.7202,  -0.8950],
        [  3.8656,  -8.4808],
        [ -3.9070,  -0.7082],
        [  4.7183,  -9.3335],
        [ -2.8296,  -1.7857],
        [ -3.4480,  -1.1672],
        [ -3.7021,  -0.9131],
        [ -8.4879,   3.8727],
        [ -4.4056,  -0.2096],
        [  5.0225,  -9.6378],
        [  4.1240,  -8.7392],
        [  2.2532,  -6.8684],
        [  3.9127,  -8.5279],
        [ -6.1975,   1.5822],
        [  1.5917,  -6.2069],
        [ -5.8478,   1.2326],
        [  2.1147,  -6.7300],
        [  4.0179,  -8.6331],
        [  4.1609,  -8.7761],
        [ -4.5518,  -0.0634],
        [ -3.5029,  -1.1123],
        [  3.2127,  -7.8279],
        [ -6.2497,   1.6344],
        [  3.4975,  -8.1128],
        [  4.5035,  -9.1188],
        [  0.4100,  -5.0252],
        [  2.3361,  -6.9513],
        [  4.5405,  -9.1557],
        [ -2.9507,  -1.6645],
        [  4.3228,  -8.9380],
        [  3.8453,  -8.4605],
        [  0.8184,  -5.4336],
        [  5.4280, -10.0432],
        [  4.5724,  -9.1876],
        [ -5.7142,   1.0990],
        [  2.3472,  -6.9624],
        [  3.3214,  -7.9366],
        [  3.7883,  -8.4035]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6413, 0.3587],
        [0.3257, 0.6743]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6473, 0.3527], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2486, 0.1006],
         [0.3996, 0.2668]],

        [[0.8804, 0.0970],
         [0.3880, 0.6432]],

        [[0.9373, 0.0963],
         [0.4950, 0.6333]],

        [[0.9713, 0.0995],
         [0.8882, 0.4988]],

        [[0.6249, 0.0968],
         [0.1154, 0.7774]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8821083056742698
time is 1
tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824124176797128
time is 2
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 94
Adjusted Rand Index: 0.7721141809334062
time is 3
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824145678101706
time is 4
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208031161549398
Global Adjusted Rand Index: 0.030459152536089863
Average Adjusted Rand Index: 0.8679705176504999
Iteration 0: Loss = -23105.855877628004
Iteration 10: Loss = -11626.218527955012
Iteration 20: Loss = -11622.81923995921
Iteration 30: Loss = -11619.113416247372
Iteration 40: Loss = -11618.99753050177
Iteration 50: Loss = -11616.57996847132
Iteration 60: Loss = -11612.273617883106
Iteration 70: Loss = -11609.839324912466
Iteration 80: Loss = -11580.498262898835
Iteration 90: Loss = -11284.413502927046
Iteration 100: Loss = -11284.336658267888
Iteration 110: Loss = -11284.336661101
1
Iteration 120: Loss = -11284.33665881622
2
Iteration 130: Loss = -11284.33665881622
3
Stopping early at iteration 129 due to no improvement.
pi: tensor([[0.7202, 0.2798],
        [0.2810, 0.7190]], dtype=torch.float64)
alpha: tensor([0.4661, 0.5339])
beta: tensor([[[0.1956, 0.1032],
         [0.0948, 0.2966]],

        [[0.2651, 0.0981],
         [0.5398, 0.9684]],

        [[0.4029, 0.0984],
         [0.6993, 0.9380]],

        [[0.3766, 0.1000],
         [0.4182, 0.9798]],

        [[0.1005, 0.0973],
         [0.8142, 0.7819]]], dtype=torch.float64)
time is 0
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 2
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208023791611811
time is 3
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208031161549398
Global Adjusted Rand Index: 0.9524807616186577
Average Adjusted Rand Index: 0.9523203153929535
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23105.602282486787
Iteration 100: Loss = -11635.464047626228
Iteration 200: Loss = -11620.59558801861
Iteration 300: Loss = -11615.015856379485
Iteration 400: Loss = -11609.517200478538
Iteration 500: Loss = -11587.310901632318
Iteration 600: Loss = -11530.720707431927
Iteration 700: Loss = -11394.683238476227
Iteration 800: Loss = -11338.046929079639
Iteration 900: Loss = -11316.450982486876
Iteration 1000: Loss = -11315.033186063538
Iteration 1100: Loss = -11311.737276491349
Iteration 1200: Loss = -11302.755534056465
Iteration 1300: Loss = -11300.963608707129
Iteration 1400: Loss = -11300.8175053721
Iteration 1500: Loss = -11289.472845824797
Iteration 1600: Loss = -11289.396999918736
Iteration 1700: Loss = -11289.348411913508
Iteration 1800: Loss = -11284.403954081723
Iteration 1900: Loss = -11281.492721875815
Iteration 2000: Loss = -11281.451615688811
Iteration 2100: Loss = -11281.429151576876
Iteration 2200: Loss = -11281.411228700117
Iteration 2300: Loss = -11281.396285511495
Iteration 2400: Loss = -11281.383478081609
Iteration 2500: Loss = -11281.371621169545
Iteration 2600: Loss = -11281.33693154468
Iteration 2700: Loss = -11281.321289289728
Iteration 2800: Loss = -11281.312733852443
Iteration 2900: Loss = -11281.293198907804
Iteration 3000: Loss = -11281.28754598633
Iteration 3100: Loss = -11281.28258646956
Iteration 3200: Loss = -11281.278107261216
Iteration 3300: Loss = -11281.273961155672
Iteration 3400: Loss = -11281.270175669511
Iteration 3500: Loss = -11281.266568637171
Iteration 3600: Loss = -11281.263052190088
Iteration 3700: Loss = -11281.259820214651
Iteration 3800: Loss = -11281.255334147272
Iteration 3900: Loss = -11281.249812367736
Iteration 4000: Loss = -11281.240061519477
Iteration 4100: Loss = -11281.230772242805
Iteration 4200: Loss = -11281.22777252676
Iteration 4300: Loss = -11281.228862988528
1
Iteration 4400: Loss = -11281.220165294211
Iteration 4500: Loss = -11281.217749511954
Iteration 4600: Loss = -11281.215277700603
Iteration 4700: Loss = -11281.212936289628
Iteration 4800: Loss = -11281.211185040751
Iteration 4900: Loss = -11281.209720825793
Iteration 5000: Loss = -11281.208070656203
Iteration 5100: Loss = -11281.206987694022
Iteration 5200: Loss = -11281.204132227287
Iteration 5300: Loss = -11281.20470681098
1
Iteration 5400: Loss = -11281.199824555277
Iteration 5500: Loss = -11281.19713694608
Iteration 5600: Loss = -11281.165008577767
Iteration 5700: Loss = -11281.167903516123
1
Iteration 5800: Loss = -11281.163745750971
Iteration 5900: Loss = -11281.167134421523
1
Iteration 6000: Loss = -11281.162771221112
Iteration 6100: Loss = -11281.184339074087
1
Iteration 6200: Loss = -11281.166399701053
2
Iteration 6300: Loss = -11281.161746060165
Iteration 6400: Loss = -11281.16145577439
Iteration 6500: Loss = -11281.17905694854
1
Iteration 6600: Loss = -11281.163246948903
2
Iteration 6700: Loss = -11281.160629565016
Iteration 6800: Loss = -11281.159497165843
Iteration 6900: Loss = -11281.118450066082
Iteration 7000: Loss = -11281.118669857391
1
Iteration 7100: Loss = -11281.11725936298
Iteration 7200: Loss = -11281.116285616628
Iteration 7300: Loss = -11281.123541515632
1
Iteration 7400: Loss = -11281.115830037283
Iteration 7500: Loss = -11281.116105721369
1
Iteration 7600: Loss = -11281.115245671032
Iteration 7700: Loss = -11281.115354808384
1
Iteration 7800: Loss = -11281.126417911748
2
Iteration 7900: Loss = -11281.114757608288
Iteration 8000: Loss = -11281.11286775733
Iteration 8100: Loss = -11281.120121594437
1
Iteration 8200: Loss = -11281.107872916245
Iteration 8300: Loss = -11281.110574742583
1
Iteration 8400: Loss = -11281.107821286863
Iteration 8500: Loss = -11281.10749979536
Iteration 8600: Loss = -11281.116413816959
1
Iteration 8700: Loss = -11281.107152664557
Iteration 8800: Loss = -11281.143190354704
1
Iteration 8900: Loss = -11281.107494418819
2
Iteration 9000: Loss = -11281.070553021436
Iteration 9100: Loss = -11281.080979371296
1
Iteration 9200: Loss = -11281.137030713193
2
Iteration 9300: Loss = -11281.068076365435
Iteration 9400: Loss = -11281.056969241046
Iteration 9500: Loss = -11281.055590688995
Iteration 9600: Loss = -11281.055842228505
1
Iteration 9700: Loss = -11281.05545666583
Iteration 9800: Loss = -11281.055535046762
1
Iteration 9900: Loss = -11281.055371101189
Iteration 10000: Loss = -11281.055467774775
1
Iteration 10100: Loss = -11281.055260293077
Iteration 10200: Loss = -11281.058388735466
1
Iteration 10300: Loss = -11281.056832765973
2
Iteration 10400: Loss = -11281.056576874851
3
Iteration 10500: Loss = -11281.056886432638
4
Iteration 10600: Loss = -11281.054796477743
Iteration 10700: Loss = -11281.056726096727
1
Iteration 10800: Loss = -11281.059726723386
2
Iteration 10900: Loss = -11281.109709012637
3
Iteration 11000: Loss = -11281.054428977557
Iteration 11100: Loss = -11281.054506778439
1
Iteration 11200: Loss = -11281.080549921664
2
Iteration 11300: Loss = -11281.057409443465
3
Iteration 11400: Loss = -11281.277942930325
4
Iteration 11500: Loss = -11281.05319519082
Iteration 11600: Loss = -11281.055520224083
1
Iteration 11700: Loss = -11281.120548736711
2
Iteration 11800: Loss = -11281.055314314617
3
Iteration 11900: Loss = -11281.053791865726
4
Iteration 12000: Loss = -11281.057740034938
5
Iteration 12100: Loss = -11281.063276800598
6
Iteration 12200: Loss = -11281.056173097551
7
Iteration 12300: Loss = -11281.12010803153
8
Iteration 12400: Loss = -11281.055767158097
9
Iteration 12500: Loss = -11281.033822656307
Iteration 12600: Loss = -11281.032480019792
Iteration 12700: Loss = -11281.033542866266
1
Iteration 12800: Loss = -11281.037948670488
2
Iteration 12900: Loss = -11281.031869427161
Iteration 13000: Loss = -11281.033077290815
1
Iteration 13100: Loss = -11281.033076411448
2
Iteration 13200: Loss = -11281.144086557715
3
Iteration 13300: Loss = -11281.031475827494
Iteration 13400: Loss = -11281.032671074325
1
Iteration 13500: Loss = -11281.031092843312
Iteration 13600: Loss = -11281.031540189784
1
Iteration 13700: Loss = -11281.036511814853
2
Iteration 13800: Loss = -11281.122762611754
3
Iteration 13900: Loss = -11281.042653422403
4
Iteration 14000: Loss = -11281.03283430155
5
Iteration 14100: Loss = -11281.031094576285
6
Iteration 14200: Loss = -11281.032306350126
7
Iteration 14300: Loss = -11281.031859666487
8
Iteration 14400: Loss = -11281.032945875191
9
Iteration 14500: Loss = -11281.036665869253
10
Stopping early at iteration 14500 due to no improvement.
tensor([[  6.0237,  -7.5220],
        [ -8.3583,   6.8275],
        [ -8.6860,   7.2480],
        [  5.4081,  -8.0901],
        [  1.0954,  -2.9584],
        [ -9.0596,   7.5663],
        [ -8.8656,   7.0074],
        [ -4.5348,   3.1359],
        [ -8.7789,   7.3921],
        [ -8.2576,   5.6946],
        [  4.7734,  -6.3227],
        [  3.7176,  -6.1841],
        [ -6.7092,   3.9623],
        [ -7.0371,   5.0554],
        [  3.7436,  -5.2351],
        [  0.6276,  -4.9309],
        [-10.7660,   8.6749],
        [  5.3264,  -8.2568],
        [ -5.7114,   3.2081],
        [  3.3474,  -4.8000],
        [  3.2440,  -5.6569],
        [  1.9757,  -3.3620],
        [ -4.2809,   2.8387],
        [  2.5710,  -3.9621],
        [ -8.6317,   7.2407],
        [-10.5391,   8.3386],
        [  2.4321,  -3.8689],
        [ -9.0558,   5.9135],
        [  1.9838,  -6.0799],
        [ -6.3753,   4.3731],
        [-11.4758,   8.0351],
        [  1.5917,  -3.3633],
        [ -9.1086,   7.4098],
        [ -9.0962,   7.4444],
        [ -8.4192,   6.9305],
        [ -5.7155,   1.7664],
        [  4.8259,  -6.2657],
        [ -6.1439,   4.6959],
        [  6.1411,  -9.1000],
        [ -2.1392,   0.4694],
        [ -8.9893,   7.0304],
        [  4.6987,  -6.6386],
        [ -3.0927,   0.4755],
        [ -6.3009,   4.4001],
        [ -5.0073,   3.1900],
        [ -8.7939,   7.3773],
        [  6.2670,  -7.6575],
        [ -8.8583,   6.6333],
        [ -3.4822,   1.9867],
        [-10.2055,   6.9808],
        [  6.8388,  -8.5156],
        [-10.2020,   8.7171],
        [  1.2595,  -2.9491],
        [ -8.3144,   6.0889],
        [  6.5358,  -8.6159],
        [ -8.7896,   7.2849],
        [ -3.2527,   1.7087],
        [  6.8865,  -8.5386],
        [  5.0826,  -6.6226],
        [  4.4517,  -5.9744],
        [ -6.0039,   3.8529],
        [ -5.1976,   3.6532],
        [  0.5255,  -5.1408],
        [-10.4269,   7.0176],
        [  3.4789,  -5.4898],
        [ -9.3222,   7.9328],
        [  1.4099,  -2.7966],
        [  6.7559,  -9.6660],
        [  2.0934,  -3.5778],
        [  6.5326,  -8.9504],
        [  1.5679,  -3.1377],
        [ -5.0206,   3.5956],
        [ -7.6494,   6.0580],
        [ -4.7249,   2.1953],
        [ -6.3145,   4.5866],
        [  3.5106,  -5.8017],
        [ -5.7863,   4.2862],
        [  4.1490,  -5.6215],
        [ -4.2809,   2.8267],
        [ -9.1882,   7.6395],
        [ -8.6004,   6.9489],
        [  2.7681,  -4.2573],
        [  1.7014,  -3.2823],
        [ -5.9028,   4.2283],
        [  4.9393,  -6.3507],
        [ -6.3789,   4.1522],
        [ -6.5193,   3.6864],
        [ -2.5942,   1.0517],
        [ -6.5507,   2.9021],
        [-10.1402,   8.2720],
        [  2.2883,  -3.8992],
        [ -9.2729,   5.3628],
        [ -6.8139,   4.9880],
        [ -3.0506,   1.4648],
        [ -4.9584,   3.2622],
        [ -6.9416,   5.0052],
        [  5.3502,  -6.8202],
        [ -3.8481,   2.4474],
        [ -5.1785,   3.7747],
        [ -5.6103,   4.2215]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7294, 0.2706],
        [0.2748, 0.7252]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3915, 0.6085], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1998, 0.1031],
         [0.0948, 0.3029]],

        [[0.2651, 0.0979],
         [0.5398, 0.9684]],

        [[0.4029, 0.0986],
         [0.6993, 0.9380]],

        [[0.3766, 0.1000],
         [0.4182, 0.9798]],

        [[0.1005, 0.0974],
         [0.8142, 0.7819]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208023791611811
time is 3
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208031161549398
Global Adjusted Rand Index: 0.9603204383279997
Average Adjusted Rand Index: 0.9603203153929535
Iteration 0: Loss = -20198.18401470149
Iteration 10: Loss = -11590.000832004083
Iteration 20: Loss = -11284.824386665357
Iteration 30: Loss = -11284.33666140081
Iteration 40: Loss = -11284.33665578057
Iteration 50: Loss = -11284.336655780573
1
Iteration 60: Loss = -11284.336655780573
2
Iteration 70: Loss = -11284.336655780573
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[0.7202, 0.2798],
        [0.2810, 0.7190]], dtype=torch.float64)
alpha: tensor([0.4661, 0.5339])
beta: tensor([[[0.1956, 0.1032],
         [0.0335, 0.2966]],

        [[0.0229, 0.0981],
         [0.4023, 0.1518]],

        [[0.5159, 0.0984],
         [0.6742, 0.9888]],

        [[0.6638, 0.1000],
         [0.2100, 0.3005]],

        [[0.2476, 0.0973],
         [0.0592, 0.0103]]], dtype=torch.float64)
time is 0
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 2
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208023791611811
time is 3
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208031161549398
Global Adjusted Rand Index: 0.9524807616186577
Average Adjusted Rand Index: 0.9523203153929535
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20198.063624743307
Iteration 100: Loss = -11633.552498787656
Iteration 200: Loss = -11627.66079954389
Iteration 300: Loss = -11625.673405451525
Iteration 400: Loss = -11624.681416582554
Iteration 500: Loss = -11623.943432023723
Iteration 600: Loss = -11622.31285828944
Iteration 700: Loss = -11617.025613822963
Iteration 800: Loss = -11513.27402240965
Iteration 900: Loss = -11468.588547412217
Iteration 1000: Loss = -11458.268161993728
Iteration 1100: Loss = -11447.545549532697
Iteration 1200: Loss = -11447.33784036303
Iteration 1300: Loss = -11447.285655460162
Iteration 1400: Loss = -11447.247986585062
Iteration 1500: Loss = -11447.2182224045
Iteration 1600: Loss = -11447.193159473642
Iteration 1700: Loss = -11447.171144417893
Iteration 1800: Loss = -11447.155022485396
Iteration 1900: Loss = -11447.141827477788
Iteration 2000: Loss = -11447.130619590173
Iteration 2100: Loss = -11447.118598243631
Iteration 2200: Loss = -11446.82167491618
Iteration 2300: Loss = -11446.811422271829
Iteration 2400: Loss = -11446.803972907815
Iteration 2500: Loss = -11446.795134212443
Iteration 2600: Loss = -11446.781006293077
Iteration 2700: Loss = -11446.720127020433
Iteration 2800: Loss = -11446.696729460884
Iteration 2900: Loss = -11446.68221836392
Iteration 3000: Loss = -11446.664268952576
Iteration 3100: Loss = -11446.637457324226
Iteration 3200: Loss = -11446.568816467303
Iteration 3300: Loss = -11431.086066361118
Iteration 3400: Loss = -11417.58612377975
Iteration 3500: Loss = -11391.78691794907
Iteration 3600: Loss = -11330.910985801165
Iteration 3700: Loss = -11288.098522892156
Iteration 3800: Loss = -11283.554179116867
Iteration 3900: Loss = -11283.47255871556
Iteration 4000: Loss = -11283.431517478468
Iteration 4100: Loss = -11283.41162467145
Iteration 4200: Loss = -11283.399744791079
Iteration 4300: Loss = -11283.391084654297
Iteration 4400: Loss = -11283.382910929615
Iteration 4500: Loss = -11283.379231888064
Iteration 4600: Loss = -11283.368798344083
Iteration 4700: Loss = -11283.364618481437
Iteration 4800: Loss = -11283.359353657119
Iteration 4900: Loss = -11283.35318213959
Iteration 5000: Loss = -11283.347386037076
Iteration 5100: Loss = -11283.345856960937
Iteration 5200: Loss = -11283.345618698002
Iteration 5300: Loss = -11283.343865140178
Iteration 5400: Loss = -11283.341660993323
Iteration 5500: Loss = -11283.120225723826
Iteration 5600: Loss = -11283.119461928549
Iteration 5700: Loss = -11283.119219120461
Iteration 5800: Loss = -11283.11809452249
Iteration 5900: Loss = -11283.117237717433
Iteration 6000: Loss = -11283.116042572183
Iteration 6100: Loss = -11283.114064677313
Iteration 6200: Loss = -11283.131090946372
1
Iteration 6300: Loss = -11283.108436500075
Iteration 6400: Loss = -11283.109290998913
1
Iteration 6500: Loss = -11283.107834535409
Iteration 6600: Loss = -11283.108386541362
1
Iteration 6700: Loss = -11283.108838994482
2
Iteration 6800: Loss = -11283.111148012256
3
Iteration 6900: Loss = -11283.109109157465
4
Iteration 7000: Loss = -11283.142034294813
5
Iteration 7100: Loss = -11283.10642336328
Iteration 7200: Loss = -11283.107488796573
1
Iteration 7300: Loss = -11283.106146426566
Iteration 7400: Loss = -11283.10606488658
Iteration 7500: Loss = -11283.108427441975
1
Iteration 7600: Loss = -11283.07804828198
Iteration 7700: Loss = -11280.683783986578
Iteration 7800: Loss = -11280.682483117469
Iteration 7900: Loss = -11280.711173179265
1
Iteration 8000: Loss = -11280.682402133163
Iteration 8100: Loss = -11280.706585464512
1
Iteration 8200: Loss = -11280.682007379886
Iteration 8300: Loss = -11280.743746043949
1
Iteration 8400: Loss = -11280.682526273875
2
Iteration 8500: Loss = -11280.682712588723
3
Iteration 8600: Loss = -11280.681346666064
Iteration 8700: Loss = -11280.679195241668
Iteration 8800: Loss = -11280.81784752569
1
Iteration 8900: Loss = -11280.680390799167
2
Iteration 9000: Loss = -11280.678544491948
Iteration 9100: Loss = -11280.684287150852
1
Iteration 9200: Loss = -11280.678302988114
Iteration 9300: Loss = -11280.680262347585
1
Iteration 9400: Loss = -11280.690857664615
2
Iteration 9500: Loss = -11280.677982779685
Iteration 9600: Loss = -11280.685841164523
1
Iteration 9700: Loss = -11280.677177058078
Iteration 9800: Loss = -11280.669702808877
Iteration 9900: Loss = -11280.622525164612
Iteration 10000: Loss = -11280.628749257497
1
Iteration 10100: Loss = -11280.622500511954
Iteration 10200: Loss = -11280.622541519278
1
Iteration 10300: Loss = -11280.622237188416
Iteration 10400: Loss = -11280.675502474372
1
Iteration 10500: Loss = -11280.621802171107
Iteration 10600: Loss = -11280.62245188448
1
Iteration 10700: Loss = -11280.669577960454
2
Iteration 10800: Loss = -11280.62176604387
Iteration 10900: Loss = -11280.621905524944
1
Iteration 11000: Loss = -11280.62310448847
2
Iteration 11100: Loss = -11280.62172766839
Iteration 11200: Loss = -11280.768931944582
1
Iteration 11300: Loss = -11280.621964980703
2
Iteration 11400: Loss = -11280.619076013085
Iteration 11500: Loss = -11280.617612596096
Iteration 11600: Loss = -11280.619281482832
1
Iteration 11700: Loss = -11280.621537064735
2
Iteration 11800: Loss = -11280.631846806344
3
Iteration 11900: Loss = -11280.61769586118
4
Iteration 12000: Loss = -11280.617706986723
5
Iteration 12100: Loss = -11280.618987553995
6
Iteration 12200: Loss = -11280.616698911159
Iteration 12300: Loss = -11280.616613051696
Iteration 12400: Loss = -11280.616586953767
Iteration 12500: Loss = -11280.61699344408
1
Iteration 12600: Loss = -11280.616530176152
Iteration 12700: Loss = -11280.616544139473
1
Iteration 12800: Loss = -11280.616526708623
Iteration 12900: Loss = -11280.616565219749
1
Iteration 13000: Loss = -11280.620522688361
2
Iteration 13100: Loss = -11280.630385720371
3
Iteration 13200: Loss = -11280.62663813049
4
Iteration 13300: Loss = -11280.719214388695
5
Iteration 13400: Loss = -11280.619073118323
6
Iteration 13500: Loss = -11280.616497514726
Iteration 13600: Loss = -11280.617556061638
1
Iteration 13700: Loss = -11280.616545674933
2
Iteration 13800: Loss = -11280.6165488685
3
Iteration 13900: Loss = -11280.616704100608
4
Iteration 14000: Loss = -11280.616496607534
Iteration 14100: Loss = -11280.616462986702
Iteration 14200: Loss = -11280.621894818887
1
Iteration 14300: Loss = -11280.61646717073
2
Iteration 14400: Loss = -11280.638623479359
3
Iteration 14500: Loss = -11280.616559299657
4
Iteration 14600: Loss = -11280.616599861982
5
Iteration 14700: Loss = -11280.61705292122
6
Iteration 14800: Loss = -11280.61760049513
7
Iteration 14900: Loss = -11280.617245812493
8
Iteration 15000: Loss = -11280.616790759987
9
Iteration 15100: Loss = -11280.628508225393
10
Stopping early at iteration 15100 due to no improvement.
tensor([[-7.5972,  5.4774],
        [ 6.6485, -8.9045],
        [ 6.6960, -8.1419],
        [-7.3282,  5.9287],
        [-2.7961,  1.3158],
        [ 3.9652, -5.8258],
        [ 4.6173, -9.1092],
        [ 2.9215, -4.7004],
        [ 6.2913, -8.2018],
        [ 6.0018, -8.3356],
        [-6.6028,  4.5102],
        [-7.2807,  2.6655],
        [ 4.5889, -6.0535],
        [ 3.6005, -4.9965],
        [-5.2735,  3.7637],
        [-3.6992,  1.9190],
        [ 6.1401, -7.6613],
        [-7.3963,  5.6723],
        [ 3.7163, -5.1608],
        [-4.8307,  3.3998],
        [-5.2750,  3.6499],
        [-3.8572,  1.5305],
        [ 1.9491, -5.1320],
        [-4.0378,  2.5663],
        [ 7.1249, -9.5053],
        [ 4.5887, -9.2039],
        [-4.3409,  2.0215],
        [ 6.5765, -7.9875],
        [-4.8234,  3.3230],
        [ 4.6356, -6.5417],
        [ 6.4508, -7.8872],
        [-3.3198,  1.6757],
        [ 7.1776, -8.5642],
        [ 5.4976, -7.6059],
        [ 1.9895, -5.7943],
        [ 2.2299, -5.2243],
        [-6.4373,  4.7065],
        [ 4.5184, -5.9813],
        [-7.1608,  5.4687],
        [ 0.5720, -1.9614],
        [ 6.5396, -8.1585],
        [-6.5633,  4.8274],
        [ 1.3188, -2.7052],
        [ 4.6484, -6.0374],
        [ 2.2330, -5.9138],
        [ 4.9938, -6.6297],
        [-7.4320,  5.9234],
        [ 6.4721, -7.9536],
        [ 0.7266, -4.7172],
        [ 3.4945, -5.0558],
        [-5.6149,  4.2133],
        [ 7.0961, -8.5159],
        [-2.8370,  0.9165],
        [ 6.2637, -7.7638],
        [-1.2923, -0.0944],
        [ 5.7169, -7.1228],
        [ 1.1261, -3.7825],
        [-6.4792,  4.9914],
        [-6.7553,  4.9715],
        [-6.1376,  4.3186],
        [ 3.7212, -6.0885],
        [ 3.5438, -5.7471],
        [-3.7038,  2.0320],
        [ 4.9631, -6.8620],
        [-5.1792,  3.3463],
        [ 6.6736, -8.1182],
        [-2.8800,  1.3773],
        [-3.0245,  1.6378],
        [-3.6487,  2.0689],
        [-8.0845,  6.6102],
        [-3.1787,  1.6031],
        [ 3.5875, -5.0003],
        [ 5.7111, -7.2181],
        [ 2.6740, -4.1994],
        [ 3.7161, -7.1507],
        [-5.3957,  3.9923],
        [ 2.6013, -4.1970],
        [-7.2250,  2.6097],
        [ 2.8225, -4.2450],
        [ 6.1810, -7.6553],
        [ 6.1997, -8.5075],
        [-4.3755,  2.7082],
        [-3.3473,  1.6692],
        [ 4.0516, -6.0378],
        [-7.9821,  3.3668],
        [ 4.2942, -6.1875],
        [ 2.9489, -7.2150],
        [ 1.1036, -2.4899],
        [ 2.9344, -6.4625],
        [ 3.7282, -5.2993],
        [-3.8465,  2.3775],
        [ 4.6443, -6.6356],
        [ 5.0607, -6.6144],
        [ 0.9845, -3.5048],
        [ 3.3676, -4.8202],
        [ 5.0930, -6.8456],
        [-6.9850,  5.2298],
        [ 2.3812, -3.8634],
        [ 2.8591, -6.0916],
        [ 4.4100, -5.8869]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7259, 0.2741],
        [0.2713, 0.7287]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6129, 0.3871], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3018, 0.1023],
         [0.0335, 0.2005]],

        [[0.0229, 0.0977],
         [0.4023, 0.1518]],

        [[0.5159, 0.0984],
         [0.6742, 0.9888]],

        [[0.6638, 0.0996],
         [0.2100, 0.3005]],

        [[0.2476, 0.0971],
         [0.0592, 0.0103]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
time is 3
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
Global Adjusted Rand Index: 0.9603204383279997
Average Adjusted Rand Index: 0.9603203153929535
Iteration 0: Loss = -17467.29579320434
Iteration 10: Loss = -11284.654535060821
Iteration 20: Loss = -11284.336677822852
Iteration 30: Loss = -11284.336666598472
Iteration 40: Loss = -11284.336666598361
Iteration 50: Loss = -11284.336666598361
1
Iteration 60: Loss = -11284.336666598361
2
Iteration 70: Loss = -11284.336666598361
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[0.7190, 0.2810],
        [0.2798, 0.7202]], dtype=torch.float64)
alpha: tensor([0.5339, 0.4661])
beta: tensor([[[0.2966, 0.1032],
         [0.2528, 0.1956]],

        [[0.5700, 0.0981],
         [0.5271, 0.7420]],

        [[0.6506, 0.0984],
         [0.2310, 0.1755]],

        [[0.7138, 0.1000],
         [0.5189, 0.9371]],

        [[0.9967, 0.0973],
         [0.5878, 0.3185]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
time is 3
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
Global Adjusted Rand Index: 0.9524807616186577
Average Adjusted Rand Index: 0.9523203153929535
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17467.001862289955
Iteration 100: Loss = -11643.502506844852
Iteration 200: Loss = -11621.74043306369
Iteration 300: Loss = -11614.162422590378
Iteration 400: Loss = -11611.164179543717
Iteration 500: Loss = -11596.49046725992
Iteration 600: Loss = -11552.98018448758
Iteration 700: Loss = -11550.671919291513
Iteration 800: Loss = -11520.392034093054
Iteration 900: Loss = -11452.34397628495
Iteration 1000: Loss = -11394.226334081617
Iteration 1100: Loss = -11378.9175128906
Iteration 1200: Loss = -11373.516789976122
Iteration 1300: Loss = -11373.24704896819
Iteration 1400: Loss = -11373.130477258164
Iteration 1500: Loss = -11373.036209839966
Iteration 1600: Loss = -11372.773456101333
Iteration 1700: Loss = -11372.679611227302
Iteration 1800: Loss = -11372.621798923963
Iteration 1900: Loss = -11372.375412950068
Iteration 2000: Loss = -11372.366184750164
Iteration 2100: Loss = -11372.359053722275
Iteration 2200: Loss = -11372.351972478595
Iteration 2300: Loss = -11372.347608641936
Iteration 2400: Loss = -11372.341117003616
Iteration 2500: Loss = -11372.337505608897
Iteration 2600: Loss = -11372.35231826214
1
Iteration 2700: Loss = -11372.32022695121
Iteration 2800: Loss = -11372.314394039418
Iteration 2900: Loss = -11372.453704217967
1
Iteration 3000: Loss = -11372.309737055131
Iteration 3100: Loss = -11372.307452877078
Iteration 3200: Loss = -11372.309559288271
1
Iteration 3300: Loss = -11372.305942144372
Iteration 3400: Loss = -11372.308832875491
1
Iteration 3500: Loss = -11372.303938846502
Iteration 3600: Loss = -11372.29966600928
Iteration 3700: Loss = -11372.298638393115
Iteration 3800: Loss = -11372.302537965445
1
Iteration 3900: Loss = -11372.302379043218
2
Iteration 4000: Loss = -11372.294123628804
Iteration 4100: Loss = -11372.260353424655
Iteration 4200: Loss = -11372.247786145219
Iteration 4300: Loss = -11372.263845031968
1
Iteration 4400: Loss = -11372.246346209062
Iteration 4500: Loss = -11372.542776401182
1
Iteration 4600: Loss = -11372.244546867942
Iteration 4700: Loss = -11372.243357430725
Iteration 4800: Loss = -11372.24713194786
1
Iteration 4900: Loss = -11372.241472511474
Iteration 5000: Loss = -11372.241130522043
Iteration 5100: Loss = -11372.242198751916
1
Iteration 5200: Loss = -11372.24039328813
Iteration 5300: Loss = -11372.240008369088
Iteration 5400: Loss = -11372.239698698026
Iteration 5500: Loss = -11372.239448287044
Iteration 5600: Loss = -11372.239096793313
Iteration 5700: Loss = -11372.246652878772
1
Iteration 5800: Loss = -11372.240168481885
2
Iteration 5900: Loss = -11372.23832176745
Iteration 6000: Loss = -11372.213310102432
Iteration 6100: Loss = -11372.181475489955
Iteration 6200: Loss = -11372.184477179007
1
Iteration 6300: Loss = -11372.181108089771
Iteration 6400: Loss = -11372.180956787995
Iteration 6500: Loss = -11372.180827831822
Iteration 6600: Loss = -11372.180745905353
Iteration 6700: Loss = -11372.180348712516
Iteration 6800: Loss = -11372.178820542378
Iteration 6900: Loss = -11371.926829716254
Iteration 7000: Loss = -11371.929819718143
1
Iteration 7100: Loss = -11371.928220782822
2
Iteration 7200: Loss = -11372.059246200266
3
Iteration 7300: Loss = -11371.928056682553
4
Iteration 7400: Loss = -11371.926518022472
Iteration 7500: Loss = -11372.13103661084
1
Iteration 7600: Loss = -11371.926751338038
2
Iteration 7700: Loss = -11371.928238610999
3
Iteration 7800: Loss = -11371.932774559365
4
Iteration 7900: Loss = -11371.934535288734
5
Iteration 8000: Loss = -11371.925928753324
Iteration 8100: Loss = -11371.927271087208
1
Iteration 8200: Loss = -11371.92579920846
Iteration 8300: Loss = -11371.926109900362
1
Iteration 8400: Loss = -11371.925896093597
2
Iteration 8500: Loss = -11371.929483659409
3
Iteration 8600: Loss = -11371.957119693328
4
Iteration 8700: Loss = -11371.987942094076
5
Iteration 8800: Loss = -11371.928635011905
6
Iteration 8900: Loss = -11368.766401763518
Iteration 9000: Loss = -11368.76680740172
1
Iteration 9100: Loss = -11368.782346321352
2
Iteration 9200: Loss = -11368.763543990688
Iteration 9300: Loss = -11368.763589042728
1
Iteration 9400: Loss = -11368.763482602915
Iteration 9500: Loss = -11368.763902726492
1
Iteration 9600: Loss = -11368.76324242568
Iteration 9700: Loss = -11368.784845808857
1
Iteration 9800: Loss = -11368.763095970015
Iteration 9900: Loss = -11368.769430886325
1
Iteration 10000: Loss = -11368.768449610343
2
Iteration 10100: Loss = -11368.763054982323
Iteration 10200: Loss = -11368.763142219064
1
Iteration 10300: Loss = -11368.763023644631
Iteration 10400: Loss = -11368.761389917134
Iteration 10500: Loss = -11368.761861759616
1
Iteration 10600: Loss = -11368.76323969567
2
Iteration 10700: Loss = -11368.762784453938
3
Iteration 10800: Loss = -11368.816257773091
4
Iteration 10900: Loss = -11368.750631791814
Iteration 11000: Loss = -11368.771935839915
1
Iteration 11100: Loss = -11368.749211682127
Iteration 11200: Loss = -11368.750122577516
1
Iteration 11300: Loss = -11368.749266178414
2
Iteration 11400: Loss = -11368.767743415941
3
Iteration 11500: Loss = -11368.769236089282
4
Iteration 11600: Loss = -11368.758571605567
5
Iteration 11700: Loss = -11368.749096200641
Iteration 11800: Loss = -11368.749651395623
1
Iteration 11900: Loss = -11368.752154192269
2
Iteration 12000: Loss = -11368.753543996734
3
Iteration 12100: Loss = -11368.748742093385
Iteration 12200: Loss = -11368.744501433248
Iteration 12300: Loss = -11368.744417795324
Iteration 12400: Loss = -11368.761577360525
1
Iteration 12500: Loss = -11368.758216582108
2
Iteration 12600: Loss = -11368.80916098711
3
Iteration 12700: Loss = -11368.879174677653
4
Iteration 12800: Loss = -11368.756216748507
5
Iteration 12900: Loss = -11368.747177908801
6
Iteration 13000: Loss = -11368.758898579039
7
Iteration 13100: Loss = -11368.723814858966
Iteration 13200: Loss = -11368.723505072598
Iteration 13300: Loss = -11368.51849867492
Iteration 13400: Loss = -11368.529083263122
1
Iteration 13500: Loss = -11368.527437205055
2
Iteration 13600: Loss = -11368.518921659683
3
Iteration 13700: Loss = -11368.53906706527
4
Iteration 13800: Loss = -11368.518835927624
5
Iteration 13900: Loss = -11368.52135086767
6
Iteration 14000: Loss = -11368.515545470218
Iteration 14100: Loss = -11368.519150665674
1
Iteration 14200: Loss = -11368.530866636987
2
Iteration 14300: Loss = -11368.515918916646
3
Iteration 14400: Loss = -11368.527395835394
4
Iteration 14500: Loss = -11368.555934005859
5
Iteration 14600: Loss = -11368.516337805411
6
Iteration 14700: Loss = -11368.516221994643
7
Iteration 14800: Loss = -11368.524866753442
8
Iteration 14900: Loss = -11368.518454299261
9
Iteration 15000: Loss = -11368.530280911593
10
Stopping early at iteration 15000 due to no improvement.
tensor([[  5.3063,  -7.6006],
        [ -8.3779,   6.7666],
        [ -9.4905,   8.1018],
        [  3.7545,  -6.6775],
        [ -1.3188,  -0.6934],
        [ -9.0883,   7.7020],
        [-10.1177,   7.0410],
        [ -7.2868,   3.0232],
        [-10.1442,   8.3817],
        [ -8.5357,   7.1349],
        [  3.4715,  -4.8597],
        [  1.7806,  -3.3980],
        [ -7.2117,   5.7808],
        [ -8.7872,   7.3801],
        [  2.3340,  -3.7892],
        [  0.4857,  -1.8808],
        [ -9.7976,   8.3895],
        [  3.2010,  -6.5299],
        [ -9.2011,   7.7453],
        [  2.4683,  -4.3496],
        [  2.1541,  -4.2584],
        [ -0.0996,  -1.4662],
        [-10.0752,   7.9015],
        [  1.7295,  -3.2843],
        [ -9.6879,   8.2914],
        [ -9.1342,   7.6988],
        [  0.7153,  -2.9877],
        [-10.3616,   8.8364],
        [  2.5446,  -4.0056],
        [ -8.3035,   5.3907],
        [ -9.0494,   7.5008],
        [  0.0750,  -1.7287],
        [-10.3013,   8.8872],
        [ -8.6101,   7.1203],
        [ -9.2978,   7.8008],
        [ -7.0039,   5.4251],
        [  3.3490,  -4.8405],
        [ -9.0759,   7.3121],
        [  3.2744,  -6.3619],
        [ -3.1911,   1.5589],
        [ -9.8039,   7.6904],
        [  3.8475,  -5.4814],
        [ -4.6022,   3.2158],
        [ -8.2141,   6.4554],
        [ -5.2812,   3.8919],
        [ -9.4360,   7.7864],
        [  3.7239,  -5.3055],
        [-10.0974,   8.1890],
        [ -9.4111,   7.6529],
        [ -8.9407,   7.3391],
        [  3.0036,  -4.5421],
        [ -9.4408,   7.6931],
        [ -1.8949,  -0.5778],
        [ -8.4527,   7.0158],
        [ -2.7463,   1.2210],
        [ -9.1149,   7.6838],
        [ -9.2608,   7.7630],
        [  7.4234,  -9.1204],
        [  4.9884,  -6.9724],
        [  1.5973,  -3.2444],
        [ -8.8285,   7.4418],
        [ -7.3089,   4.6752],
        [  0.6242,  -2.1827],
        [ -9.1710,   7.7670],
        [  6.8269,  -8.4515],
        [ -9.4792,   8.0861],
        [ -0.1922,  -1.2232],
        [  0.3814,  -1.8365],
        [ -0.8238,  -3.5753],
        [  4.6686,  -8.7584],
        [  0.7904,  -3.3625],
        [ -8.8048,   7.3597],
        [ -9.7317,   8.3447],
        [ -8.1019,   6.7028],
        [ -7.0944,   5.4851],
        [  6.8623,  -8.7732],
        [ -8.8763,   6.8023],
        [  2.5949,  -4.4628],
        [-10.3190,   8.6542],
        [ -9.7908,   7.8173],
        [ -9.5609,   8.1742],
        [  7.3775,  -9.5201],
        [  7.8817,  -9.3993],
        [ -8.3823,   6.0159],
        [  3.3268,  -4.7824],
        [ -6.8288,   5.2492],
        [ -8.1065,   6.0402],
        [ -9.6866,   7.1401],
        [ -5.3843,   3.9644],
        [ -9.0553,   5.5227],
        [ -0.2838,  -1.5225],
        [ -9.0335,   7.2193],
        [-11.3550,   7.1650],
        [ -8.8001,   7.4121],
        [-10.0139,   7.1706],
        [ -9.2461,   5.3096],
        [  1.7945,  -4.9985],
        [ -5.5448,   3.8422],
        [ -9.5552,   7.9314],
        [ -7.6687,   5.6075]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6730, 0.3270],
        [0.3584, 0.6416]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3530, 0.6470], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2670, 0.1007],
         [0.2528, 0.2486]],

        [[0.5700, 0.0966],
         [0.5271, 0.7420]],

        [[0.6506, 0.0958],
         [0.2310, 0.1755]],

        [[0.7138, 0.0992],
         [0.5189, 0.9371]],

        [[0.9967, 0.0968],
         [0.5878, 0.3185]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8821083056742698
time is 1
tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824124176797128
time is 2
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 6
Adjusted Rand Index: 0.7721141809334062
time is 3
tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 4
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
Global Adjusted Rand Index: 0.030459152536089863
Average Adjusted Rand Index: 0.8679705176504999
Iteration 0: Loss = -19672.780466197564
Iteration 10: Loss = -11616.08084264723
Iteration 20: Loss = -11594.562539942304
Iteration 30: Loss = -11286.143484668815
Iteration 40: Loss = -11284.336679791773
Iteration 50: Loss = -11284.336658919507
Iteration 60: Loss = -11284.336656109555
Iteration 70: Loss = -11284.336656109555
1
Iteration 80: Loss = -11284.336656109555
2
Iteration 90: Loss = -11284.336656109555
3
Stopping early at iteration 89 due to no improvement.
pi: tensor([[0.7202, 0.2798],
        [0.2810, 0.7190]], dtype=torch.float64)
alpha: tensor([0.4661, 0.5339])
beta: tensor([[[0.1956, 0.1032],
         [0.8159, 0.2966]],

        [[0.4676, 0.0981],
         [0.8637, 0.8022]],

        [[0.1145, 0.0984],
         [0.1479, 0.0314]],

        [[0.4559, 0.1000],
         [0.0364, 0.2100]],

        [[0.0531, 0.0973],
         [0.7178, 0.4364]]], dtype=torch.float64)
time is 0
tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 2
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208023791611811
time is 3
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208031161549398
Global Adjusted Rand Index: 0.9524807616186577
Average Adjusted Rand Index: 0.9523203153929535
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19672.825046866303
Iteration 100: Loss = -11630.110172558965
Iteration 200: Loss = -11625.594622236113
Iteration 300: Loss = -11623.273999975778
Iteration 400: Loss = -11619.720913149253
Iteration 500: Loss = -11601.774575931473
Iteration 600: Loss = -11564.264944361743
Iteration 700: Loss = -11543.629666167712
Iteration 800: Loss = -11480.925537067345
Iteration 900: Loss = -11454.243145538754
Iteration 1000: Loss = -11446.760046779184
Iteration 1100: Loss = -11446.489492422528
Iteration 1200: Loss = -11446.408871364518
Iteration 1300: Loss = -11446.355456890342
Iteration 1400: Loss = -11446.317484920315
Iteration 1500: Loss = -11446.28789138856
Iteration 1600: Loss = -11446.26157275649
Iteration 1700: Loss = -11446.1922328888
Iteration 1800: Loss = -11445.634471134965
Iteration 1900: Loss = -11445.605305301642
Iteration 2000: Loss = -11445.582056656778
Iteration 2100: Loss = -11445.330920470482
Iteration 2200: Loss = -11445.315831713902
Iteration 2300: Loss = -11445.285016301454
Iteration 2400: Loss = -11445.29848200134
1
Iteration 2500: Loss = -11445.27105445956
Iteration 2600: Loss = -11445.265265384733
Iteration 2700: Loss = -11445.26001838137
Iteration 2800: Loss = -11445.254996606585
Iteration 2900: Loss = -11445.253239104459
Iteration 3000: Loss = -11445.239795843809
Iteration 3100: Loss = -11445.232096038735
Iteration 3200: Loss = -11445.238145748272
1
Iteration 3300: Loss = -11445.225485172548
Iteration 3400: Loss = -11445.222018781375
Iteration 3500: Loss = -11445.234371252416
1
Iteration 3600: Loss = -11445.211114231623
Iteration 3700: Loss = -11445.207427162944
Iteration 3800: Loss = -11445.21354768101
1
Iteration 3900: Loss = -11445.208025819202
2
Iteration 4000: Loss = -11445.201675668868
Iteration 4100: Loss = -11445.186309155048
Iteration 4200: Loss = -11445.166221467143
Iteration 4300: Loss = -11443.06120030332
Iteration 4400: Loss = -11429.777589606225
Iteration 4500: Loss = -11417.720486258624
Iteration 4600: Loss = -11403.45877876591
Iteration 4700: Loss = -11403.059957347123
Iteration 4800: Loss = -11402.738353416667
Iteration 4900: Loss = -11398.121964714632
Iteration 5000: Loss = -11398.098100181378
Iteration 5100: Loss = -11398.01235066865
Iteration 5200: Loss = -11397.727448429536
Iteration 5300: Loss = -11381.100969202624
Iteration 5400: Loss = -11380.806320697167
Iteration 5500: Loss = -11375.097762681895
Iteration 5600: Loss = -11373.195005087577
Iteration 5700: Loss = -11373.10235003978
Iteration 5800: Loss = -11361.332092338946
Iteration 5900: Loss = -11346.616670652898
Iteration 6000: Loss = -11337.570145211506
Iteration 6100: Loss = -11330.357014659345
Iteration 6200: Loss = -11324.237308815935
Iteration 6300: Loss = -11313.173407248067
Iteration 6400: Loss = -11313.224436872566
1
Iteration 6500: Loss = -11303.438512362129
Iteration 6600: Loss = -11303.42927710109
Iteration 6700: Loss = -11303.427844370788
Iteration 6800: Loss = -11303.426643405113
Iteration 6900: Loss = -11297.558831120134
Iteration 7000: Loss = -11297.55583699843
Iteration 7100: Loss = -11297.557960563936
1
Iteration 7200: Loss = -11297.530196727454
Iteration 7300: Loss = -11280.740573216002
Iteration 7400: Loss = -11280.71978144494
Iteration 7500: Loss = -11280.713696857089
Iteration 7600: Loss = -11280.641699097901
Iteration 7700: Loss = -11280.639138599834
Iteration 7800: Loss = -11280.657592963407
1
Iteration 7900: Loss = -11280.637730346501
Iteration 8000: Loss = -11280.637191289621
Iteration 8100: Loss = -11280.636723114509
Iteration 8200: Loss = -11280.640053200346
1
Iteration 8300: Loss = -11280.635065874058
Iteration 8400: Loss = -11280.633978058637
Iteration 8500: Loss = -11280.676564315683
1
Iteration 8600: Loss = -11280.633604797793
Iteration 8700: Loss = -11280.629862187583
Iteration 8800: Loss = -11280.62804062711
Iteration 8900: Loss = -11280.720538095626
1
Iteration 9000: Loss = -11280.683899349138
2
Iteration 9100: Loss = -11280.625789167852
Iteration 9200: Loss = -11280.620369082273
Iteration 9300: Loss = -11280.637494470597
1
Iteration 9400: Loss = -11280.620188975225
Iteration 9500: Loss = -11280.696990013997
1
Iteration 9600: Loss = -11280.620091815128
Iteration 9700: Loss = -11280.625881160637
1
Iteration 9800: Loss = -11280.698828483002
2
Iteration 9900: Loss = -11280.620173977584
3
Iteration 10000: Loss = -11280.619947809108
Iteration 10100: Loss = -11280.622161830832
1
Iteration 10200: Loss = -11280.620157431711
2
Iteration 10300: Loss = -11280.619694774019
Iteration 10400: Loss = -11280.65750149639
1
Iteration 10500: Loss = -11280.621454351258
2
Iteration 10600: Loss = -11280.618969092471
Iteration 10700: Loss = -11280.621554504598
1
Iteration 10800: Loss = -11280.618278353673
Iteration 10900: Loss = -11280.619168480096
1
Iteration 11000: Loss = -11280.61806540868
Iteration 11100: Loss = -11280.618081544397
1
Iteration 11200: Loss = -11280.618144805187
2
Iteration 11300: Loss = -11280.636630724613
3
Iteration 11400: Loss = -11280.62816903684
4
Iteration 11500: Loss = -11280.638039250192
5
Iteration 11600: Loss = -11280.614781641254
Iteration 11700: Loss = -11280.615184103417
1
Iteration 11800: Loss = -11280.615059440965
2
Iteration 11900: Loss = -11280.698059718854
3
Iteration 12000: Loss = -11280.614710097154
Iteration 12100: Loss = -11280.633137334402
1
Iteration 12200: Loss = -11280.614685179835
Iteration 12300: Loss = -11280.617217713216
1
Iteration 12400: Loss = -11280.61467249066
Iteration 12500: Loss = -11280.614827127363
1
Iteration 12600: Loss = -11280.617353630429
2
Iteration 12700: Loss = -11280.635347911893
3
Iteration 12800: Loss = -11280.627371137512
4
Iteration 12900: Loss = -11280.615466138395
5
Iteration 13000: Loss = -11280.615363921936
6
Iteration 13100: Loss = -11280.62053989146
7
Iteration 13200: Loss = -11280.7545742518
8
Iteration 13300: Loss = -11280.614649467696
Iteration 13400: Loss = -11280.614853202787
1
Iteration 13500: Loss = -11280.614630681368
Iteration 13600: Loss = -11280.615041825577
1
Iteration 13700: Loss = -11280.614619693792
Iteration 13800: Loss = -11280.627738212715
1
Iteration 13900: Loss = -11280.614595060544
Iteration 14000: Loss = -11280.614586223372
Iteration 14100: Loss = -11280.621352711558
1
Iteration 14200: Loss = -11280.629231375904
2
Iteration 14300: Loss = -11280.615126266779
3
Iteration 14400: Loss = -11280.614563435069
Iteration 14500: Loss = -11280.615208600962
1
Iteration 14600: Loss = -11280.61451667129
Iteration 14700: Loss = -11280.614776078324
1
Iteration 14800: Loss = -11280.744811016732
2
Iteration 14900: Loss = -11280.614314274631
Iteration 15000: Loss = -11280.621232206326
1
Iteration 15100: Loss = -11280.62073340468
2
Iteration 15200: Loss = -11280.615941618538
3
Iteration 15300: Loss = -11280.614324632232
4
Iteration 15400: Loss = -11280.628410615416
5
Iteration 15500: Loss = -11280.614446108259
6
Iteration 15600: Loss = -11280.614323109081
7
Iteration 15700: Loss = -11280.615866974169
8
Iteration 15800: Loss = -11280.637877734365
9
Iteration 15900: Loss = -11280.614205097292
Iteration 16000: Loss = -11280.615430111195
1
Iteration 16100: Loss = -11280.63448144405
2
Iteration 16200: Loss = -11280.614241395797
3
Iteration 16300: Loss = -11280.615028626811
4
Iteration 16400: Loss = -11280.753302984569
5
Iteration 16500: Loss = -11280.614164762803
Iteration 16600: Loss = -11280.619801392571
1
Iteration 16700: Loss = -11280.614125337972
Iteration 16800: Loss = -11280.614441795033
1
Iteration 16900: Loss = -11280.630799721243
2
Iteration 17000: Loss = -11280.622738048925
3
Iteration 17100: Loss = -11280.61415740278
4
Iteration 17200: Loss = -11280.61972268175
5
Iteration 17300: Loss = -11280.614130877733
6
Iteration 17400: Loss = -11280.614210374913
7
Iteration 17500: Loss = -11280.619434186669
8
Iteration 17600: Loss = -11280.628226131566
9
Iteration 17700: Loss = -11280.61413586681
10
Stopping early at iteration 17700 due to no improvement.
tensor([[-7.7629,  6.3465],
        [ 7.5290, -9.5022],
        [ 7.0614, -8.6463],
        [-7.6965,  6.0316],
        [-2.7507,  1.3594],
        [ 3.8948, -5.8966],
        [ 5.9822, -7.7314],
        [ 3.0710, -4.5510],
        [ 6.3815, -8.2786],
        [ 5.8528, -8.3986],
        [-6.2850,  4.8349],
        [-5.8555,  4.0907],
        [ 5.9843, -9.4522],
        [ 3.3148, -5.2822],
        [-6.1834,  2.8535],
        [-3.6507,  1.9671],
        [ 6.1760, -8.4036],
        [-8.4855,  5.2147],
        [ 3.5051, -5.3717],
        [-5.2330,  2.9975],
        [-5.2482,  3.6765],
        [-3.7584,  1.6269],
        [ 2.4935, -4.5884],
        [-4.0023,  2.6014],
        [ 7.5921, -9.0241],
        [ 5.6423, -8.9907],
        [-3.9655,  2.3967],
        [ 6.9646, -8.3534],
        [-4.8933,  3.2530],
        [ 4.1605, -7.0170],
        [ 6.6790, -8.6484],
        [-3.3486,  1.6461],
        [ 7.6119, -9.0126],
        [ 5.8627, -7.2492],
        [ 3.1570, -4.6267],
        [ 2.9163, -4.5411],
        [-6.3331,  4.8117],
        [ 3.7553, -6.7465],
        [-8.4928,  4.8911],
        [-0.1485, -2.6866],
        [ 6.7043, -8.8658],
        [-6.4593,  4.9309],
        [ 1.1399, -2.8900],
        [ 4.6379, -6.0500],
        [ 2.9000, -5.2495],
        [ 4.9077, -6.7200],
        [-8.7307,  5.5484],
        [ 7.1502, -8.6094],
        [ 1.9657, -3.4790],
        [ 3.4187, -5.1317],
        [-5.8657,  3.9622],
        [ 7.7973, -9.1880],
        [-3.0686,  0.6818],
        [ 6.5135, -8.0367],
        [-1.2902, -0.0961],
        [ 5.5707, -7.3396],
        [ 1.7467, -3.1694],
        [-6.5148,  4.9563],
        [-6.5743,  5.1865],
        [-5.9668,  4.4894],
        [ 4.0749, -5.7349],
        [ 3.9349, -5.3568],
        [-3.7996,  1.9349],
        [ 4.7273, -7.0978],
        [-5.7057,  2.8196],
        [ 7.5006, -8.8918],
        [-3.3704,  0.8846],
        [-3.1927,  1.4687],
        [-3.7484,  1.9688],
        [-8.9636,  5.9032],
        [-3.3363,  1.4452],
        [ 3.5034, -5.0846],
        [ 5.8843, -7.3118],
        [ 1.6921, -5.1817],
        [ 4.3077, -6.5594],
        [-5.3874,  4.0006],
        [ 2.4772, -4.3215],
        [-5.6860,  4.1486],
        [ 2.1358, -4.9325],
        [ 6.3602, -7.7473],
        [ 6.3624, -8.2806],
        [-4.6765,  2.4070],
        [-3.2593,  1.7565],
        [ 2.7371, -7.3523],
        [-6.4104,  4.9385],
        [ 4.0463, -6.4350],
        [ 4.3710, -5.7940],
        [ 0.8223, -2.7778],
        [ 6.5867, -9.1816],
        [ 3.8187, -5.2090],
        [-3.9361,  2.2876],
        [ 4.8736, -6.4070],
        [ 4.7415, -6.9406],
        [ 0.8143, -3.6772],
        [ 3.3300, -4.8579],
        [ 6.8906, -8.9608],
        [-7.4084,  4.8071],
        [ 1.5410, -4.7046],
        [ 3.7813, -5.1691],
        [ 4.2698, -6.0273]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7256, 0.2744],
        [0.2707, 0.7293]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6118, 0.3882], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3021, 0.1027],
         [0.8159, 0.2002]],

        [[0.4676, 0.0979],
         [0.8637, 0.8022]],

        [[0.1145, 0.0985],
         [0.1479, 0.0314]],

        [[0.4559, 0.0999],
         [0.0364, 0.2100]],

        [[0.0531, 0.0972],
         [0.7178, 0.4364]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208023791611811
time is 3
tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
Global Adjusted Rand Index: 0.9603204383279997
Average Adjusted Rand Index: 0.9603203153929535
11297.408809010527
new:  [0.9603204383279997, 0.9603204383279997, 0.030459152536089863, 0.9603204383279997] [0.9603203153929535, 0.9603203153929535, 0.8679705176504999, 0.9603203153929535] [11281.036665869253, 11280.628508225393, 11368.530280911593, 11280.61413586681]
prior:  [0.9524807616186577, 0.9524807616186577, 0.9524807616186577, 0.9524807616186577] [0.9523203153929535, 0.9523203153929535, 0.9523203153929535, 0.9523203153929535] [11284.33665881622, 11284.336655780573, 11284.336666598361, 11284.336656109555]
-----------------------------------------------------------------------------------------
This iteration is 12
True Objective function: Loss = -11060.163568680035
Iteration 0: Loss = -27063.70991100116
Iteration 10: Loss = -11331.669514147752
Iteration 20: Loss = -11331.66952777643
1
Iteration 30: Loss = -11331.66596715454
Iteration 40: Loss = -11327.567314483193
Iteration 50: Loss = -11322.569293123825
Iteration 60: Loss = -11319.94902874726
Iteration 70: Loss = -11313.980080817608
Iteration 80: Loss = -11291.079588758517
Iteration 90: Loss = -11167.450814740247
Iteration 100: Loss = -11040.615874415893
Iteration 110: Loss = -11040.989388219643
1
Iteration 120: Loss = -11041.011893347426
2
Iteration 130: Loss = -11041.01254063089
3
Stopping early at iteration 129 due to no improvement.
pi: tensor([[0.7287, 0.2713],
        [0.3146, 0.6854]], dtype=torch.float64)
alpha: tensor([0.5108, 0.4892])
beta: tensor([[[0.1923, 0.0840],
         [0.8986, 0.2918]],

        [[0.9831, 0.0994],
         [0.4862, 0.4643]],

        [[0.0240, 0.1068],
         [0.7399, 0.7954]],

        [[0.8926, 0.1035],
         [0.8225, 0.6969]],

        [[0.0841, 0.0936],
         [0.1722, 0.7143]]], dtype=torch.float64)
time is 0
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 1
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
time is 2
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 3
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9207675179163246
time is 4
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9368976526043716
Average Adjusted Rand Index: 0.9364753795053573
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26551.97488126984
Iteration 100: Loss = -11333.706035691517
Iteration 200: Loss = -11332.792324217937
Iteration 300: Loss = -11332.331736672297
Iteration 400: Loss = -11332.01539237969
Iteration 500: Loss = -11331.742312053928
Iteration 600: Loss = -11331.35827387757
Iteration 700: Loss = -11328.328654483008
Iteration 800: Loss = -11313.353753209714
Iteration 900: Loss = -11147.667603389706
Iteration 1000: Loss = -11055.343680611219
Iteration 1100: Loss = -11039.692072413929
Iteration 1200: Loss = -11039.259650323153
Iteration 1300: Loss = -11039.064689482952
Iteration 1400: Loss = -11038.873480839799
Iteration 1500: Loss = -11036.857528799173
Iteration 1600: Loss = -11036.708181266571
Iteration 1700: Loss = -11036.661485090295
Iteration 1800: Loss = -11036.629792273894
Iteration 1900: Loss = -11036.605735449517
Iteration 2000: Loss = -11036.586602973726
Iteration 2100: Loss = -11036.570992901728
Iteration 2200: Loss = -11036.557962323708
Iteration 2300: Loss = -11036.546950384432
Iteration 2400: Loss = -11036.537592700157
Iteration 2500: Loss = -11036.529563631024
Iteration 2600: Loss = -11036.522562886135
Iteration 2700: Loss = -11036.516108219155
Iteration 2800: Loss = -11036.50762763316
Iteration 2900: Loss = -11036.495804758624
Iteration 3000: Loss = -11036.482134707334
Iteration 3100: Loss = -11036.432292806225
Iteration 3200: Loss = -11036.425764108135
Iteration 3300: Loss = -11036.421239719062
Iteration 3400: Loss = -11036.420265642691
Iteration 3500: Loss = -11036.408471157027
Iteration 3600: Loss = -11036.402373876424
Iteration 3700: Loss = -11036.367594755147
Iteration 3800: Loss = -11036.365061321874
Iteration 3900: Loss = -11036.362756011224
Iteration 4000: Loss = -11036.367558906873
1
Iteration 4100: Loss = -11036.35836882076
Iteration 4200: Loss = -11036.355737838441
Iteration 4300: Loss = -11036.322971157997
Iteration 4400: Loss = -11036.300162189276
Iteration 4500: Loss = -11036.296440100727
Iteration 4600: Loss = -11036.265845438573
Iteration 4700: Loss = -11036.261302854611
Iteration 4800: Loss = -11036.260361507426
Iteration 4900: Loss = -11036.262369598235
1
Iteration 5000: Loss = -11036.258446965665
Iteration 5100: Loss = -11036.256729903604
Iteration 5200: Loss = -11036.274701015593
1
Iteration 5300: Loss = -11036.254652941287
Iteration 5400: Loss = -11036.253967134739
Iteration 5500: Loss = -11036.256470302602
1
Iteration 5600: Loss = -11036.246825375756
Iteration 5700: Loss = -11036.245280623316
Iteration 5800: Loss = -11036.254854618888
1
Iteration 5900: Loss = -11036.243894479896
Iteration 6000: Loss = -11036.243470988038
Iteration 6100: Loss = -11036.265604107031
1
Iteration 6200: Loss = -11036.24265913577
Iteration 6300: Loss = -11036.242291823697
Iteration 6400: Loss = -11036.272958048121
1
Iteration 6500: Loss = -11036.241524013883
Iteration 6600: Loss = -11036.240902384425
Iteration 6700: Loss = -11036.282043181147
1
Iteration 6800: Loss = -11036.239172320511
Iteration 6900: Loss = -11036.2389918531
Iteration 7000: Loss = -11036.239676433199
1
Iteration 7100: Loss = -11036.241975991425
2
Iteration 7200: Loss = -11036.242806004057
3
Iteration 7300: Loss = -11036.228143064614
Iteration 7400: Loss = -11036.264815433819
1
Iteration 7500: Loss = -11036.22777640143
Iteration 7600: Loss = -11036.228222145564
1
Iteration 7700: Loss = -11036.227416815442
Iteration 7800: Loss = -11036.227193919676
Iteration 7900: Loss = -11036.229429445755
1
Iteration 8000: Loss = -11036.226924000255
Iteration 8100: Loss = -11036.23214236243
1
Iteration 8200: Loss = -11036.22628131464
Iteration 8300: Loss = -11036.226077050856
Iteration 8400: Loss = -11036.233288933101
1
Iteration 8500: Loss = -11036.225814284793
Iteration 8600: Loss = -11036.225788064614
Iteration 8700: Loss = -11036.305746077434
1
Iteration 8800: Loss = -11036.223101462328
Iteration 8900: Loss = -11036.222742609623
Iteration 9000: Loss = -11036.222737963384
Iteration 9100: Loss = -11036.22242250574
Iteration 9200: Loss = -11036.244771533462
1
Iteration 9300: Loss = -11036.255015634852
2
Iteration 9400: Loss = -11036.227624758052
3
Iteration 9500: Loss = -11036.2296200371
4
Iteration 9600: Loss = -11036.23599762679
5
Iteration 9700: Loss = -11036.232475975174
6
Iteration 9800: Loss = -11036.22797871014
7
Iteration 9900: Loss = -11036.230282037775
8
Iteration 10000: Loss = -11036.235223368169
9
Iteration 10100: Loss = -11036.227984605215
10
Stopping early at iteration 10100 due to no improvement.
tensor([[-4.8694e+00,  2.5413e-01],
        [ 3.9825e+00, -8.5977e+00],
        [ 4.0553e+00, -8.6706e+00],
        [ 3.8588e+00, -8.4740e+00],
        [ 5.4643e+00, -1.0079e+01],
        [ 2.3920e+00, -7.0072e+00],
        [ 4.5389e+00, -9.1541e+00],
        [-8.1867e+00,  3.5715e+00],
        [ 3.6390e+00, -8.2542e+00],
        [ 2.9943e+00, -7.6095e+00],
        [-7.3883e+00,  2.7731e+00],
        [ 3.6575e-01, -4.9810e+00],
        [-6.2846e+00,  1.6694e+00],
        [-7.7663e+00,  3.1511e+00],
        [ 2.5626e+00, -7.1778e+00],
        [ 4.5676e+00, -9.1828e+00],
        [ 4.3801e+00, -8.9953e+00],
        [ 4.2637e+00, -8.8790e+00],
        [ 4.2423e+00, -8.8576e+00],
        [ 4.2369e+00, -8.8521e+00],
        [ 4.0407e+00, -8.6559e+00],
        [ 4.2477e+00, -8.8629e+00],
        [ 4.1343e+00, -8.7495e+00],
        [ 3.9683e+00, -8.5836e+00],
        [-9.3718e+00,  4.7566e+00],
        [ 3.9524e+00, -8.5676e+00],
        [-6.8573e+00,  2.2421e+00],
        [ 3.0683e+00, -7.6835e+00],
        [ 4.1315e+00, -8.7467e+00],
        [ 2.8792e+00, -7.4944e+00],
        [-7.0221e+00,  2.4069e+00],
        [-9.7269e+00,  5.1116e+00],
        [-3.5595e+00, -1.0558e+00],
        [ 3.8420e+00, -8.4573e+00],
        [ 3.4023e+00, -8.0175e+00],
        [-5.5397e+00,  9.2449e-01],
        [ 2.3120e+00, -6.9272e+00],
        [-9.3887e+00,  4.7735e+00],
        [-7.6597e+00,  3.0444e+00],
        [ 5.1963e+00, -9.8115e+00],
        [-9.0131e+00,  4.3979e+00],
        [ 4.2918e+00, -8.9070e+00],
        [-2.1552e+00, -2.4600e+00],
        [-8.0338e+00,  3.4186e+00],
        [-3.8284e+00, -7.8680e-01],
        [ 4.9467e+00, -9.5619e+00],
        [ 3.4751e+00, -8.0903e+00],
        [-8.7319e+00,  4.1167e+00],
        [ 3.4382e+00, -8.0534e+00],
        [-5.9204e+00,  1.3052e+00],
        [-8.2654e+00,  3.6502e+00],
        [-5.4757e+00,  8.6049e-01],
        [-9.3442e+00,  4.7290e+00],
        [ 1.0122e+00, -5.6275e+00],
        [-8.3658e+00,  3.7506e+00],
        [ 3.3558e+00, -7.9710e+00],
        [-8.5149e+00,  3.8997e+00],
        [ 2.5136e+00, -7.1288e+00],
        [-5.8960e+00,  1.2808e+00],
        [-2.6119e+00, -2.0033e+00],
        [-8.7344e+00,  4.1192e+00],
        [ 3.1999e+00, -7.8151e+00],
        [ 3.6975e+00, -8.3128e+00],
        [-5.6744e+00,  1.0592e+00],
        [ 8.8125e-03, -4.6240e+00],
        [ 2.7930e+00, -7.4082e+00],
        [ 4.8867e+00, -9.5019e+00],
        [-6.6300e+00,  2.0147e+00],
        [ 3.5845e-01, -4.9737e+00],
        [ 2.3703e+00, -6.9856e+00],
        [-8.5440e+00,  3.9287e+00],
        [ 4.2562e+00, -8.8714e+00],
        [ 4.4585e+00, -9.0737e+00],
        [-8.3150e+00,  3.6998e+00],
        [ 5.0736e+00, -9.6888e+00],
        [-8.4928e+00,  3.8776e+00],
        [ 1.3614e+00, -5.9766e+00],
        [ 2.3536e+00, -6.9688e+00],
        [-3.7909e+00, -8.2431e-01],
        [-5.6822e+00,  1.0670e+00],
        [-4.7187e+00,  1.0349e-01],
        [-7.6075e+00,  2.9922e+00],
        [ 4.3054e+00, -8.9206e+00],
        [-8.6799e+00,  4.0647e+00],
        [-6.1420e+00,  1.5268e+00],
        [ 9.9622e-01, -5.6114e+00],
        [-8.3972e+00,  3.7820e+00],
        [-7.8461e+00,  3.2309e+00],
        [ 4.7210e+00, -9.3363e+00],
        [ 5.0305e+00, -9.6457e+00],
        [ 4.1891e+00, -8.8043e+00],
        [-8.1611e+00,  3.5459e+00],
        [ 3.3136e+00, -7.9288e+00],
        [-5.2094e+00,  5.9420e-01],
        [-8.2008e+00,  3.5855e+00],
        [ 1.0065e+00, -5.6217e+00],
        [ 3.5576e+00, -8.1728e+00],
        [-8.2611e+00,  3.6459e+00],
        [-6.1822e+00,  1.5670e+00],
        [-9.8200e+00,  5.2048e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7064, 0.2936],
        [0.2495, 0.7505]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5506, 0.4494], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2979, 0.0839],
         [0.8986, 0.1953]],

        [[0.9831, 0.0988],
         [0.4862, 0.4643]],

        [[0.0240, 0.1067],
         [0.7399, 0.7954]],

        [[0.8926, 0.1047],
         [0.8225, 0.6969]],

        [[0.0841, 0.0934],
         [0.1722, 0.7143]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 1
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208070336283601
time is 2
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8823435719624108
time is 4
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.929154120624323
Average Adjusted Rand Index: 0.9287906401706234
Iteration 0: Loss = -28852.373030013117
Iteration 10: Loss = -11323.20946511526
Iteration 20: Loss = -11320.189074357051
Iteration 30: Loss = -11304.556118749726
Iteration 40: Loss = -11122.00748571972
Iteration 50: Loss = -11040.838897027292
Iteration 60: Loss = -11041.00647536084
1
Iteration 70: Loss = -11041.012399053125
2
Iteration 80: Loss = -11041.012564140357
3
Stopping early at iteration 79 due to no improvement.
pi: tensor([[0.7287, 0.2713],
        [0.3146, 0.6854]], dtype=torch.float64)
alpha: tensor([0.5108, 0.4892])
beta: tensor([[[0.1923, 0.0840],
         [0.4698, 0.2918]],

        [[0.8043, 0.0994],
         [0.0277, 0.1850]],

        [[0.1282, 0.1068],
         [0.2773, 0.0886]],

        [[0.3799, 0.1035],
         [0.5871, 0.2293]],

        [[0.0482, 0.0936],
         [0.5150, 0.2726]]], dtype=torch.float64)
time is 0
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 1
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
time is 2
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 3
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9207675179163246
time is 4
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9368976526043716
Average Adjusted Rand Index: 0.9364753795053573
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -28852.59693540608
Iteration 100: Loss = -11378.078713275083
Iteration 200: Loss = -11354.25477083022
Iteration 300: Loss = -11334.170871160219
Iteration 400: Loss = -11331.306376078017
Iteration 500: Loss = -11329.442558463847
Iteration 600: Loss = -11328.701158416336
Iteration 700: Loss = -11328.169066593198
Iteration 800: Loss = -11327.712259148295
Iteration 900: Loss = -11327.280371601302
Iteration 1000: Loss = -11326.848524952937
Iteration 1100: Loss = -11326.40322370882
Iteration 1200: Loss = -11325.979770230471
Iteration 1300: Loss = -11325.272930754218
Iteration 1400: Loss = -11324.229263318322
Iteration 1500: Loss = -11314.737894136577
Iteration 1600: Loss = -11295.440864778626
Iteration 1700: Loss = -11254.757629043392
Iteration 1800: Loss = -11229.76798271153
Iteration 1900: Loss = -11215.64230958582
Iteration 2000: Loss = -11209.433779017203
Iteration 2100: Loss = -11201.099166258864
Iteration 2200: Loss = -11166.92834005471
Iteration 2300: Loss = -11155.92983761308
Iteration 2400: Loss = -11149.565634493798
Iteration 2500: Loss = -11144.524960781602
Iteration 2600: Loss = -11132.253053753488
Iteration 2700: Loss = -11126.80876712284
Iteration 2800: Loss = -11125.654140644509
Iteration 2900: Loss = -11125.40550975246
Iteration 3000: Loss = -11124.31452039527
Iteration 3100: Loss = -11123.664671369783
Iteration 3200: Loss = -11123.518574815263
Iteration 3300: Loss = -11119.131561537044
Iteration 3400: Loss = -11114.011148983966
Iteration 3500: Loss = -11113.967254304143
Iteration 3600: Loss = -11113.955330982793
Iteration 3700: Loss = -11113.941576670451
Iteration 3800: Loss = -11113.912568767246
Iteration 3900: Loss = -11113.363367731807
Iteration 4000: Loss = -11113.353791909938
Iteration 4100: Loss = -11113.339561396542
Iteration 4200: Loss = -11113.072673446419
Iteration 4300: Loss = -11108.690918443795
Iteration 4400: Loss = -11106.349551984118
Iteration 4500: Loss = -11106.267620077499
Iteration 4600: Loss = -11106.255521941266
Iteration 4700: Loss = -11101.610484127988
Iteration 4800: Loss = -11101.588409591186
Iteration 4900: Loss = -11101.544968094766
Iteration 5000: Loss = -11098.51951374786
Iteration 5100: Loss = -11097.271404979787
Iteration 5200: Loss = -11097.047276745161
Iteration 5300: Loss = -11095.907636610144
Iteration 5400: Loss = -11095.857821082203
Iteration 5500: Loss = -11095.84669993673
Iteration 5600: Loss = -11095.842469215628
Iteration 5700: Loss = -11095.842708228898
1
Iteration 5800: Loss = -11095.830992549041
Iteration 5900: Loss = -11095.83009260165
Iteration 6000: Loss = -11095.829579875255
Iteration 6100: Loss = -11095.827975271754
Iteration 6200: Loss = -11095.827354049774
Iteration 6300: Loss = -11095.826611659488
Iteration 6400: Loss = -11095.825801452169
Iteration 6500: Loss = -11095.8263602777
1
Iteration 6600: Loss = -11095.824861649737
Iteration 6700: Loss = -11095.822243857094
Iteration 6800: Loss = -11095.821747679693
Iteration 6900: Loss = -11095.82144862942
Iteration 7000: Loss = -11095.821702048259
1
Iteration 7100: Loss = -11095.822633542532
2
Iteration 7200: Loss = -11095.820961930454
Iteration 7300: Loss = -11095.624007039076
Iteration 7400: Loss = -11095.109777736183
Iteration 7500: Loss = -11095.10897120449
Iteration 7600: Loss = -11095.10804262288
Iteration 7700: Loss = -11095.080432724148
Iteration 7800: Loss = -11095.10031360623
1
Iteration 7900: Loss = -11095.0839404126
2
Iteration 8000: Loss = -11095.056061449806
Iteration 8100: Loss = -11095.055395368361
Iteration 8200: Loss = -11095.063633163314
1
Iteration 8300: Loss = -11094.99845265935
Iteration 8400: Loss = -11094.99786682619
Iteration 8500: Loss = -11095.001591600774
1
Iteration 8600: Loss = -11094.994389094822
Iteration 8700: Loss = -11094.987457097708
Iteration 8800: Loss = -11095.23682448802
1
Iteration 8900: Loss = -11094.986276966303
Iteration 9000: Loss = -11095.124573031086
1
Iteration 9100: Loss = -11094.978641018686
Iteration 9200: Loss = -11093.914398490904
Iteration 9300: Loss = -11093.919129567405
1
Iteration 9400: Loss = -11093.912975593561
Iteration 9500: Loss = -11093.912948672069
Iteration 9600: Loss = -11093.91280259613
Iteration 9700: Loss = -11093.912925044311
1
Iteration 9800: Loss = -11093.912688707405
Iteration 9900: Loss = -11093.922423067785
1
Iteration 10000: Loss = -11093.912631091947
Iteration 10100: Loss = -11093.956841148953
1
Iteration 10200: Loss = -11093.917140473402
2
Iteration 10300: Loss = -11093.912356055756
Iteration 10400: Loss = -11093.912554511748
1
Iteration 10500: Loss = -11093.91215348661
Iteration 10600: Loss = -11093.91559902801
1
Iteration 10700: Loss = -11093.911466609234
Iteration 10800: Loss = -11093.909620699113
Iteration 10900: Loss = -11093.905325267966
Iteration 11000: Loss = -11093.905156035407
Iteration 11100: Loss = -11093.985515242362
1
Iteration 11200: Loss = -11093.900547467047
Iteration 11300: Loss = -11093.905207511707
1
Iteration 11400: Loss = -11093.822196770232
Iteration 11500: Loss = -11093.820891389987
Iteration 11600: Loss = -11093.820790857097
Iteration 11700: Loss = -11093.822397364807
1
Iteration 11800: Loss = -11093.820163621665
Iteration 11900: Loss = -11093.820100601133
Iteration 12000: Loss = -11093.819686907946
Iteration 12100: Loss = -11093.805430339735
Iteration 12200: Loss = -11093.793200810604
Iteration 12300: Loss = -11093.79294818561
Iteration 12400: Loss = -11093.792001389606
Iteration 12500: Loss = -11093.881277774648
1
Iteration 12600: Loss = -11093.795403380389
2
Iteration 12700: Loss = -11093.794220051692
3
Iteration 12800: Loss = -11093.791812038264
Iteration 12900: Loss = -11093.791641620279
Iteration 13000: Loss = -11087.864982064555
Iteration 13100: Loss = -11087.762049306695
Iteration 13200: Loss = -11081.884972215692
Iteration 13300: Loss = -11081.884103002143
Iteration 13400: Loss = -11081.895549640933
1
Iteration 13500: Loss = -11081.87718325908
Iteration 13600: Loss = -11081.875527690921
Iteration 13700: Loss = -11081.881116282031
1
Iteration 13800: Loss = -11081.87469376431
Iteration 13900: Loss = -11081.84301441134
Iteration 14000: Loss = -11081.877677775372
1
Iteration 14100: Loss = -11081.840843994883
Iteration 14200: Loss = -11081.837917101186
Iteration 14300: Loss = -11081.838183063992
1
Iteration 14400: Loss = -11081.838696679308
2
Iteration 14500: Loss = -11081.915143878163
3
Iteration 14600: Loss = -11081.837638951567
Iteration 14700: Loss = -11081.580968506856
Iteration 14800: Loss = -11081.573734402815
Iteration 14900: Loss = -11081.577672641159
1
Iteration 15000: Loss = -11081.572925134546
Iteration 15100: Loss = -11081.573147098043
1
Iteration 15200: Loss = -11081.572877162593
Iteration 15300: Loss = -11081.573439158497
1
Iteration 15400: Loss = -11081.572850554923
Iteration 15500: Loss = -11081.574387037752
1
Iteration 15600: Loss = -11081.572786996792
Iteration 15700: Loss = -11081.573969414845
1
Iteration 15800: Loss = -11081.57277460828
Iteration 15900: Loss = -11081.592730112327
1
Iteration 16000: Loss = -11081.572770348052
Iteration 16100: Loss = -11081.57864258071
1
Iteration 16200: Loss = -11081.599040130122
2
Iteration 16300: Loss = -11081.567067483415
Iteration 16400: Loss = -11081.571787573546
1
Iteration 16500: Loss = -11081.577398319208
2
Iteration 16600: Loss = -11081.567535481996
3
Iteration 16700: Loss = -11081.574716888716
4
Iteration 16800: Loss = -11081.584061272404
5
Iteration 16900: Loss = -11081.567085559334
6
Iteration 17000: Loss = -11081.593409386944
7
Iteration 17100: Loss = -11081.566882473398
Iteration 17200: Loss = -11081.567848880342
1
Iteration 17300: Loss = -11081.572046278645
2
Iteration 17400: Loss = -11081.577721973434
3
Iteration 17500: Loss = -11081.56644638744
Iteration 17600: Loss = -11081.568582546393
1
Iteration 17700: Loss = -11081.567033997202
2
Iteration 17800: Loss = -11081.5662692819
Iteration 17900: Loss = -11081.567393266701
1
Iteration 18000: Loss = -11081.566676184146
2
Iteration 18100: Loss = -11081.566286136163
3
Iteration 18200: Loss = -11081.58527166503
4
Iteration 18300: Loss = -11081.56628519742
5
Iteration 18400: Loss = -11081.572516402011
6
Iteration 18500: Loss = -11081.564986207622
Iteration 18600: Loss = -11081.555110617648
Iteration 18700: Loss = -11081.554489027158
Iteration 18800: Loss = -11081.555029974785
1
Iteration 18900: Loss = -11081.566569868652
2
Iteration 19000: Loss = -11081.554346650262
Iteration 19100: Loss = -11081.556597172792
1
Iteration 19200: Loss = -11081.554353268446
2
Iteration 19300: Loss = -11081.635049713417
3
Iteration 19400: Loss = -11081.554344397582
Iteration 19500: Loss = -11081.55430973113
Iteration 19600: Loss = -11081.5550280266
1
Iteration 19700: Loss = -11081.554284288037
Iteration 19800: Loss = -11081.55429723292
1
Iteration 19900: Loss = -11081.554448421533
2
tensor([[  1.3080,  -3.2089],
        [ -9.0052,   7.5938],
        [ -9.7098,   8.3234],
        [ -7.8062,   6.4139],
        [-10.8518,   8.7258],
        [ -7.1961,   4.3606],
        [ -9.7686,   8.3641],
        [  4.8590,  -6.3120],
        [ -8.5723,   4.9507],
        [ -6.2693,   4.7923],
        [  3.4381,  -4.8918],
        [ -3.9305,   2.3269],
        [  2.7345,  -4.1232],
        [  4.6273,  -6.0146],
        [ -6.1776,   4.0441],
        [-10.1632,   8.7385],
        [-10.2051,   6.9490],
        [ -8.8121,   7.2998],
        [ -9.3459,   7.6282],
        [ -8.6583,   7.2644],
        [ -9.1204,   6.8360],
        [-10.7790,   8.9953],
        [ -8.9099,   7.1426],
        [ -8.8401,   6.8520],
        [  5.7985,  -8.3124],
        [ -9.9402,   5.3250],
        [  2.6942,  -4.8643],
        [ -6.5545,   5.1487],
        [ -7.6638,   6.2000],
        [ -6.3261,   4.5768],
        [  3.2866,  -5.0999],
        [  3.7122,  -5.2250],
        [ -0.4945,  -1.1597],
        [ -8.0945,   6.6342],
        [ -8.9705,   4.9980],
        [  2.0003,  -3.4163],
        [ -8.3900,   6.7588],
        [  5.9221,  -7.5850],
        [  3.1541,  -4.7441],
        [ -9.6896,   7.9989],
        [  7.9502,  -9.3371],
        [ -9.5463,   7.6059],
        [ -1.2516,  -0.2626],
        [  4.1676,  -5.8289],
        [  0.2121,  -1.8131],
        [ -9.4377,   7.2183],
        [ -8.2699,   6.3727],
        [  8.5441,  -9.9355],
        [ -9.0834,   7.6433],
        [  2.7677,  -4.1791],
        [  4.9884,  -6.9858],
        [  2.1624,  -3.7041],
        [  2.8416,  -4.5210],
        [ -4.3996,   3.0024],
        [ -0.0264,  -1.4385],
        [ -7.6979,   6.2097],
        [  6.2573,  -7.6449],
        [ -9.6678,   8.1734],
        [  2.0774,  -3.5048],
        [ -0.5867,  -0.8237],
        [  7.2426,  -8.6339],
        [ -6.7673,   4.9524],
        [ -7.7085,   5.8264],
        [  2.5516,  -4.0208],
        [ -4.3565,   2.7542],
        [ -6.1932,   4.5842],
        [-10.5015,   9.0566],
        [  2.5063,  -3.9852],
        [ -4.0198,   2.3503],
        [-10.3211,   8.5004],
        [  5.8476,  -7.8678],
        [ -9.0385,   7.4004],
        [-10.3587,   8.1132],
        [  4.8445,  -6.6760],
        [ -9.6451,   8.0652],
        [  4.8238,  -6.7741],
        [ -4.7345,   3.3111],
        [ -8.7316,   7.2063],
        [  0.0907,  -1.6066],
        [  0.7427,  -5.3579],
        [  1.0253,  -2.9956],
        [  3.2300,  -6.4954],
        [ -9.2478,   7.2046],
        [  6.7839,  -8.1710],
        [  2.0360,  -3.4306],
        [ -5.0288,   3.6333],
        [  5.4631,  -6.8660],
        [  3.9977,  -6.4422],
        [-10.4051,   7.7200],
        [-10.0790,   7.9411],
        [ -8.7771,   7.3218],
        [  5.0327,  -6.5848],
        [ -8.0321,   3.9513],
        [  1.4664,  -3.2033],
        [  3.6187,  -5.1496],
        [ -4.5980,   2.7532],
        [ -7.6389,   6.1646],
        [  5.3409,  -7.0586],
        [  2.2833,  -4.8025],
        [  3.0691,  -6.3254]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.3497, 0.6503],
        [0.7749, 0.2251]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4397, 0.5603], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2163, 0.0834],
         [0.4698, 0.2788]],

        [[0.8043, 0.0916],
         [0.0277, 0.1850]],

        [[0.1282, 0.1064],
         [0.2773, 0.0886]],

        [[0.3799, 0.1065],
         [0.5871, 0.2293]],

        [[0.0482, 0.0932],
         [0.5150, 0.2726]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 1
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 12
Adjusted Rand Index: 0.573478092283831
time is 2
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 11
Adjusted Rand Index: 0.6045203687862557
time is 4
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.07655585947417852
Average Adjusted Rand Index: 0.8117606501029234
Iteration 0: Loss = -18214.48938557193
Iteration 10: Loss = -11322.181654103977
Iteration 20: Loss = -11317.605570857719
Iteration 30: Loss = -11264.477256865102
Iteration 40: Loss = -11041.895757875394
Iteration 50: Loss = -11040.97414792607
Iteration 60: Loss = -11041.011471602113
1
Iteration 70: Loss = -11041.012538203538
2
Iteration 80: Loss = -11041.012575508841
3
Stopping early at iteration 79 due to no improvement.
pi: tensor([[0.7287, 0.2713],
        [0.3146, 0.6854]], dtype=torch.float64)
alpha: tensor([0.5108, 0.4892])
beta: tensor([[[0.1923, 0.0840],
         [0.5419, 0.2918]],

        [[0.7099, 0.0994],
         [0.4415, 0.4131]],

        [[0.4593, 0.1068],
         [0.8314, 0.8533]],

        [[0.6768, 0.1035],
         [0.4413, 0.5817]],

        [[0.7395, 0.0936],
         [0.8124, 0.7701]]], dtype=torch.float64)
time is 0
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 1
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
time is 2
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 3
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9207675179163246
time is 4
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9368976526043716
Average Adjusted Rand Index: 0.9364753795053573
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18214.5402582853
Iteration 100: Loss = -11340.910619576835
Iteration 200: Loss = -11327.606463682128
Iteration 300: Loss = -11324.981162331203
Iteration 400: Loss = -11287.7804869618
Iteration 500: Loss = -11239.576347910557
Iteration 600: Loss = -11239.145412183685
Iteration 700: Loss = -11238.915655564591
Iteration 800: Loss = -11238.679582312297
Iteration 900: Loss = -11238.429235874051
Iteration 1000: Loss = -11238.089674929348
Iteration 1100: Loss = -11125.95355588823
Iteration 1200: Loss = -11041.392561773919
Iteration 1300: Loss = -11036.903264568025
Iteration 1400: Loss = -11036.706432569454
Iteration 1500: Loss = -11036.618141912944
Iteration 1600: Loss = -11036.539797750213
Iteration 1700: Loss = -11036.490735263711
Iteration 1800: Loss = -11036.463840778317
Iteration 1900: Loss = -11036.442135390625
Iteration 2000: Loss = -11036.421589600552
Iteration 2100: Loss = -11036.367022821052
Iteration 2200: Loss = -11036.341652363893
Iteration 2300: Loss = -11036.331323440007
Iteration 2400: Loss = -11036.32399269
Iteration 2500: Loss = -11036.318195241905
Iteration 2600: Loss = -11036.313351750712
Iteration 2700: Loss = -11036.309180900756
Iteration 2800: Loss = -11036.305590757507
Iteration 2900: Loss = -11036.30235659966
Iteration 3000: Loss = -11036.29924035023
Iteration 3100: Loss = -11036.295314378107
Iteration 3200: Loss = -11036.28248383785
Iteration 3300: Loss = -11036.280379431684
Iteration 3400: Loss = -11036.278482366419
Iteration 3500: Loss = -11036.276738618804
Iteration 3600: Loss = -11036.27511822276
Iteration 3700: Loss = -11036.273597043539
Iteration 3800: Loss = -11036.27202683068
Iteration 3900: Loss = -11036.270253123188
Iteration 4000: Loss = -11036.267205695134
Iteration 4100: Loss = -11036.251669275533
Iteration 4200: Loss = -11036.225784754173
Iteration 4300: Loss = -11036.224747523609
Iteration 4400: Loss = -11036.223915485138
Iteration 4500: Loss = -11036.222928373545
Iteration 4600: Loss = -11036.22025678851
Iteration 4700: Loss = -11036.218866322271
Iteration 4800: Loss = -11036.2182226293
Iteration 4900: Loss = -11036.217678393345
Iteration 5000: Loss = -11036.21723291641
Iteration 5100: Loss = -11036.216826198992
Iteration 5200: Loss = -11036.216440691333
Iteration 5300: Loss = -11036.215919025295
Iteration 5400: Loss = -11036.217700082705
1
Iteration 5500: Loss = -11036.212345599364
Iteration 5600: Loss = -11036.209506583611
Iteration 5700: Loss = -11036.203248547297
Iteration 5800: Loss = -11036.20241122941
Iteration 5900: Loss = -11036.202107618248
Iteration 6000: Loss = -11036.20170040741
Iteration 6100: Loss = -11036.201653866752
Iteration 6200: Loss = -11036.20127716183
Iteration 6300: Loss = -11036.201067906177
Iteration 6400: Loss = -11036.20096044936
Iteration 6500: Loss = -11036.200623811408
Iteration 6600: Loss = -11036.20034064723
Iteration 6700: Loss = -11036.199734734775
Iteration 6800: Loss = -11036.19907320562
Iteration 6900: Loss = -11036.200661290777
1
Iteration 7000: Loss = -11036.228707628903
2
Iteration 7100: Loss = -11036.198559489516
Iteration 7200: Loss = -11036.25595384998
1
Iteration 7300: Loss = -11036.198250898173
Iteration 7400: Loss = -11036.200342512151
1
Iteration 7500: Loss = -11036.2853861372
2
Iteration 7600: Loss = -11036.1978513707
Iteration 7700: Loss = -11036.195924811698
Iteration 7800: Loss = -11036.205332585518
1
Iteration 7900: Loss = -11036.19493733789
Iteration 8000: Loss = -11036.21893856375
1
Iteration 8100: Loss = -11036.194927269853
Iteration 8200: Loss = -11036.194810828056
Iteration 8300: Loss = -11036.195306604654
1
Iteration 8400: Loss = -11036.198964917035
2
Iteration 8500: Loss = -11036.19433531534
Iteration 8600: Loss = -11036.191914667143
Iteration 8700: Loss = -11036.191695866333
Iteration 8800: Loss = -11036.189454903852
Iteration 8900: Loss = -11036.163336941669
Iteration 9000: Loss = -11036.157119214215
Iteration 9100: Loss = -11036.157088489523
Iteration 9200: Loss = -11036.161681516814
1
Iteration 9300: Loss = -11036.15702958068
Iteration 9400: Loss = -11036.156886332088
Iteration 9500: Loss = -11036.196593100804
1
Iteration 9600: Loss = -11036.15605346504
Iteration 9700: Loss = -11036.155665970706
Iteration 9800: Loss = -11036.155403242947
Iteration 9900: Loss = -11036.180358285947
1
Iteration 10000: Loss = -11036.15696860433
2
Iteration 10100: Loss = -11036.15529372127
Iteration 10200: Loss = -11036.155148943744
Iteration 10300: Loss = -11036.248002117296
1
Iteration 10400: Loss = -11036.155008940766
Iteration 10500: Loss = -11036.169238248453
1
Iteration 10600: Loss = -11036.15494467864
Iteration 10700: Loss = -11036.155688535573
1
Iteration 10800: Loss = -11036.171146122653
2
Iteration 10900: Loss = -11036.15494033301
Iteration 11000: Loss = -11036.154620554527
Iteration 11100: Loss = -11036.292414512494
1
Iteration 11200: Loss = -11036.149348323914
Iteration 11300: Loss = -11036.150607078998
1
Iteration 11400: Loss = -11036.309393185455
2
Iteration 11500: Loss = -11036.149295712852
Iteration 11600: Loss = -11036.149609826522
1
Iteration 11700: Loss = -11036.149170398448
Iteration 11800: Loss = -11036.14913353829
Iteration 11900: Loss = -11036.149016884578
Iteration 12000: Loss = -11036.150462698712
1
Iteration 12100: Loss = -11036.230732745436
2
Iteration 12200: Loss = -11036.150657621056
3
Iteration 12300: Loss = -11036.150092231324
4
Iteration 12400: Loss = -11036.166581657319
5
Iteration 12500: Loss = -11036.167352536328
6
Iteration 12600: Loss = -11036.179552157584
7
Iteration 12700: Loss = -11036.146714908133
Iteration 12800: Loss = -11036.147404264193
1
Iteration 12900: Loss = -11036.14667337866
Iteration 13000: Loss = -11036.177068357822
1
Iteration 13100: Loss = -11036.144700264602
Iteration 13200: Loss = -11036.150213204335
1
Iteration 13300: Loss = -11036.144726695053
2
Iteration 13400: Loss = -11036.147447693183
3
Iteration 13500: Loss = -11036.276975546483
4
Iteration 13600: Loss = -11036.144665527656
Iteration 13700: Loss = -11036.145124013321
1
Iteration 13800: Loss = -11036.172330220614
2
Iteration 13900: Loss = -11036.153304105414
3
Iteration 14000: Loss = -11036.154758507293
4
Iteration 14100: Loss = -11036.14451159402
Iteration 14200: Loss = -11036.137208728118
Iteration 14300: Loss = -11036.192908983614
1
Iteration 14400: Loss = -11036.13769980516
2
Iteration 14500: Loss = -11036.137154181739
Iteration 14600: Loss = -11036.146688235182
1
Iteration 14700: Loss = -11036.14171854581
2
Iteration 14800: Loss = -11036.136815261272
Iteration 14900: Loss = -11036.143526780268
1
Iteration 15000: Loss = -11036.147065625128
2
Iteration 15100: Loss = -11036.137720942352
3
Iteration 15200: Loss = -11036.136845047347
4
Iteration 15300: Loss = -11036.142741939802
5
Iteration 15400: Loss = -11036.15730135831
6
Iteration 15500: Loss = -11036.136790393784
Iteration 15600: Loss = -11036.138170868522
1
Iteration 15700: Loss = -11036.136770646948
Iteration 15800: Loss = -11036.138248317879
1
Iteration 15900: Loss = -11036.136905704187
2
Iteration 16000: Loss = -11036.137442141246
3
Iteration 16100: Loss = -11036.242000615632
4
Iteration 16200: Loss = -11036.136910368148
5
Iteration 16300: Loss = -11036.13677871884
6
Iteration 16400: Loss = -11036.220420087546
7
Iteration 16500: Loss = -11036.136689596215
Iteration 16600: Loss = -11036.154131282017
1
Iteration 16700: Loss = -11036.136701604833
2
Iteration 16800: Loss = -11036.137162001292
3
Iteration 16900: Loss = -11036.136697148875
4
Iteration 17000: Loss = -11036.13700935346
5
Iteration 17100: Loss = -11036.136694165323
6
Iteration 17200: Loss = -11036.13714401215
7
Iteration 17300: Loss = -11036.136697009923
8
Iteration 17400: Loss = -11036.136816609658
9
Iteration 17500: Loss = -11036.167343571136
10
Stopping early at iteration 17500 due to no improvement.
tensor([[  1.6067,  -3.5504],
        [ -8.1060,   6.4766],
        [ -7.6231,   6.2119],
        [ -7.5606,   6.1734],
        [-10.7097,   9.3152],
        [ -6.4074,   2.8812],
        [ -9.9813,   8.5399],
        [  5.6579,  -7.0505],
        [ -8.3820,   6.9907],
        [ -6.3662,   4.2156],
        [  3.0711,  -6.9294],
        [ -3.6350,   1.9109],
        [  3.2022,  -4.7938],
        [  4.7474,  -6.3809],
        [ -6.9636,   2.7113],
        [ -9.2622,   7.5336],
        [ -9.5532,   8.1321],
        [-10.2176,   8.8305],
        [ -9.5003,   7.1368],
        [ -8.9555,   6.9682],
        [-11.9318,   7.3166],
        [ -9.1873,   6.2737],
        [ -9.7036,   8.1620],
        [ -8.8051,   7.1386],
        [  6.4548,  -8.4334],
        [ -9.8340,   8.4419],
        [  2.5860,  -6.5506],
        [ -6.0668,   4.6752],
        [ -8.4593,   7.0316],
        [ -5.9578,   4.3975],
        [  4.0164,  -5.4344],
        [  4.5119,  -5.9971],
        [  0.5127,  -2.0195],
        [-10.7558,   7.8627],
        [ -6.4448,   4.9677],
        [  2.4686,  -4.0246],
        [ -5.3982,   3.7170],
        [  6.1902,  -8.3101],
        [  4.5481,  -6.0481],
        [ -9.6325,   8.2339],
        [  7.3691, -10.6965],
        [ -9.9964,   8.2928],
        [ -1.0084,  -0.7335],
        [  5.1082,  -6.7194],
        [  0.8285,  -2.2463],
        [-10.7604,   8.7523],
        [ -6.5482,   5.0844],
        [  7.3356,  -8.8492],
        [ -6.9373,   4.5436],
        [  2.5070,  -4.7721],
        [  5.4120,  -7.6267],
        [  1.6985,  -4.6716],
        [  2.5960,  -5.1779],
        [ -4.1123,   2.7241],
        [  0.4562,  -1.8790],
        [ -6.7852,   4.5908],
        [  6.5041,  -7.9325],
        [ -5.4059,   3.9539],
        [  2.8201,  -4.3818],
        [ -0.3750,  -1.0131],
        [  6.2354, -10.6775],
        [ -8.3754,   6.7979],
        [ -7.5892,   5.0428],
        [  2.6199,  -4.1475],
        [ -3.7996,   1.0408],
        [ -9.9065,   8.1939],
        [-10.6231,   8.5743],
        [  3.1749,  -5.2812],
        [ -3.3697,   1.9462],
        [ -6.5975,   2.8411],
        [  6.0919,  -8.9111],
        [-10.2073,   8.7846],
        [-10.5494,   8.6920],
        [  5.7590,  -7.4870],
        [-10.6368,   9.1486],
        [  5.5305,  -6.9518],
        [ -4.3683,   2.9373],
        [ -5.7564,   3.4686],
        [  0.5235,  -2.4675],
        [  2.1728,  -4.3859],
        [  1.6624,  -3.1850],
        [  4.6311,  -6.0861],
        [ -9.4609,   7.3308],
        [  7.1902,  -8.8787],
        [  1.5430,  -6.1582],
        [ -4.0798,   2.5062],
        [  6.1018,  -7.6434],
        [  4.9873,  -6.4136],
        [-10.8008,   9.1027],
        [-11.1335,   8.6441],
        [-11.2624,   8.5010],
        [  5.5227,  -6.9454],
        [-12.1797,   7.5645],
        [  2.1914,  -3.6378],
        [  4.8365,  -6.3355],
        [ -4.1575,   2.4506],
        [ -6.9304,   5.3607],
        [  8.5379, -11.0128],
        [  2.7060,  -5.0755],
        [  4.6528,  -6.1901]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7497, 0.2503],
        [0.2941, 0.7059]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4483, 0.5517], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1962, 0.0843],
         [0.5419, 0.2968]],

        [[0.7099, 0.0984],
         [0.4415, 0.4131]],

        [[0.4593, 0.1072],
         [0.8314, 0.8533]],

        [[0.6768, 0.1049],
         [0.4413, 0.5817]],

        [[0.7395, 0.0932],
         [0.8124, 0.7701]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 1
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
time is 2
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8823435719624108
time is 4
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.929154120624323
Average Adjusted Rand Index: 0.9287906401706234
Iteration 0: Loss = -14322.721719083296
Iteration 10: Loss = -11321.888106950631
Iteration 20: Loss = -11316.974873980987
Iteration 30: Loss = -11250.82141829037
Iteration 40: Loss = -11040.811404373819
Iteration 50: Loss = -11040.981812811999
1
Iteration 60: Loss = -11041.011673579627
2
Iteration 70: Loss = -11041.012551824733
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[0.7287, 0.2713],
        [0.3146, 0.6854]], dtype=torch.float64)
alpha: tensor([0.5108, 0.4892])
beta: tensor([[[0.1923, 0.0840],
         [0.4727, 0.2918]],

        [[0.3872, 0.0994],
         [0.6390, 0.0809]],

        [[0.4208, 0.1068],
         [0.2245, 0.2215]],

        [[0.4309, 0.1035],
         [0.6517, 0.7415]],

        [[0.4893, 0.0936],
         [0.5904, 0.7681]]], dtype=torch.float64)
time is 0
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 1
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
time is 2
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 3
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9207675179163246
time is 4
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9368976526043716
Average Adjusted Rand Index: 0.9364753795053573
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -14322.472456651156
Iteration 100: Loss = -11329.226648790196
Iteration 200: Loss = -11325.218313267469
Iteration 300: Loss = -11316.439176402524
Iteration 400: Loss = -11049.817302822203
Iteration 500: Loss = -11043.52157298086
Iteration 600: Loss = -11040.723801011007
Iteration 700: Loss = -11040.119509614426
Iteration 800: Loss = -11037.813840832707
Iteration 900: Loss = -11037.617758004842
Iteration 1000: Loss = -11037.572628436552
Iteration 1100: Loss = -11037.52410650293
Iteration 1200: Loss = -11037.486430070147
Iteration 1300: Loss = -11037.470623381647
Iteration 1400: Loss = -11037.458535284326
Iteration 1500: Loss = -11037.447974711164
Iteration 1600: Loss = -11037.437600155765
Iteration 1700: Loss = -11036.299229027143
Iteration 1800: Loss = -11036.258206554043
Iteration 1900: Loss = -11036.253240073285
Iteration 2000: Loss = -11036.247786426711
Iteration 2100: Loss = -11036.212004105
Iteration 2200: Loss = -11036.20266369621
Iteration 2300: Loss = -11036.193233520129
Iteration 2400: Loss = -11036.159796780856
Iteration 2500: Loss = -11036.155946017723
Iteration 2600: Loss = -11036.154122441218
Iteration 2700: Loss = -11036.152713921367
Iteration 2800: Loss = -11036.171774701254
1
Iteration 2900: Loss = -11036.150260650154
Iteration 3000: Loss = -11036.1492893669
Iteration 3100: Loss = -11036.148315685965
Iteration 3200: Loss = -11036.147460616421
Iteration 3300: Loss = -11036.14768903519
1
Iteration 3400: Loss = -11036.145759505238
Iteration 3500: Loss = -11036.144887441713
Iteration 3600: Loss = -11036.144051246478
Iteration 3700: Loss = -11036.143305065016
Iteration 3800: Loss = -11036.145899689933
1
Iteration 3900: Loss = -11036.141914122847
Iteration 4000: Loss = -11036.146251977196
1
Iteration 4100: Loss = -11036.146144981747
2
Iteration 4200: Loss = -11036.141525534513
Iteration 4300: Loss = -11036.152169848448
1
Iteration 4400: Loss = -11036.139387713438
Iteration 4500: Loss = -11036.138961078099
Iteration 4600: Loss = -11036.139930494188
1
Iteration 4700: Loss = -11036.140027412455
2
Iteration 4800: Loss = -11036.148575000276
3
Iteration 4900: Loss = -11036.13794662459
Iteration 5000: Loss = -11036.137766008058
Iteration 5100: Loss = -11036.167527413747
1
Iteration 5200: Loss = -11036.149781110096
2
Iteration 5300: Loss = -11036.147297795185
3
Iteration 5400: Loss = -11036.137217046498
Iteration 5500: Loss = -11036.149758594967
1
Iteration 5600: Loss = -11036.134769289756
Iteration 5700: Loss = -11036.133253931062
Iteration 5800: Loss = -11036.129820774291
Iteration 5900: Loss = -11036.129374059687
Iteration 6000: Loss = -11036.13445640901
1
Iteration 6100: Loss = -11036.13373885858
2
Iteration 6200: Loss = -11036.129043015719
Iteration 6300: Loss = -11036.128928444161
Iteration 6400: Loss = -11036.129542678726
1
Iteration 6500: Loss = -11036.134009950478
2
Iteration 6600: Loss = -11036.134901226433
3
Iteration 6700: Loss = -11036.148386741868
4
Iteration 6800: Loss = -11036.155963535546
5
Iteration 6900: Loss = -11036.13322997262
6
Iteration 7000: Loss = -11036.129538686959
7
Iteration 7100: Loss = -11036.128912350276
Iteration 7200: Loss = -11036.128876466202
Iteration 7300: Loss = -11036.126607655095
Iteration 7400: Loss = -11036.128955139697
1
Iteration 7500: Loss = -11036.19646093734
2
Iteration 7600: Loss = -11036.125164004143
Iteration 7700: Loss = -11036.127854901126
1
Iteration 7800: Loss = -11036.125055565444
Iteration 7900: Loss = -11036.12971427449
1
Iteration 8000: Loss = -11036.13640122652
2
Iteration 8100: Loss = -11036.125005491522
Iteration 8200: Loss = -11036.128755998177
1
Iteration 8300: Loss = -11036.124913005833
Iteration 8400: Loss = -11036.130325930959
1
Iteration 8500: Loss = -11036.12489438503
Iteration 8600: Loss = -11036.129575024272
1
Iteration 8700: Loss = -11036.124878977394
Iteration 8800: Loss = -11036.138190132051
1
Iteration 8900: Loss = -11036.12486088788
Iteration 9000: Loss = -11036.167113839661
1
Iteration 9100: Loss = -11036.124782055133
Iteration 9200: Loss = -11036.148070787727
1
Iteration 9300: Loss = -11036.156193657926
2
Iteration 9400: Loss = -11036.12511073775
3
Iteration 9500: Loss = -11036.12689713433
4
Iteration 9600: Loss = -11036.158512611677
5
Iteration 9700: Loss = -11036.124635580481
Iteration 9800: Loss = -11036.137433885908
1
Iteration 9900: Loss = -11036.124685300334
2
Iteration 10000: Loss = -11036.124672053329
3
Iteration 10100: Loss = -11036.13058405082
4
Iteration 10200: Loss = -11036.170721528057
5
Iteration 10300: Loss = -11036.124546168556
Iteration 10400: Loss = -11036.127732638663
1
Iteration 10500: Loss = -11036.124527201793
Iteration 10600: Loss = -11036.127968603927
1
Iteration 10700: Loss = -11036.124534600709
2
Iteration 10800: Loss = -11036.128082375908
3
Iteration 10900: Loss = -11036.124589723568
4
Iteration 11000: Loss = -11036.124570927002
5
Iteration 11100: Loss = -11036.190837204937
6
Iteration 11200: Loss = -11036.124516901558
Iteration 11300: Loss = -11036.12544307335
1
Iteration 11400: Loss = -11036.124499419133
Iteration 11500: Loss = -11036.124475771923
Iteration 11600: Loss = -11036.125790630442
1
Iteration 11700: Loss = -11036.124578812743
2
Iteration 11800: Loss = -11036.170864589634
3
Iteration 11900: Loss = -11036.124539336408
4
Iteration 12000: Loss = -11036.12459161045
5
Iteration 12100: Loss = -11036.23565035658
6
Iteration 12200: Loss = -11036.124626415685
7
Iteration 12300: Loss = -11036.12576697173
8
Iteration 12400: Loss = -11036.124663506622
9
Iteration 12500: Loss = -11036.124693212096
10
Stopping early at iteration 12500 due to no improvement.
tensor([[ 1.7475, -3.3995],
        [-8.1432,  6.4453],
        [-8.0413,  5.8009],
        [-7.4592,  6.0724],
        [-9.1340,  7.7292],
        [-5.3368,  3.9501],
        [-9.1784,  7.3979],
        [ 5.0372, -7.6727],
        [-7.0909,  5.5957],
        [-6.0774,  4.5085],
        [ 4.1630, -5.8331],
        [-3.7772,  1.7686],
        [ 3.0155, -4.9718],
        [ 4.7860, -6.3653],
        [-5.9493,  3.7234],
        [-8.4639,  7.0310],
        [-8.9173,  7.1823],
        [-9.4000,  7.8500],
        [-8.6188,  6.7022],
        [-9.4176,  5.9762],
        [-8.9710,  7.5770],
        [-9.8970,  5.2817],
        [-8.7077,  7.0567],
        [-7.7118,  6.2344],
        [ 6.4363, -7.8238],
        [-8.7930,  5.3575],
        [ 3.7604, -5.3733],
        [-6.1695,  4.5724],
        [-8.8218,  6.0660],
        [-6.3505,  4.0046],
        [ 3.8655, -5.5846],
        [ 4.1842, -6.3380],
        [-0.7745, -3.2970],
        [-7.3585,  5.8659],
        [-7.2581,  4.1538],
        [ 2.5208, -3.9647],
        [-5.4784,  3.6381],
        [ 5.8748, -8.2035],
        [ 4.5713, -6.0241],
        [-8.8842,  7.1326],
        [ 7.3380, -9.9470],
        [-9.1534,  6.9799],
        [-1.7076, -1.4255],
        [ 5.2012, -6.6360],
        [ 0.5689, -2.4985],
        [-8.7470,  7.2849],
        [-6.6421,  4.9862],
        [ 6.5369, -8.0123],
        [-6.4625,  5.0326],
        [ 1.8460, -5.4208],
        [ 4.9223, -8.1254],
        [ 2.3344, -4.0262],
        [ 2.9369, -4.8300],
        [-5.0702,  1.7623],
        [ 0.4713, -1.8588],
        [-6.5072,  4.8755],
        [ 6.1715, -8.2005],
        [-5.9510,  3.4101],
        [ 2.7851, -4.4106],
        [-0.4620, -1.0946],
        [ 5.9280, -9.3319],
        [-6.2900,  4.8102],
        [-7.2170,  5.4355],
        [ 2.5672, -4.1892],
        [-3.2345,  1.6098],
        [-5.8482,  4.4119],
        [-9.2857,  7.8858],
        [ 3.4616, -4.9841],
        [-3.4203,  1.8944],
        [-5.4950,  3.9432],
        [ 6.3125, -7.7301],
        [-8.3677,  6.9552],
        [-9.2216,  7.6884],
        [ 5.7767, -7.2333],
        [-8.5399,  7.1156],
        [ 5.3074, -7.1788],
        [-4.6870,  2.6149],
        [-5.6499,  3.5731],
        [ 0.4519, -2.5331],
        [ 1.9009, -4.6467],
        [ 1.7208, -3.1194],
        [ 4.6705, -6.0581],
        [-8.3155,  6.7740],
        [ 6.7824, -8.6331],
        [ 2.8882, -4.8031],
        [-4.2435,  2.3465],
        [ 6.0403, -8.9839],
        [ 4.8682, -6.5317],
        [-9.3289,  7.9308],
        [-9.2360,  7.2495],
        [-8.5997,  6.9303],
        [ 5.4269, -7.0133],
        [-6.5541,  4.6722],
        [ 2.1657, -3.6551],
        [ 4.8702, -6.3013],
        [-4.0226,  2.5856],
        [-7.9226,  4.4041],
        [ 7.6271, -9.5068],
        [ 3.1464, -4.6251],
        [ 4.3754, -6.4667]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7520, 0.2480],
        [0.2933, 0.7067]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4483, 0.5517], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1953, 0.0838],
         [0.4727, 0.2980]],

        [[0.3872, 0.0989],
         [0.6390, 0.0809]],

        [[0.4208, 0.1069],
         [0.2245, 0.2215]],

        [[0.4309, 0.1052],
         [0.6517, 0.7415]],

        [[0.4893, 0.0936],
         [0.5904, 0.7681]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 1
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
time is 2
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8823435719624108
time is 4
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.929154120624323
Average Adjusted Rand Index: 0.9287906401706234
Iteration 0: Loss = -18492.581395265806
Iteration 10: Loss = -11322.944159500248
Iteration 20: Loss = -11319.71884629491
Iteration 30: Loss = -11297.788746431379
Iteration 40: Loss = -11102.96766225449
Iteration 50: Loss = -11040.892298369794
Iteration 60: Loss = -11041.008650749867
1
Iteration 70: Loss = -11041.01250019507
2
Iteration 80: Loss = -11041.012572572337
3
Stopping early at iteration 79 due to no improvement.
pi: tensor([[0.7287, 0.2713],
        [0.3146, 0.6854]], dtype=torch.float64)
alpha: tensor([0.5108, 0.4892])
beta: tensor([[[0.1923, 0.0840],
         [0.0748, 0.2918]],

        [[0.7128, 0.0994],
         [0.4045, 0.5984]],

        [[0.3893, 0.1068],
         [0.5684, 0.2278]],

        [[0.6956, 0.1035],
         [0.5297, 0.0410]],

        [[0.1063, 0.0936],
         [0.8010, 0.6794]]], dtype=torch.float64)
time is 0
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 1
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
time is 2
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 3
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9207675179163246
time is 4
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
Global Adjusted Rand Index: 0.9368976526043716
Average Adjusted Rand Index: 0.9364753795053573
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18492.332112865184
Iteration 100: Loss = -11327.160247480037
Iteration 200: Loss = -11312.514194024925
Iteration 300: Loss = -11242.206894317791
Iteration 400: Loss = -11208.19597768982
Iteration 500: Loss = -11202.430650109518
Iteration 600: Loss = -11201.702831178987
Iteration 700: Loss = -11193.393984740558
Iteration 800: Loss = -11191.284974530816
Iteration 900: Loss = -11190.801753231402
Iteration 1000: Loss = -11188.185918423116
Iteration 1100: Loss = -11188.10407187136
Iteration 1200: Loss = -11188.050774306063
Iteration 1300: Loss = -11187.958614104904
Iteration 1400: Loss = -11187.534540763687
Iteration 1500: Loss = -11187.493298944153
Iteration 1600: Loss = -11187.455324806073
Iteration 1700: Loss = -11187.44310767268
Iteration 1800: Loss = -11187.433460228698
Iteration 1900: Loss = -11187.42544823721
Iteration 2000: Loss = -11187.418484382348
Iteration 2100: Loss = -11187.408714226292
Iteration 2200: Loss = -11187.00610052942
Iteration 2300: Loss = -11186.995907160048
Iteration 2400: Loss = -11186.991257146008
Iteration 2500: Loss = -11186.979069019219
Iteration 2600: Loss = -11186.954670762889
Iteration 2700: Loss = -11186.897388683392
Iteration 2800: Loss = -11186.89283652189
Iteration 2900: Loss = -11186.890008973041
Iteration 3000: Loss = -11186.885602526772
Iteration 3100: Loss = -11186.755555607147
Iteration 3200: Loss = -11186.748626460543
Iteration 3300: Loss = -11186.74612139628
Iteration 3400: Loss = -11186.744052160166
Iteration 3500: Loss = -11186.743410759927
Iteration 3600: Loss = -11186.658379710207
Iteration 3700: Loss = -11183.119264858104
Iteration 3800: Loss = -11183.064172467703
Iteration 3900: Loss = -11183.04769335588
Iteration 4000: Loss = -11183.041177524863
Iteration 4100: Loss = -11183.029539189227
Iteration 4200: Loss = -11183.029059680617
Iteration 4300: Loss = -11183.013674789097
Iteration 4400: Loss = -11183.01343805121
Iteration 4500: Loss = -11183.011849412791
Iteration 4600: Loss = -11183.009347969628
Iteration 4700: Loss = -11183.009595814923
1
Iteration 4800: Loss = -11183.009711178269
2
Iteration 4900: Loss = -11183.008030302082
Iteration 5000: Loss = -11183.006981536306
Iteration 5100: Loss = -11182.993393844887
Iteration 5200: Loss = -11182.992677945364
Iteration 5300: Loss = -11182.99288515084
1
Iteration 5400: Loss = -11182.991747672708
Iteration 5500: Loss = -11182.99258768483
1
Iteration 5600: Loss = -11182.990835543975
Iteration 5700: Loss = -11183.010218678697
1
Iteration 5800: Loss = -11182.989945672818
Iteration 5900: Loss = -11182.989542810927
Iteration 6000: Loss = -11182.988338933183
Iteration 6100: Loss = -11182.982659170522
Iteration 6200: Loss = -11182.981909123699
Iteration 6300: Loss = -11182.981602584721
Iteration 6400: Loss = -11182.991317344919
1
Iteration 6500: Loss = -11182.979829818214
Iteration 6600: Loss = -11182.980725839026
1
Iteration 6700: Loss = -11182.978966213677
Iteration 6800: Loss = -11182.98677623406
1
Iteration 6900: Loss = -11182.98381816359
2
Iteration 7000: Loss = -11182.985641301331
3
Iteration 7100: Loss = -11182.985421628819
4
Iteration 7200: Loss = -11183.007632955076
5
Iteration 7300: Loss = -11182.983443866393
6
Iteration 7400: Loss = -11182.976243660456
Iteration 7500: Loss = -11182.97668933377
1
Iteration 7600: Loss = -11182.975813799689
Iteration 7700: Loss = -11182.9756977674
Iteration 7800: Loss = -11182.975539107327
Iteration 7900: Loss = -11182.975473592804
Iteration 8000: Loss = -11182.975403250588
Iteration 8100: Loss = -11182.976635131798
1
Iteration 8200: Loss = -11182.975253659275
Iteration 8300: Loss = -11182.975152001309
Iteration 8400: Loss = -11182.97501494703
Iteration 8500: Loss = -11182.974824607847
Iteration 8600: Loss = -11183.437006428554
1
Iteration 8700: Loss = -11182.974340485585
Iteration 8800: Loss = -11182.974156401307
Iteration 8900: Loss = -11183.165155286511
1
Iteration 9000: Loss = -11182.974085286165
Iteration 9100: Loss = -11182.974045646662
Iteration 9200: Loss = -11183.046690432488
1
Iteration 9300: Loss = -11182.973924138412
Iteration 9400: Loss = -11182.97388916265
Iteration 9500: Loss = -11182.976955903956
1
Iteration 9600: Loss = -11182.972465481207
Iteration 9700: Loss = -11182.97239216499
Iteration 9800: Loss = -11182.98069304305
1
Iteration 9900: Loss = -11182.972251808393
Iteration 10000: Loss = -11182.972198908732
Iteration 10100: Loss = -11182.972371139007
1
Iteration 10200: Loss = -11182.972121760096
Iteration 10300: Loss = -11182.974419985423
1
Iteration 10400: Loss = -11182.972099774319
Iteration 10500: Loss = -11182.975390212374
1
Iteration 10600: Loss = -11182.97177494366
Iteration 10700: Loss = -11182.97143356465
Iteration 10800: Loss = -11182.971510909709
1
Iteration 10900: Loss = -11182.959461873425
Iteration 11000: Loss = -11182.959630791604
1
Iteration 11100: Loss = -11182.958996866244
Iteration 11200: Loss = -11182.967727265208
1
Iteration 11300: Loss = -11182.958899909652
Iteration 11400: Loss = -11182.961879004048
1
Iteration 11500: Loss = -11182.958836613152
Iteration 11600: Loss = -11182.958780319934
Iteration 11700: Loss = -11182.981071048982
1
Iteration 11800: Loss = -11182.958743380435
Iteration 11900: Loss = -11182.9587432022
Iteration 12000: Loss = -11182.959242918474
1
Iteration 12100: Loss = -11182.95864299445
Iteration 12200: Loss = -11183.25227821438
1
Iteration 12300: Loss = -11182.958615218391
Iteration 12400: Loss = -11182.958619419996
1
Iteration 12500: Loss = -11182.972995263362
2
Iteration 12600: Loss = -11182.958606055114
Iteration 12700: Loss = -11182.958617751608
1
Iteration 12800: Loss = -11182.958871980285
2
Iteration 12900: Loss = -11182.958557533782
Iteration 13000: Loss = -11182.971559789945
1
Iteration 13100: Loss = -11182.958544560957
Iteration 13200: Loss = -11182.958506896042
Iteration 13300: Loss = -11182.957926392764
Iteration 13400: Loss = -11182.95359840114
Iteration 13500: Loss = -11182.978173297395
1
Iteration 13600: Loss = -11182.955320419016
2
Iteration 13700: Loss = -11182.966735133352
3
Iteration 13800: Loss = -11182.953504730556
Iteration 13900: Loss = -11182.956572846539
1
Iteration 14000: Loss = -11182.95241210738
Iteration 14100: Loss = -11182.955994021038
1
Iteration 14200: Loss = -11182.96179743028
2
Iteration 14300: Loss = -11182.952236078327
Iteration 14400: Loss = -11182.95239348037
1
Iteration 14500: Loss = -11182.981509451869
2
Iteration 14600: Loss = -11182.95218155541
Iteration 14700: Loss = -11183.108124377864
1
Iteration 14800: Loss = -11182.954158557335
2
Iteration 14900: Loss = -11182.95233575884
3
Iteration 15000: Loss = -11182.952182011853
4
Iteration 15100: Loss = -11182.95216074371
Iteration 15200: Loss = -11182.952724596755
1
Iteration 15300: Loss = -11182.952152657843
Iteration 15400: Loss = -11182.95214354151
Iteration 15500: Loss = -11182.952411322274
1
Iteration 15600: Loss = -11182.952153651677
2
Iteration 15700: Loss = -11182.952173816675
3
Iteration 15800: Loss = -11182.95281675589
4
Iteration 15900: Loss = -11182.952139658622
Iteration 16000: Loss = -11182.952163187949
1
Iteration 16100: Loss = -11182.953678794403
2
Iteration 16200: Loss = -11182.952172257515
3
Iteration 16300: Loss = -11183.224097110893
4
Iteration 16400: Loss = -11182.952139793753
5
Iteration 16500: Loss = -11182.952143129307
6
Iteration 16600: Loss = -11182.952161491949
7
Iteration 16700: Loss = -11182.957353899576
8
Iteration 16800: Loss = -11182.952599365253
9
Iteration 16900: Loss = -11182.952231225001
10
Stopping early at iteration 16900 due to no improvement.
tensor([[  5.9355,  -8.5210],
        [  5.9970,  -7.3935],
        [  5.5557,  -7.1383],
        [  6.3970,  -8.4182],
        [  4.9415,  -6.5281],
        [  6.5376,  -8.0932],
        [  3.6029,  -6.1467],
        [  6.7280,  -8.1174],
        [  6.1589,  -7.6364],
        [  4.9710,  -6.9818],
        [  6.6926,  -8.0953],
        [  6.3526,  -7.9353],
        [  6.7248,  -8.3881],
        [  6.1420,  -7.9935],
        [  6.5802,  -8.3660],
        [  3.8955,  -5.3637],
        [  6.2274,  -7.6420],
        [  4.2901,  -5.6964],
        [  7.9505, -10.2445],
        [  5.9660,  -7.4146],
        [  5.6801,  -8.7222],
        [  5.3834,  -6.8209],
        [  5.4546,  -6.9776],
        [  6.3850,  -7.8086],
        [  6.8337,  -8.9877],
        [  6.3355,  -7.8522],
        [  7.3213,  -8.8167],
        [  6.5632,  -8.0023],
        [  5.9705,  -7.9394],
        [  6.4269,  -7.8388],
        [  7.4631,  -9.0418],
        [  7.4057,  -8.9313],
        [  7.1449,  -8.5403],
        [  6.6905, -10.7536],
        [  5.5537,  -7.1328],
        [  7.3334,  -8.8758],
        [  6.1028,  -7.4922],
        [  7.3952,  -8.9686],
        [  6.8661,  -8.2971],
        [  5.7151,  -7.1882],
        [  6.6016,  -9.2277],
        [  5.9356,  -7.3676],
        [  6.8268,  -8.3612],
        [  7.4756,  -8.9146],
        [  6.6236,  -8.3566],
        [  4.7979,  -6.6707],
        [  6.4156,  -7.9326],
        [  7.1851,  -9.1997],
        [  5.5180,  -7.1556],
        [  6.1211,  -7.7552],
        [  7.4507,  -9.3501],
        [  7.0869,  -8.4981],
        [  6.4751,  -7.8640],
        [  5.7829,  -7.5891],
        [  6.9297,  -8.4038],
        [  5.6713,  -7.0650],
        [  6.5074,  -8.0304],
        [  5.1416,  -6.5807],
        [  7.1531,  -8.6642],
        [  6.7584,  -8.3737],
        [  6.5789,  -9.2243],
        [  6.4316,  -7.8552],
        [  6.4920,  -8.0623],
        [  6.1769,  -8.4393],
        [  6.7821,  -8.2858],
        [  4.2742,  -5.7137],
        [  3.3787,  -4.8906],
        [  7.0695,  -8.5686],
        [  5.3604,  -7.0693],
        [  6.2148,  -7.7883],
        [  6.7655,  -9.6303],
        [  6.0118,  -7.8966],
        [  6.1401,  -7.7195],
        [  7.3104,  -8.8571],
        [  5.6255,  -7.8148],
        [  7.4249,  -8.8178],
        [  5.2983,  -8.1096],
        [  6.2714,  -7.7494],
        [  7.6490,  -9.6719],
        [  6.1616,  -8.6432],
        [  6.9706,  -8.9190],
        [  6.1772,  -8.9320],
        [  4.0266,  -7.7035],
        [  7.1077,  -8.5503],
        [  6.4652,  -9.3173],
        [  6.2120,  -7.6640],
        [  7.2377,  -8.9288],
        [  7.1181,  -8.5201],
        [  3.1428,  -5.8461],
        [  5.2362,  -6.7142],
        [  5.3815,  -8.3661],
        [  7.3371,  -9.8901],
        [  7.9378, -10.1332],
        [  6.7867,  -8.2933],
        [  7.2003,  -8.6251],
        [  7.0050,  -8.4281],
        [  5.5823,  -8.2612],
        [  7.1195,  -9.3407],
        [  6.5117,  -7.9018],
        [  7.4078,  -8.7956]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.4226, 0.5774],
        [0.6029, 0.3971]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9999e-01, 7.4306e-06], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1865, 0.2733],
         [0.0748, 0.2791]],

        [[0.7128, 0.0946],
         [0.4045, 0.5984]],

        [[0.3893, 0.1067],
         [0.5684, 0.2278]],

        [[0.6956, 0.1078],
         [0.5297, 0.0410]],

        [[0.1063, 0.0934],
         [0.8010, 0.6794]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 93
Adjusted Rand Index: 0.7369913366172994
time is 2
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824177928584268
time is 3
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 12
Adjusted Rand Index: 0.573395733075917
time is 4
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.14889697714099973
Average Adjusted Rand Index: 0.6385609725103286
11060.163568680035
new:  [0.07655585947417852, 0.929154120624323, 0.929154120624323, 0.14889697714099973] [0.8117606501029234, 0.9287906401706234, 0.9287906401706234, 0.6385609725103286] [11081.554319818362, 11036.167343571136, 11036.124693212096, 11182.952231225001]
prior:  [0.9368976526043716, 0.9368976526043716, 0.9368976526043716, 0.9368976526043716] [0.9364753795053573, 0.9364753795053573, 0.9364753795053573, 0.9364753795053573] [11041.012564140357, 11041.012575508841, 11041.012551824733, 11041.012572572337]
-----------------------------------------------------------------------------------------
This iteration is 13
True Objective function: Loss = -10989.752095543823
Iteration 0: Loss = -47732.12587525013
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.3066,    nan]],

        [[0.0504,    nan],
         [0.0345, 0.6915]],

        [[0.3512,    nan],
         [0.1757, 0.9518]],

        [[0.6919,    nan],
         [0.9808, 0.6882]],

        [[0.8949,    nan],
         [0.6192, 0.6507]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -46458.77320419125
Iteration 100: Loss = -11308.46102226848
Iteration 200: Loss = -11305.69483068991
Iteration 300: Loss = -11304.66840424023
Iteration 400: Loss = -11303.937749161818
Iteration 500: Loss = -11302.822191375411
Iteration 600: Loss = -11301.866315080102
Iteration 700: Loss = -11301.540665018645
Iteration 800: Loss = -11301.278344767492
Iteration 900: Loss = -11301.060637319077
Iteration 1000: Loss = -11300.888733663562
Iteration 1100: Loss = -11300.7418842122
Iteration 1200: Loss = -11300.594184002122
Iteration 1300: Loss = -11300.416257515148
Iteration 1400: Loss = -11300.172171439164
Iteration 1500: Loss = -11299.487199792571
Iteration 1600: Loss = -11297.23523856772
Iteration 1700: Loss = -11294.604989402816
Iteration 1800: Loss = -11277.437338369271
Iteration 1900: Loss = -11252.345273584388
Iteration 2000: Loss = -11245.490423866351
Iteration 2100: Loss = -11238.788885800015
Iteration 2200: Loss = -11229.280848998904
Iteration 2300: Loss = -11226.635222057097
Iteration 2400: Loss = -11224.755568214801
Iteration 2500: Loss = -11188.629671119726
Iteration 2600: Loss = -11124.008799510524
Iteration 2700: Loss = -11114.644011295444
Iteration 2800: Loss = -11103.353164568744
Iteration 2900: Loss = -11089.857954686151
Iteration 3000: Loss = -11073.830511856819
Iteration 3100: Loss = -11068.470315104
Iteration 3200: Loss = -11068.304304099063
Iteration 3300: Loss = -11068.21660475473
Iteration 3400: Loss = -11068.153301757211
Iteration 3500: Loss = -11068.101030508333
Iteration 3600: Loss = -11068.054498020187
Iteration 3700: Loss = -11067.983993073123
Iteration 3800: Loss = -11067.792626436441
Iteration 3900: Loss = -11067.761214867916
Iteration 4000: Loss = -11067.743309411799
Iteration 4100: Loss = -11067.71453783257
Iteration 4200: Loss = -11067.52339807657
Iteration 4300: Loss = -11067.513940403618
Iteration 4400: Loss = -11067.504594482496
Iteration 4500: Loss = -11067.49544525616
Iteration 4600: Loss = -11067.48374331065
Iteration 4700: Loss = -11067.469192472412
Iteration 4800: Loss = -11067.458328106823
Iteration 4900: Loss = -11067.452451913932
Iteration 5000: Loss = -11067.44816930745
Iteration 5100: Loss = -11067.444247404892
Iteration 5200: Loss = -11067.440385537146
Iteration 5300: Loss = -11067.435098415699
Iteration 5400: Loss = -11067.429862997422
Iteration 5500: Loss = -11067.42495487788
Iteration 5600: Loss = -11067.427351121401
1
Iteration 5700: Loss = -11067.418959628154
Iteration 5800: Loss = -11067.421196975529
1
Iteration 5900: Loss = -11067.414057513895
Iteration 6000: Loss = -11067.411832568838
Iteration 6100: Loss = -11067.410109177226
Iteration 6200: Loss = -11067.407852372651
Iteration 6300: Loss = -11067.412922021322
1
Iteration 6400: Loss = -11067.398066804148
Iteration 6500: Loss = -11067.39677749668
Iteration 6600: Loss = -11067.414240890217
1
Iteration 6700: Loss = -11067.395474861429
Iteration 6800: Loss = -11067.407041863531
1
Iteration 6900: Loss = -11067.391931488488
Iteration 7000: Loss = -11067.397569024988
1
Iteration 7100: Loss = -11067.38675563659
Iteration 7200: Loss = -11067.380159604303
Iteration 7300: Loss = -11067.38073161465
1
Iteration 7400: Loss = -11067.377764558381
Iteration 7500: Loss = -11067.376582562447
Iteration 7600: Loss = -11067.0801043408
Iteration 7700: Loss = -11067.067136882439
Iteration 7800: Loss = -11066.166436154354
Iteration 7900: Loss = -11065.76838993928
Iteration 8000: Loss = -11065.752315837697
Iteration 8100: Loss = -11065.75261441684
1
Iteration 8200: Loss = -11065.747630286682
Iteration 8300: Loss = -11065.725096910372
Iteration 8400: Loss = -11065.722754264269
Iteration 8500: Loss = -11065.717943117335
Iteration 8600: Loss = -11065.719803249045
1
Iteration 8700: Loss = -11065.716888242643
Iteration 8800: Loss = -11065.71641482039
Iteration 8900: Loss = -11065.767316443642
1
Iteration 9000: Loss = -11065.713735736343
Iteration 9100: Loss = -11065.713587544547
Iteration 9200: Loss = -11065.746917680808
1
Iteration 9300: Loss = -11065.712554282729
Iteration 9400: Loss = -11065.713607101918
1
Iteration 9500: Loss = -11065.711397388113
Iteration 9600: Loss = -11065.71380922682
1
Iteration 9700: Loss = -11065.710923598037
Iteration 9800: Loss = -11065.711187734902
1
Iteration 9900: Loss = -11065.719746498498
2
Iteration 10000: Loss = -11065.710350619773
Iteration 10100: Loss = -11065.815170317373
1
Iteration 10200: Loss = -11065.709654919772
Iteration 10300: Loss = -11065.709904214986
1
Iteration 10400: Loss = -11065.709079475784
Iteration 10500: Loss = -11065.70993677933
1
Iteration 10600: Loss = -11065.79088515676
2
Iteration 10700: Loss = -11065.709542764898
3
Iteration 10800: Loss = -11065.708349410635
Iteration 10900: Loss = -11065.707823793025
Iteration 11000: Loss = -11065.708816260685
1
Iteration 11100: Loss = -11065.714285730313
2
Iteration 11200: Loss = -11065.7085330117
3
Iteration 11300: Loss = -11065.709798244297
4
Iteration 11400: Loss = -11065.70537197218
Iteration 11500: Loss = -11065.705567633531
1
Iteration 11600: Loss = -11065.704377460028
Iteration 11700: Loss = -11065.703002676297
Iteration 11800: Loss = -11065.70278684226
Iteration 11900: Loss = -11065.71451893558
1
Iteration 12000: Loss = -11065.702593940101
Iteration 12100: Loss = -11065.701661806701
Iteration 12200: Loss = -11065.701678839008
1
Iteration 12300: Loss = -11065.749324724284
2
Iteration 12400: Loss = -11065.700925242196
Iteration 12500: Loss = -11065.70173439581
1
Iteration 12600: Loss = -11065.70080850147
Iteration 12700: Loss = -11065.702066607993
1
Iteration 12800: Loss = -11065.700767178674
Iteration 12900: Loss = -11065.705592205184
1
Iteration 13000: Loss = -11065.70261800571
2
Iteration 13100: Loss = -11065.710320910779
3
Iteration 13200: Loss = -11065.7006165779
Iteration 13300: Loss = -11065.700300132967
Iteration 13400: Loss = -11065.708439497
1
Iteration 13500: Loss = -11065.709519722976
2
Iteration 13600: Loss = -11065.700112035058
Iteration 13700: Loss = -11065.700311163986
1
Iteration 13800: Loss = -11065.700080477065
Iteration 13900: Loss = -11066.052894478205
1
Iteration 14000: Loss = -11065.700005489862
Iteration 14100: Loss = -11065.777336371806
1
Iteration 14200: Loss = -11065.703671221052
2
Iteration 14300: Loss = -11065.700132834168
3
Iteration 14400: Loss = -11065.700089382888
4
Iteration 14500: Loss = -11065.702503247108
5
Iteration 14600: Loss = -11065.738971268787
6
Iteration 14700: Loss = -11065.699914747336
Iteration 14800: Loss = -11065.701880057179
1
Iteration 14900: Loss = -11065.699931667701
2
Iteration 15000: Loss = -11065.70006997631
3
Iteration 15100: Loss = -11065.699890424963
Iteration 15200: Loss = -11065.70028550471
1
Iteration 15300: Loss = -11065.700893130785
2
Iteration 15400: Loss = -11065.701528232788
3
Iteration 15500: Loss = -11065.701780646741
4
Iteration 15600: Loss = -11065.699868560465
Iteration 15700: Loss = -11065.700023288904
1
Iteration 15800: Loss = -11065.699880364626
2
Iteration 15900: Loss = -11065.700009725379
3
Iteration 16000: Loss = -11065.699924071376
4
Iteration 16100: Loss = -11065.699876697438
5
Iteration 16200: Loss = -11065.720429990479
6
Iteration 16300: Loss = -11065.70569004926
7
Iteration 16400: Loss = -11065.699870130255
8
Iteration 16500: Loss = -11065.69990983807
9
Iteration 16600: Loss = -11065.705959366427
10
Stopping early at iteration 16600 due to no improvement.
tensor([[ -9.9973,   5.3821],
        [-11.7769,   7.1617],
        [  2.5566,  -7.1718],
        [ -9.3893,   4.7741],
        [  2.6928,  -7.3080],
        [ -6.5421,   1.9269],
        [ -1.0612,  -3.5540],
        [  3.9232,  -8.5385],
        [ -8.9367,   4.3215],
        [  1.2801,  -5.8953],
        [ -9.3050,   4.6898],
        [-11.5276,   6.9124],
        [  1.0695,  -5.6847],
        [ -0.1340,  -4.4812],
        [ -9.9117,   5.2964],
        [-11.2717,   6.6565],
        [ -9.9938,   5.3785],
        [ -7.1524,   2.5372],
        [ -5.3429,   0.7277],
        [ -5.3868,   0.7716],
        [  3.4966,  -8.1118],
        [-10.2579,   5.6427],
        [  4.6640,  -9.2792],
        [-11.5881,   6.9729],
        [  2.4432,  -7.0584],
        [  3.5947,  -8.2099],
        [-11.4680,   6.8528],
        [-11.8016,   7.1864],
        [ -4.6755,   0.0603],
        [ -8.5539,   3.9387],
        [  4.1381,  -8.7534],
        [ -6.6516,   2.0363],
        [ -7.0786,   2.4633],
        [ -9.2227,   4.6075],
        [ -9.9345,   5.3193],
        [ -5.8874,   1.2722],
        [  4.6042,  -9.2194],
        [  1.2094,  -5.8246],
        [  3.4579,  -8.0731],
        [ -7.2376,   2.6223],
        [ -2.2156,  -2.3996],
        [ -1.1348,  -3.4804],
        [  4.3504,  -8.9656],
        [  2.3309,  -6.9461],
        [  3.8617,  -8.4769],
        [  2.4472,  -7.0624],
        [  0.6748,  -5.2900],
        [-11.5202,   6.9050],
        [  3.4009,  -8.0161],
        [ -5.4172,   0.8020],
        [ -5.7776,   1.1623],
        [  0.9221,  -5.5373],
        [ -0.0597,  -4.5556],
        [  0.5142,  -5.1294],
        [  0.3330,  -4.9482],
        [  3.7821,  -8.3973],
        [ -4.9859,   0.3707],
        [ -4.0304,  -0.5848],
        [ -9.9997,   5.3845],
        [  1.1053,  -5.7205],
        [ -9.8182,   5.2030],
        [  0.9597,  -5.5749],
        [-11.5445,   6.9292],
        [  2.3134,  -6.9286],
        [ -3.4867,  -1.1285],
        [ -9.6709,   5.0557],
        [ -4.8745,   0.2593],
        [ -9.8794,   5.2642],
        [ -6.6050,   1.9897],
        [ -6.5429,   1.9277],
        [-10.1655,   5.5503],
        [  2.2008,  -6.8160],
        [ -7.1510,   2.5358],
        [ -5.3908,   0.7756],
        [ -9.0079,   4.3927],
        [  1.2855,  -5.9007],
        [  4.0195,  -8.6347],
        [ -7.9164,   3.3012],
        [  3.9454,  -8.5607],
        [-11.8066,   7.1914],
        [  1.0192,  -5.6344],
        [  1.0236,  -5.6388],
        [ -1.9552,  -2.6600],
        [ -0.3367,  -4.2785],
        [ -8.9334,   4.3182],
        [  0.9689,  -5.5841],
        [ -0.8066,  -3.8086],
        [ -6.4738,   1.8586],
        [-11.7305,   7.1153],
        [-10.0750,   5.4597],
        [ -9.4715,   4.8563],
        [  3.9916,  -8.6068],
        [ -6.5793,   1.9641],
        [-10.0171,   5.4018],
        [ -9.6833,   5.0680],
        [ -7.6055,   2.9903],
        [ -6.1165,   1.5013],
        [  3.5232,  -8.1384],
        [ -7.3735,   2.7583],
        [ -6.0861,   1.4709]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6835, 0.3165],
        [0.4245, 0.5755]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4200, 0.5800], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2353, 0.0877],
         [0.3066, 0.2592]],

        [[0.0504, 0.0931],
         [0.0345, 0.6915]],

        [[0.3512, 0.1007],
         [0.1757, 0.9518]],

        [[0.6919, 0.0995],
         [0.9808, 0.6882]],

        [[0.8949, 0.0836],
         [0.6192, 0.6507]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 1
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080682750429576
time is 2
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 95
Adjusted Rand Index: 0.8080725364594837
time is 3
tensor([1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 17
Adjusted Rand Index: 0.4299793895002505
time is 4
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 7
Adjusted Rand Index: 0.7370212140872312
Global Adjusted Rand Index: 0.061632040603541036
Average Adjusted Rand Index: 0.74862793822198
Iteration 0: Loss = -15084.724957335568
Iteration 10: Loss = -11295.806560131798
Iteration 20: Loss = -11289.332202265012
Iteration 30: Loss = -11244.987609557622
Iteration 40: Loss = -11015.499798570805
Iteration 50: Loss = -10971.367542297552
Iteration 60: Loss = -10971.360810031156
Iteration 70: Loss = -10971.360821770253
1
Iteration 80: Loss = -10971.360817375771
2
Iteration 90: Loss = -10971.360817375771
3
Stopping early at iteration 89 due to no improvement.
pi: tensor([[0.7967, 0.2033],
        [0.2385, 0.7615]], dtype=torch.float64)
alpha: tensor([0.5106, 0.4894])
beta: tensor([[[0.1928, 0.0898],
         [0.6865, 0.2921]],

        [[0.1970, 0.0961],
         [0.9960, 0.3405]],

        [[0.1524, 0.1026],
         [0.5927, 0.4125]],

        [[0.4944, 0.1010],
         [0.0898, 0.8886]],

        [[0.5779, 0.0893],
         [0.8263, 0.3784]]], dtype=torch.float64)
time is 0
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 1
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448351863643042
time is 2
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824062740165256
time is 3
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 4
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9214426683307702
Average Adjusted Rand Index: 0.9214475084437244
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -15084.628009578226
Iteration 100: Loss = -11300.574707149095
Iteration 200: Loss = -11294.801185274795
Iteration 300: Loss = -11228.951538501875
Iteration 400: Loss = -11022.885994117607
Iteration 500: Loss = -10996.533991744027
Iteration 600: Loss = -10984.077090777664
Iteration 700: Loss = -10983.86442117758
Iteration 800: Loss = -10983.434679106205
Iteration 900: Loss = -10983.340743926396
Iteration 1000: Loss = -10983.1364578242
Iteration 1100: Loss = -10981.014973859228
Iteration 1200: Loss = -10979.125827940377
Iteration 1300: Loss = -10978.890278647177
Iteration 1400: Loss = -10977.88564513507
Iteration 1500: Loss = -10977.67443047594
Iteration 1600: Loss = -10971.971526188272
Iteration 1700: Loss = -10971.961178006208
Iteration 1800: Loss = -10971.953773122188
Iteration 1900: Loss = -10971.947845208808
Iteration 2000: Loss = -10971.942951185816
Iteration 2100: Loss = -10971.93874862827
Iteration 2200: Loss = -10971.946875730559
1
Iteration 2300: Loss = -10971.931639360013
Iteration 2400: Loss = -10971.928763958382
Iteration 2500: Loss = -10971.928208244864
Iteration 2600: Loss = -10971.92398650874
Iteration 2700: Loss = -10971.922009980117
Iteration 2800: Loss = -10971.923352420466
1
Iteration 2900: Loss = -10971.918875894831
Iteration 3000: Loss = -10971.917593940925
Iteration 3100: Loss = -10971.916413764682
Iteration 3200: Loss = -10971.915139412858
Iteration 3300: Loss = -10965.256617853829
Iteration 3400: Loss = -10965.253675339272
Iteration 3500: Loss = -10965.252313377587
Iteration 3600: Loss = -10965.251341505433
Iteration 3700: Loss = -10965.2522582084
1
Iteration 3800: Loss = -10965.249729508176
Iteration 3900: Loss = -10965.249061301216
Iteration 4000: Loss = -10965.248413506846
Iteration 4100: Loss = -10965.247653698087
Iteration 4200: Loss = -10965.247044080565
Iteration 4300: Loss = -10965.246519561033
Iteration 4400: Loss = -10965.246018912516
Iteration 4500: Loss = -10965.24559875064
Iteration 4600: Loss = -10965.246157896134
1
Iteration 4700: Loss = -10965.244660347187
Iteration 4800: Loss = -10965.244123823508
Iteration 4900: Loss = -10965.244371058907
1
Iteration 5000: Loss = -10965.243252575565
Iteration 5100: Loss = -10965.244960033422
1
Iteration 5200: Loss = -10965.242770202412
Iteration 5300: Loss = -10965.242477463577
Iteration 5400: Loss = -10965.263611898723
1
Iteration 5500: Loss = -10965.237957527668
Iteration 5600: Loss = -10965.237660024975
Iteration 5700: Loss = -10965.237390291382
Iteration 5800: Loss = -10965.237354502833
Iteration 5900: Loss = -10965.238336498056
1
Iteration 6000: Loss = -10965.238214646877
2
Iteration 6100: Loss = -10965.236646155563
Iteration 6200: Loss = -10965.235748214804
Iteration 6300: Loss = -10965.247092984415
1
Iteration 6400: Loss = -10965.238698725761
2
Iteration 6500: Loss = -10965.235256037864
Iteration 6600: Loss = -10965.243401533664
1
Iteration 6700: Loss = -10965.23462601927
Iteration 6800: Loss = -10965.249265293985
1
Iteration 6900: Loss = -10965.246968840584
2
Iteration 7000: Loss = -10965.234261116393
Iteration 7100: Loss = -10965.234596962382
1
Iteration 7200: Loss = -10965.23410095992
Iteration 7300: Loss = -10965.234297417206
1
Iteration 7400: Loss = -10965.24871328423
2
Iteration 7500: Loss = -10965.28759920001
3
Iteration 7600: Loss = -10965.276730190704
4
Iteration 7700: Loss = -10965.233842904487
Iteration 7800: Loss = -10965.333124095117
1
Iteration 7900: Loss = -10965.233756624413
Iteration 8000: Loss = -10965.233714757494
Iteration 8100: Loss = -10965.236369885115
1
Iteration 8200: Loss = -10965.233659604728
Iteration 8300: Loss = -10965.457887853592
1
Iteration 8400: Loss = -10965.234002273368
2
Iteration 8500: Loss = -10965.234178263243
3
Iteration 8600: Loss = -10965.233527123899
Iteration 8700: Loss = -10965.23345500421
Iteration 8800: Loss = -10965.234132333024
1
Iteration 8900: Loss = -10965.233419598646
Iteration 9000: Loss = -10965.233434125172
1
Iteration 9100: Loss = -10965.233673386121
2
Iteration 9200: Loss = -10965.240717078515
3
Iteration 9300: Loss = -10965.40890474236
4
Iteration 9400: Loss = -10965.234712975158
5
Iteration 9500: Loss = -10965.265697342214
6
Iteration 9600: Loss = -10965.232403163021
Iteration 9700: Loss = -10965.354527126015
1
Iteration 9800: Loss = -10965.231620879653
Iteration 9900: Loss = -10965.230839979318
Iteration 10000: Loss = -10965.231087560873
1
Iteration 10100: Loss = -10965.231747404809
2
Iteration 10200: Loss = -10965.23045193023
Iteration 10300: Loss = -10965.22951032281
Iteration 10400: Loss = -10965.230717807175
1
Iteration 10500: Loss = -10965.229487279388
Iteration 10600: Loss = -10965.230782959292
1
Iteration 10700: Loss = -10965.2289365818
Iteration 10800: Loss = -10965.226323894973
Iteration 10900: Loss = -10965.227446942077
1
Iteration 11000: Loss = -10965.226453604519
2
Iteration 11100: Loss = -10965.225468607838
Iteration 11200: Loss = -10965.241246470096
1
Iteration 11300: Loss = -10965.225169272091
Iteration 11400: Loss = -10965.247879874843
1
Iteration 11500: Loss = -10965.225526775144
2
Iteration 11600: Loss = -10965.42265062552
3
Iteration 11700: Loss = -10965.22585777727
4
Iteration 11800: Loss = -10965.246406052482
5
Iteration 11900: Loss = -10965.22509048881
Iteration 12000: Loss = -10965.230209509933
1
Iteration 12100: Loss = -10965.225098549608
2
Iteration 12200: Loss = -10965.225576523957
3
Iteration 12300: Loss = -10965.225050931396
Iteration 12400: Loss = -10965.22446474145
Iteration 12500: Loss = -10965.224614402066
1
Iteration 12600: Loss = -10965.224404362172
Iteration 12700: Loss = -10965.224831640791
1
Iteration 12800: Loss = -10965.224434294718
2
Iteration 12900: Loss = -10965.224357928528
Iteration 13000: Loss = -10965.22670191633
1
Iteration 13100: Loss = -10965.224344985549
Iteration 13200: Loss = -10965.403213267802
1
Iteration 13300: Loss = -10965.22433710215
Iteration 13400: Loss = -10965.22421853208
Iteration 13500: Loss = -10965.2303564491
1
Iteration 13600: Loss = -10965.233852723075
2
Iteration 13700: Loss = -10965.232192444768
3
Iteration 13800: Loss = -10965.24533766292
4
Iteration 13900: Loss = -10965.224121681153
Iteration 14000: Loss = -10965.444036356615
1
Iteration 14100: Loss = -10965.224264580951
2
Iteration 14200: Loss = -10965.225009103035
3
Iteration 14300: Loss = -10965.2243908998
4
Iteration 14400: Loss = -10965.226129038905
5
Iteration 14500: Loss = -10965.224051914469
Iteration 14600: Loss = -10965.226942087354
1
Iteration 14700: Loss = -10965.286886351154
2
Iteration 14800: Loss = -10965.224040244142
Iteration 14900: Loss = -10965.22610462766
1
Iteration 15000: Loss = -10965.224038108214
Iteration 15100: Loss = -10965.224193694488
1
Iteration 15200: Loss = -10965.224050854371
2
Iteration 15300: Loss = -10965.22414593472
3
Iteration 15400: Loss = -10965.224041496906
4
Iteration 15500: Loss = -10965.22462018869
5
Iteration 15600: Loss = -10965.226076293335
6
Iteration 15700: Loss = -10965.253190683521
7
Iteration 15800: Loss = -10965.224540450175
8
Iteration 15900: Loss = -10965.22881957195
9
Iteration 16000: Loss = -10965.233409589384
10
Stopping early at iteration 16000 due to no improvement.
tensor([[ -9.5596,   7.8546],
        [-12.1971,   8.3134],
        [  4.1188,  -6.5279],
        [ -7.0010,   5.4516],
        [  4.4541,  -6.1069],
        [ -7.2203,   5.7654],
        [  1.2734,  -2.6610],
        [  5.6481,  -7.1538],
        [ -6.8288,   5.1973],
        [  5.0930,  -6.4955],
        [-10.8253,   7.5429],
        [ -6.3934,   4.6704],
        [  3.2872,  -6.4419],
        [  2.0499,  -5.3271],
        [-10.3716,   8.0320],
        [-10.4819,   7.8822],
        [ -9.0703,   7.6334],
        [ -5.0509,   3.3371],
        [ -1.9549,   0.5468],
        [ -4.6938,   1.0460],
        [  6.4840,  -7.9469],
        [ -9.7917,   8.4043],
        [  7.5307,  -8.9853],
        [ -9.5143,   8.0993],
        [  5.7956,  -8.5113],
        [  6.5828,  -8.3796],
        [ -6.2533,   4.8669],
        [ -9.4855,   7.7295],
        [ -1.7172,   0.2252],
        [ -6.4018,   5.0101],
        [  6.3317,  -7.7206],
        [ -5.0168,   3.2962],
        [ -3.6430,   2.0381],
        [ -7.2287,   3.9699],
        [-11.3352,   7.6411],
        [ -4.5988,   1.5705],
        [  7.3630,  -9.3082],
        [  3.8922,  -5.3716],
        [  6.3258,  -7.9084],
        [ -9.6535,   7.7672],
        [  0.7326,  -2.2797],
        [  1.9339,  -3.3663],
        [  6.9441,  -8.3522],
        [  5.5720,  -8.3606],
        [  6.5250,  -7.9578],
        [  5.6803,  -7.4852],
        [  3.6399,  -5.0656],
        [ -9.3021,   7.7099],
        [  5.9640,  -7.3960],
        [ -2.5165,   0.9983],
        [ -5.0301,   2.4852],
        [  2.8102,  -4.1973],
        [  2.0944,  -3.6439],
        [  3.8541,  -5.2444],
        [  2.1150,  -3.5287],
        [  5.9056,  -7.5403],
        [ -3.4904,   1.8421],
        [ -1.8377,   0.4345],
        [-12.1492,   8.1154],
        [  4.1776,  -5.6281],
        [ -8.9937,   6.8392],
        [  3.3409,  -5.6301],
        [-10.7109,   8.1005],
        [  4.4961,  -7.8478],
        [ -1.7362,  -2.3474],
        [ -8.9348,   7.5468],
        [ -1.9220,   0.0909],
        [ -8.7562,   6.5109],
        [ -5.1007,   3.1043],
        [ -4.0003,   2.2596],
        [ -9.5472,   8.0864],
        [  4.9046,  -6.7353],
        [ -4.9290,   3.1532],
        [ -1.7724,   0.2833],
        [ -8.9734,   7.4276],
        [  3.2631,  -5.4521],
        [  6.3675,  -7.9685],
        [ -5.7830,   4.3958],
        [  5.6364,  -8.1070],
        [-10.1617,   8.7668],
        [  3.6904,  -5.4946],
        [  3.8425,  -5.6060],
        [  1.6971,  -3.6821],
        [  0.7963,  -5.2252],
        [ -9.2407,   7.6238],
        [  4.1604,  -5.5980],
        [  2.6348,  -4.0706],
        [ -4.3150,   2.8718],
        [ -6.9325,   4.7966],
        [-10.0738,   8.6650],
        [ -9.9159,   8.2029],
        [  6.1277,  -8.1794],
        [ -4.2452,   2.8527],
        [-10.4880,   8.0322],
        [-10.1190,   8.7157],
        [-10.0458,   8.3019],
        [ -4.9060,   2.9934],
        [  6.6555,  -8.3248],
        [ -5.9521,   3.5667],
        [ -4.2776,   2.3760]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.8119, 0.1881],
        [0.2156, 0.7844]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4400, 0.5600], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1976, 0.0898],
         [0.6865, 0.2960]],

        [[0.1970, 0.0956],
         [0.9960, 0.3405]],

        [[0.1524, 0.1033],
         [0.5927, 0.4125]],

        [[0.4944, 0.1006],
         [0.0898, 0.8886]],

        [[0.5779, 0.0891],
         [0.8263, 0.3784]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 2
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 3
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9603205178782778
Average Adjusted Rand Index: 0.9603199464905721
Iteration 0: Loss = -28311.109606704646
Iteration 10: Loss = -11303.237482542696
Iteration 20: Loss = -11298.214076012424
Iteration 30: Loss = -11295.12824324112
Iteration 40: Loss = -11292.327780365114
Iteration 50: Loss = -11270.924865163386
Iteration 60: Loss = -11123.349074081192
Iteration 70: Loss = -10971.62740318435
Iteration 80: Loss = -10971.360906638525
Iteration 90: Loss = -10971.360816774803
Iteration 100: Loss = -10971.360817375808
1
Iteration 110: Loss = -10971.360817375771
2
Iteration 120: Loss = -10971.360817375771
3
Stopping early at iteration 119 due to no improvement.
pi: tensor([[0.7967, 0.2033],
        [0.2385, 0.7615]], dtype=torch.float64)
alpha: tensor([0.5106, 0.4894])
beta: tensor([[[0.1928, 0.0898],
         [0.0788, 0.2921]],

        [[0.0102, 0.0961],
         [0.4183, 0.7277]],

        [[0.5605, 0.1026],
         [0.3254, 0.5164]],

        [[0.4977, 0.1010],
         [0.0460, 0.1554]],

        [[0.3366, 0.0893],
         [0.3670, 0.3215]]], dtype=torch.float64)
time is 0
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 1
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448351863643042
time is 2
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824062740165256
time is 3
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 4
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9214426683307702
Average Adjusted Rand Index: 0.9214475084437244
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -28311.781310595266
Iteration 100: Loss = -11311.162784109356
Iteration 200: Loss = -11301.331316200469
Iteration 300: Loss = -11294.598356020468
Iteration 400: Loss = -11288.239424260026
Iteration 500: Loss = -11278.394244319285
Iteration 600: Loss = -11270.73360863437
Iteration 700: Loss = -11251.486066521273
Iteration 800: Loss = -11242.936200179383
Iteration 900: Loss = -11235.098382696046
Iteration 1000: Loss = -11226.444856067093
Iteration 1100: Loss = -11224.792837948253
Iteration 1200: Loss = -11223.874097948143
Iteration 1300: Loss = -11223.27591021847
Iteration 1400: Loss = -11222.23621238098
Iteration 1500: Loss = -11220.09182470041
Iteration 1600: Loss = -11184.113388788437
Iteration 1700: Loss = -11154.596694525206
Iteration 1800: Loss = -11085.73372549553
Iteration 1900: Loss = -11076.365087595374
Iteration 2000: Loss = -11068.164391031785
Iteration 2100: Loss = -11058.544244778888
Iteration 2200: Loss = -11056.153087823457
Iteration 2300: Loss = -11055.654185274734
Iteration 2400: Loss = -11053.026789472915
Iteration 2500: Loss = -11052.384706808332
Iteration 2600: Loss = -11052.041532158912
Iteration 2700: Loss = -11051.902956119418
Iteration 2800: Loss = -11051.403008421603
Iteration 2900: Loss = -11051.331960555075
Iteration 3000: Loss = -11051.297315852134
Iteration 3100: Loss = -11051.269949546242
Iteration 3200: Loss = -11051.247844991596
Iteration 3300: Loss = -11051.23282438309
Iteration 3400: Loss = -11051.22108180103
Iteration 3500: Loss = -11051.210954597109
Iteration 3600: Loss = -11051.202037202744
Iteration 3700: Loss = -11051.196293947794
Iteration 3800: Loss = -11051.187067684281
Iteration 3900: Loss = -11051.180751959186
Iteration 4000: Loss = -11051.175075834824
Iteration 4100: Loss = -11051.169726282436
Iteration 4200: Loss = -11051.164240619633
Iteration 4300: Loss = -11051.157479110952
Iteration 4400: Loss = -11051.15232753158
Iteration 4500: Loss = -11051.147430635969
Iteration 4600: Loss = -11051.143618552134
Iteration 4700: Loss = -11051.139647726985
Iteration 4800: Loss = -11051.134964881187
Iteration 4900: Loss = -11051.12872787054
Iteration 5000: Loss = -11051.124048418082
Iteration 5100: Loss = -11051.119577545875
Iteration 5200: Loss = -11051.112662590158
Iteration 5300: Loss = -11051.106657047929
Iteration 5400: Loss = -11051.107751753445
1
Iteration 5500: Loss = -11051.098191463936
Iteration 5600: Loss = -11051.08925368725
Iteration 5700: Loss = -11050.98510543104
Iteration 5800: Loss = -11050.879163662083
Iteration 5900: Loss = -11050.255944559161
Iteration 6000: Loss = -11050.23793596658
Iteration 6100: Loss = -11050.233559445936
Iteration 6200: Loss = -11050.226404787792
Iteration 6300: Loss = -11050.22439008328
Iteration 6400: Loss = -11050.223274565898
Iteration 6500: Loss = -11050.22123836489
Iteration 6600: Loss = -11050.22092410291
Iteration 6700: Loss = -11050.218940578761
Iteration 6800: Loss = -11050.218058734645
Iteration 6900: Loss = -11050.217803105345
Iteration 7000: Loss = -11050.216190031177
Iteration 7100: Loss = -11050.213649211664
Iteration 7200: Loss = -11050.165109114883
Iteration 7300: Loss = -11050.163804636852
Iteration 7400: Loss = -11050.163311447634
Iteration 7500: Loss = -11050.164151537563
1
Iteration 7600: Loss = -11050.162348897822
Iteration 7700: Loss = -11050.160971680702
Iteration 7800: Loss = -11050.15914121236
Iteration 7900: Loss = -11050.158001576938
Iteration 8000: Loss = -11050.163134764516
1
Iteration 8100: Loss = -11050.142081982172
Iteration 8200: Loss = -11050.14353273896
1
Iteration 8300: Loss = -11050.139554158508
Iteration 8400: Loss = -11050.136750753352
Iteration 8500: Loss = -11050.127032769504
Iteration 8600: Loss = -11050.12005904416
Iteration 8700: Loss = -11050.118521277906
Iteration 8800: Loss = -11050.06806306951
Iteration 8900: Loss = -11050.066405818463
Iteration 9000: Loss = -11050.0632338794
Iteration 9100: Loss = -11050.062534841658
Iteration 9200: Loss = -11050.071947835038
1
Iteration 9300: Loss = -11050.06505911901
2
Iteration 9400: Loss = -11050.06170645049
Iteration 9500: Loss = -11050.062914672657
1
Iteration 9600: Loss = -11050.181698169934
2
Iteration 9700: Loss = -11050.064506659328
3
Iteration 9800: Loss = -11050.061182684472
Iteration 9900: Loss = -11050.09960572754
1
Iteration 10000: Loss = -11050.068917860306
2
Iteration 10100: Loss = -11050.059873986873
Iteration 10200: Loss = -11050.0712404435
1
Iteration 10300: Loss = -11050.04774786546
Iteration 10400: Loss = -11050.136370404814
1
Iteration 10500: Loss = -11050.047571548028
Iteration 10600: Loss = -11050.046340914516
Iteration 10700: Loss = -11050.046463495324
1
Iteration 10800: Loss = -11050.275278001458
2
Iteration 10900: Loss = -11050.046158915096
Iteration 11000: Loss = -11050.047658794687
1
Iteration 11100: Loss = -11050.050684944514
2
Iteration 11200: Loss = -11050.080022694612
3
Iteration 11300: Loss = -11050.045988094244
Iteration 11400: Loss = -11050.045886118158
Iteration 11500: Loss = -11050.060855812939
1
Iteration 11600: Loss = -11050.045568841988
Iteration 11700: Loss = -11050.046047020385
1
Iteration 11800: Loss = -11050.047475741274
2
Iteration 11900: Loss = -11050.045898636448
3
Iteration 12000: Loss = -11050.113621848644
4
Iteration 12100: Loss = -11050.052587441438
5
Iteration 12200: Loss = -11050.06631723192
6
Iteration 12300: Loss = -11050.043840057508
Iteration 12400: Loss = -11050.047976141152
1
Iteration 12500: Loss = -11050.056709744995
2
Iteration 12600: Loss = -11050.043618623515
Iteration 12700: Loss = -11050.049557795748
1
Iteration 12800: Loss = -11050.042728688408
Iteration 12900: Loss = -11050.04416777188
1
Iteration 13000: Loss = -11050.042601226882
Iteration 13100: Loss = -11050.04740489047
1
Iteration 13200: Loss = -11050.04252597087
Iteration 13300: Loss = -11050.042497713002
Iteration 13400: Loss = -11050.042617375631
1
Iteration 13500: Loss = -11050.04243590065
Iteration 13600: Loss = -11050.074939323584
1
Iteration 13700: Loss = -11050.040928576196
Iteration 13800: Loss = -11050.041845405145
1
Iteration 13900: Loss = -11050.041422446702
2
Iteration 14000: Loss = -11050.043741599746
3
Iteration 14100: Loss = -11050.03975215585
Iteration 14200: Loss = -11050.039553007311
Iteration 14300: Loss = -11050.195563212834
1
Iteration 14400: Loss = -11050.03891226638
Iteration 14500: Loss = -11050.042528028183
1
Iteration 14600: Loss = -11050.038429609636
Iteration 14700: Loss = -11050.038520929933
1
Iteration 14800: Loss = -11050.040801731584
2
Iteration 14900: Loss = -11050.05986022796
3
Iteration 15000: Loss = -11050.04279117923
4
Iteration 15100: Loss = -11050.048584609818
5
Iteration 15200: Loss = -11050.043774903308
6
Iteration 15300: Loss = -11050.037893514751
Iteration 15400: Loss = -11050.053129658721
1
Iteration 15500: Loss = -11050.037063709759
Iteration 15600: Loss = -11050.037066196453
1
Iteration 15700: Loss = -11050.037819943765
2
Iteration 15800: Loss = -11050.040000029203
3
Iteration 15900: Loss = -11050.174416289703
4
Iteration 16000: Loss = -11050.03289139673
Iteration 16100: Loss = -11050.03240566144
Iteration 16200: Loss = -11050.100253340683
1
Iteration 16300: Loss = -11049.187821657957
Iteration 16400: Loss = -11049.026169843703
Iteration 16500: Loss = -11049.025992974097
Iteration 16600: Loss = -11049.030451295344
1
Iteration 16700: Loss = -11049.034864135281
2
Iteration 16800: Loss = -11049.02388578657
Iteration 16900: Loss = -11049.024145421867
1
Iteration 17000: Loss = -11049.134189903658
2
Iteration 17100: Loss = -11049.02717915942
3
Iteration 17200: Loss = -11049.02548124105
4
Iteration 17300: Loss = -11049.023886939325
5
Iteration 17400: Loss = -11049.024242270649
6
Iteration 17500: Loss = -11049.024163265194
7
Iteration 17600: Loss = -11049.027648651925
8
Iteration 17700: Loss = -11049.02569498471
9
Iteration 17800: Loss = -11049.05104922283
10
Stopping early at iteration 17800 due to no improvement.
tensor([[ -9.9767,   8.0851],
        [-10.2646,   8.6195],
        [  4.2307,  -6.0762],
        [ -8.0987,   6.4729],
        [  3.9647,  -6.7586],
        [ -9.2943,   6.0724],
        [  0.7556,  -2.1764],
        [  5.5244,  -7.1041],
        [ -7.5112,   5.3277],
        [  3.8867,  -5.3153],
        [ -9.6149,   8.2018],
        [ -9.5809,   7.5485],
        [  3.1264,  -4.9297],
        [  2.0771,  -3.5463],
        [ -9.7154,   8.3278],
        [-10.4738,   8.7144],
        [ -8.9174,   7.4011],
        [ -5.1285,   3.6452],
        [ -3.1150,   1.6637],
        [ -3.6419,   2.1501],
        [  5.7462,  -7.1516],
        [-10.3686,   8.4209],
        [  6.7771,  -8.2601],
        [-10.1915,   8.2854],
        [  5.2420,  -6.8263],
        [  5.8794,  -7.4928],
        [ -6.6622,   5.2191],
        [ -9.4968,   7.5706],
        [ -2.6626,   0.8753],
        [ -7.2783,   5.8350],
        [  6.0664,  -7.5206],
        [ -5.5911,   2.8024],
        [ -5.6689,   2.5566],
        [ -7.7968,   5.8130],
        [ -9.5827,   8.1905],
        [ -3.9991,   2.5068],
        [  6.6687,  -8.2966],
        [  3.3134,  -5.9861],
        [  4.9540,  -7.7922],
        [ -9.3339,   7.9475],
        [ -0.1595,  -1.2826],
        [  1.0491,  -2.6590],
        [  5.6719,  -9.0530],
        [  4.7127,  -6.3958],
        [  5.7679,  -7.4490],
        [  4.6040,  -6.5520],
        [  2.3416,  -4.8676],
        [ -8.7663,   7.1910],
        [  5.1546,  -7.1059],
        [ -3.3045,   1.9030],
        [ -9.3735,   7.9534],
        [  2.5432,  -4.6521],
        [  2.0064,  -3.4575],
        [  2.6365,  -4.4096],
        [  1.8848,  -3.8718],
        [  5.3219,  -7.3110],
        [-10.5603,   8.2381],
        [ -3.2436,  -0.5644],
        [ -9.0661,   7.6642],
        [  1.8764,  -6.4916],
        [ -8.9304,   7.0651],
        [  2.9744,  -4.4835],
        [ -8.9865,   7.5451],
        [  4.5661,  -5.9525],
        [ -2.3796,  -1.3274],
        [-11.4581,   7.7306],
        [ -3.1931,   0.7556],
        [ -8.9347,   7.5304],
        [ -8.4542,   6.9770],
        [ -4.3625,   2.8794],
        [ -9.7445,   8.2804],
        [  3.7768,  -6.1933],
        [ -8.0142,   6.6014],
        [ -3.1783,   1.5698],
        [ -9.8033,   7.5684],
        [  3.0945,  -4.6279],
        [  5.9729,  -7.3701],
        [-10.2122,   7.9836],
        [  5.8089,  -7.2111],
        [-12.7709,   9.0595],
        [  3.1528,  -4.5782],
        [  2.4361,  -5.3193],
        [ -0.7396,  -3.2102],
        [  1.0332,  -3.6224],
        [ -9.6202,   8.2338],
        [  3.2751,  -4.9821],
        [  1.0388,  -3.2301],
        [-10.6579,   8.5382],
        [ -9.4425,   8.0512],
        [-10.4474,   8.6652],
        [ -9.6087,   8.1107],
        [  5.9505,  -7.3687],
        [ -7.2250,   5.6277],
        [ -9.5522,   7.7389],
        [-11.2361,   8.3350],
        [ -9.8282,   8.2504],
        [ -4.8865,   2.6496],
        [  5.9644,  -7.3941],
        [ -9.7665,   8.2190],
        [ -4.1688,   2.7486]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7177, 0.2823],
        [0.3961, 0.6039]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4275, 0.5725], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2187, 0.0881],
         [0.0788, 0.2787]],

        [[0.0102, 0.0941],
         [0.4183, 0.7277]],

        [[0.5605, 0.1015],
         [0.3254, 0.5164]],

        [[0.4977, 0.0997],
         [0.0460, 0.1554]],

        [[0.3366, 0.0844],
         [0.3670, 0.3215]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 1
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208
time is 2
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448338943900694
time is 3
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.844846433231073
time is 4
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 10
Adjusted Rand Index: 0.6365003576249006
Global Adjusted Rand Index: 0.35392714295470945
Average Adjusted Rand Index: 0.8413957922532038
Iteration 0: Loss = -35393.20057448482
Iteration 10: Loss = -11303.188042523403
Iteration 20: Loss = -11296.800927446742
Iteration 30: Loss = -11294.44964377658
Iteration 40: Loss = -11287.57005587283
Iteration 50: Loss = -11228.488841893142
Iteration 60: Loss = -10979.190117234531
Iteration 70: Loss = -10971.363029117423
Iteration 80: Loss = -10971.360810664766
Iteration 90: Loss = -10971.36082494442
1
Iteration 100: Loss = -10971.36081841047
2
Iteration 110: Loss = -10971.36081841047
3
Stopping early at iteration 109 due to no improvement.
pi: tensor([[0.7967, 0.2033],
        [0.2385, 0.7615]], dtype=torch.float64)
alpha: tensor([0.5106, 0.4894])
beta: tensor([[[0.1928, 0.0898],
         [0.7219, 0.2921]],

        [[0.2698, 0.0961],
         [0.9252, 0.0048]],

        [[0.1993, 0.1026],
         [0.5792, 0.9491]],

        [[0.5255, 0.1010],
         [0.1116, 0.5698]],

        [[0.8580, 0.0893],
         [0.1995, 0.1524]]], dtype=torch.float64)
time is 0
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 1
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448351863643042
time is 2
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824062740165256
time is 3
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 4
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9214426683307702
Average Adjusted Rand Index: 0.9214475084437244
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -35393.644395088886
Iteration 100: Loss = -11309.507846156595
Iteration 200: Loss = -11305.193845577276
Iteration 300: Loss = -11304.317440958906
Iteration 400: Loss = -11303.895031443091
Iteration 500: Loss = -11303.645473744313
Iteration 600: Loss = -11303.476935613951
Iteration 700: Loss = -11303.35088984997
Iteration 800: Loss = -11303.244212805697
Iteration 900: Loss = -11303.131017521968
Iteration 1000: Loss = -11302.89720123094
Iteration 1100: Loss = -11300.888866413028
Iteration 1200: Loss = -11296.231162786755
Iteration 1300: Loss = -11279.60401721714
Iteration 1400: Loss = -11266.508221302256
Iteration 1500: Loss = -11243.54842037179
Iteration 1600: Loss = -11211.964731795615
Iteration 1700: Loss = -11082.327303871618
Iteration 1800: Loss = -11050.801198348727
Iteration 1900: Loss = -11004.39430089896
Iteration 2000: Loss = -11000.5903638402
Iteration 2100: Loss = -10994.395042979828
Iteration 2200: Loss = -10993.805441376207
Iteration 2300: Loss = -10987.694706708462
Iteration 2400: Loss = -10987.564787666264
Iteration 2500: Loss = -10983.476289540897
Iteration 2600: Loss = -10983.432412002034
Iteration 2700: Loss = -10983.001426344443
Iteration 2800: Loss = -10982.899442110194
Iteration 2900: Loss = -10981.381654644949
Iteration 3000: Loss = -10981.354240738152
Iteration 3100: Loss = -10981.319805126612
Iteration 3200: Loss = -10981.287235677182
Iteration 3300: Loss = -10981.263882835554
Iteration 3400: Loss = -10981.2253632915
Iteration 3500: Loss = -10981.212956423737
Iteration 3600: Loss = -10981.173669999236
Iteration 3700: Loss = -10976.050925376021
Iteration 3800: Loss = -10967.94353705925
Iteration 3900: Loss = -10967.933051588547
Iteration 4000: Loss = -10967.927961801346
Iteration 4100: Loss = -10967.923764616475
Iteration 4200: Loss = -10967.919869389494
Iteration 4300: Loss = -10967.92010765801
1
Iteration 4400: Loss = -10967.91191055914
Iteration 4500: Loss = -10967.90434742655
Iteration 4600: Loss = -10967.862727422784
Iteration 4700: Loss = -10967.849852121344
Iteration 4800: Loss = -10967.848276188068
Iteration 4900: Loss = -10967.850187776123
1
Iteration 5000: Loss = -10967.844195280315
Iteration 5100: Loss = -10967.842075459714
Iteration 5200: Loss = -10967.836286771033
Iteration 5300: Loss = -10966.904106576183
Iteration 5400: Loss = -10966.90083307044
Iteration 5500: Loss = -10966.899366158344
Iteration 5600: Loss = -10966.904620946467
1
Iteration 5700: Loss = -10966.898981242608
Iteration 5800: Loss = -10966.87766199465
Iteration 5900: Loss = -10966.878988615905
1
Iteration 6000: Loss = -10966.882608230628
2
Iteration 6100: Loss = -10966.875755579487
Iteration 6200: Loss = -10966.307761734704
Iteration 6300: Loss = -10966.307036017952
Iteration 6400: Loss = -10966.308477683244
1
Iteration 6500: Loss = -10966.307083234762
2
Iteration 6600: Loss = -10966.305882311319
Iteration 6700: Loss = -10966.314795354243
1
Iteration 6800: Loss = -10966.30939989925
2
Iteration 6900: Loss = -10966.3038750713
Iteration 7000: Loss = -10966.299273072585
Iteration 7100: Loss = -10966.30061106425
1
Iteration 7200: Loss = -10966.294206583156
Iteration 7300: Loss = -10966.293210387177
Iteration 7400: Loss = -10966.291380169689
Iteration 7500: Loss = -10965.843247721301
Iteration 7600: Loss = -10965.843006586267
Iteration 7700: Loss = -10965.84136819261
Iteration 7800: Loss = -10965.841929334678
1
Iteration 7900: Loss = -10965.844838724806
2
Iteration 8000: Loss = -10965.840026739368
Iteration 8100: Loss = -10965.840851164747
1
Iteration 8200: Loss = -10965.838994052738
Iteration 8300: Loss = -10965.366339987884
Iteration 8400: Loss = -10965.36939176531
1
Iteration 8500: Loss = -10965.365755027475
Iteration 8600: Loss = -10965.365813073791
1
Iteration 8700: Loss = -10965.372398605481
2
Iteration 8800: Loss = -10965.364992027737
Iteration 8900: Loss = -10965.364863082477
Iteration 9000: Loss = -10965.36465606523
Iteration 9100: Loss = -10965.364730087926
1
Iteration 9200: Loss = -10965.364470702612
Iteration 9300: Loss = -10965.364074726289
Iteration 9400: Loss = -10965.364470535094
1
Iteration 9500: Loss = -10965.363896814175
Iteration 9600: Loss = -10965.364643335868
1
Iteration 9700: Loss = -10965.364358379229
2
Iteration 9800: Loss = -10965.3635153722
Iteration 9900: Loss = -10965.363671795734
1
Iteration 10000: Loss = -10965.374051464918
2
Iteration 10100: Loss = -10965.363409125224
Iteration 10200: Loss = -10965.363377753085
Iteration 10300: Loss = -10965.363639251482
1
Iteration 10400: Loss = -10965.41486769182
2
Iteration 10500: Loss = -10965.36289641257
Iteration 10600: Loss = -10965.305846308953
Iteration 10700: Loss = -10965.291177975034
Iteration 10800: Loss = -10965.291409750642
1
Iteration 10900: Loss = -10965.32077693723
2
Iteration 11000: Loss = -10965.291525791183
3
Iteration 11100: Loss = -10965.291108178302
Iteration 11200: Loss = -10965.291139435592
1
Iteration 11300: Loss = -10965.29743047889
2
Iteration 11400: Loss = -10965.289915608584
Iteration 11500: Loss = -10965.547775901861
1
Iteration 11600: Loss = -10965.290226429093
2
Iteration 11700: Loss = -10965.329148693094
3
Iteration 11800: Loss = -10965.288499775237
Iteration 11900: Loss = -10965.300024204798
1
Iteration 12000: Loss = -10965.298349006263
2
Iteration 12100: Loss = -10965.29641065851
3
Iteration 12200: Loss = -10965.288158583602
Iteration 12300: Loss = -10965.288082199562
Iteration 12400: Loss = -10965.288892577968
1
Iteration 12500: Loss = -10965.287818163799
Iteration 12600: Loss = -10965.35458041575
1
Iteration 12700: Loss = -10965.285887226477
Iteration 12800: Loss = -10965.29236671444
1
Iteration 12900: Loss = -10965.285812574606
Iteration 13000: Loss = -10965.307582917445
1
Iteration 13100: Loss = -10965.26626400558
Iteration 13200: Loss = -10965.265936945158
Iteration 13300: Loss = -10965.269863755653
1
Iteration 13400: Loss = -10965.326153882046
2
Iteration 13500: Loss = -10965.264199704154
Iteration 13600: Loss = -10965.275125492542
1
Iteration 13700: Loss = -10965.264187630819
Iteration 13800: Loss = -10965.55257828543
1
Iteration 13900: Loss = -10965.264152640615
Iteration 14000: Loss = -10965.270106051754
1
Iteration 14100: Loss = -10965.269906740543
2
Iteration 14200: Loss = -10965.26471574586
3
Iteration 14300: Loss = -10965.264190927008
4
Iteration 14400: Loss = -10965.264550056956
5
Iteration 14500: Loss = -10965.274871917754
6
Iteration 14600: Loss = -10965.264316509047
7
Iteration 14700: Loss = -10965.264181844406
8
Iteration 14800: Loss = -10965.441693351204
9
Iteration 14900: Loss = -10965.264101052075
Iteration 15000: Loss = -10965.513333510919
1
Iteration 15100: Loss = -10965.264108042944
2
Iteration 15200: Loss = -10965.34166190603
3
Iteration 15300: Loss = -10965.269154313512
4
Iteration 15400: Loss = -10965.383750045217
5
Iteration 15500: Loss = -10965.265840976377
6
Iteration 15600: Loss = -10965.265981575656
7
Iteration 15700: Loss = -10965.265895155128
8
Iteration 15800: Loss = -10965.623235114876
9
Iteration 15900: Loss = -10965.263586376615
Iteration 16000: Loss = -10965.273384939583
1
Iteration 16100: Loss = -10965.263553665045
Iteration 16200: Loss = -10965.265358119594
1
Iteration 16300: Loss = -10965.263651667927
2
Iteration 16400: Loss = -10965.263606801822
3
Iteration 16500: Loss = -10965.335116609544
4
Iteration 16600: Loss = -10965.268742662585
5
Iteration 16700: Loss = -10965.266762133215
6
Iteration 16800: Loss = -10965.265274944937
7
Iteration 16900: Loss = -10965.263978593903
8
Iteration 17000: Loss = -10965.264237486292
9
Iteration 17100: Loss = -10965.286765150564
10
Stopping early at iteration 17100 due to no improvement.
tensor([[ -9.4607,   6.8443],
        [ -9.7456,   8.0577],
        [  4.4672,  -6.1849],
        [ -6.9140,   5.5240],
        [  7.3593,  -9.7974],
        [ -7.4263,   5.8345],
        [  0.3406,  -3.5961],
        [  5.4785,  -7.3270],
        [ -6.7137,   5.3115],
        [  4.7616,  -6.8353],
        [-11.6708,   7.2521],
        [ -6.2418,   4.8198],
        [  3.5289,  -6.2027],
        [  2.6605,  -4.7207],
        [-10.8958,   7.7453],
        [ -9.4378,   7.8533],
        [ -9.2019,   7.8049],
        [ -5.4233,   2.9633],
        [ -1.9466,   0.5531],
        [ -4.0914,   1.6481],
        [  6.4253,  -7.8541],
        [ -9.5404,   8.0628],
        [  6.9779,  -8.5352],
        [-10.0360,   8.0754],
        [  6.3777,  -7.8720],
        [  6.3015, -10.0960],
        [ -6.2906,   4.8311],
        [ -8.7910,   7.2536],
        [ -1.6662,   0.2782],
        [ -6.4359,   4.9653],
        [  6.0918,  -7.9557],
        [ -4.8792,   3.4324],
        [ -3.5483,   2.1332],
        [ -6.3546,   4.8701],
        [-10.3874,   7.4681],
        [ -3.8567,   2.3101],
        [  7.0589,  -8.9988],
        [  3.7698,  -5.5050],
        [  5.8336,  -8.3312],
        [ -9.6429,   8.0541],
        [  0.4541,  -2.5602],
        [  1.9313,  -3.3732],
        [  6.3536,  -8.1292],
        [  6.0421,  -7.8649],
        [  6.3872,  -8.0284],
        [  5.2481,  -7.9214],
        [  3.6607,  -5.0474],
        [ -8.8733,   7.2214],
        [  5.9599,  -7.4058],
        [ -2.4838,   1.0318],
        [ -4.6369,   2.8800],
        [  2.8008,  -4.2140],
        [  2.1263,  -3.6237],
        [  3.1519,  -5.9516],
        [  1.1327,  -4.5259],
        [  5.5334,  -7.9371],
        [ -4.5860,   0.7443],
        [ -1.9881,   0.2779],
        [ -9.9034,   6.7858],
        [  3.6844,  -6.1296],
        [ -8.9904,   6.7967],
        [  3.7707,  -5.2027],
        [ -9.8747,   6.4592],
        [  4.5549,  -7.8357],
        [ -0.6314,  -1.2422],
        [ -8.9008,   7.3807],
        [ -1.7336,   0.2820],
        [ -8.2535,   6.7224],
        [ -5.2835,   2.9208],
        [ -3.9223,   2.3352],
        [ -9.4738,   7.2722],
        [  4.5599,  -7.0934],
        [ -7.0446,   5.0229],
        [ -2.1334,  -0.0759],
        [ -9.8751,   7.7075],
        [  3.1626,  -5.5551],
        [  6.1788,  -8.0787],
        [ -5.8254,   4.3474],
        [  6.1539,  -7.5458],
        [-10.1691,   8.7715],
        [  3.6708,  -5.5191],
        [  2.4207,  -7.0359],
        [  1.6802,  -3.7045],
        [  2.3180,  -3.7044],
        [ -8.7262,   7.2388],
        [  4.1846,  -5.5759],
        [  1.8252,  -4.8864],
        [-12.3871,   7.7719],
        [ -9.4494,   7.7599],
        [ -9.4494,   8.0237],
        [ -9.2753,   7.8435],
        [  6.2851,  -7.9531],
        [ -5.8552,   1.2400],
        [ -8.8430,   7.4347],
        [ -9.6609,   8.2654],
        [-10.5671,   7.8135],
        [ -4.8169,   3.0821],
        [  6.6309,  -8.4054],
        [ -5.5400,   3.9757],
        [ -4.0865,   2.5651]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.8120, 0.1880],
        [0.2149, 0.7851]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4416, 0.5584], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1978, 0.0901],
         [0.7219, 0.2957]],

        [[0.2698, 0.0952],
         [0.9252, 0.0048]],

        [[0.1993, 0.1031],
         [0.5792, 0.9491]],

        [[0.5255, 0.1011],
         [0.1116, 0.5698]],

        [[0.8580, 0.0889],
         [0.1995, 0.1524]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
time is 2
tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 3
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9603205178782778
Average Adjusted Rand Index: 0.9603199464905721
Iteration 0: Loss = -21922.214723376987
Iteration 10: Loss = -11294.807211852785
Iteration 20: Loss = -11287.623689843624
Iteration 30: Loss = -11224.48622303137
Iteration 40: Loss = -10977.563263825445
Iteration 50: Loss = -10971.362550286834
Iteration 60: Loss = -10971.360807343177
Iteration 70: Loss = -10971.360829183124
1
Iteration 80: Loss = -10971.360822838762
2
Iteration 90: Loss = -10971.360822838762
3
Stopping early at iteration 89 due to no improvement.
pi: tensor([[0.7615, 0.2385],
        [0.2033, 0.7967]], dtype=torch.float64)
alpha: tensor([0.4894, 0.5106])
beta: tensor([[[0.2921, 0.0898],
         [0.8166, 0.1928]],

        [[0.4550, 0.0961],
         [0.6982, 0.9635]],

        [[0.8448, 0.1026],
         [0.7083, 0.1480]],

        [[0.9777, 0.1010],
         [0.9274, 0.9902]],

        [[0.3669, 0.0893],
         [0.7860, 0.9557]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 1
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448351863643042
time is 2
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824062740165256
time is 3
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 4
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9214426683307702
Average Adjusted Rand Index: 0.9214475084437244
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21910.656260796615
Iteration 100: Loss = -11383.152163769657
Iteration 200: Loss = -11328.172339708533
Iteration 300: Loss = -11296.550522980888
Iteration 400: Loss = -11229.112227897194
Iteration 500: Loss = -11227.289584785354
Iteration 600: Loss = -11226.146098657227
Iteration 700: Loss = -11225.288470714322
Iteration 800: Loss = -11224.97300230078
Iteration 900: Loss = -11224.73946244313
Iteration 1000: Loss = -11224.455709438978
Iteration 1100: Loss = -11224.284736784539
Iteration 1200: Loss = -11224.108152187115
Iteration 1300: Loss = -11224.005185669397
Iteration 1400: Loss = -11223.872811057947
Iteration 1500: Loss = -11223.74180321683
Iteration 1600: Loss = -11223.650918248764
Iteration 1700: Loss = -11223.547448042325
Iteration 1800: Loss = -11223.414365524408
Iteration 1900: Loss = -11223.287649482396
Iteration 2000: Loss = -11222.80726433003
Iteration 2100: Loss = -11205.54617421013
Iteration 2200: Loss = -11188.882844737964
Iteration 2300: Loss = -11021.185374569
Iteration 2400: Loss = -11000.859965563246
Iteration 2500: Loss = -10990.225253989458
Iteration 2600: Loss = -10981.842540660664
Iteration 2700: Loss = -10981.708276715342
Iteration 2800: Loss = -10978.009538979331
Iteration 2900: Loss = -10977.272780553187
Iteration 3000: Loss = -10977.216342805244
Iteration 3100: Loss = -10977.192019510492
Iteration 3200: Loss = -10977.180601135029
Iteration 3300: Loss = -10977.170797498296
Iteration 3400: Loss = -10977.016362077078
Iteration 3500: Loss = -10976.737272415285
Iteration 3600: Loss = -10976.725716936166
Iteration 3700: Loss = -10976.716118582028
Iteration 3800: Loss = -10976.710575335652
Iteration 3900: Loss = -10976.40216306437
Iteration 4000: Loss = -10976.311851708384
Iteration 4100: Loss = -10976.308675350287
Iteration 4200: Loss = -10976.306162740864
Iteration 4300: Loss = -10976.303914009395
Iteration 4400: Loss = -10976.30193682985
Iteration 4500: Loss = -10976.300163795171
Iteration 4600: Loss = -10976.299113524126
Iteration 4700: Loss = -10976.295537643942
Iteration 4800: Loss = -10976.205473181042
Iteration 4900: Loss = -10976.32583812639
1
Iteration 5000: Loss = -10976.200866907846
Iteration 5100: Loss = -10976.199759415837
Iteration 5200: Loss = -10976.204885915977
1
Iteration 5300: Loss = -10976.197565445598
Iteration 5400: Loss = -10976.196235090376
Iteration 5500: Loss = -10976.196458886956
1
Iteration 5600: Loss = -10976.20064132957
2
Iteration 5700: Loss = -10976.192578294584
Iteration 5800: Loss = -10976.206234113235
1
Iteration 5900: Loss = -10976.190671926048
Iteration 6000: Loss = -10976.189865762353
Iteration 6100: Loss = -10976.188732079527
Iteration 6200: Loss = -10976.183004216995
Iteration 6300: Loss = -10976.181388957653
Iteration 6400: Loss = -10976.180144948052
Iteration 6500: Loss = -10976.186328554686
1
Iteration 6600: Loss = -10976.179161600412
Iteration 6700: Loss = -10976.178831812438
Iteration 6800: Loss = -10976.178685839015
Iteration 6900: Loss = -10976.178258546492
Iteration 7000: Loss = -10976.19367284162
1
Iteration 7100: Loss = -10976.177743812004
Iteration 7200: Loss = -10976.183581855568
1
Iteration 7300: Loss = -10976.176464345095
Iteration 7400: Loss = -10966.406258152947
Iteration 7500: Loss = -10966.405907915305
Iteration 7600: Loss = -10966.405618739249
Iteration 7700: Loss = -10966.40554660686
Iteration 7800: Loss = -10966.469450216962
1
Iteration 7900: Loss = -10966.405017585406
Iteration 8000: Loss = -10966.424274457273
1
Iteration 8100: Loss = -10966.404769707437
Iteration 8200: Loss = -10966.390633311106
Iteration 8300: Loss = -10966.480602887395
1
Iteration 8400: Loss = -10966.388703970031
Iteration 8500: Loss = -10966.38886786949
1
Iteration 8600: Loss = -10966.389419857
2
Iteration 8700: Loss = -10966.417040859196
3
Iteration 8800: Loss = -10966.396210097046
4
Iteration 8900: Loss = -10966.388685839462
Iteration 9000: Loss = -10966.388083921944
Iteration 9100: Loss = -10965.401383872842
Iteration 9200: Loss = -10965.411623108224
1
Iteration 9300: Loss = -10965.409527834723
2
Iteration 9400: Loss = -10965.401241306796
Iteration 9500: Loss = -10965.400077769782
Iteration 9600: Loss = -10965.402929027368
1
Iteration 9700: Loss = -10965.404631969941
2
Iteration 9800: Loss = -10965.433357784597
3
Iteration 9900: Loss = -10965.490612214886
4
Iteration 10000: Loss = -10965.409353768935
5
Iteration 10100: Loss = -10965.399598156891
Iteration 10200: Loss = -10965.399456610772
Iteration 10300: Loss = -10965.416867223224
1
Iteration 10400: Loss = -10965.40658753893
2
Iteration 10500: Loss = -10965.39961379164
3
Iteration 10600: Loss = -10965.399291539014
Iteration 10700: Loss = -10965.408492498744
1
Iteration 10800: Loss = -10965.4005898935
2
Iteration 10900: Loss = -10965.399380674236
3
Iteration 11000: Loss = -10965.405775556977
4
Iteration 11100: Loss = -10965.369584103411
Iteration 11200: Loss = -10965.386926779875
1
Iteration 11300: Loss = -10965.366233981587
Iteration 11400: Loss = -10965.446538826405
1
Iteration 11500: Loss = -10965.366197647916
Iteration 11600: Loss = -10965.382804415149
1
Iteration 11700: Loss = -10965.374018970488
2
Iteration 11800: Loss = -10965.366338397978
3
Iteration 11900: Loss = -10965.366362166138
4
Iteration 12000: Loss = -10965.36718993771
5
Iteration 12100: Loss = -10965.36713988069
6
Iteration 12200: Loss = -10965.366291395689
7
Iteration 12300: Loss = -10965.366331209503
8
Iteration 12400: Loss = -10965.365981877836
Iteration 12500: Loss = -10965.407899601601
1
Iteration 12600: Loss = -10965.364008687002
Iteration 12700: Loss = -10965.380027235315
1
Iteration 12800: Loss = -10965.363874376682
Iteration 12900: Loss = -10965.368502464906
1
Iteration 13000: Loss = -10965.269016927614
Iteration 13100: Loss = -10965.292044542373
1
Iteration 13200: Loss = -10965.268758119119
Iteration 13300: Loss = -10965.321586308559
1
Iteration 13400: Loss = -10965.26829841531
Iteration 13500: Loss = -10965.273848230448
1
Iteration 13600: Loss = -10965.278137797633
2
Iteration 13700: Loss = -10965.270554250834
3
Iteration 13800: Loss = -10965.269579851805
4
Iteration 13900: Loss = -10965.26838454027
5
Iteration 14000: Loss = -10965.2671598961
Iteration 14100: Loss = -10965.275514231214
1
Iteration 14200: Loss = -10965.267113939057
Iteration 14300: Loss = -10965.267138119736
1
Iteration 14400: Loss = -10965.273553351164
2
Iteration 14500: Loss = -10965.265947681904
Iteration 14600: Loss = -10965.265919151494
Iteration 14700: Loss = -10965.266086757232
1
Iteration 14800: Loss = -10965.266470343502
2
Iteration 14900: Loss = -10965.266018628992
3
Iteration 15000: Loss = -10965.268745243535
4
Iteration 15100: Loss = -10965.319785144378
5
Iteration 15200: Loss = -10965.267382915838
6
Iteration 15300: Loss = -10965.267066122666
7
Iteration 15400: Loss = -10965.266455295347
8
Iteration 15500: Loss = -10965.264168684527
Iteration 15600: Loss = -10965.264327462917
1
Iteration 15700: Loss = -10965.308325309996
2
Iteration 15800: Loss = -10965.26866383379
3
Iteration 15900: Loss = -10965.441275898314
4
Iteration 16000: Loss = -10965.264324554722
5
Iteration 16100: Loss = -10965.264716219117
6
Iteration 16200: Loss = -10965.268595363323
7
Iteration 16300: Loss = -10965.264166507202
Iteration 16400: Loss = -10965.265137847058
1
Iteration 16500: Loss = -10965.26421023853
2
Iteration 16600: Loss = -10965.264170397211
3
Iteration 16700: Loss = -10965.26270212501
Iteration 16800: Loss = -10965.262922265742
1
Iteration 16900: Loss = -10965.262722314128
2
Iteration 17000: Loss = -10965.283565635682
3
Iteration 17100: Loss = -10965.262701395688
Iteration 17200: Loss = -10965.256248795462
Iteration 17300: Loss = -10965.256675645314
1
Iteration 17400: Loss = -10965.26134348845
2
Iteration 17500: Loss = -10965.255903939913
Iteration 17600: Loss = -10965.335749370175
1
Iteration 17700: Loss = -10965.254170293447
Iteration 17800: Loss = -10965.254413341569
1
Iteration 17900: Loss = -10965.365818821336
2
Iteration 18000: Loss = -10965.254144829058
Iteration 18100: Loss = -10965.256115941573
1
Iteration 18200: Loss = -10965.250976069678
Iteration 18300: Loss = -10965.252169118105
1
Iteration 18400: Loss = -10965.25098930804
2
Iteration 18500: Loss = -10965.25222812969
3
Iteration 18600: Loss = -10965.250991106388
4
Iteration 18700: Loss = -10965.255669330329
5
Iteration 18800: Loss = -10965.250828657874
Iteration 18900: Loss = -10965.260240839123
1
Iteration 19000: Loss = -10965.25083057863
2
Iteration 19100: Loss = -10965.267455364157
3
Iteration 19200: Loss = -10965.254034091904
4
Iteration 19300: Loss = -10965.251396595244
5
Iteration 19400: Loss = -10965.250793856783
Iteration 19500: Loss = -10965.251488070178
1
Iteration 19600: Loss = -10965.379248672178
2
Iteration 19700: Loss = -10965.250794742935
3
Iteration 19800: Loss = -10965.25256478166
4
Iteration 19900: Loss = -10965.251432234585
5
tensor([[  8.4837,  -9.8723],
        [  8.9990, -10.4322],
        [ -6.1236,   4.5346],
        [  5.3683,  -7.0762],
        [-11.9401,   7.3249],
        [  2.8115,  -4.9263],
        [ -2.8865,   1.0338],
        [-10.7200,   9.1829],
        [  7.9895, -10.0736],
        [ -7.0620,   4.5443],
        [  8.0024,  -9.4041],
        [  9.0739, -10.5996],
        [ -5.6516,   4.0764],
        [ -4.4260,   2.9470],
        [  8.7828, -10.6572],
        [  7.8761,  -9.7750],
        [  8.3692, -10.1416],
        [  8.1666,  -9.7889],
        [  0.5528,  -1.9671],
        [  2.1770,  -3.5698],
        [ -9.2064,   5.2330],
        [  8.6311, -10.0664],
        [ -9.3559,   7.7127],
        [  8.8002, -10.7634],
        [ -7.9393,   6.3694],
        [ -8.3872,   6.8528],
        [  8.3715, -12.9867],
        [  9.2821, -13.1291],
        [ -1.0312,  -2.9733],
        [  4.9673,  -6.4365],
        [ -7.9094,   6.1726],
        [  8.4031, -10.8701],
        [  0.5383,  -5.1535],
        [  4.8923,  -6.3225],
        [  9.1863, -10.9719],
        [  1.4503,  -4.7274],
        [ -9.1244,   7.7313],
        [ -5.3300,   3.9388],
        [ -7.8150,   6.3448],
        [  3.8347,  -5.5866],
        [ -2.1978,   0.7995],
        [ -3.4963,   1.7981],
        [-10.0390,   5.4238],
        [ -8.0547,   5.8190],
        [ -8.0367,   6.4499],
        [ -8.0079,   5.1530],
        [ -5.0457,   3.6585],
        [  7.8410,  -9.2273],
        [ -7.9963,   5.3602],
        [  0.9479,  -2.5769],
        [  8.4627,  -9.9131],
        [ -8.2278,   6.4889],
        [ -8.7603,   7.3673],
        [ -5.2439,   3.8560],
        [-10.6107,   9.1553],
        [ -7.4305,   6.0172],
        [  1.4850,  -3.8421],
        [  0.3321,  -1.9397],
        [  9.4996, -10.9036],
        [ -5.7346,   4.0719],
        [  7.2580,  -8.8980],
        [ -5.7257,   3.2446],
        [  9.1406, -10.7641],
        [ -7.4235,   4.9243],
        [ -1.5522,  -0.9506],
        [  8.6960, -10.3252],
        [  0.3129,  -1.7088],
        [  6.7647,  -8.4070],
        [  6.7576,  -8.1442],
        [  5.8542,  -8.4270],
        [  8.9520, -10.3752],
        [ -6.5203,   5.1222],
        [  5.1932,  -6.8031],
        [ -0.9199,  -2.9894],
        [  8.8622, -10.4642],
        [ -5.8759,   2.8463],
        [ -7.8797,   6.3990],
        [  2.7849,  -7.4002],
        [ -8.5012,   5.2302],
        [  9.2554, -10.8599],
        [ -5.5174,   3.6681],
        [ -6.1804,   3.2674],
        [ -3.8066,   1.5650],
        [ -4.1025,   1.9167],
        [  7.7887,  -9.3410],
        [ -5.6132,   4.1475],
        [ -4.5397,   2.1595],
        [  8.4952, -10.3060],
        [  8.8576, -11.0948],
        [  8.4203, -12.0580],
        [  8.8702, -12.2535],
        [ -7.8374,   6.4436],
        [  2.1905,  -4.9155],
        [  8.5664, -10.5941],
        [  9.5158, -11.6660],
        [  7.9607, -10.3340],
        [  2.8188,  -5.0877],
        [ -8.6031,   6.6199],
        [  9.4926, -10.9747],
        [  6.6101,  -9.3582]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7843, 0.2157],
        [0.1870, 0.8130]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5584, 0.4416], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2966, 0.0901],
         [0.8166, 0.1971]],

        [[0.4550, 0.0958],
         [0.6982, 0.9635]],

        [[0.8448, 0.1030],
         [0.7083, 0.1480]],

        [[0.9777, 0.1011],
         [0.9274, 0.9902]],

        [[0.3669, 0.0893],
         [0.7860, 0.9557]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 2
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 3
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9603205178782778
Average Adjusted Rand Index: 0.9603199464905721
10989.752095543823
new:  [0.9603205178782778, 0.35392714295470945, 0.9603205178782778, 0.9603205178782778] [0.9603199464905721, 0.8413957922532038, 0.9603199464905721, 0.9603199464905721] [10965.233409589384, 11049.05104922283, 10965.286765150564, 10965.2509160167]
prior:  [0.9214426683307702, 0.9214426683307702, 0.9214426683307702, 0.9214426683307702] [0.9214475084437244, 0.9214475084437244, 0.9214475084437244, 0.9214475084437244] [10971.360817375771, 10971.360817375771, 10971.36081841047, 10971.360822838762]
-----------------------------------------------------------------------------------------
This iteration is 14
True Objective function: Loss = -11113.854306225014
Iteration 0: Loss = -39343.42402227703
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.6820,    nan]],

        [[0.6954,    nan],
         [0.4465, 0.0114]],

        [[0.0191,    nan],
         [0.7798, 0.6775]],

        [[0.9788,    nan],
         [0.6987, 0.0876]],

        [[0.3514,    nan],
         [0.2273, 0.4671]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -38344.96891023567
Iteration 100: Loss = -11473.37132066049
Iteration 200: Loss = -11469.562805112953
Iteration 300: Loss = -11468.55352370508
Iteration 400: Loss = -11467.92752162809
Iteration 500: Loss = -11467.232024307328
Iteration 600: Loss = -11466.584426970332
Iteration 700: Loss = -11466.21400669121
Iteration 800: Loss = -11465.973245876654
Iteration 900: Loss = -11465.443896871455
Iteration 1000: Loss = -11465.135655233791
Iteration 1100: Loss = -11464.77308890358
Iteration 1200: Loss = -11463.47399008997
Iteration 1300: Loss = -11457.511114730696
Iteration 1400: Loss = -11421.26436102489
Iteration 1500: Loss = -11380.777047002008
Iteration 1600: Loss = -11358.822049785307
Iteration 1700: Loss = -11300.297770902469
Iteration 1800: Loss = -11269.333059425207
Iteration 1900: Loss = -11246.811801442782
Iteration 2000: Loss = -11222.091852406229
Iteration 2100: Loss = -11215.245059169356
Iteration 2200: Loss = -11210.565453675163
Iteration 2300: Loss = -11200.22027414791
Iteration 2400: Loss = -11189.7311632013
Iteration 2500: Loss = -11189.306403742845
Iteration 2600: Loss = -11188.997387238473
Iteration 2700: Loss = -11179.183022026464
Iteration 2800: Loss = -11178.803033922608
Iteration 2900: Loss = -11178.573172357344
Iteration 3000: Loss = -11178.127282036745
Iteration 3100: Loss = -11178.07548628683
Iteration 3200: Loss = -11178.029940072738
Iteration 3300: Loss = -11174.77699913213
Iteration 3400: Loss = -11173.477798288777
Iteration 3500: Loss = -11173.450051737209
Iteration 3600: Loss = -11173.42823131873
Iteration 3700: Loss = -11173.399987709396
Iteration 3800: Loss = -11173.306047784436
Iteration 3900: Loss = -11173.236479630212
Iteration 4000: Loss = -11173.219138610515
Iteration 4100: Loss = -11173.197238740791
Iteration 4200: Loss = -11173.090831080322
Iteration 4300: Loss = -11172.942480551681
Iteration 4400: Loss = -11172.922081627956
Iteration 4500: Loss = -11172.909207801114
Iteration 4600: Loss = -11172.898695693835
Iteration 4700: Loss = -11172.889917236182
Iteration 4800: Loss = -11172.87855795048
Iteration 4900: Loss = -11172.848889797213
Iteration 5000: Loss = -11172.800487455526
Iteration 5100: Loss = -11172.78728897028
Iteration 5200: Loss = -11172.777099132887
Iteration 5300: Loss = -11172.763101584043
Iteration 5400: Loss = -11171.826971393059
Iteration 5500: Loss = -11171.602001566625
Iteration 5600: Loss = -11171.595112541858
Iteration 5700: Loss = -11171.580626217947
Iteration 5800: Loss = -11171.524424281382
Iteration 5900: Loss = -11171.520377875104
Iteration 6000: Loss = -11171.517434256513
Iteration 6100: Loss = -11171.518919986818
1
Iteration 6200: Loss = -11171.513664871876
Iteration 6300: Loss = -11171.50866915266
Iteration 6400: Loss = -11171.078166466654
Iteration 6500: Loss = -11170.844313871326
Iteration 6600: Loss = -11170.84583878072
1
Iteration 6700: Loss = -11170.839720457967
Iteration 6800: Loss = -11170.870737473046
1
Iteration 6900: Loss = -11170.836711918613
Iteration 7000: Loss = -11170.835614200607
Iteration 7100: Loss = -11170.83595298078
1
Iteration 7200: Loss = -11170.834045079488
Iteration 7300: Loss = -11170.833387276958
Iteration 7400: Loss = -11170.83316847263
Iteration 7500: Loss = -11170.839241912872
1
Iteration 7600: Loss = -11170.831480057399
Iteration 7700: Loss = -11170.831592581699
1
Iteration 7800: Loss = -11170.830032678085
Iteration 7900: Loss = -11170.830388456061
1
Iteration 8000: Loss = -11170.830460428504
2
Iteration 8100: Loss = -11170.830256530417
3
Iteration 8200: Loss = -11170.827851690878
Iteration 8300: Loss = -11170.825914288067
Iteration 8400: Loss = -11170.82767849492
1
Iteration 8500: Loss = -11170.824095687221
Iteration 8600: Loss = -11170.806289740884
Iteration 8700: Loss = -11170.807746019253
1
Iteration 8800: Loss = -11170.765627482111
Iteration 8900: Loss = -11170.765865609359
1
Iteration 9000: Loss = -11170.763743081738
Iteration 9100: Loss = -11170.764659758112
1
Iteration 9200: Loss = -11170.782978562358
2
Iteration 9300: Loss = -11170.764807737618
3
Iteration 9400: Loss = -11170.769178932997
4
Iteration 9500: Loss = -11170.76556255828
5
Iteration 9600: Loss = -11170.762725929544
Iteration 9700: Loss = -11164.447134766682
Iteration 9800: Loss = -11164.355041257788
Iteration 9900: Loss = -11164.353099933081
Iteration 10000: Loss = -11164.364698668582
1
Iteration 10100: Loss = -11164.355452041027
2
Iteration 10200: Loss = -11164.348262525926
Iteration 10300: Loss = -11164.348757865615
1
Iteration 10400: Loss = -11164.357794043503
2
Iteration 10500: Loss = -11164.347950823041
Iteration 10600: Loss = -11164.349696983094
1
Iteration 10700: Loss = -11164.353026001334
2
Iteration 10800: Loss = -11164.349254552306
3
Iteration 10900: Loss = -11164.34906768349
4
Iteration 11000: Loss = -11164.411883742623
5
Iteration 11100: Loss = -11164.347559197211
Iteration 11200: Loss = -11164.354411012706
1
Iteration 11300: Loss = -11164.378181414177
2
Iteration 11400: Loss = -11164.352100742472
3
Iteration 11500: Loss = -11164.351960588592
4
Iteration 11600: Loss = -11164.352401545655
5
Iteration 11700: Loss = -11164.353974808611
6
Iteration 11800: Loss = -11164.34889726924
7
Iteration 11900: Loss = -11164.347403313144
Iteration 12000: Loss = -11164.34930494813
1
Iteration 12100: Loss = -11164.347136198197
Iteration 12200: Loss = -11164.347613972663
1
Iteration 12300: Loss = -11164.347101549787
Iteration 12400: Loss = -11164.347983200902
1
Iteration 12500: Loss = -11164.347025232742
Iteration 12600: Loss = -11164.346956810134
Iteration 12700: Loss = -11164.369575829176
1
Iteration 12800: Loss = -11164.346876787253
Iteration 12900: Loss = -11164.34744355776
1
Iteration 13000: Loss = -11164.34824473961
2
Iteration 13100: Loss = -11164.350893294115
3
Iteration 13200: Loss = -11164.358772882024
4
Iteration 13300: Loss = -11164.346890623072
5
Iteration 13400: Loss = -11164.347162190394
6
Iteration 13500: Loss = -11164.348089483425
7
Iteration 13600: Loss = -11164.347816439195
8
Iteration 13700: Loss = -11164.390105567629
9
Iteration 13800: Loss = -11164.358364972162
10
Stopping early at iteration 13800 due to no improvement.
tensor([[-1.0624e+01,  6.0085e+00],
        [-7.4080e+00,  2.7927e+00],
        [-1.3499e+00, -3.2653e+00],
        [-9.1985e-01, -3.6954e+00],
        [ 3.1732e-01, -4.9325e+00],
        [ 2.0691e+00, -6.6844e+00],
        [-8.8506e+00,  4.2354e+00],
        [ 3.5384e-01, -4.9691e+00],
        [-8.1662e+00,  3.5510e+00],
        [-8.2560e+00,  3.6408e+00],
        [-7.5320e+00,  2.9168e+00],
        [-8.4096e+00,  3.7943e+00],
        [-9.1360e+00,  4.5208e+00],
        [-8.5445e+00,  3.9293e+00],
        [-6.3006e+00,  1.6853e+00],
        [-9.1281e+00,  4.5129e+00],
        [-3.3367e+00, -1.2785e+00],
        [-5.1602e+00,  5.4500e-01],
        [-3.7833e+00, -8.3196e-01],
        [ 6.2719e-01, -5.2424e+00],
        [ 2.9017e+00, -7.5169e+00],
        [-6.4742e+00,  1.8589e+00],
        [-7.8523e+00,  3.2371e+00],
        [-2.5053e-01, -4.3647e+00],
        [-2.6403e+00, -1.9749e+00],
        [-9.5660e+00,  4.9507e+00],
        [-8.1492e+00,  3.5340e+00],
        [-7.8453e+00,  3.2301e+00],
        [-4.3478e-01, -4.1804e+00],
        [-4.4255e+00, -1.8974e-01],
        [ 7.4843e-01, -5.3637e+00],
        [ 2.0293e+00, -6.6445e+00],
        [-9.0504e+00,  4.4352e+00],
        [-8.7293e-03, -4.6065e+00],
        [-3.0531e+00, -1.5622e+00],
        [-4.1204e+00, -4.9484e-01],
        [ 1.2279e+00, -5.8431e+00],
        [-2.8333e+00, -1.7819e+00],
        [-9.0516e+00,  4.4364e+00],
        [-1.4484e+00, -3.1668e+00],
        [-5.1020e+00,  4.8678e-01],
        [-3.4941e-01, -4.2658e+00],
        [-7.8443e+00,  3.2291e+00],
        [-9.3582e+00,  4.7430e+00],
        [-5.3537e+00,  7.3843e-01],
        [ 3.8642e+00, -8.4794e+00],
        [-8.4412e+00,  3.8260e+00],
        [ 1.0416e+00, -5.6569e+00],
        [-8.2409e+00,  3.6256e+00],
        [-6.7741e+00,  2.1589e+00],
        [-7.5039e+00,  2.8887e+00],
        [-6.0635e-01, -4.0089e+00],
        [ 7.9899e-01, -5.4142e+00],
        [-5.7096e+00,  1.0944e+00],
        [ 3.9837e+00, -8.5989e+00],
        [-8.5772e+00,  3.9620e+00],
        [-1.0963e+00, -3.5189e+00],
        [-8.6636e+00,  4.0484e+00],
        [-8.1276e+00,  3.5124e+00],
        [-5.3438e+00,  7.2857e-01],
        [ 2.7972e+00, -7.4124e+00],
        [-8.7640e+00,  4.1488e+00],
        [-7.9499e+00,  3.3347e+00],
        [ 2.0534e+00, -6.6686e+00],
        [ 9.6389e-01, -5.5791e+00],
        [-1.5615e+00, -3.0538e+00],
        [-9.2437e+00,  4.6285e+00],
        [-9.5605e+00,  4.9453e+00],
        [-7.9421e+00,  3.3269e+00],
        [-7.2882e+00,  2.6729e+00],
        [-1.0295e+01,  5.6798e+00],
        [-8.9799e+00,  4.3647e+00],
        [-9.5570e+00,  4.9418e+00],
        [-1.5378e+00, -3.0775e+00],
        [-9.0997e+00,  4.4845e+00],
        [-9.3003e+00,  4.6851e+00],
        [-3.2626e+00, -1.3526e+00],
        [ 2.6847e+00, -7.2999e+00],
        [ 2.2420e+00, -6.8572e+00],
        [-5.7391e+00,  1.1239e+00],
        [ 1.8395e+00, -6.4547e+00],
        [-4.7839e+00,  1.6871e-01],
        [ 1.3666e+00, -5.9818e+00],
        [-9.0089e+00,  4.3937e+00],
        [ 2.6238e+00, -7.2390e+00],
        [-9.1790e+00,  4.5638e+00],
        [-9.2316e+00,  4.6164e+00],
        [-3.4211e-01, -4.2731e+00],
        [-5.8554e+00,  1.2401e+00],
        [-6.3501e-01, -3.9802e+00],
        [-6.4330e+00,  1.8178e+00],
        [-7.9437e+00,  3.3285e+00],
        [-8.9201e+00,  4.3049e+00],
        [-3.9486e+00, -6.6666e-01],
        [-7.4260e-02, -4.5410e+00],
        [-9.0136e+00,  4.3983e+00],
        [-7.9771e+00,  3.3619e+00],
        [-6.9078e+00,  2.2926e+00],
        [-9.2261e+00,  4.6109e+00],
        [-7.7254e+00,  3.1102e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6952, 0.3048],
        [0.3661, 0.6339]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3426, 0.6574], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2886, 0.0912],
         [0.6820, 0.2200]],

        [[0.6954, 0.0882],
         [0.4465, 0.0114]],

        [[0.0191, 0.0906],
         [0.7798, 0.6775]],

        [[0.9788, 0.0908],
         [0.6987, 0.0876]],

        [[0.3514, 0.1071],
         [0.2273, 0.4671]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 85
Adjusted Rand Index: 0.4853143013999451
time is 1
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 4
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824165642894751
Global Adjusted Rand Index: 0.38815048037122996
Average Adjusted Rand Index: 0.8266769975767113
Iteration 0: Loss = -25561.454024468596
Iteration 10: Loss = -11461.801099319628
Iteration 20: Loss = -11450.41928667726
Iteration 30: Loss = -11324.35391404638
Iteration 40: Loss = -11100.89671270935
Iteration 50: Loss = -11100.936404137818
1
Iteration 60: Loss = -11100.937970689427
2
Iteration 70: Loss = -11100.93800610327
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[0.7430, 0.2570],
        [0.2747, 0.7253]], dtype=torch.float64)
alpha: tensor([0.5184, 0.4816])
beta: tensor([[[0.2955, 0.0950],
         [0.3181, 0.1970]],

        [[0.0324, 0.0883],
         [0.3840, 0.4672]],

        [[0.0165, 0.0909],
         [0.4102, 0.2857]],

        [[0.8670, 0.0923],
         [0.1584, 0.8189]],

        [[0.0406, 0.1086],
         [0.5874, 0.7914]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 4
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
Global Adjusted Rand Index: 0.9291542558103376
Average Adjusted Rand Index: 0.92929042477414
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25561.330968371494
Iteration 100: Loss = -11503.559755355664
Iteration 200: Loss = -11473.596662929973
Iteration 300: Loss = -11467.540583346821
Iteration 400: Loss = -11464.611305884631
Iteration 500: Loss = -11462.348060744394
Iteration 600: Loss = -11460.481532519465
Iteration 700: Loss = -11458.113912947028
Iteration 800: Loss = -11456.774918204823
Iteration 900: Loss = -11455.821218120684
Iteration 1000: Loss = -11454.496977082737
Iteration 1100: Loss = -11450.257304766868
Iteration 1200: Loss = -11369.160574779273
Iteration 1300: Loss = -11261.24664454565
Iteration 1400: Loss = -11235.078680747356
Iteration 1500: Loss = -11224.627620542286
Iteration 1600: Loss = -11211.710695022311
Iteration 1700: Loss = -11210.983303269211
Iteration 1800: Loss = -11210.597600654788
Iteration 1900: Loss = -11210.350579527141
Iteration 2000: Loss = -11210.075008277467
Iteration 2100: Loss = -11207.747292286378
Iteration 2200: Loss = -11167.170968782266
Iteration 2300: Loss = -11165.892419704573
Iteration 2400: Loss = -11151.112709416537
Iteration 2500: Loss = -11134.985392241606
Iteration 2600: Loss = -11127.544200315582
Iteration 2700: Loss = -11117.635978741351
Iteration 2800: Loss = -11117.41051097932
Iteration 2900: Loss = -11117.344683237425
Iteration 3000: Loss = -11117.301099622255
Iteration 3100: Loss = -11117.27757389947
Iteration 3200: Loss = -11117.257945577148
Iteration 3300: Loss = -11117.245563886485
Iteration 3400: Loss = -11117.237371488769
Iteration 3500: Loss = -11117.229463715486
Iteration 3600: Loss = -11117.223200586152
Iteration 3700: Loss = -11117.218308555955
Iteration 3800: Loss = -11117.213115096522
Iteration 3900: Loss = -11117.217292960324
1
Iteration 4000: Loss = -11117.205207090772
Iteration 4100: Loss = -11117.201870514344
Iteration 4200: Loss = -11117.19930996604
Iteration 4300: Loss = -11117.196651097747
Iteration 4400: Loss = -11117.207451976014
1
Iteration 4500: Loss = -11117.191587962581
Iteration 4600: Loss = -11117.18956678281
Iteration 4700: Loss = -11117.19041006705
1
Iteration 4800: Loss = -11117.234188526232
2
Iteration 4900: Loss = -11117.18512204042
Iteration 5000: Loss = -11117.183809512657
Iteration 5100: Loss = -11117.181560977268
Iteration 5200: Loss = -11117.183671691944
1
Iteration 5300: Loss = -11117.17911887082
Iteration 5400: Loss = -11117.193116880488
1
Iteration 5500: Loss = -11117.176711397406
Iteration 5600: Loss = -11117.177113579028
1
Iteration 5700: Loss = -11111.532042360797
Iteration 5800: Loss = -11111.415210021778
Iteration 5900: Loss = -11111.408154899618
Iteration 6000: Loss = -11111.40552541181
Iteration 6100: Loss = -11111.404822604349
Iteration 6200: Loss = -11111.405297067977
1
Iteration 6300: Loss = -11111.403293570327
Iteration 6400: Loss = -11111.406295144734
1
Iteration 6500: Loss = -11111.40387173623
2
Iteration 6600: Loss = -11111.438429377398
3
Iteration 6700: Loss = -11111.394823557976
Iteration 6800: Loss = -11102.568923562316
Iteration 6900: Loss = -11102.56483902022
Iteration 7000: Loss = -11101.164567571295
Iteration 7100: Loss = -11101.158549212872
Iteration 7200: Loss = -11101.156166970777
Iteration 7300: Loss = -11101.156675350625
1
Iteration 7400: Loss = -11101.154234736365
Iteration 7500: Loss = -11101.153680596295
Iteration 7600: Loss = -11101.160595020978
1
Iteration 7700: Loss = -11101.15296481535
Iteration 7800: Loss = -11101.152989021692
1
Iteration 7900: Loss = -11101.152718056597
Iteration 8000: Loss = -11101.154212294341
1
Iteration 8100: Loss = -11101.157217475893
2
Iteration 8200: Loss = -11101.154357437257
3
Iteration 8300: Loss = -11101.151662439597
Iteration 8400: Loss = -11101.169706247401
1
Iteration 8500: Loss = -11101.153434947037
2
Iteration 8600: Loss = -11101.15796233304
3
Iteration 8700: Loss = -11099.423521056571
Iteration 8800: Loss = -11099.483535854659
1
Iteration 8900: Loss = -11099.439505655288
2
Iteration 9000: Loss = -11099.436556290317
3
Iteration 9100: Loss = -11099.42109321808
Iteration 9200: Loss = -11099.404406598747
Iteration 9300: Loss = -11099.317967643063
Iteration 9400: Loss = -11099.346208860043
1
Iteration 9500: Loss = -11099.326091670622
2
Iteration 9600: Loss = -11099.31780714698
Iteration 9700: Loss = -11099.317643971062
Iteration 9800: Loss = -11099.317290024599
Iteration 9900: Loss = -11099.315771923066
Iteration 10000: Loss = -11099.315921312651
1
Iteration 10100: Loss = -11099.327205040485
2
Iteration 10200: Loss = -11099.316254859043
3
Iteration 10300: Loss = -11099.315576841971
Iteration 10400: Loss = -11099.334860982555
1
Iteration 10500: Loss = -11099.32742485743
2
Iteration 10600: Loss = -11099.316144655022
3
Iteration 10700: Loss = -11099.327322610303
4
Iteration 10800: Loss = -11099.320275390131
5
Iteration 10900: Loss = -11099.317203658451
6
Iteration 11000: Loss = -11099.316006322264
7
Iteration 11100: Loss = -11099.32851032421
8
Iteration 11200: Loss = -11099.315127360138
Iteration 11300: Loss = -11099.315268371995
1
Iteration 11400: Loss = -11099.330776032783
2
Iteration 11500: Loss = -11099.315135004967
3
Iteration 11600: Loss = -11099.322925962473
4
Iteration 11700: Loss = -11099.315065284149
Iteration 11800: Loss = -11099.33555578771
1
Iteration 11900: Loss = -11099.314898433704
Iteration 12000: Loss = -11099.426170472183
1
Iteration 12100: Loss = -11099.321170425097
2
Iteration 12200: Loss = -11099.314835821177
Iteration 12300: Loss = -11099.319623257652
1
Iteration 12400: Loss = -11098.05238161742
Iteration 12500: Loss = -11098.006378640626
Iteration 12600: Loss = -11097.89282335642
Iteration 12700: Loss = -11097.88642317691
Iteration 12800: Loss = -11097.882886876974
Iteration 12900: Loss = -11097.864825904513
Iteration 13000: Loss = -11097.864544350907
Iteration 13100: Loss = -11097.86904426067
1
Iteration 13200: Loss = -11097.87705187596
2
Iteration 13300: Loss = -11097.865429452113
3
Iteration 13400: Loss = -11097.86449273298
Iteration 13500: Loss = -11097.874007066963
1
Iteration 13600: Loss = -11097.864409921043
Iteration 13700: Loss = -11097.866544388788
1
Iteration 13800: Loss = -11097.86445141894
2
Iteration 13900: Loss = -11097.863784179583
Iteration 14000: Loss = -11097.850112765787
Iteration 14100: Loss = -11097.846583185927
Iteration 14200: Loss = -11097.861367068552
1
Iteration 14300: Loss = -11097.849252731172
2
Iteration 14400: Loss = -11097.842816292889
Iteration 14500: Loss = -11097.870443151478
1
Iteration 14600: Loss = -11097.842179431977
Iteration 14700: Loss = -11097.98553194074
1
Iteration 14800: Loss = -11097.841852123627
Iteration 14900: Loss = -11097.841813705847
Iteration 15000: Loss = -11097.843737212119
1
Iteration 15100: Loss = -11097.852162567782
2
Iteration 15200: Loss = -11097.842680769365
3
Iteration 15300: Loss = -11097.845336290518
4
Iteration 15400: Loss = -11097.841778180391
Iteration 15500: Loss = -11097.855368419865
1
Iteration 15600: Loss = -11097.841797512305
2
Iteration 15700: Loss = -11097.843298533297
3
Iteration 15800: Loss = -11097.84821868535
4
Iteration 15900: Loss = -11097.841865658642
5
Iteration 16000: Loss = -11097.84265097088
6
Iteration 16100: Loss = -11097.842373385121
7
Iteration 16200: Loss = -11097.842170701379
8
Iteration 16300: Loss = -11097.845843397621
9
Iteration 16400: Loss = -11097.854033823196
10
Stopping early at iteration 16400 due to no improvement.
tensor([[  4.7678,  -6.1590],
        [  3.8566,  -5.6531],
        [ -6.2616,   4.7236],
        [ -5.2518,   3.5146],
        [ -3.9306,   2.3755],
        [ -7.1184,   5.5796],
        [  7.0418,  -8.5400],
        [ -6.6181,   5.2299],
        [  2.1647,  -6.3377],
        [  3.3140,  -6.9545],
        [  5.7284,  -7.1949],
        [  4.1459,  -5.8404],
        [  6.5256,  -8.5766],
        [  4.3392,  -5.7329],
        [  3.0320,  -4.4184],
        [  8.1495, -10.0600],
        [ -5.7646,   1.1493],
        [ -2.5506,   1.1334],
        [ -2.9253,   0.8479],
        [ -3.7599,   2.2252],
        [ -7.7689,   5.5867],
        [  3.8565,  -5.5434],
        [  4.8853,  -6.2943],
        [ -6.6326,   4.5925],
        [ -4.5852,   3.0882],
        [  7.3778,  -8.7642],
        [  4.0001,  -5.8172],
        [  4.5780,  -6.5389],
        [ -5.0113,   3.4543],
        [ -2.3317,   0.3905],
        [ -6.4688,   5.0679],
        [ -6.0466,   4.5299],
        [  5.1008,  -6.4938],
        [ -9.2023,   6.3167],
        [ -1.7081,   0.1493],
        [ -3.7202,   2.2219],
        [ -4.6137,   2.7847],
        [ -5.7714,   4.3321],
        [  6.9317,  -8.3257],
        [ -3.4638,   2.0472],
        [ -5.2364,   3.7732],
        [ -3.9585,   2.0306],
        [  4.2686,  -6.7009],
        [  5.4405,  -9.4321],
        [ -4.4714,   2.7611],
        [ -7.0432,   5.3097],
        [  3.3375,  -4.7474],
        [ -4.4144,   2.4449],
        [  4.5268,  -5.9350],
        [  2.4584,  -3.8447],
        [  4.3657,  -5.8511],
        [ -5.9981,   4.4150],
        [ -6.3776,   4.3592],
        [  2.7398,  -4.4453],
        [ -8.4203,   6.7331],
        [  6.2092,  -7.6207],
        [ -4.3083,   2.4910],
        [  5.9962,  -7.3942],
        [  3.0358,  -6.7783],
        [  1.7812,  -3.4218],
        [ -7.1950,   5.5283],
        [  7.0811,  -8.4706],
        [  4.7308,  -6.1942],
        [ -6.0681,   4.4237],
        [ -7.7173,   5.2768],
        [ -4.2672,   2.1785],
        [  6.9491,  -8.5417],
        [  7.3187,  -9.3625],
        [  3.0214,  -6.4739],
        [  4.0498,  -5.7063],
        [  3.4234,  -5.4545],
        [  3.1432,  -4.9093],
        [  6.2853,  -8.8873],
        [ -5.0206,   3.4410],
        [  7.4151,  -9.6123],
        [  5.6905,  -7.5268],
        [ -4.5509,   2.6808],
        [ -8.4997,   4.8244],
        [ -8.4490,   5.6519],
        [ -2.0346,   0.6276],
        [ -7.4146,   5.9933],
        [ -3.7563,   2.3560],
        [ -5.4179,   4.0134],
        [  6.3482,  -8.3364],
        [ -6.8993,   3.8336],
        [  6.4547,  -7.8596],
        [  5.8615,  -9.5333],
        [ -5.1052,   3.6611],
        [ -1.3576,  -0.1705],
        [ -5.4706,   4.0492],
        [  5.3493,  -6.8636],
        [  2.4151,  -6.0762],
        [  6.2884,  -8.6103],
        [ -4.1512,   2.4384],
        [ -3.3020,   1.7870],
        [  4.9991,  -6.6290],
        [  4.3617,  -5.9050],
        [  2.0386,  -3.5069],
        [  6.5843,  -9.2055],
        [  3.6037,  -7.3325]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7507, 0.2493],
        [0.2608, 0.7392]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5158, 0.4842], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3031, 0.0956],
         [0.3181, 0.2003]],

        [[0.0324, 0.0887],
         [0.3840, 0.4672]],

        [[0.0165, 0.0908],
         [0.4102, 0.2857]],

        [[0.8670, 0.0933],
         [0.1584, 0.8189]],

        [[0.0406, 0.1087],
         [0.5874, 0.7914]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 4
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
Global Adjusted Rand Index: 0.960320569116647
Average Adjusted Rand Index: 0.960321435933202
Iteration 0: Loss = -26250.500862673845
Iteration 10: Loss = -11467.796112796083
Iteration 20: Loss = -11467.78644050339
Iteration 30: Loss = -11459.105348246185
Iteration 40: Loss = -11454.836074281988
Iteration 50: Loss = -11445.242516334423
Iteration 60: Loss = -11108.490409235053
Iteration 70: Loss = -11100.919563055677
Iteration 80: Loss = -11100.937153869116
1
Iteration 90: Loss = -11100.937947567454
2
Iteration 100: Loss = -11100.937997753228
3
Stopping early at iteration 99 due to no improvement.
pi: tensor([[0.7253, 0.2747],
        [0.2570, 0.7430]], dtype=torch.float64)
alpha: tensor([0.4816, 0.5184])
beta: tensor([[[0.1970, 0.0950],
         [0.8228, 0.2955]],

        [[0.9948, 0.0883],
         [0.7727, 0.1078]],

        [[0.8321, 0.0909],
         [0.6649, 0.0675]],

        [[0.5697, 0.0923],
         [0.1423, 0.1750]],

        [[0.8754, 0.1086],
         [0.0464, 0.8397]]], dtype=torch.float64)
time is 0
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 3
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 4
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
Global Adjusted Rand Index: 0.9291542558103376
Average Adjusted Rand Index: 0.92929042477414
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26251.200856533287
Iteration 100: Loss = -11494.004496773092
Iteration 200: Loss = -11474.365980562921
Iteration 300: Loss = -11467.268837263045
Iteration 400: Loss = -11465.619958632215
Iteration 500: Loss = -11464.310495585372
Iteration 600: Loss = -11463.294925535762
Iteration 700: Loss = -11462.076910751368
Iteration 800: Loss = -11460.205178721675
Iteration 900: Loss = -11458.294757020243
Iteration 1000: Loss = -11454.388185146014
Iteration 1100: Loss = -11448.518130684808
Iteration 1200: Loss = -11361.713940209484
Iteration 1300: Loss = -11255.74191582801
Iteration 1400: Loss = -11207.819159514504
Iteration 1500: Loss = -11180.414791735553
Iteration 1600: Loss = -11174.300895012591
Iteration 1700: Loss = -11154.264882492334
Iteration 1800: Loss = -11153.787858499141
Iteration 1900: Loss = -11137.930731387225
Iteration 2000: Loss = -11128.485570497965
Iteration 2100: Loss = -11128.404615995769
Iteration 2200: Loss = -11128.34937393715
Iteration 2300: Loss = -11128.310708898962
Iteration 2400: Loss = -11128.2652967371
Iteration 2500: Loss = -11125.477424468705
Iteration 2600: Loss = -11125.419852858699
Iteration 2700: Loss = -11125.361372622627
Iteration 2800: Loss = -11125.339823185725
Iteration 2900: Loss = -11125.325853083055
Iteration 3000: Loss = -11125.314037033808
Iteration 3100: Loss = -11125.304088873876
Iteration 3200: Loss = -11125.294943609522
Iteration 3300: Loss = -11125.28715368867
Iteration 3400: Loss = -11125.280314653133
Iteration 3500: Loss = -11125.273808067259
Iteration 3600: Loss = -11125.267895460414
Iteration 3700: Loss = -11125.255412105646
Iteration 3800: Loss = -11118.215582785891
Iteration 3900: Loss = -11118.218086397914
1
Iteration 4000: Loss = -11118.205471558986
Iteration 4100: Loss = -11118.201843593473
Iteration 4200: Loss = -11118.204524847153
1
Iteration 4300: Loss = -11118.193844
Iteration 4400: Loss = -11118.19031222144
Iteration 4500: Loss = -11117.66367615414
Iteration 4600: Loss = -11117.66212589544
Iteration 4700: Loss = -11112.423428820635
Iteration 4800: Loss = -11112.375269537826
Iteration 4900: Loss = -11112.37530668669
1
Iteration 5000: Loss = -11112.369383757437
Iteration 5100: Loss = -11112.367474857889
Iteration 5200: Loss = -11112.365634146536
Iteration 5300: Loss = -11112.364722119664
Iteration 5400: Loss = -11112.36164361379
Iteration 5500: Loss = -11112.358803088766
Iteration 5600: Loss = -11112.344717142503
Iteration 5700: Loss = -11112.340225864495
Iteration 5800: Loss = -11112.350432451849
1
Iteration 5900: Loss = -11112.336737929732
Iteration 6000: Loss = -11112.335415666328
Iteration 6100: Loss = -11112.358253446842
1
Iteration 6200: Loss = -11112.33253834896
Iteration 6300: Loss = -11112.336101108049
1
Iteration 6400: Loss = -11112.330533908744
Iteration 6500: Loss = -11112.330757301841
1
Iteration 6600: Loss = -11112.328963364236
Iteration 6700: Loss = -11112.327921445689
Iteration 6800: Loss = -11112.32703066584
Iteration 6900: Loss = -11112.330596638136
1
Iteration 7000: Loss = -11112.262702133026
Iteration 7100: Loss = -11107.097690443099
Iteration 7200: Loss = -11107.096587945955
Iteration 7300: Loss = -11107.09629329437
Iteration 7400: Loss = -11107.095590946923
Iteration 7500: Loss = -11107.095628979185
1
Iteration 7600: Loss = -11107.095703431703
2
Iteration 7700: Loss = -11107.095634051517
3
Iteration 7800: Loss = -11107.128926153728
4
Iteration 7900: Loss = -11107.093390434491
Iteration 8000: Loss = -11107.148106876939
1
Iteration 8100: Loss = -11107.098101050276
2
Iteration 8200: Loss = -11107.092238973868
Iteration 8300: Loss = -11107.091675268262
Iteration 8400: Loss = -11107.091238301835
Iteration 8500: Loss = -11107.090800775832
Iteration 8600: Loss = -11107.10782157365
1
Iteration 8700: Loss = -11107.09286286248
2
Iteration 8800: Loss = -11107.089815933876
Iteration 8900: Loss = -11107.095149779865
1
Iteration 9000: Loss = -11107.089063118545
Iteration 9100: Loss = -11107.08799157525
Iteration 9200: Loss = -11107.08457171822
Iteration 9300: Loss = -11107.086605401602
1
Iteration 9400: Loss = -11107.096989038853
2
Iteration 9500: Loss = -11107.077859025001
Iteration 9600: Loss = -11107.08489508886
1
Iteration 9700: Loss = -11107.077138290182
Iteration 9800: Loss = -11107.080475344172
1
Iteration 9900: Loss = -11107.07603823533
Iteration 10000: Loss = -11107.075726731753
Iteration 10100: Loss = -11107.076187317567
1
Iteration 10200: Loss = -11107.07546275137
Iteration 10300: Loss = -11107.08015871432
1
Iteration 10400: Loss = -11107.075204835553
Iteration 10500: Loss = -11107.075916808926
1
Iteration 10600: Loss = -11107.075042666427
Iteration 10700: Loss = -11107.22870076831
1
Iteration 10800: Loss = -11107.074869960425
Iteration 10900: Loss = -11107.07576185113
1
Iteration 11000: Loss = -11107.080701043098
2
Iteration 11100: Loss = -11107.074444453508
Iteration 11200: Loss = -11107.076898306363
1
Iteration 11300: Loss = -11107.074200682278
Iteration 11400: Loss = -11107.081762322328
1
Iteration 11500: Loss = -11107.073567050124
Iteration 11600: Loss = -11107.17144603451
1
Iteration 11700: Loss = -11107.073414563125
Iteration 11800: Loss = -11107.102499391382
1
Iteration 11900: Loss = -11107.072356007046
Iteration 12000: Loss = -11107.071587632086
Iteration 12100: Loss = -11107.081705555716
1
Iteration 12200: Loss = -11107.071200140319
Iteration 12300: Loss = -11107.071133383304
Iteration 12400: Loss = -11107.071290746577
1
Iteration 12500: Loss = -11107.070759817896
Iteration 12600: Loss = -11107.067682845714
Iteration 12700: Loss = -11107.22916113345
1
Iteration 12800: Loss = -11107.060352105706
Iteration 12900: Loss = -11107.063634910295
1
Iteration 13000: Loss = -11107.076079482082
2
Iteration 13100: Loss = -11107.071147304254
3
Iteration 13200: Loss = -11107.060331987977
Iteration 13300: Loss = -11107.064322175445
1
Iteration 13400: Loss = -11107.099131786788
2
Iteration 13500: Loss = -11107.061969124761
3
Iteration 13600: Loss = -11107.062637331299
4
Iteration 13700: Loss = -11107.092329872865
5
Iteration 13800: Loss = -11107.05473567974
Iteration 13900: Loss = -11107.06478119044
1
Iteration 14000: Loss = -11107.054622782938
Iteration 14100: Loss = -11107.055340272957
1
Iteration 14200: Loss = -11107.061634310669
2
Iteration 14300: Loss = -11107.063294019905
3
Iteration 14400: Loss = -11107.054650441116
4
Iteration 14500: Loss = -11107.054696833355
5
Iteration 14600: Loss = -11107.094815375902
6
Iteration 14700: Loss = -11107.054503607678
Iteration 14800: Loss = -11107.199800616152
1
Iteration 14900: Loss = -11107.054502045898
Iteration 15000: Loss = -11107.05448970304
Iteration 15100: Loss = -11107.05483084581
1
Iteration 15200: Loss = -11107.056048231141
2
Iteration 15300: Loss = -11107.057131893318
3
Iteration 15400: Loss = -11107.056613152028
4
Iteration 15500: Loss = -11107.053461353786
Iteration 15600: Loss = -11107.070894022912
1
Iteration 15700: Loss = -11107.053474582895
2
Iteration 15800: Loss = -11107.053748826975
3
Iteration 15900: Loss = -11107.064821514623
4
Iteration 16000: Loss = -11107.053523990504
5
Iteration 16100: Loss = -11107.05403889425
6
Iteration 16200: Loss = -11107.054067863663
7
Iteration 16300: Loss = -11107.054664967516
8
Iteration 16400: Loss = -11107.053248013726
Iteration 16500: Loss = -11107.05353905151
1
Iteration 16600: Loss = -11107.053452680713
2
Iteration 16700: Loss = -11107.0571773441
3
Iteration 16800: Loss = -11107.053125150647
Iteration 16900: Loss = -11107.054276209261
1
Iteration 17000: Loss = -11107.053166335603
2
Iteration 17100: Loss = -11107.093287308224
3
Iteration 17200: Loss = -11107.05316346936
4
Iteration 17300: Loss = -11107.053318668639
5
Iteration 17400: Loss = -11107.053631910247
6
Iteration 17500: Loss = -11107.053121025783
Iteration 17600: Loss = -11107.055977660608
1
Iteration 17700: Loss = -11107.067566914482
2
Iteration 17800: Loss = -11107.053108052925
Iteration 17900: Loss = -11107.053685435012
1
Iteration 18000: Loss = -11107.053110836168
2
Iteration 18100: Loss = -11107.053589541682
3
Iteration 18200: Loss = -11107.086677379299
4
Iteration 18300: Loss = -11107.053951493734
5
Iteration 18400: Loss = -11107.053003506539
Iteration 18500: Loss = -11100.143026448284
Iteration 18600: Loss = -11100.11349634673
Iteration 18700: Loss = -11100.110357016903
Iteration 18800: Loss = -11100.135808535495
1
Iteration 18900: Loss = -11100.11961302301
2
Iteration 19000: Loss = -11100.109920245777
Iteration 19100: Loss = -11100.109642662177
Iteration 19200: Loss = -11100.113191804794
1
Iteration 19300: Loss = -11100.112608183748
2
Iteration 19400: Loss = -11100.142255339784
3
Iteration 19500: Loss = -11100.109540733712
Iteration 19600: Loss = -11100.110399602863
1
Iteration 19700: Loss = -11100.120712625892
2
Iteration 19800: Loss = -11100.109560219245
3
Iteration 19900: Loss = -11100.109714865117
4
tensor([[ -6.4215,   4.4935],
        [ -6.5291,   2.9579],
        [  4.7479,  -6.2783],
        [  3.6887,  -5.0985],
        [  1.7079,  -4.6416],
        [  5.6679,  -7.0701],
        [-10.0614,   8.4572],
        [  4.9219,  -6.9292],
        [ -5.6410,   2.8268],
        [-10.3813,   8.1428],
        [ -9.7375,   7.9072],
        [ -5.7641,   4.2048],
        [ -9.4786,   7.2156],
        [-10.2340,   8.4157],
        [ -4.6062,   2.8011],
        [-10.7181,   8.9849],
        [  2.7854,  -4.1735],
        [  1.0277,  -2.7148],
        [  0.4181,  -3.4137],
        [  2.2659,  -3.7843],
        [  5.9062,  -7.6860],
        [ -5.4658,   3.9087],
        [ -6.3774,   4.7718],
        [  4.9176,  -6.3505],
        [  2.0835,  -5.5951],
        [-10.3110,   8.7142],
        [ -6.2511,   3.5334],
        [ -7.1096,   3.9771],
        [  3.5499,  -4.9376],
        [  0.6060,  -2.1833],
        [  4.9693,  -6.6233],
        [  3.9899,  -6.6250],
        [ -6.5508,   5.0336],
        [  1.6626,  -3.5325],
        [  0.2315,  -1.6810],
        [  2.2464,  -3.7183],
        [  2.5897,  -4.8763],
        [  2.7766,  -7.3918],
        [ -9.1342,   7.2720],
        [  1.8809,  -3.6764],
        [  2.3406,  -6.7159],
        [  2.3242,  -3.7295],
        [ -6.1769,   4.7763],
        [ -8.1513,   6.7500],
        [  2.9474,  -4.3476],
        [  5.0576,  -7.3222],
        [ -4.8276,   3.2278],
        [  2.6286,  -4.2459],
        [ -6.3123,   4.1130],
        [ -3.8364,   2.4023],
        [ -6.2711,   3.9263],
        [  4.0863,  -6.3714],
        [  4.6844,  -6.0714],
        [ -4.8792,   2.2657],
        [  7.5645,  -9.3718],
        [ -7.7459,   6.1897],
        [  2.3430,  -4.4697],
        [ -7.9201,   5.4943],
        [ -5.6462,   4.1401],
        [ -3.4144,   1.7355],
        [  4.2567,  -8.4932],
        [-10.7504,   8.6364],
        [ -6.1623,   4.7496],
        [  3.8452,  -6.6573],
        [  5.5082,  -7.5177],
        [  2.5603,  -3.9469],
        [-10.6848,   8.6527],
        [ -9.6626,   8.2678],
        [ -6.5727,   2.9027],
        [ -5.7153,   4.0029],
        [ -5.1357,   3.7489],
        [ -4.7042,   3.3141],
        [-10.7097,   8.4308],
        [  3.2782,  -5.2176],
        [-10.4539,   8.5284],
        [ -7.2852,   5.8896],
        [  2.8151,  -4.4765],
        [  5.7778,  -7.5649],
        [  6.2501,  -7.8903],
        [  0.5355,  -2.2001],
        [  6.0847,  -7.5081],
        [  2.4038,  -3.7922],
        [  3.8232,  -5.6430],
        [ -9.4047,   6.7254],
        [  4.2600,  -6.4715],
        [ -8.0168,   6.6232],
        [ -8.4625,   7.0476],
        [  3.3746,  -5.4000],
        [ -0.4666,  -1.7259],
        [  4.0797,  -5.4916],
        [ -6.9308,   5.2826],
        [ -4.9509,   3.5306],
        [ -9.5337,   8.1402],
        [  2.5019,  -4.1500],
        [  1.1336,  -3.9842],
        [ -6.7406,   4.8790],
        [ -5.8130,   4.4267],
        [ -3.6512,   1.8547],
        [ -9.6063,   8.0250],
        [ -6.1990,   4.7268]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7376, 0.2624],
        [0.2463, 0.7537]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4811, 0.5189], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2010, 0.0946],
         [0.8228, 0.3023]],

        [[0.9948, 0.0882],
         [0.7727, 0.1078]],

        [[0.8321, 0.0920],
         [0.6649, 0.0675]],

        [[0.5697, 0.0929],
         [0.1423, 0.1750]],

        [[0.8754, 0.1078],
         [0.0464, 0.8397]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208023791611811
time is 3
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 4
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
Global Adjusted Rand Index: 0.9524808542478043
Average Adjusted Rand Index: 0.9524823506018751
Iteration 0: Loss = -29793.11774684111
Iteration 10: Loss = -11448.65763266853
Iteration 20: Loss = -11221.00450331958
Iteration 30: Loss = -11100.904302220077
Iteration 40: Loss = -11100.936442405198
1
Iteration 50: Loss = -11100.937898388609
2
Iteration 60: Loss = -11100.938000073835
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7253, 0.2747],
        [0.2570, 0.7430]], dtype=torch.float64)
alpha: tensor([0.4816, 0.5184])
beta: tensor([[[0.1970, 0.0950],
         [0.8400, 0.2955]],

        [[0.9820, 0.0883],
         [0.7850, 0.8720]],

        [[0.7214, 0.0909],
         [0.1344, 0.6197]],

        [[0.3830, 0.0923],
         [0.5886, 0.5981]],

        [[0.5604, 0.1086],
         [0.1459, 0.4807]]], dtype=torch.float64)
time is 0
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 3
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 4
tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208012930401136
Global Adjusted Rand Index: 0.9291542558103376
Average Adjusted Rand Index: 0.92929042477414
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -29792.767976630035
Iteration 100: Loss = -11566.258466395846
Iteration 200: Loss = -11518.336220333778
Iteration 300: Loss = -11498.062408751446
Iteration 400: Loss = -11482.95059678958
Iteration 500: Loss = -11476.92154906132
Iteration 600: Loss = -11469.658692774039
Iteration 700: Loss = -11466.796290505827
Iteration 800: Loss = -11466.013975519667
Iteration 900: Loss = -11465.366301766195
Iteration 1000: Loss = -11464.389391038732
Iteration 1100: Loss = -11402.681098257885
Iteration 1200: Loss = -11400.800128972804
Iteration 1300: Loss = -11397.770390475873
Iteration 1400: Loss = -11397.489468201684
Iteration 1500: Loss = -11389.686034114473
Iteration 1600: Loss = -11389.543266241195
Iteration 1700: Loss = -11389.394121937441
Iteration 1800: Loss = -11389.203136647177
Iteration 1900: Loss = -11388.951529109341
Iteration 2000: Loss = -11388.695774639249
Iteration 2100: Loss = -11388.507605476203
Iteration 2200: Loss = -11388.383273791327
Iteration 2300: Loss = -11388.297836744545
Iteration 2400: Loss = -11388.235686949287
Iteration 2500: Loss = -11388.188520098132
Iteration 2600: Loss = -11388.151810694919
Iteration 2700: Loss = -11388.122182004992
Iteration 2800: Loss = -11388.09754464577
Iteration 2900: Loss = -11388.07665538841
Iteration 3000: Loss = -11388.058708000952
Iteration 3100: Loss = -11388.04301304464
Iteration 3200: Loss = -11388.029238948558
Iteration 3300: Loss = -11388.016971329704
Iteration 3400: Loss = -11388.006090831865
Iteration 3500: Loss = -11387.996257205486
Iteration 3600: Loss = -11387.987387258121
Iteration 3700: Loss = -11387.979348974062
Iteration 3800: Loss = -11387.97203566295
Iteration 3900: Loss = -11387.965398658986
Iteration 4000: Loss = -11387.959235973469
Iteration 4100: Loss = -11387.953658529686
Iteration 4200: Loss = -11387.94859854444
Iteration 4300: Loss = -11387.943755909033
Iteration 4400: Loss = -11387.939346886664
Iteration 4500: Loss = -11387.944041620645
1
Iteration 4600: Loss = -11387.93154074022
Iteration 4700: Loss = -11387.928072019948
Iteration 4800: Loss = -11388.3663445149
1
Iteration 4900: Loss = -11387.921788467862
Iteration 5000: Loss = -11387.919007953804
Iteration 5100: Loss = -11387.91637569371
Iteration 5200: Loss = -11387.913950969038
Iteration 5300: Loss = -11387.911628018248
Iteration 5400: Loss = -11387.909528889162
Iteration 5500: Loss = -11387.91311588335
1
Iteration 5600: Loss = -11387.905625452813
Iteration 5700: Loss = -11387.918308833749
1
Iteration 5800: Loss = -11387.903687811191
Iteration 5900: Loss = -11387.900720777887
Iteration 6000: Loss = -11387.962288653305
1
Iteration 6100: Loss = -11387.897917259108
Iteration 6200: Loss = -11387.896688586756
Iteration 6300: Loss = -11387.901101908785
1
Iteration 6400: Loss = -11387.894305058304
Iteration 6500: Loss = -11387.898700591735
1
Iteration 6600: Loss = -11387.892224255953
Iteration 6700: Loss = -11387.891345492148
Iteration 6800: Loss = -11387.890421668666
Iteration 6900: Loss = -11387.889565694586
Iteration 7000: Loss = -11387.893467781863
1
Iteration 7100: Loss = -11387.888035215366
Iteration 7200: Loss = -11387.887333252042
Iteration 7300: Loss = -11387.887482534192
1
Iteration 7400: Loss = -11387.886067678926
Iteration 7500: Loss = -11387.895016119357
1
Iteration 7600: Loss = -11387.887005935918
2
Iteration 7700: Loss = -11387.885091100778
Iteration 7800: Loss = -11387.884491244069
Iteration 7900: Loss = -11387.88325840744
Iteration 8000: Loss = -11387.883601256952
1
Iteration 8100: Loss = -11387.882382484135
Iteration 8200: Loss = -11387.881976789671
Iteration 8300: Loss = -11387.907385335564
1
Iteration 8400: Loss = -11387.882753219055
2
Iteration 8500: Loss = -11388.122173046286
3
Iteration 8600: Loss = -11387.880604739401
Iteration 8700: Loss = -11387.880380107117
Iteration 8800: Loss = -11387.879963766016
Iteration 8900: Loss = -11387.879693121957
Iteration 9000: Loss = -11387.879578980477
Iteration 9100: Loss = -11387.879110071091
Iteration 9200: Loss = -11387.87958558254
1
Iteration 9300: Loss = -11387.87865284487
Iteration 9400: Loss = -11387.879471044482
1
Iteration 9500: Loss = -11387.878195516176
Iteration 9600: Loss = -11387.877906032303
Iteration 9700: Loss = -11387.877634329527
Iteration 9800: Loss = -11387.877469312321
Iteration 9900: Loss = -11387.877029095429
Iteration 10000: Loss = -11387.882594260005
1
Iteration 10100: Loss = -11387.300787066535
Iteration 10200: Loss = -11387.273278337789
Iteration 10300: Loss = -11386.66996431055
Iteration 10400: Loss = -11386.653405377469
Iteration 10500: Loss = -11386.651956517499
Iteration 10600: Loss = -11386.623683209551
Iteration 10700: Loss = -11385.493217821997
Iteration 10800: Loss = -11385.212132071245
Iteration 10900: Loss = -11384.839186476245
Iteration 11000: Loss = -11384.805308967605
Iteration 11100: Loss = -11384.626563427539
Iteration 11200: Loss = -11384.493666418202
Iteration 11300: Loss = -11384.345838978517
Iteration 11400: Loss = -11384.17828991585
Iteration 11500: Loss = -11383.83181249643
Iteration 11600: Loss = -11383.825843707837
Iteration 11700: Loss = -11383.576465775992
Iteration 11800: Loss = -11383.552767034824
Iteration 11900: Loss = -11383.48646276034
Iteration 12000: Loss = -11383.351970950822
Iteration 12100: Loss = -11383.309806270792
Iteration 12200: Loss = -11383.147921620368
Iteration 12300: Loss = -11383.029933846157
Iteration 12400: Loss = -11383.172603826008
1
Iteration 12500: Loss = -11382.926482960947
Iteration 12600: Loss = -11382.87480726914
Iteration 12700: Loss = -11382.847037038175
Iteration 12800: Loss = -11378.46663441756
Iteration 12900: Loss = -11378.118706826997
Iteration 13000: Loss = -11378.053192928835
Iteration 13100: Loss = -11377.991673221299
Iteration 13200: Loss = -11300.406039890608
Iteration 13300: Loss = -11297.281685052269
Iteration 13400: Loss = -11283.686815393925
Iteration 13500: Loss = -11282.506440478153
Iteration 13600: Loss = -11280.690835089257
Iteration 13700: Loss = -11280.397142517284
Iteration 13800: Loss = -11275.394792645417
Iteration 13900: Loss = -11274.42437126308
Iteration 14000: Loss = -11274.336086620264
Iteration 14100: Loss = -11274.298630608426
Iteration 14200: Loss = -11274.286749531184
Iteration 14300: Loss = -11274.240670513365
Iteration 14400: Loss = -11274.235469052599
Iteration 14500: Loss = -11274.276680859904
1
Iteration 14600: Loss = -11274.231250866509
Iteration 14700: Loss = -11274.227068955593
Iteration 14800: Loss = -11273.004675678401
Iteration 14900: Loss = -11272.960650964706
Iteration 15000: Loss = -11272.966651924171
1
Iteration 15100: Loss = -11272.956071005972
Iteration 15200: Loss = -11272.956424412936
1
Iteration 15300: Loss = -11272.895708254655
Iteration 15400: Loss = -11272.74499299881
Iteration 15500: Loss = -11272.744124496001
Iteration 15600: Loss = -11272.742389064068
Iteration 15700: Loss = -11272.717687977134
Iteration 15800: Loss = -11272.715545379584
Iteration 15900: Loss = -11272.582670875649
Iteration 16000: Loss = -11272.58268943544
1
Iteration 16100: Loss = -11272.483566281877
Iteration 16200: Loss = -11272.489112788822
1
Iteration 16300: Loss = -11272.481796910131
Iteration 16400: Loss = -11272.13339332268
Iteration 16500: Loss = -11272.124186946769
Iteration 16600: Loss = -11272.112462335122
Iteration 16700: Loss = -11271.329346760922
Iteration 16800: Loss = -11271.453433218761
1
Iteration 16900: Loss = -11271.307158947577
Iteration 17000: Loss = -11271.307054524223
Iteration 17100: Loss = -11271.306690074205
Iteration 17200: Loss = -11271.30495208219
Iteration 17300: Loss = -11269.775192871291
Iteration 17400: Loss = -11269.755564371262
Iteration 17500: Loss = -11269.708959259588
Iteration 17600: Loss = -11269.69338597298
Iteration 17700: Loss = -11269.69458597164
1
Iteration 17800: Loss = -11269.577598593994
Iteration 17900: Loss = -11269.575820737347
Iteration 18000: Loss = -11265.352096380171
Iteration 18100: Loss = -11265.34702117832
Iteration 18200: Loss = -11265.345811553025
Iteration 18300: Loss = -11265.32229448942
Iteration 18400: Loss = -11265.3236281357
1
Iteration 18500: Loss = -11265.321462903801
Iteration 18600: Loss = -11265.329367767763
1
Iteration 18700: Loss = -11265.320684217226
Iteration 18800: Loss = -11265.324644842029
1
Iteration 18900: Loss = -11265.319081607651
Iteration 19000: Loss = -11265.321899144968
1
Iteration 19100: Loss = -11265.314579600195
Iteration 19200: Loss = -11265.569861536473
1
Iteration 19300: Loss = -11265.314487058919
Iteration 19400: Loss = -11254.938958605635
Iteration 19500: Loss = -11235.63097627244
Iteration 19600: Loss = -11232.399584688917
Iteration 19700: Loss = -11232.354139044526
Iteration 19800: Loss = -11232.350974399757
Iteration 19900: Loss = -11232.33946214235
tensor([[-10.0629,   8.6739],
        [ -6.5852,   4.0029],
        [  3.2625,  -4.6634],
        [  2.7794,  -4.6725],
        [  1.4948,  -3.8716],
        [  6.5070,  -8.0416],
        [ -9.4968,   8.0956],
        [  5.6350,  -9.0148],
        [ -5.5920,   3.0503],
        [ -6.1770,   4.2256],
        [ -8.1492,   6.0667],
        [ -6.5603,   4.2521],
        [ -9.7907,   8.4017],
        [ -7.4920,   5.5366],
        [ -4.8489,   3.2996],
        [-10.0090,   8.4485],
        [  1.1940,  -2.6823],
        [ -0.5734,  -1.0362],
        [ -1.3644,  -0.8314],
        [  1.6032,  -3.0551],
        [  7.7517,  -9.2645],
        [ -5.9710,   3.2158],
        [ -7.2653,   5.1358],
        [  2.0826,  -5.2704],
        [  2.8514,  -4.3777],
        [ -9.8848,   8.4673],
        [ -6.6375,   4.9880],
        [ -6.4255,   5.0388],
        [  6.9235,  -8.7761],
        [ -1.0572,  -0.4057],
        [  3.2020,  -4.6109],
        [  3.3269,  -5.0184],
        [-11.6479,   8.4160],
        [  0.9686,  -2.5505],
        [ -1.0143,  -1.0472],
        [  1.5151,  -3.6979],
        [  2.2151,  -3.6738],
        [ -0.1333,  -4.4820],
        [ -8.9620,   7.5466],
        [ -0.3463,  -2.3187],
        [  2.4259,  -3.8496],
        [  1.3986,  -3.2212],
        [ -6.9431,   5.3663],
        [-11.9202,   8.8824],
        [  0.3698,  -3.1772],
        [  3.7601,  -5.1517],
        [ -6.9232,   5.0152],
        [  2.4594,  -4.4014],
        [ -8.9994,   5.0371],
        [ -5.1537,   3.5972],
        [ -6.5010,   4.9064],
        [  3.2788,  -4.6651],
        [  6.3245,  -8.4206],
        [ -5.1428,   3.5020],
        [  8.3010, -10.7569],
        [ -8.2388,   5.7644],
        [  1.6292,  -4.2113],
        [ -8.5190,   6.7787],
        [-11.1541,   6.9526],
        [ -5.0296,   2.4590],
        [  5.2675,  -6.6993],
        [ -9.3077,   7.9130],
        [ -9.9413,   8.3072],
        [  3.3785,  -6.2972],
        [  8.2885,  -9.7759],
        [ -0.5746,  -2.7998],
        [-11.9946,   7.5918],
        [-10.5473,   9.1610],
        [ -5.9924,   4.5729],
        [ -6.6892,   5.0297],
        [ -5.0086,   3.5844],
        [ -6.8603,   2.7858],
        [ -9.2707,   7.7473],
        [  4.2651,  -6.2396],
        [ -9.2714,   7.8631],
        [ -9.6216,   8.1787],
        [  1.0715,  -2.4899],
        [  7.8976,  -9.3980],
        [  4.5756,  -7.1642],
        [ -2.0266,   0.6269],
        [  7.9485,  -9.3493],
        [  2.7781,  -4.8041],
        [  3.5234,  -5.4084],
        [ -9.3242,   7.0091],
        [  2.4449,  -6.8045],
        [-10.0074,   8.3406],
        [ -8.8660,   7.3513],
        [  6.9878,  -8.3762],
        [ -2.1040,   0.6719],
        [  2.3302,  -3.8120],
        [ -6.9325,   4.7759],
        [ -5.9967,   4.3560],
        [ -7.9842,   6.4444],
        [  0.3173,  -1.7465],
        [  4.8166,  -6.2073],
        [ -9.6231,   8.2238],
        [ -6.0864,   4.5059],
        [ -4.2474,   2.8589],
        [ -9.7876,   7.9002],
        [ -6.6177,   5.2119]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.4120, 0.5880],
        [0.6514, 0.3486]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4448, 0.5552], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2426, 0.0927],
         [0.8400, 0.2596]],

        [[0.9820, 0.0915],
         [0.7850, 0.8720]],

        [[0.7214, 0.0948],
         [0.1344, 0.6197]],

        [[0.3830, 0.0893],
         [0.5886, 0.5981]],

        [[0.5604, 0.1059],
         [0.1459, 0.4807]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 1
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 14
Adjusted Rand Index: 0.5139202893869905
time is 3
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 4
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 9
Adjusted Rand Index: 0.6691665204565022
Global Adjusted Rand Index: 0.014411532691088079
Average Adjusted Rand Index: 0.751041604392941
Iteration 0: Loss = -19997.252911618598
Iteration 10: Loss = -11467.79611585447
Iteration 20: Loss = -11466.420711895462
Iteration 30: Loss = -11457.016519056186
Iteration 40: Loss = -11452.833243637433
Iteration 50: Loss = -11397.207509102362
Iteration 60: Loss = -11100.885864648673
Iteration 70: Loss = -11100.936241652824
1
Iteration 80: Loss = -11100.93807498799
2
Iteration 90: Loss = -11100.93801810557
3
Stopping early at iteration 89 due to no improvement.
pi: tensor([[0.7430, 0.2570],
        [0.2747, 0.7253]], dtype=torch.float64)
alpha: tensor([0.5184, 0.4816])
beta: tensor([[[0.2955, 0.0950],
         [0.0886, 0.1970]],

        [[0.7065, 0.0883],
         [0.4310, 0.8386]],

        [[0.2306, 0.0909],
         [0.9095, 0.8997]],

        [[0.8467, 0.0923],
         [0.7729, 0.4541]],

        [[0.9684, 0.1086],
         [0.2250, 0.1464]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 4
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
Global Adjusted Rand Index: 0.9291542558103376
Average Adjusted Rand Index: 0.92929042477414
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19996.444215624484
Iteration 100: Loss = -11470.7393251183
Iteration 200: Loss = -11469.09458019208
Iteration 300: Loss = -11467.968452915096
Iteration 400: Loss = -11466.016497707129
Iteration 500: Loss = -11463.521521550396
Iteration 600: Loss = -11461.853318068934
Iteration 700: Loss = -11459.161907382686
Iteration 800: Loss = -11457.598340967705
Iteration 900: Loss = -11456.755393997268
Iteration 1000: Loss = -11446.678020178933
Iteration 1100: Loss = -11384.33680384993
Iteration 1200: Loss = -11383.055570321718
Iteration 1300: Loss = -11382.711182925286
Iteration 1400: Loss = -11382.513666486355
Iteration 1500: Loss = -11382.373672694124
Iteration 1600: Loss = -11382.27267229261
Iteration 1700: Loss = -11382.210378410415
Iteration 1800: Loss = -11382.167204086127
Iteration 1900: Loss = -11382.13357030514
Iteration 2000: Loss = -11382.103560240597
Iteration 2100: Loss = -11382.073930010385
Iteration 2200: Loss = -11382.038544572733
Iteration 2300: Loss = -11382.003334255207
Iteration 2400: Loss = -11381.968232599731
Iteration 2500: Loss = -11381.892161578386
Iteration 2600: Loss = -11380.763871717782
Iteration 2700: Loss = -11380.145102730236
Iteration 2800: Loss = -11188.262633054446
Iteration 2900: Loss = -11182.87546509827
Iteration 3000: Loss = -11181.29034286289
Iteration 3100: Loss = -11175.994444117467
Iteration 3200: Loss = -11175.84681194372
Iteration 3300: Loss = -11175.765904238304
Iteration 3400: Loss = -11175.7332128751
Iteration 3500: Loss = -11175.70940263642
Iteration 3600: Loss = -11175.692452779274
Iteration 3700: Loss = -11175.679716737475
Iteration 3800: Loss = -11175.667887292078
Iteration 3900: Loss = -11175.6327777361
Iteration 4000: Loss = -11175.626485137287
Iteration 4100: Loss = -11175.61074530938
Iteration 4200: Loss = -11175.604608676327
Iteration 4300: Loss = -11175.594128402941
Iteration 4400: Loss = -11175.531288952825
Iteration 4500: Loss = -11175.52339989263
Iteration 4600: Loss = -11175.519948416118
Iteration 4700: Loss = -11175.518337004807
Iteration 4800: Loss = -11175.52383710021
1
Iteration 4900: Loss = -11175.515207709028
Iteration 5000: Loss = -11175.509033256452
Iteration 5100: Loss = -11175.506651318543
Iteration 5200: Loss = -11175.504699134535
Iteration 5300: Loss = -11175.501997972813
Iteration 5400: Loss = -11175.492231122196
Iteration 5500: Loss = -11175.490621678531
Iteration 5600: Loss = -11175.49236286573
1
Iteration 5700: Loss = -11175.489961270816
Iteration 5800: Loss = -11175.487967755893
Iteration 5900: Loss = -11175.487148775966
Iteration 6000: Loss = -11175.486170693926
Iteration 6100: Loss = -11175.485622084347
Iteration 6200: Loss = -11175.48608512269
1
Iteration 6300: Loss = -11175.483902434928
Iteration 6400: Loss = -11175.486044087416
1
Iteration 6500: Loss = -11175.486941474297
2
Iteration 6600: Loss = -11175.486841468502
3
Iteration 6700: Loss = -11175.479860640848
Iteration 6800: Loss = -11175.490362885374
1
Iteration 6900: Loss = -11175.478513018861
Iteration 7000: Loss = -11175.479064597175
1
Iteration 7100: Loss = -11175.498903874362
2
Iteration 7200: Loss = -11175.478826616521
3
Iteration 7300: Loss = -11175.47765202718
Iteration 7400: Loss = -11175.476644949344
Iteration 7500: Loss = -11175.477857940117
1
Iteration 7600: Loss = -11175.479291250353
2
Iteration 7700: Loss = -11175.471240797493
Iteration 7800: Loss = -11175.472777189585
1
Iteration 7900: Loss = -11175.470148481052
Iteration 8000: Loss = -11175.468134362494
Iteration 8100: Loss = -11175.468122115037
Iteration 8200: Loss = -11175.507616733468
1
Iteration 8300: Loss = -11175.480130839702
2
Iteration 8400: Loss = -11175.46767682226
Iteration 8500: Loss = -11175.467256287768
Iteration 8600: Loss = -11175.560139047782
1
Iteration 8700: Loss = -11175.466992320658
Iteration 8800: Loss = -11175.486479757064
1
Iteration 8900: Loss = -11175.46689912616
Iteration 9000: Loss = -11175.511165524156
1
Iteration 9100: Loss = -11175.475241377444
2
Iteration 9200: Loss = -11175.466336093303
Iteration 9300: Loss = -11175.491391734153
1
Iteration 9400: Loss = -11175.46592162733
Iteration 9500: Loss = -11175.465950556829
1
Iteration 9600: Loss = -11175.468641719506
2
Iteration 9700: Loss = -11175.482923159798
3
Iteration 9800: Loss = -11175.470609177692
4
Iteration 9900: Loss = -11175.46438894729
Iteration 10000: Loss = -11175.464315524681
Iteration 10100: Loss = -11175.473174050938
1
Iteration 10200: Loss = -11175.463863281973
Iteration 10300: Loss = -11175.461490700616
Iteration 10400: Loss = -11175.469227649837
1
Iteration 10500: Loss = -11175.697445567821
2
Iteration 10600: Loss = -11175.460934903742
Iteration 10700: Loss = -11175.46297120561
1
Iteration 10800: Loss = -11175.481564779355
2
Iteration 10900: Loss = -11175.457147537409
Iteration 11000: Loss = -11175.457422737683
1
Iteration 11100: Loss = -11175.45679758021
Iteration 11200: Loss = -11175.456727696543
Iteration 11300: Loss = -11175.456851466446
1
Iteration 11400: Loss = -11175.45663845831
Iteration 11500: Loss = -11175.462794339364
1
Iteration 11600: Loss = -11175.456565602948
Iteration 11700: Loss = -11175.456501156874
Iteration 11800: Loss = -11175.476676340468
1
Iteration 11900: Loss = -11175.45632843494
Iteration 12000: Loss = -11175.456736611257
1
Iteration 12100: Loss = -11175.456381643527
2
Iteration 12200: Loss = -11175.45643273521
3
Iteration 12300: Loss = -11175.458927227748
4
Iteration 12400: Loss = -11175.456972100743
5
Iteration 12500: Loss = -11175.468672150864
6
Iteration 12600: Loss = -11175.456250410954
Iteration 12700: Loss = -11175.471944616684
1
Iteration 12800: Loss = -11175.456149193267
Iteration 12900: Loss = -11175.456273621958
1
Iteration 13000: Loss = -11175.510884354811
2
Iteration 13100: Loss = -11175.454494121292
Iteration 13200: Loss = -11175.469109188716
1
Iteration 13300: Loss = -11175.453876043868
Iteration 13400: Loss = -11175.455361631877
1
Iteration 13500: Loss = -11175.466018796853
2
Iteration 13600: Loss = -11175.44854936203
Iteration 13700: Loss = -11175.546484040975
1
Iteration 13800: Loss = -11175.564480944815
2
Iteration 13900: Loss = -11175.44211781344
Iteration 14000: Loss = -11175.442094815813
Iteration 14100: Loss = -11175.516702178675
1
Iteration 14200: Loss = -11175.445413667961
2
Iteration 14300: Loss = -11175.839291757693
3
Iteration 14400: Loss = -11175.443342154975
4
Iteration 14500: Loss = -11175.442033648127
Iteration 14600: Loss = -11175.442111308092
1
Iteration 14700: Loss = -11175.44321536279
2
Iteration 14800: Loss = -11175.442012457908
Iteration 14900: Loss = -11175.649326244467
1
Iteration 15000: Loss = -11175.441968922945
Iteration 15100: Loss = -11175.499570733542
1
Iteration 15200: Loss = -11175.441931392234
Iteration 15300: Loss = -11175.441889078937
Iteration 15400: Loss = -11175.442182780534
1
Iteration 15500: Loss = -11175.441387308405
Iteration 15600: Loss = -11175.49963841966
1
Iteration 15700: Loss = -11175.441407643124
2
Iteration 15800: Loss = -11175.441345097131
Iteration 15900: Loss = -11175.442694829013
1
Iteration 16000: Loss = -11175.441329510844
Iteration 16100: Loss = -11175.441816452732
1
Iteration 16200: Loss = -11175.44141797679
2
Iteration 16300: Loss = -11175.441455315138
3
Iteration 16400: Loss = -11174.378575541165
Iteration 16500: Loss = -11174.365647482422
Iteration 16600: Loss = -11174.375323588982
1
Iteration 16700: Loss = -11174.364011144316
Iteration 16800: Loss = -11174.39522966485
1
Iteration 16900: Loss = -11174.364031765044
2
Iteration 17000: Loss = -11174.364993011866
3
Iteration 17100: Loss = -11174.367042320991
4
Iteration 17200: Loss = -11174.508304583042
5
Iteration 17300: Loss = -11174.36553145971
6
Iteration 17400: Loss = -11174.420029224682
7
Iteration 17500: Loss = -11174.36689828279
8
Iteration 17600: Loss = -11174.364640134476
9
Iteration 17700: Loss = -11174.432779146317
10
Stopping early at iteration 17700 due to no improvement.
tensor([[  4.9319,  -7.3929],
        [  4.7352,  -6.1504],
        [ -6.6680,   3.0500],
        [ -4.5703,   3.1715],
        [ -5.3086,   2.2739],
        [ -7.5856,   3.6316],
        [  6.9569,  -8.8979],
        [ -6.2969,   4.1590],
        [  2.7193,  -4.9741],
        [  7.1028,  -9.1547],
        [  6.5286,  -7.9621],
        [  2.5874,  -6.8700],
        [  6.4566, -10.7250],
        [  3.7034,  -7.3191],
        [  1.5362,  -5.3997],
        [  8.0315, -11.2022],
        [ -3.8354,   1.4138],
        [ -1.3755,  -0.3682],
        [ -1.3244,  -0.1366],
        [ -4.0158,   2.5369],
        [ -6.8370,   5.4297],
        [  3.5127,  -5.0355],
        [  7.9656,  -9.4792],
        [ -5.5278,   3.7569],
        [ -4.2625,   2.7081],
        [  8.1397, -11.3690],
        [  4.8834,  -6.6170],
        [  4.5690,  -5.9655],
        [ -4.2872,   2.8875],
        [ -1.3204,  -0.1427],
        [ -5.7556,   4.3622],
        [ -9.4835,   7.8320],
        [  4.9572,  -8.2933],
        [ -4.1401,   1.4222],
        [ -2.9737,  -0.5186],
        [ -3.2468,   1.7374],
        [ -4.6615,   3.2685],
        [ -3.9111,   2.5196],
        [  7.6979,  -9.2938],
        [ -2.7465,   0.4979],
        [ -5.3697,   2.5329],
        [ -4.1374,   2.4098],
        [  5.4622,  -6.8828],
        [  7.5894,  -8.9907],
        [ -3.2717,   1.6265],
        [ -5.8757,   4.4679],
        [  4.0662,  -5.4525],
        [ -4.9263,   2.7895],
        [  3.3310,  -7.9462],
        [  1.3756,  -5.9908],
        [  3.9042,  -5.8345],
        [ -5.1741,   3.7219],
        [ -5.4901,   4.0337],
        [  2.1719,  -4.1889],
        [ -8.7976,   7.3896],
        [  7.0560,  -8.5346],
        [ -3.7322,   2.1686],
        [  7.9188,  -9.9495],
        [  4.7926,  -6.2836],
        [  1.1869,  -3.8937],
        [ -9.0642,   4.8111],
        [  7.9242,  -9.3438],
        [  5.4015,  -6.9004],
        [ -5.6745,   4.0174],
        [ -7.2931,   4.4633],
        [ -3.3206,   1.7101],
        [  8.0659,  -9.8952],
        [  7.9590,  -9.3537],
        [  4.6578,  -6.2325],
        [  4.1564,  -5.5437],
        [  4.1418,  -5.5563],
        [  7.6829,  -9.8556],
        [  8.2401,  -9.6288],
        [ -4.6952,   2.8912],
        [  8.1892,  -9.6640],
        [  6.8805,  -8.2831],
        [ -3.1720,   1.7611],
        [ -7.2962,   5.2135],
        [ -8.1697,   6.4865],
        [ -0.9282,  -0.9537],
        [ -6.8896,   5.2950],
        [ -3.3518,   0.5864],
        [ -9.2791,   7.6600],
        [  8.2738,  -9.8481],
        [ -5.5954,   4.1112],
        [  6.4517,  -9.0847],
        [  8.6342, -10.0885],
        [ -4.7677,   2.9976],
        [  0.0219,  -1.4497],
        [ -4.7453,   3.1574],
        [  7.4919,  -9.3461],
        [  3.7393,  -6.1771],
        [  8.0400, -10.4214],
        [ -2.7580,   1.2340],
        [ -2.7656,   1.2944],
        [  5.4992,  -7.4161],
        [  3.6012,  -5.9742],
        [  7.2789,  -8.7071],
        [  7.1763, -10.0615],
        [  5.4630,  -6.8550]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.4915, 0.5085],
        [0.4908, 0.5092]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5318, 0.4682], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2907, 0.0942],
         [0.0886, 0.2184]],

        [[0.7065, 0.0882],
         [0.4310, 0.8386]],

        [[0.2306, 0.0893],
         [0.9095, 0.8997]],

        [[0.8467, 0.0905],
         [0.7729, 0.4541]],

        [[0.9684, 0.1075],
         [0.2250, 0.1464]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 1
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 93
Adjusted Rand Index: 0.737020183942648
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 5
Adjusted Rand Index: 0.8080808080808081
time is 4
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824165642894751
Global Adjusted Rand Index: 0.3444304224940144
Average Adjusted Rand Index: 0.8696641344935742
11113.854306225014
new:  [0.960320569116647, 0.9524808542478043, 0.014411532691088079, 0.3444304224940144] [0.960321435933202, 0.9524823506018751, 0.751041604392941, 0.8696641344935742] [11097.854033823196, 11100.17975433569, 11232.339726483719, 11174.432779146317]
prior:  [0.9291542558103376, 0.9291542558103376, 0.9291542558103376, 0.9291542558103376] [0.92929042477414, 0.92929042477414, 0.92929042477414, 0.92929042477414] [11100.93800610327, 11100.937997753228, 11100.938000073835, 11100.93801810557]
-----------------------------------------------------------------------------------------
This iteration is 15
True Objective function: Loss = -11154.820126537514
Iteration 0: Loss = -18085.089086262033
Iteration 10: Loss = -11368.397886516726
Iteration 20: Loss = -11131.438538847215
Iteration 30: Loss = -11131.04984240654
Iteration 40: Loss = -11131.049554764058
Iteration 50: Loss = -11131.04955709106
1
Iteration 60: Loss = -11131.049552199278
Iteration 70: Loss = -11131.049547460327
Iteration 80: Loss = -11131.049551639879
1
Iteration 90: Loss = -11131.049549872645
2
Iteration 100: Loss = -11131.049552703289
3
Stopping early at iteration 99 due to no improvement.
pi: tensor([[0.7432, 0.2568],
        [0.2784, 0.7216]], dtype=torch.float64)
alpha: tensor([0.4883, 0.5117])
beta: tensor([[[0.2901, 0.0983],
         [0.0376, 0.1936]],

        [[0.1959, 0.1038],
         [0.6425, 0.7791]],

        [[0.8864, 0.1013],
         [0.5853, 0.4523]],

        [[0.5847, 0.0966],
         [0.9148, 0.2091]],

        [[0.8480, 0.1064],
         [0.6516, 0.4315]]], dtype=torch.float64)
time is 0
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 1
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 2
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824062740165256
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.9214428106396227
Average Adjusted Rand Index: 0.9211251262542455
tensor([[-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18030.167055165803
Iteration 100: Loss = -11392.787559630011
Iteration 200: Loss = -11388.696764144968
Iteration 300: Loss = -11387.841759294502
Iteration 400: Loss = -11387.456662771365
Iteration 500: Loss = -11387.107312193904
Iteration 600: Loss = -11386.41861121087
Iteration 700: Loss = -11385.917260251226
Iteration 800: Loss = -11385.431297919871
Iteration 900: Loss = -11384.854436231322
Iteration 1000: Loss = -11383.65229237248
Iteration 1100: Loss = -11383.279238642526
Iteration 1200: Loss = -11382.960586691075
Iteration 1300: Loss = -11381.864333259009
Iteration 1400: Loss = -11381.042306120393
Iteration 1500: Loss = -11380.446997906403
Iteration 1600: Loss = -11379.79884798478
Iteration 1700: Loss = -11379.121394546626
Iteration 1800: Loss = -11378.749563614834
Iteration 1900: Loss = -11378.371287998682
Iteration 2000: Loss = -11377.885255002944
Iteration 2100: Loss = -11377.595574479174
Iteration 2200: Loss = -11377.177168305287
Iteration 2300: Loss = -11376.274280922014
Iteration 2400: Loss = -11368.263296050127
Iteration 2500: Loss = -11194.35181423755
Iteration 2600: Loss = -11168.385554450091
Iteration 2700: Loss = -11147.472413779074
Iteration 2800: Loss = -11138.659453698137
Iteration 2900: Loss = -11138.579996708579
Iteration 3000: Loss = -11138.562878693574
Iteration 3100: Loss = -11138.553531633284
Iteration 3200: Loss = -11138.547477246435
Iteration 3300: Loss = -11138.542644481313
Iteration 3400: Loss = -11138.537161565962
Iteration 3500: Loss = -11138.533763081354
Iteration 3600: Loss = -11138.531152942822
Iteration 3700: Loss = -11138.529533380073
Iteration 3800: Loss = -11138.527216642724
Iteration 3900: Loss = -11138.525779373189
Iteration 4000: Loss = -11138.53312607121
1
Iteration 4100: Loss = -11138.523507929507
Iteration 4200: Loss = -11138.522791009265
Iteration 4300: Loss = -11138.521658886744
Iteration 4400: Loss = -11138.520796009767
Iteration 4500: Loss = -11138.519897849841
Iteration 4600: Loss = -11138.542465526963
1
Iteration 4700: Loss = -11138.518530537833
Iteration 4800: Loss = -11138.517849587122
Iteration 4900: Loss = -11138.520009387663
1
Iteration 5000: Loss = -11138.510652680587
Iteration 5100: Loss = -11138.508803058823
Iteration 5200: Loss = -11138.507669793593
Iteration 5300: Loss = -11138.476401640091
Iteration 5400: Loss = -11126.934446786294
Iteration 5500: Loss = -11126.93288399071
Iteration 5600: Loss = -11126.933553504303
1
Iteration 5700: Loss = -11126.97218941553
2
Iteration 5800: Loss = -11126.940696719807
3
Iteration 5900: Loss = -11126.930296223945
Iteration 6000: Loss = -11126.930236363705
Iteration 6100: Loss = -11127.015985543252
1
Iteration 6200: Loss = -11126.929211818255
Iteration 6300: Loss = -11126.965152669227
1
Iteration 6400: Loss = -11126.928604107105
Iteration 6500: Loss = -11127.206233768808
1
Iteration 6600: Loss = -11126.928221387754
Iteration 6700: Loss = -11126.928092565235
Iteration 6800: Loss = -11126.928194601778
1
Iteration 6900: Loss = -11126.92787656976
Iteration 7000: Loss = -11126.927972386111
1
Iteration 7100: Loss = -11126.929394807985
2
Iteration 7200: Loss = -11126.964025029969
3
Iteration 7300: Loss = -11126.927446213058
Iteration 7400: Loss = -11126.93291916073
1
Iteration 7500: Loss = -11126.926994696043
Iteration 7600: Loss = -11126.93409198811
1
Iteration 7700: Loss = -11126.9247774452
Iteration 7800: Loss = -11126.924635784932
Iteration 7900: Loss = -11126.924645620238
1
Iteration 8000: Loss = -11126.924371256922
Iteration 8100: Loss = -11126.927613326077
1
Iteration 8200: Loss = -11126.923357057722
Iteration 8300: Loss = -11126.911791145434
Iteration 8400: Loss = -11126.912582714951
1
Iteration 8500: Loss = -11126.927018552999
2
Iteration 8600: Loss = -11126.911274820659
Iteration 8700: Loss = -11126.972358053054
1
Iteration 8800: Loss = -11126.909000528336
Iteration 8900: Loss = -11127.093384389129
1
Iteration 9000: Loss = -11126.864048802658
Iteration 9100: Loss = -11126.901339521048
1
Iteration 9200: Loss = -11126.864672008638
2
Iteration 9300: Loss = -11126.866508845465
3
Iteration 9400: Loss = -11126.863504801111
Iteration 9500: Loss = -11126.869663375352
1
Iteration 9600: Loss = -11126.862576132065
Iteration 9700: Loss = -11126.864798923518
1
Iteration 9800: Loss = -11126.862431778449
Iteration 9900: Loss = -11126.862745285087
1
Iteration 10000: Loss = -11126.863826708903
2
Iteration 10100: Loss = -11126.862679234177
3
Iteration 10200: Loss = -11126.878471853033
4
Iteration 10300: Loss = -11126.86139910044
Iteration 10400: Loss = -11126.860094013353
Iteration 10500: Loss = -11126.859509380687
Iteration 10600: Loss = -11126.859517312541
1
Iteration 10700: Loss = -11126.931297407757
2
Iteration 10800: Loss = -11126.935159484461
3
Iteration 10900: Loss = -11126.86007279212
4
Iteration 11000: Loss = -11126.859534305448
5
Iteration 11100: Loss = -11126.859349829945
Iteration 11200: Loss = -11126.859298313339
Iteration 11300: Loss = -11126.897263821906
1
Iteration 11400: Loss = -11126.85919341607
Iteration 11500: Loss = -11126.859300730774
1
Iteration 11600: Loss = -11126.859817006965
2
Iteration 11700: Loss = -11126.867384064619
3
Iteration 11800: Loss = -11126.874679454926
4
Iteration 11900: Loss = -11126.86578220804
5
Iteration 12000: Loss = -11126.859125080395
Iteration 12100: Loss = -11126.863148798238
1
Iteration 12200: Loss = -11126.85975871506
2
Iteration 12300: Loss = -11126.859121182273
Iteration 12400: Loss = -11126.866725425321
1
Iteration 12500: Loss = -11126.859090580783
Iteration 12600: Loss = -11126.859316161148
1
Iteration 12700: Loss = -11126.869228631505
2
Iteration 12800: Loss = -11126.85984151694
3
Iteration 12900: Loss = -11126.859282778913
4
Iteration 13000: Loss = -11126.874011808435
5
Iteration 13100: Loss = -11126.858320519164
Iteration 13200: Loss = -11126.86961720801
1
Iteration 13300: Loss = -11126.860225576787
2
Iteration 13400: Loss = -11126.858342050711
3
Iteration 13500: Loss = -11126.86084033332
4
Iteration 13600: Loss = -11126.860740337976
5
Iteration 13700: Loss = -11126.85831019299
Iteration 13800: Loss = -11126.8588006511
1
Iteration 13900: Loss = -11126.862007738959
2
Iteration 14000: Loss = -11126.859590853948
3
Iteration 14100: Loss = -11126.88627394204
4
Iteration 14200: Loss = -11126.858295797974
Iteration 14300: Loss = -11126.85829547573
Iteration 14400: Loss = -11126.862192064953
1
Iteration 14500: Loss = -11126.859016006527
2
Iteration 14600: Loss = -11126.862286635562
3
Iteration 14700: Loss = -11126.858395712947
4
Iteration 14800: Loss = -11126.858435725111
5
Iteration 14900: Loss = -11126.86006831682
6
Iteration 15000: Loss = -11126.858319546578
7
Iteration 15100: Loss = -11126.858276060124
Iteration 15200: Loss = -11126.858225601063
Iteration 15300: Loss = -11126.858354979267
1
Iteration 15400: Loss = -11126.868014610034
2
Iteration 15500: Loss = -11126.858251229613
3
Iteration 15600: Loss = -11126.888086181702
4
Iteration 15700: Loss = -11126.867677222455
5
Iteration 15800: Loss = -11126.858335418112
6
Iteration 15900: Loss = -11126.87540043686
7
Iteration 16000: Loss = -11126.858234336401
8
Iteration 16100: Loss = -11126.858895349784
9
Iteration 16200: Loss = -11126.858226394806
10
Stopping early at iteration 16200 due to no improvement.
tensor([[ -4.5012,  -0.1140],
        [ -6.9052,   2.2900],
        [  3.7320,  -8.3472],
        [ -5.6917,   1.0765],
        [ -7.0260,   2.4108],
        [ -1.2376,  -3.3776],
        [ -5.1242,   0.5090],
        [ -6.0902,   1.4750],
        [  2.5327,  -7.1479],
        [  4.8555,  -9.4708],
        [ -7.9452,   3.3300],
        [-11.8308,   7.2156],
        [ -7.8960,   3.2808],
        [  3.4278,  -8.0430],
        [ -0.1142,  -4.5010],
        [  1.4289,  -6.0441],
        [ -5.3389,   0.7237],
        [  2.1672,  -6.7824],
        [ -7.7792,   3.1640],
        [  2.7233,  -7.3385],
        [ -2.0124,  -2.6028],
        [ -9.5375,   4.9223],
        [ -5.4299,   0.8147],
        [ -7.4108,   2.7956],
        [ -5.4511,   0.8359],
        [ -3.9099,  -0.7053],
        [ -5.3429,   0.7277],
        [ -9.7222,   5.1069],
        [ -7.7750,   3.1597],
        [ -3.1834,  -1.4318],
        [ -9.5873,   4.9721],
        [ -6.1895,   1.5742],
        [ -6.6950,   2.0798],
        [  5.3718,  -9.9870],
        [ -5.2843,   0.6691],
        [  1.7379,  -6.3531],
        [ -7.3916,   2.7764],
        [  3.0334,  -7.6486],
        [  0.9918,  -5.6070],
        [ -0.4614,  -4.1539],
        [ -7.9325,   3.3172],
        [ -5.6629,   1.0477],
        [ -5.3743,   0.7591],
        [ -3.8407,  -0.7746],
        [ -6.9930,   2.3778],
        [  1.8738,  -6.4890],
        [ -5.4637,   0.8485],
        [ -4.3536,  -0.2616],
        [  3.4369,  -8.0521],
        [ -2.0669,  -2.5483],
        [ -5.6895,   1.0743],
        [  3.5836,  -8.1988],
        [  1.4503,  -6.0655],
        [ -8.3418,   3.7266],
        [ -6.1016,   1.4864],
        [ -8.0574,   3.4422],
        [ -3.6097,  -1.0055],
        [  0.1510,  -4.7662],
        [ -5.6508,   1.0356],
        [  1.4055,  -6.0207],
        [ -4.4656,  -0.1496],
        [  2.8786,  -7.4938],
        [ -6.2150,   1.5997],
        [  2.0887,  -6.7039],
        [ -0.9842,  -3.6310],
        [  1.3488,  -5.9641],
        [ -6.5256,   1.9104],
        [ -5.8560,   1.2408],
        [ -1.5825,  -3.0327],
        [ -7.5572,   2.9419],
        [ -7.0550,   2.4398],
        [  1.5137,  -6.1290],
        [  2.4699,  -7.0851],
        [ -4.5726,  -0.0426],
        [ -4.7607,   0.1455],
        [  2.3979,  -7.0131],
        [ -1.2232,  -3.3920],
        [ -0.2231,  -4.3921],
        [  0.3901,  -5.0053],
        [ -2.8326,  -1.7827],
        [  2.4625,  -7.0777],
        [ -2.7411,  -1.8741],
        [ -8.8836,   4.2683],
        [  0.5296,  -5.1448],
        [ -5.7476,   1.1324],
        [  0.4447,  -5.0599],
        [  1.5141,  -6.1293],
        [  1.6507,  -6.2659],
        [  1.0933,  -5.7085],
        [  0.7952,  -5.4104],
        [ -6.2931,   1.6778],
        [  3.0980,  -7.7133],
        [  3.9696,  -8.5849],
        [ -7.1812,   2.5660],
        [ -8.1710,   3.5558],
        [  3.2092,  -7.8245],
        [ -1.0469,  -3.5683],
        [  5.4544, -10.0696],
        [  2.1770,  -6.7922],
        [ -9.7814,   5.1661]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7562, 0.2438],
        [0.2615, 0.7385]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4559, 0.5441], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2971, 0.0977],
         [0.0376, 0.1971]],

        [[0.1959, 0.1036],
         [0.6425, 0.7791]],

        [[0.8864, 0.1014],
         [0.5853, 0.4523]],

        [[0.5847, 0.0974],
         [0.9148, 0.2091]],

        [[0.8480, 0.1064],
         [0.6516, 0.4315]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 1
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8448338943900694
time is 2
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.9214428106396227
Average Adjusted Rand Index: 0.9214497464974624
Iteration 0: Loss = -23662.362335955255
Iteration 10: Loss = -11383.206037798203
Iteration 20: Loss = -11381.50207390771
Iteration 30: Loss = -11381.738917110284
1
Iteration 40: Loss = -11375.206877341196
Iteration 50: Loss = -11138.693370122832
Iteration 60: Loss = -11131.054453533663
Iteration 70: Loss = -11131.049590618355
Iteration 80: Loss = -11131.049556930295
Iteration 90: Loss = -11131.049562230182
1
Iteration 100: Loss = -11131.049567320766
2
Iteration 110: Loss = -11131.049568984043
3
Stopping early at iteration 109 due to no improvement.
pi: tensor([[0.7432, 0.2568],
        [0.2784, 0.7216]], dtype=torch.float64)
alpha: tensor([0.4883, 0.5117])
beta: tensor([[[0.2901, 0.0983],
         [0.0912, 0.1936]],

        [[0.7700, 0.1038],
         [0.0804, 0.4404]],

        [[0.5374, 0.1013],
         [0.9455, 0.4808]],

        [[0.7952, 0.0966],
         [0.0577, 0.3496]],

        [[0.2360, 0.1064],
         [0.7964, 0.4934]]], dtype=torch.float64)
time is 0
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 1
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 2
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824062740165256
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.9214428106396227
Average Adjusted Rand Index: 0.9211251262542455
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23660.960470311733
Iteration 100: Loss = -11374.681739486017
Iteration 200: Loss = -11280.281828191817
Iteration 300: Loss = -11225.896999274895
Iteration 400: Loss = -11208.345447697619
Iteration 500: Loss = -11196.45212115987
Iteration 600: Loss = -11185.236770428719
Iteration 700: Loss = -11183.192574652036
Iteration 800: Loss = -11182.909856795259
Iteration 900: Loss = -11182.683098718853
Iteration 1000: Loss = -11182.52032529739
Iteration 1100: Loss = -11182.35813042116
Iteration 1200: Loss = -11182.17483981261
Iteration 1300: Loss = -11181.995356574822
Iteration 1400: Loss = -11181.854502435328
Iteration 1500: Loss = -11181.712065006124
Iteration 1600: Loss = -11180.62587694846
Iteration 1700: Loss = -11180.300900976068
Iteration 1800: Loss = -11180.24250672058
Iteration 1900: Loss = -11180.204367023693
Iteration 2000: Loss = -11177.5975927936
Iteration 2100: Loss = -11177.494214061497
Iteration 2200: Loss = -11177.474013105506
Iteration 2300: Loss = -11177.44650431094
Iteration 2400: Loss = -11177.103149761688
Iteration 2500: Loss = -11176.67423592134
Iteration 2600: Loss = -11176.651775246506
Iteration 2700: Loss = -11176.641080981924
Iteration 2800: Loss = -11176.634004621224
Iteration 2900: Loss = -11176.62617184745
Iteration 3000: Loss = -11176.597924646398
Iteration 3100: Loss = -11176.4418151992
Iteration 3200: Loss = -11176.43334134464
Iteration 3300: Loss = -11176.430296803128
Iteration 3400: Loss = -11176.426644410818
Iteration 3500: Loss = -11176.401115939227
Iteration 3600: Loss = -11175.671224435886
Iteration 3700: Loss = -11175.668022395814
Iteration 3800: Loss = -11175.666553702518
Iteration 3900: Loss = -11175.665316173352
Iteration 4000: Loss = -11175.665825666905
1
Iteration 4100: Loss = -11175.663383950236
Iteration 4200: Loss = -11175.662595253043
Iteration 4300: Loss = -11175.661855249553
Iteration 4400: Loss = -11175.673238360263
1
Iteration 4500: Loss = -11175.660591457046
Iteration 4600: Loss = -11175.660004457455
Iteration 4700: Loss = -11175.68960611879
1
Iteration 4800: Loss = -11175.658982964896
Iteration 4900: Loss = -11175.658545522803
Iteration 5000: Loss = -11175.65812833021
Iteration 5100: Loss = -11175.657706686137
Iteration 5200: Loss = -11175.657333930574
Iteration 5300: Loss = -11175.656958528281
Iteration 5400: Loss = -11175.656495424819
Iteration 5500: Loss = -11175.655144419048
Iteration 5600: Loss = -11175.533030958815
Iteration 5700: Loss = -11175.532656618754
Iteration 5800: Loss = -11175.531588571632
Iteration 5900: Loss = -11175.519444088812
Iteration 6000: Loss = -11175.518649218455
Iteration 6100: Loss = -11175.518296284696
Iteration 6200: Loss = -11175.518770808883
1
Iteration 6300: Loss = -11175.504825029073
Iteration 6400: Loss = -11175.504539348825
Iteration 6500: Loss = -11175.508744168641
1
Iteration 6600: Loss = -11175.503706155512
Iteration 6700: Loss = -11175.503435026787
Iteration 6800: Loss = -11175.503263764344
Iteration 6900: Loss = -11175.50335583533
1
Iteration 7000: Loss = -11175.504840808893
2
Iteration 7100: Loss = -11175.502575671422
Iteration 7200: Loss = -11175.503030168633
1
Iteration 7300: Loss = -11175.502404936084
Iteration 7400: Loss = -11175.502719878856
1
Iteration 7500: Loss = -11175.50374139932
2
Iteration 7600: Loss = -11175.502727630683
3
Iteration 7700: Loss = -11175.50331411781
4
Iteration 7800: Loss = -11175.527328083252
5
Iteration 7900: Loss = -11175.50064223316
Iteration 8000: Loss = -11175.495567351374
Iteration 8100: Loss = -11175.484138483404
Iteration 8200: Loss = -11175.485910601106
1
Iteration 8300: Loss = -11175.47601037735
Iteration 8400: Loss = -11175.473938654999
Iteration 8500: Loss = -11175.473068988154
Iteration 8600: Loss = -11175.440983344015
Iteration 8700: Loss = -11175.440346790776
Iteration 8800: Loss = -11175.439085170865
Iteration 8900: Loss = -11175.439018205554
Iteration 9000: Loss = -11175.452417727001
1
Iteration 9100: Loss = -11175.438876389924
Iteration 9200: Loss = -11175.47913689528
1
Iteration 9300: Loss = -11175.438804687006
Iteration 9400: Loss = -11175.446154477613
1
Iteration 9500: Loss = -11175.438766905027
Iteration 9600: Loss = -11175.61745876594
1
Iteration 9700: Loss = -11175.43869851083
Iteration 9800: Loss = -11175.438708078267
1
Iteration 9900: Loss = -11175.43870038698
2
Iteration 10000: Loss = -11175.438592105398
Iteration 10100: Loss = -11175.445859647609
1
Iteration 10200: Loss = -11175.438564401893
Iteration 10300: Loss = -11175.438550252182
Iteration 10400: Loss = -11175.438685300996
1
Iteration 10500: Loss = -11175.438476734922
Iteration 10600: Loss = -11175.458642980973
1
Iteration 10700: Loss = -11175.438432039253
Iteration 10800: Loss = -11175.441597536717
1
Iteration 10900: Loss = -11175.438236243926
Iteration 11000: Loss = -11175.443730060168
1
Iteration 11100: Loss = -11175.437990690754
Iteration 11200: Loss = -11175.48778065807
1
Iteration 11300: Loss = -11175.437916688452
Iteration 11400: Loss = -11175.44632054189
1
Iteration 11500: Loss = -11175.437853900487
Iteration 11600: Loss = -11175.439921318035
1
Iteration 11700: Loss = -11175.438024200288
2
Iteration 11800: Loss = -11175.438022398223
3
Iteration 11900: Loss = -11175.47195118011
4
Iteration 12000: Loss = -11175.4372710939
Iteration 12100: Loss = -11175.437489094551
1
Iteration 12200: Loss = -11175.467479583105
2
Iteration 12300: Loss = -11175.521177464485
3
Iteration 12400: Loss = -11175.437232235676
Iteration 12500: Loss = -11175.437378038407
1
Iteration 12600: Loss = -11175.505956837458
2
Iteration 12700: Loss = -11175.437185370547
Iteration 12800: Loss = -11175.43804102962
1
Iteration 12900: Loss = -11175.437136021847
Iteration 13000: Loss = -11175.43725188084
1
Iteration 13100: Loss = -11175.476951272793
2
Iteration 13200: Loss = -11175.437051920806
Iteration 13300: Loss = -11175.465020895102
1
Iteration 13400: Loss = -11175.423552375161
Iteration 13500: Loss = -11175.304240776517
Iteration 13600: Loss = -11175.315398168737
1
Iteration 13700: Loss = -11175.300536990502
Iteration 13800: Loss = -11175.300575794045
1
Iteration 13900: Loss = -11175.306773670227
2
Iteration 14000: Loss = -11175.300928689434
3
Iteration 14100: Loss = -11175.300553042829
4
Iteration 14200: Loss = -11175.44102200019
5
Iteration 14300: Loss = -11175.30047140161
Iteration 14400: Loss = -11175.305958045565
1
Iteration 14500: Loss = -11175.300462483714
Iteration 14600: Loss = -11175.30076313389
1
Iteration 14700: Loss = -11175.301376998235
2
Iteration 14800: Loss = -11175.300567039481
3
Iteration 14900: Loss = -11175.301196626351
4
Iteration 15000: Loss = -11175.398301834372
5
Iteration 15100: Loss = -11175.300461355602
Iteration 15200: Loss = -11175.302124996957
1
Iteration 15300: Loss = -11175.309825774253
2
Iteration 15400: Loss = -11175.322062502652
3
Iteration 15500: Loss = -11175.31171587449
4
Iteration 15600: Loss = -11175.32487070304
5
Iteration 15700: Loss = -11175.300173336947
Iteration 15800: Loss = -11175.302243548325
1
Iteration 15900: Loss = -11175.306559157967
2
Iteration 16000: Loss = -11175.30167990711
3
Iteration 16100: Loss = -11175.328795776004
4
Iteration 16200: Loss = -11175.300169769598
Iteration 16300: Loss = -11175.302549152108
1
Iteration 16400: Loss = -11175.300184578657
2
Iteration 16500: Loss = -11175.331657686884
3
Iteration 16600: Loss = -11175.300191193508
4
Iteration 16700: Loss = -11175.422694936686
5
Iteration 16800: Loss = -11175.307326390079
6
Iteration 16900: Loss = -11175.300178596051
7
Iteration 17000: Loss = -11175.300628498695
8
Iteration 17100: Loss = -11175.30414789836
9
Iteration 17200: Loss = -11175.306255560889
10
Stopping early at iteration 17200 due to no improvement.
tensor([[-2.5107,  1.1243],
        [-3.0738,  1.2247],
        [-5.2034,  3.8150],
        [-0.5004, -0.9630],
        [-1.3085, -0.0905],
        [-0.7539, -1.0069],
        [-3.4178,  1.8644],
        [ 1.0112, -2.4947],
        [-3.5351,  1.7205],
        [-3.4370,  2.0202],
        [-2.4038, -0.5869],
        [-1.2364, -0.1499],
        [-2.1344,  0.4110],
        [-3.7763,  2.3898],
        [-1.7525,  0.3628],
        [-5.1717,  3.2024],
        [ 0.3129, -1.7015],
        [-4.1525,  2.7043],
        [-4.2553,  2.8020],
        [-4.6098,  3.1798],
        [-1.9643,  0.4632],
        [-4.7268,  1.8543],
        [-2.7772,  1.1561],
        [-2.9383,  1.5494],
        [-0.9897, -0.7846],
        [-2.9105, -0.6696],
        [-3.4349,  1.8835],
        [-3.5263,  1.5808],
        [-0.8937, -0.6958],
        [-1.9620,  0.4989],
        [-2.5858,  1.1872],
        [-3.3131,  0.1308],
        [-4.0486,  2.0145],
        [-5.3886,  3.3515],
        [-0.9491, -1.0527],
        [-4.0902,  2.2748],
        [-4.0012,  2.3915],
        [-2.6452,  1.1537],
        [-2.9684,  1.4932],
        [-5.0678,  3.6690],
        [-3.0983,  1.6909],
        [-1.7585,  0.2750],
        [-3.8457,  2.2655],
        [-3.2019,  1.7019],
        [-4.6079,  2.2005],
        [-2.8337,  0.7332],
        [-4.1821,  1.3851],
        [-1.6642,  0.2399],
        [-3.8342,  2.3331],
        [ 0.5745, -2.1967],
        [-2.7498,  0.8696],
        [-4.2867,  2.8111],
        [-4.8900,  2.6088],
        [ 2.7856, -4.3689],
        [-2.5529,  1.0710],
        [-2.4064,  0.9351],
        [ 0.2230, -1.7150],
        [-3.1156,  1.7166],
        [-1.8029, -0.0845],
        [-3.6266,  1.8046],
        [-3.7587,  2.0178],
        [-3.6777,  2.0644],
        [-3.7555,  1.4146],
        [-1.7961,  0.3247],
        [-1.5020,  0.0987],
        [-3.2127,  0.9483],
        [-2.8503,  1.0194],
        [-3.4458, -1.1694],
        [-2.8603,  1.2655],
        [-3.8702,  2.3682],
        [-3.1209,  1.2858],
        [-3.8731,  2.4778],
        [-2.7955,  1.0900],
        [-1.4438, -1.9713],
        [-3.2410,  0.3537],
        [-3.1468,  0.7738],
        [-1.7608,  0.2344],
        [-2.6857,  0.9019],
        [-2.2633,  0.8523],
        [-2.6612,  1.2556],
        [-1.6210,  0.2307],
        [-1.9595,  0.5565],
        [-2.9155,  1.2768],
        [-4.0916,  2.5718],
        [-3.7290,  1.6953],
        [-3.3146,  1.7902],
        [-3.2966,  0.6956],
        [-3.6488,  0.9019],
        [-3.6902,  1.5796],
        [-2.8101,  1.1676],
        [-1.4609,  0.0135],
        [-4.7071,  3.2481],
        [-4.8145,  1.5112],
        [-4.2000,  1.7751],
        [-2.8494,  1.1873],
        [-4.3935,  0.8403],
        [-2.6104,  0.7018],
        [-4.1101,  1.5523],
        [-6.8629,  5.4748],
        [-4.1191,  1.9583]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7541, 0.2459],
        [0.3294, 0.6706]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1091, 0.8909], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3036, 0.1110],
         [0.0912, 0.1863]],

        [[0.7700, 0.1047],
         [0.0804, 0.4404]],

        [[0.5374, 0.1020],
         [0.9455, 0.4808]],

        [[0.7952, 0.0977],
         [0.0577, 0.3496]],

        [[0.2360, 0.1066],
         [0.7964, 0.4934]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.008484848484848486
time is 1
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 5
Adjusted Rand Index: 0.8080620079101718
time is 2
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9207907017983009
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.5706913820725206
Average Adjusted Rand Index: 0.7280729139720148
Iteration 0: Loss = -24869.632107425346
Iteration 10: Loss = -11384.750733795774
Iteration 20: Loss = -11381.936485253254
Iteration 30: Loss = -11381.703096235058
Iteration 40: Loss = -11381.693121822995
Iteration 50: Loss = -11381.710372776357
1
Iteration 60: Loss = -11381.722549407246
2
Iteration 70: Loss = -11381.729396246108
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[0.8885, 0.1115],
        [0.9640, 0.0360]], dtype=torch.float64)
alpha: tensor([0.9002, 0.0998])
beta: tensor([[[0.1670, 0.1486],
         [0.6163, 0.2532]],

        [[0.2624, 0.2119],
         [0.8986, 0.5563]],

        [[0.8090, 0.1976],
         [0.1643, 0.4078]],

        [[0.1238, 0.1016],
         [0.9676, 0.1605]],

        [[0.0786, 0.2228],
         [0.7631, 0.6124]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: -0.014778186472389411
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.006467401572035518
Global Adjusted Rand Index: -7.67562268970562e-05
Average Adjusted Rand Index: -0.004558265739409537
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24868.94008832412
Iteration 100: Loss = -11468.77440620459
Iteration 200: Loss = -11427.319498881405
Iteration 300: Loss = -11402.022456735169
Iteration 400: Loss = -11391.631251000033
Iteration 500: Loss = -11389.69966647316
Iteration 600: Loss = -11389.055812688443
Iteration 700: Loss = -11388.578798559725
Iteration 800: Loss = -11388.19433808944
Iteration 900: Loss = -11387.908251674136
Iteration 1000: Loss = -11387.707189523328
Iteration 1100: Loss = -11387.54255689146
Iteration 1200: Loss = -11387.22695814703
Iteration 1300: Loss = -11386.50322896283
Iteration 1400: Loss = -11385.685588688038
Iteration 1500: Loss = -11384.430439569325
Iteration 1600: Loss = -11383.642363658686
Iteration 1700: Loss = -11382.738759340411
Iteration 1800: Loss = -11379.48676389265
Iteration 1900: Loss = -11316.802369071442
Iteration 2000: Loss = -11272.111328477626
Iteration 2100: Loss = -11241.598342578096
Iteration 2200: Loss = -11201.977255472297
Iteration 2300: Loss = -11201.165975626222
Iteration 2400: Loss = -11192.552933345232
Iteration 2500: Loss = -11189.121041492486
Iteration 2600: Loss = -11188.981998427085
Iteration 2700: Loss = -11188.883734894736
Iteration 2800: Loss = -11187.677967309104
Iteration 2900: Loss = -11187.504429989725
Iteration 3000: Loss = -11187.242882930777
Iteration 3100: Loss = -11187.213281640628
Iteration 3200: Loss = -11187.195004705864
Iteration 3300: Loss = -11183.616737157214
Iteration 3400: Loss = -11180.41291043199
Iteration 3500: Loss = -11180.328740677469
Iteration 3600: Loss = -11180.318643958231
Iteration 3700: Loss = -11180.311282609833
Iteration 3800: Loss = -11180.304923488078
Iteration 3900: Loss = -11180.29817758317
Iteration 4000: Loss = -11180.283936327649
Iteration 4100: Loss = -11180.269726487259
Iteration 4200: Loss = -11180.266253232112
Iteration 4300: Loss = -11180.261367589268
Iteration 4400: Loss = -11180.189597204377
Iteration 4500: Loss = -11180.182606940729
Iteration 4600: Loss = -11180.180500316694
Iteration 4700: Loss = -11180.229243186775
1
Iteration 4800: Loss = -11180.176103479555
Iteration 4900: Loss = -11180.173840620851
Iteration 5000: Loss = -11180.171545123041
Iteration 5100: Loss = -11180.16632776452
Iteration 5200: Loss = -11180.163197272921
Iteration 5300: Loss = -11180.161587899116
Iteration 5400: Loss = -11180.159839555909
Iteration 5500: Loss = -11180.1580527098
Iteration 5600: Loss = -11180.156149183533
Iteration 5700: Loss = -11180.154217592832
Iteration 5800: Loss = -11180.152029870289
Iteration 5900: Loss = -11180.149743984222
Iteration 6000: Loss = -11180.14749806501
Iteration 6100: Loss = -11180.147808058035
1
Iteration 6200: Loss = -11180.143813610468
Iteration 6300: Loss = -11180.14232572196
Iteration 6400: Loss = -11180.297085153348
1
Iteration 6500: Loss = -11180.139957045567
Iteration 6600: Loss = -11180.138923353805
Iteration 6700: Loss = -11180.137878254649
Iteration 6800: Loss = -11180.137135600777
Iteration 6900: Loss = -11180.119377282956
Iteration 7000: Loss = -11180.116650813752
Iteration 7100: Loss = -11179.973883292292
Iteration 7200: Loss = -11179.099533099537
Iteration 7300: Loss = -11176.95635695405
Iteration 7400: Loss = -11176.925189886135
Iteration 7500: Loss = -11176.923064379198
Iteration 7600: Loss = -11176.899844483962
Iteration 7700: Loss = -11176.878464649113
Iteration 7800: Loss = -11176.903711105184
1
Iteration 7900: Loss = -11176.875893641574
Iteration 8000: Loss = -11176.875405720137
Iteration 8100: Loss = -11176.890433984265
1
Iteration 8200: Loss = -11176.87367803525
Iteration 8300: Loss = -11176.873729769915
1
Iteration 8400: Loss = -11176.869644148335
Iteration 8500: Loss = -11177.04565352506
1
Iteration 8600: Loss = -11176.861783833097
Iteration 8700: Loss = -11176.860732331908
Iteration 8800: Loss = -11176.844999896603
Iteration 8900: Loss = -11176.84458620011
Iteration 9000: Loss = -11176.844203992236
Iteration 9100: Loss = -11176.879430585783
1
Iteration 9200: Loss = -11176.815754830912
Iteration 9300: Loss = -11176.765801497393
Iteration 9400: Loss = -11176.765254000371
Iteration 9500: Loss = -11176.71754196785
Iteration 9600: Loss = -11176.725708483411
1
Iteration 9700: Loss = -11176.717149519274
Iteration 9800: Loss = -11176.728299007538
1
Iteration 9900: Loss = -11176.71625021403
Iteration 10000: Loss = -11176.6696820864
Iteration 10100: Loss = -11176.669958371353
1
Iteration 10200: Loss = -11176.645140477329
Iteration 10300: Loss = -11176.65224429262
1
Iteration 10400: Loss = -11176.64395082213
Iteration 10500: Loss = -11176.643935764625
Iteration 10600: Loss = -11176.644049099708
1
Iteration 10700: Loss = -11176.643855836333
Iteration 10800: Loss = -11176.683892023979
1
Iteration 10900: Loss = -11176.643816296992
Iteration 11000: Loss = -11176.645397119166
1
Iteration 11100: Loss = -11176.643693735487
Iteration 11200: Loss = -11176.64396194778
1
Iteration 11300: Loss = -11176.658654150911
2
Iteration 11400: Loss = -11176.641326465588
Iteration 11500: Loss = -11176.640712640195
Iteration 11600: Loss = -11176.63968670345
Iteration 11700: Loss = -11176.490027196995
Iteration 11800: Loss = -11176.489524681929
Iteration 11900: Loss = -11176.489239269247
Iteration 12000: Loss = -11176.482506656692
Iteration 12100: Loss = -11176.48490435417
1
Iteration 12200: Loss = -11176.482431258446
Iteration 12300: Loss = -11176.522607645613
1
Iteration 12400: Loss = -11176.482195139248
Iteration 12500: Loss = -11176.48029265945
Iteration 12600: Loss = -11176.462772528022
Iteration 12700: Loss = -11176.462699490012
Iteration 12800: Loss = -11176.478667327718
1
Iteration 12900: Loss = -11176.461330650467
Iteration 13000: Loss = -11176.47303449297
1
Iteration 13100: Loss = -11176.461318151545
Iteration 13200: Loss = -11176.461257396686
Iteration 13300: Loss = -11175.963831122683
Iteration 13400: Loss = -11175.915928702745
Iteration 13500: Loss = -11175.922645361876
1
Iteration 13600: Loss = -11175.89394901911
Iteration 13700: Loss = -11175.918193083922
1
Iteration 13800: Loss = -11175.893329373514
Iteration 13900: Loss = -11175.94145602442
1
Iteration 14000: Loss = -11175.890924947475
Iteration 14100: Loss = -11175.93069870458
1
Iteration 14200: Loss = -11175.893030793693
2
Iteration 14300: Loss = -11175.890815357163
Iteration 14400: Loss = -11175.94594504437
1
Iteration 14500: Loss = -11175.890162062475
Iteration 14600: Loss = -11175.890333902153
1
Iteration 14700: Loss = -11175.8702550534
Iteration 14800: Loss = -11175.873865259431
1
Iteration 14900: Loss = -11175.816927961892
Iteration 15000: Loss = -11175.814583010599
Iteration 15100: Loss = -11175.8138160234
Iteration 15200: Loss = -11175.799690875152
Iteration 15300: Loss = -11175.77772000057
Iteration 15400: Loss = -11175.816757118228
1
Iteration 15500: Loss = -11175.776118608112
Iteration 15600: Loss = -11175.893877033963
1
Iteration 15700: Loss = -11175.776090307761
Iteration 15800: Loss = -11175.77677695399
1
Iteration 15900: Loss = -11175.757668778142
Iteration 16000: Loss = -11175.757716429822
1
Iteration 16100: Loss = -11175.77631767345
2
Iteration 16200: Loss = -11175.6292061121
Iteration 16300: Loss = -11175.627381425687
Iteration 16400: Loss = -11175.632510941085
1
Iteration 16500: Loss = -11175.627434554712
2
Iteration 16600: Loss = -11175.62741789218
3
Iteration 16700: Loss = -11175.68867930877
4
Iteration 16800: Loss = -11175.627300762924
Iteration 16900: Loss = -11175.627544214445
1
Iteration 17000: Loss = -11175.630782317778
2
Iteration 17100: Loss = -11175.624922087854
Iteration 17200: Loss = -11175.624474144244
Iteration 17300: Loss = -11175.628472937957
1
Iteration 17400: Loss = -11175.60244810767
Iteration 17500: Loss = -11175.59116433184
Iteration 17600: Loss = -11175.585222031857
Iteration 17700: Loss = -11175.597595779764
1
Iteration 17800: Loss = -11175.584083236377
Iteration 17900: Loss = -11175.584160874098
1
Iteration 18000: Loss = -11175.595573322622
2
Iteration 18100: Loss = -11175.712447456435
3
Iteration 18200: Loss = -11175.57865697326
Iteration 18300: Loss = -11175.578772558238
1
Iteration 18400: Loss = -11175.580765921548
2
Iteration 18500: Loss = -11175.578678810405
3
Iteration 18600: Loss = -11175.59613137997
4
Iteration 18700: Loss = -11175.76469120614
5
Iteration 18800: Loss = -11175.578686051505
6
Iteration 18900: Loss = -11175.579301014925
7
Iteration 19000: Loss = -11175.671137207768
8
Iteration 19100: Loss = -11175.577672793126
Iteration 19200: Loss = -11175.488972109562
Iteration 19300: Loss = -11175.483642546149
Iteration 19400: Loss = -11175.494164006515
1
Iteration 19500: Loss = -11175.483640370578
Iteration 19600: Loss = -11175.52328958979
1
Iteration 19700: Loss = -11175.483620682911
Iteration 19800: Loss = -11175.483611695483
Iteration 19900: Loss = -11175.483636261295
1
tensor([[ -0.1432,  -3.7967],
        [  1.6305,  -3.0218],
        [  8.4118,  -9.9433],
        [ -1.3136,  -0.6595],
        [ -0.1710,  -1.2727],
        [ -1.6067,  -1.1260],
        [  1.7400,  -3.7768],
        [ -2.3671,   0.9756],
        [  1.9711,  -3.3910],
        [  2.1374,  -3.5615],
        [  6.7619,  -9.1042],
        [ -0.2927,  -1.2717],
        [  0.3256,  -2.0502],
        [  2.4185,  -3.8049],
        [  0.2640,  -1.8761],
        [  3.4831,  -5.0175],
        [ -2.0425,  -0.1447],
        [  2.7900,  -4.2054],
        [  2.7497,  -4.5838],
        [  2.8885,  -4.9810],
        [  0.3915,  -2.1558],
        [  1.7692,  -4.8410],
        [  0.7422,  -3.0759],
        [  0.6323,  -3.9788],
        [ -0.6671,  -0.9049],
        [  0.2894,  -2.0428],
        [  1.9299,  -3.3836],
        [  1.6688,  -3.4802],
        [ -1.5958,  -1.7211],
        [  0.5626,  -2.1539],
        [  1.1663,  -2.5649],
        [  0.2283,  -3.2380],
        [  2.0276,  -4.0428],
        [  2.0924,  -6.7076],
        [ -0.7524,  -0.6378],
        [  2.1683,  -4.5293],
        [  2.2444,  -4.1389],
        [  0.3440,  -3.4047],
        [ -0.0808,  -4.5345],
        [  3.5446,  -5.2430],
        [  1.5593,  -3.4976],
        [ -0.1259,  -2.0671],
        [  2.3306,  -3.8350],
        [  0.9137,  -4.0188],
        [  2.1966,  -4.6227],
        [  0.9285,  -2.6540],
        [  2.1266,  -3.5153],
        [ -0.2309,  -2.1541],
        [  2.4199,  -3.8374],
        [ -2.5979,   0.4173],
        [  1.1313,  -2.5315],
        [  2.8439,  -4.2839],
        [  8.0006, -10.5290],
        [ -4.5022,   2.8508],
        [  1.1007,  -2.5136],
        [  1.1113,  -2.5502],
        [ -2.6309,  -0.8319],
        [  1.5670,  -3.2817],
        [  0.1242,  -1.5608],
        [  1.9641,  -3.6042],
        [  1.8489,  -4.0911],
        [  1.4478,  -4.3384],
        [  1.6949,  -3.4412],
        [ -0.1969,  -2.3893],
        [ -0.2577,  -1.7932],
        [  1.1927,  -2.9020],
        [  1.1601,  -2.5897],
        [  0.2623,  -1.8038],
        [  1.3216,  -2.7304],
        [  2.3811,  -4.2042],
        [  1.5228,  -3.1545],
        [  2.4299,  -3.9591],
        [  0.9375,  -2.8936],
        [ -1.6353,  -1.4256],
        [  0.3046,  -3.5232],
        [  1.2519,  -2.6867],
        [  0.3571,  -1.7458],
        [  1.0180,  -2.8397],
        [ -0.3128,  -3.7246],
        [  0.6117,  -3.2379],
        [  0.1531,  -1.6113],
        [  0.7233,  -2.1098],
        [  1.3731,  -2.8411],
        [  2.6501,  -4.0641],
        [  1.9690,  -3.4690],
        [  1.8144,  -3.3255],
        [  0.7969,  -3.4841],
        [  8.9801, -10.4769],
        [  1.5991,  -3.7050],
        [  0.4786,  -3.5484],
        [ -1.5045,  -3.1107],
        [  3.3206,  -4.7555],
        [  2.6322,  -4.0430],
        [  1.7360,  -4.4695],
        [ -0.1722,  -4.3040],
        [  1.9661,  -3.3687],
        [  0.6560,  -2.6219],
        [  2.1555,  -3.6111],
        [  5.1225,  -7.4241],
        [  2.4493,  -3.8408]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6736, 0.3264],
        [0.2426, 0.7574]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8942, 0.1058], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1854, 0.1095],
         [0.6163, 0.3051]],

        [[0.2624, 0.1053],
         [0.8986, 0.5563]],

        [[0.8090, 0.1022],
         [0.1643, 0.4078]],

        [[0.1238, 0.0984],
         [0.9676, 0.1605]],

        [[0.0786, 0.1071],
         [0.7631, 0.6124]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.008484848484848486
time is 1
tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 95
Adjusted Rand Index: 0.8080620079101718
time is 2
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9207907017983009
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.5706913820725206
Average Adjusted Rand Index: 0.7280729139720148
Iteration 0: Loss = -14261.522876879639
Iteration 10: Loss = -11384.126450973345
Iteration 20: Loss = -11368.08128061987
Iteration 30: Loss = -11321.159414247975
Iteration 40: Loss = -11131.051594093127
Iteration 50: Loss = -11131.049511483432
Iteration 60: Loss = -11131.04956949712
1
Iteration 70: Loss = -11131.049557476801
2
Iteration 80: Loss = -11131.0495519502
3
Stopping early at iteration 79 due to no improvement.
pi: tensor([[0.7432, 0.2568],
        [0.2784, 0.7216]], dtype=torch.float64)
alpha: tensor([0.4883, 0.5117])
beta: tensor([[[0.2901, 0.0983],
         [0.2408, 0.1936]],

        [[0.7126, 0.1038],
         [0.8344, 0.7483]],

        [[0.9938, 0.1013],
         [0.0327, 0.1163]],

        [[0.7478, 0.0966],
         [0.1266, 0.4000]],

        [[0.8972, 0.1064],
         [0.1100, 0.4796]]], dtype=torch.float64)
time is 0
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 1
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 2
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824062740165256
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.9214428106396227
Average Adjusted Rand Index: 0.9211251262542455
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -14261.283342747647
Iteration 100: Loss = -11386.17178816186
Iteration 200: Loss = -11380.314292012985
Iteration 300: Loss = -11376.718822905696
Iteration 400: Loss = -11186.005175223932
Iteration 500: Loss = -11140.103029641774
Iteration 600: Loss = -11137.470741969853
Iteration 700: Loss = -11137.372976475468
Iteration 800: Loss = -11137.312739770543
Iteration 900: Loss = -11137.275394100488
Iteration 1000: Loss = -11137.257239929242
Iteration 1100: Loss = -11137.243851226303
Iteration 1200: Loss = -11137.232493369478
Iteration 1300: Loss = -11129.40043667882
Iteration 1400: Loss = -11127.26729855277
Iteration 1500: Loss = -11127.159217890645
Iteration 1600: Loss = -11127.154775212275
Iteration 1700: Loss = -11127.151057630264
Iteration 1800: Loss = -11127.14757253038
Iteration 1900: Loss = -11127.143955460411
Iteration 2000: Loss = -11127.221996182656
1
Iteration 2100: Loss = -11127.135405360108
Iteration 2200: Loss = -11127.113216134734
Iteration 2300: Loss = -11127.111171941837
Iteration 2400: Loss = -11127.109954254369
Iteration 2500: Loss = -11127.108748690505
Iteration 2600: Loss = -11127.107604246894
Iteration 2700: Loss = -11127.1062057485
Iteration 2800: Loss = -11127.104658944283
Iteration 2900: Loss = -11127.103754894362
Iteration 3000: Loss = -11127.102116367272
Iteration 3100: Loss = -11127.082509457057
Iteration 3200: Loss = -11127.085623038653
1
Iteration 3300: Loss = -11127.08095050319
Iteration 3400: Loss = -11127.080732881672
Iteration 3500: Loss = -11127.08031073806
Iteration 3600: Loss = -11127.080469316594
1
Iteration 3700: Loss = -11127.07971095134
Iteration 3800: Loss = -11127.082554230306
1
Iteration 3900: Loss = -11127.078872691445
Iteration 4000: Loss = -11127.038758136081
Iteration 4100: Loss = -11127.034125821856
Iteration 4200: Loss = -11127.033922482837
Iteration 4300: Loss = -11127.075910416139
1
Iteration 4400: Loss = -11127.033423482388
Iteration 4500: Loss = -11127.03316730512
Iteration 4600: Loss = -11127.033051279257
Iteration 4700: Loss = -11127.032465810184
Iteration 4800: Loss = -11127.066164535476
1
Iteration 4900: Loss = -11127.031180864728
Iteration 5000: Loss = -11127.031041168817
Iteration 5100: Loss = -11127.031340564483
1
Iteration 5200: Loss = -11127.030822641012
Iteration 5300: Loss = -11127.08303539671
1
Iteration 5400: Loss = -11127.03072181873
Iteration 5500: Loss = -11127.03062140096
Iteration 5600: Loss = -11127.030567239908
Iteration 5700: Loss = -11127.030313108322
Iteration 5800: Loss = -11127.037313628482
1
Iteration 5900: Loss = -11127.030061821704
Iteration 6000: Loss = -11127.039806409128
1
Iteration 6100: Loss = -11127.029227948056
Iteration 6200: Loss = -11127.029138014108
Iteration 6300: Loss = -11127.028965472318
Iteration 6400: Loss = -11127.028740360118
Iteration 6500: Loss = -11127.036285326694
1
Iteration 6600: Loss = -11127.028572849595
Iteration 6700: Loss = -11127.028531971146
Iteration 6800: Loss = -11127.053784287496
1
Iteration 6900: Loss = -11127.028426860841
Iteration 7000: Loss = -11127.028350865807
Iteration 7100: Loss = -11127.028245477968
Iteration 7200: Loss = -11127.028249707055
1
Iteration 7300: Loss = -11127.028101786991
Iteration 7400: Loss = -11127.027978437705
Iteration 7500: Loss = -11127.026937738177
Iteration 7600: Loss = -11127.027053968424
1
Iteration 7700: Loss = -11127.039707080621
2
Iteration 7800: Loss = -11127.027420740367
3
Iteration 7900: Loss = -11127.027244690964
4
Iteration 8000: Loss = -11127.032320789496
5
Iteration 8100: Loss = -11127.024107098403
Iteration 8200: Loss = -11127.024978253168
1
Iteration 8300: Loss = -11127.022623342149
Iteration 8400: Loss = -11127.022382975314
Iteration 8500: Loss = -11126.886480829024
Iteration 8600: Loss = -11126.975428705498
1
Iteration 8700: Loss = -11126.98140993961
2
Iteration 8800: Loss = -11126.879774544372
Iteration 8900: Loss = -11126.87996483657
1
Iteration 9000: Loss = -11126.967623739094
2
Iteration 9100: Loss = -11126.880055824979
3
Iteration 9200: Loss = -11126.885883815827
4
Iteration 9300: Loss = -11126.878185584414
Iteration 9400: Loss = -11126.87924140315
1
Iteration 9500: Loss = -11126.905851486277
2
Iteration 9600: Loss = -11126.99435167626
3
Iteration 9700: Loss = -11126.885480679346
4
Iteration 9800: Loss = -11126.877722946405
Iteration 9900: Loss = -11126.879230491135
1
Iteration 10000: Loss = -11126.880623015477
2
Iteration 10100: Loss = -11126.876829956715
Iteration 10200: Loss = -11126.876755524556
Iteration 10300: Loss = -11126.885443667858
1
Iteration 10400: Loss = -11126.876690167654
Iteration 10500: Loss = -11126.877609833544
1
Iteration 10600: Loss = -11126.87662056474
Iteration 10700: Loss = -11126.871694242633
Iteration 10800: Loss = -11126.872026374069
1
Iteration 10900: Loss = -11126.870631265472
Iteration 11000: Loss = -11126.875695383827
1
Iteration 11100: Loss = -11126.860941164427
Iteration 11200: Loss = -11126.829828808857
Iteration 11300: Loss = -11126.829803629465
Iteration 11400: Loss = -11126.83022148498
1
Iteration 11500: Loss = -11126.830534806984
2
Iteration 11600: Loss = -11126.831256340018
3
Iteration 11700: Loss = -11126.853611258271
4
Iteration 11800: Loss = -11126.836739170123
5
Iteration 11900: Loss = -11126.831599303086
6
Iteration 12000: Loss = -11126.809225054418
Iteration 12100: Loss = -11126.8139042918
1
Iteration 12200: Loss = -11126.812044589788
2
Iteration 12300: Loss = -11126.79976609728
Iteration 12400: Loss = -11126.795463564422
Iteration 12500: Loss = -11126.85190014065
1
Iteration 12600: Loss = -11126.795223630763
Iteration 12700: Loss = -11126.805978613605
1
Iteration 12800: Loss = -11126.82986935048
2
Iteration 12900: Loss = -11126.800597044683
3
Iteration 13000: Loss = -11126.794578916588
Iteration 13100: Loss = -11126.79470736887
1
Iteration 13200: Loss = -11126.943892048233
2
Iteration 13300: Loss = -11126.794156902331
Iteration 13400: Loss = -11126.795075935395
1
Iteration 13500: Loss = -11126.793272267214
Iteration 13600: Loss = -11126.79385900789
1
Iteration 13700: Loss = -11126.79324130449
Iteration 13800: Loss = -11126.793938637515
1
Iteration 13900: Loss = -11126.79322183973
Iteration 14000: Loss = -11126.79672105971
1
Iteration 14100: Loss = -11126.793203917216
Iteration 14200: Loss = -11126.793363628403
1
Iteration 14300: Loss = -11126.793241536277
2
Iteration 14400: Loss = -11126.793356035192
3
Iteration 14500: Loss = -11126.79745838116
4
Iteration 14600: Loss = -11126.793021079679
Iteration 14700: Loss = -11126.792909525555
Iteration 14800: Loss = -11126.800373354685
1
Iteration 14900: Loss = -11126.792259831202
Iteration 15000: Loss = -11126.792984992859
1
Iteration 15100: Loss = -11126.792092716369
Iteration 15200: Loss = -11126.79902315251
1
Iteration 15300: Loss = -11126.792298073278
2
Iteration 15400: Loss = -11126.79191505535
Iteration 15500: Loss = -11126.796711782967
1
Iteration 15600: Loss = -11126.791858730998
Iteration 15700: Loss = -11126.792775291682
1
Iteration 15800: Loss = -11126.79918743945
2
Iteration 15900: Loss = -11126.791832611865
Iteration 16000: Loss = -11126.792827397729
1
Iteration 16100: Loss = -11126.797851350433
2
Iteration 16200: Loss = -11126.803574133348
3
Iteration 16300: Loss = -11126.853089458134
4
Iteration 16400: Loss = -11126.791988749686
5
Iteration 16500: Loss = -11126.791701623246
Iteration 16600: Loss = -11126.799363771379
1
Iteration 16700: Loss = -11126.791589574432
Iteration 16800: Loss = -11126.802043203226
1
Iteration 16900: Loss = -11126.791615363205
2
Iteration 17000: Loss = -11126.802120821201
3
Iteration 17100: Loss = -11126.808745779725
4
Iteration 17200: Loss = -11126.791654306171
5
Iteration 17300: Loss = -11126.805423351263
6
Iteration 17400: Loss = -11126.791600896557
7
Iteration 17500: Loss = -11126.812824111144
8
Iteration 17600: Loss = -11126.792208043087
9
Iteration 17700: Loss = -11127.011451655693
10
Stopping early at iteration 17700 due to no improvement.
tensor([[ -2.8832,   1.4832],
        [ -5.3711,   3.8158],
        [  4.1826,  -7.8705],
        [ -4.0749,   2.6877],
        [ -6.0027,   3.4263],
        [  0.3480,  -1.7629],
        [ -3.7283,   1.8854],
        [ -4.5045,   3.0478],
        [  4.0605,  -5.6275],
        [  6.5042,  -7.8921],
        [ -6.8386,   4.4175],
        [ -3.5509,   1.8917],
        [ -6.3051,   4.8749],
        [  5.0410,  -6.4273],
        [  1.2222,  -3.1945],
        [  2.9274,  -4.5768],
        [ -3.9214,   2.1172],
        [  3.3070,  -5.6786],
        [ -6.5868,   4.3324],
        [  2.7933,  -7.2970],
        [ -0.4022,  -1.0147],
        [ -7.8941,   6.5044],
        [ -4.5503,   1.6820],
        [ -6.4151,   3.7804],
        [ -3.8836,   2.4544],
        [ -2.5600,   0.6268],
        [ -5.3344,   0.7192],
        [ -8.2954,   6.4897],
        [ -6.1787,   4.7385],
        [ -1.5902,   0.1446],
        [ -8.1295,   6.7272],
        [ -4.7801,   2.9717],
        [ -5.1128,   3.6512],
        [  6.6861,  -8.6648],
        [ -3.6598,   2.2732],
        [  3.2798,  -4.8460],
        [ -6.2851,   3.8703],
        [  4.6171,  -6.0578],
        [  2.4994,  -4.1138],
        [  1.1633,  -2.5576],
        [ -6.3674,   4.8605],
        [ -4.0468,   2.6377],
        [ -4.7778,   1.3291],
        [ -2.6834,   0.3644],
        [ -5.5157,   3.8444],
        [  2.8783,  -5.4911],
        [ -3.8461,   2.4415],
        [ -3.1989,   0.8798],
        [  4.7289,  -6.7542],
        [ -0.5448,  -0.9461],
        [ -4.3922,   2.3739],
        [  5.1871,  -6.6016],
        [  2.7243,  -4.8333],
        [ -6.8632,   5.2535],
        [ -4.5013,   2.9176],
        [ -6.5163,   4.9717],
        [ -2.0729,   0.5155],
        [  1.5963,  -3.3474],
        [ -4.3175,   2.3530],
        [  1.4271,  -6.0246],
        [ -4.0028,   0.3018],
        [  4.1599,  -6.2107],
        [ -4.6625,   3.1418],
        [  3.2345,  -5.5283],
        [  0.5496,  -2.0700],
        [  2.9033,  -4.4209],
        [ -4.9977,   3.4803],
        [ -4.4483,   2.6912],
        [  0.0277,  -1.4430],
        [ -5.9489,   4.5464],
        [ -6.3068,   3.1841],
        [  3.0536,  -4.6153],
        [  3.9457,  -5.6140],
        [ -2.9923,   1.5205],
        [ -3.2089,   1.6766],
        [  4.0111,  -5.4090],
        [  0.3952,  -1.8022],
        [  1.4004,  -2.7957],
        [  0.7084,  -4.7159],
        [ -1.2391,  -0.2099],
        [  4.0542,  -5.4430],
        [ -1.6476,  -0.8028],
        [-10.5860,   8.8537],
        [  2.1258,  -3.5784],
        [ -4.1237,   2.7367],
        [  1.2551,  -4.2277],
        [  3.1233,  -4.5485],
        [  2.8312,  -5.1086],
        [  2.3747,  -4.4568],
        [  2.2010,  -4.0334],
        [ -5.1917,   2.7649],
        [  3.9470,  -6.8467],
        [  5.1103,  -7.4362],
        [ -5.7340,   4.0040],
        [ -6.5535,   5.1626],
        [  4.7995,  -6.2340],
        [  0.4506,  -2.1060],
        [  6.8647,  -8.7367],
        [  3.4512,  -5.5365],
        [ -8.6662,   6.6870]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7575, 0.2425],
        [0.2573, 0.7427]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4518, 0.5482], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2987, 0.0987],
         [0.2408, 0.1955]],

        [[0.7126, 0.1046],
         [0.8344, 0.7483]],

        [[0.9938, 0.1024],
         [0.0327, 0.1163]],

        [[0.7478, 0.0984],
         [0.1266, 0.4000]],

        [[0.8972, 0.1074],
         [0.1100, 0.4796]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 1
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8448338943900694
time is 2
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.9214428106396227
Average Adjusted Rand Index: 0.9214497464974624
Iteration 0: Loss = -31508.617724666223
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.2085,    nan]],

        [[0.6733,    nan],
         [0.8500, 0.9876]],

        [[0.0705,    nan],
         [0.9676, 0.9580]],

        [[0.2585,    nan],
         [0.8522, 0.1623]],

        [[0.4791,    nan],
         [0.2203, 0.2678]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -31507.60401283028
Iteration 100: Loss = -11396.619171383229
Iteration 200: Loss = -11393.313803457506
Iteration 300: Loss = -11392.11249036133
Iteration 400: Loss = -11391.337781703613
Iteration 500: Loss = -11390.812764041424
Iteration 600: Loss = -11390.421154507807
Iteration 700: Loss = -11390.107587444298
Iteration 800: Loss = -11389.847477977251
Iteration 900: Loss = -11389.631683431155
Iteration 1000: Loss = -11389.449007327665
Iteration 1100: Loss = -11389.285419412658
Iteration 1200: Loss = -11389.12790625145
Iteration 1300: Loss = -11388.967548080907
Iteration 1400: Loss = -11388.797869036269
Iteration 1500: Loss = -11388.606502823057
Iteration 1600: Loss = -11388.36507768196
Iteration 1700: Loss = -11388.019643040776
Iteration 1800: Loss = -11387.497784905921
Iteration 1900: Loss = -11386.749225820298
Iteration 2000: Loss = -11385.612740250714
Iteration 2100: Loss = -11383.466051449195
Iteration 2200: Loss = -11381.85626926275
Iteration 2300: Loss = -11380.921213636522
Iteration 2400: Loss = -11380.360282800064
Iteration 2500: Loss = -11379.955533786193
Iteration 2600: Loss = -11379.392248646374
Iteration 2700: Loss = -11373.066550671749
Iteration 2800: Loss = -11336.547190223488
Iteration 2900: Loss = -11315.079579275585
Iteration 3000: Loss = -11304.425946556941
Iteration 3100: Loss = -11296.867661918162
Iteration 3200: Loss = -11288.752932905623
Iteration 3300: Loss = -11287.830578852116
Iteration 3400: Loss = -11287.497878685355
Iteration 3500: Loss = -11287.166230672032
Iteration 3600: Loss = -11275.317843147483
Iteration 3700: Loss = -11274.526092629592
Iteration 3800: Loss = -11274.394196108227
Iteration 3900: Loss = -11274.249353060619
Iteration 4000: Loss = -11272.692752837846
Iteration 4100: Loss = -11271.825421748292
Iteration 4200: Loss = -11260.126615319145
Iteration 4300: Loss = -11255.373014225452
Iteration 4400: Loss = -11251.549962179291
Iteration 4500: Loss = -11250.801011664065
Iteration 4600: Loss = -11250.73885953499
Iteration 4700: Loss = -11247.328911747183
Iteration 4800: Loss = -11247.160463378985
Iteration 4900: Loss = -11244.790736862553
Iteration 5000: Loss = -11244.534235400071
Iteration 5100: Loss = -11244.322319529261
Iteration 5200: Loss = -11231.02759959272
Iteration 5300: Loss = -11226.318555614864
Iteration 5400: Loss = -11223.726083039623
Iteration 5500: Loss = -11222.581385801373
Iteration 5600: Loss = -11222.439937897623
Iteration 5700: Loss = -11222.404922186774
Iteration 5800: Loss = -11220.038253915433
Iteration 5900: Loss = -11217.676792411881
Iteration 6000: Loss = -11217.63244238885
Iteration 6100: Loss = -11217.599283987496
Iteration 6200: Loss = -11216.6951923036
Iteration 6300: Loss = -11216.68888309328
Iteration 6400: Loss = -11216.685102665357
Iteration 6500: Loss = -11216.67060190173
Iteration 6600: Loss = -11216.617269984688
Iteration 6700: Loss = -11216.610039974039
Iteration 6800: Loss = -11216.602075862793
Iteration 6900: Loss = -11216.041585456911
Iteration 7000: Loss = -11216.016807461894
Iteration 7100: Loss = -11215.99967736482
Iteration 7200: Loss = -11215.852509374616
Iteration 7300: Loss = -11215.840126356668
Iteration 7400: Loss = -11215.83617775501
Iteration 7500: Loss = -11215.82826940129
Iteration 7600: Loss = -11213.162490627245
Iteration 7700: Loss = -11213.159679546667
Iteration 7800: Loss = -11213.152922651834
Iteration 7900: Loss = -11213.151987874022
Iteration 8000: Loss = -11213.063572128338
Iteration 8100: Loss = -11213.072529187497
1
Iteration 8200: Loss = -11213.058488499228
Iteration 8300: Loss = -11213.057941554476
Iteration 8400: Loss = -11213.059680690989
1
Iteration 8500: Loss = -11213.059581061014
2
Iteration 8600: Loss = -11211.512217167401
Iteration 8700: Loss = -11211.508314528557
Iteration 8800: Loss = -11209.944618396505
Iteration 8900: Loss = -11209.941559506298
Iteration 9000: Loss = -11209.94050745755
Iteration 9100: Loss = -11209.941465636788
1
Iteration 9200: Loss = -11209.939437358684
Iteration 9300: Loss = -11209.939823136752
1
Iteration 9400: Loss = -11209.936482674231
Iteration 9500: Loss = -11209.933636647233
Iteration 9600: Loss = -11209.920961658238
Iteration 9700: Loss = -11209.921320199725
1
Iteration 9800: Loss = -11209.920567448751
Iteration 9900: Loss = -11209.925061907836
1
Iteration 10000: Loss = -11209.920319741981
Iteration 10100: Loss = -11209.94576730734
1
Iteration 10200: Loss = -11209.920045874147
Iteration 10300: Loss = -11209.919884947803
Iteration 10400: Loss = -11209.920046358717
1
Iteration 10500: Loss = -11209.919488731455
Iteration 10600: Loss = -11209.928226513448
1
Iteration 10700: Loss = -11209.919245163945
Iteration 10800: Loss = -11209.920670185345
1
Iteration 10900: Loss = -11209.933734755714
2
Iteration 11000: Loss = -11209.915427342283
Iteration 11100: Loss = -11209.912121992487
Iteration 11200: Loss = -11209.910073171177
Iteration 11300: Loss = -11209.917795850693
1
Iteration 11400: Loss = -11209.909694865191
Iteration 11500: Loss = -11209.91077417206
1
Iteration 11600: Loss = -11209.907035714717
Iteration 11700: Loss = -11209.271142571473
Iteration 11800: Loss = -11209.267245639952
Iteration 11900: Loss = -11209.251137528416
Iteration 12000: Loss = -11209.246274391431
Iteration 12100: Loss = -11209.263056857351
1
Iteration 12200: Loss = -11209.242913040249
Iteration 12300: Loss = -11209.266986605438
1
Iteration 12400: Loss = -11209.239862583923
Iteration 12500: Loss = -11209.239763964133
Iteration 12600: Loss = -11209.239966757108
1
Iteration 12700: Loss = -11209.239630677697
Iteration 12800: Loss = -11209.282876549356
1
Iteration 12900: Loss = -11209.239188996524
Iteration 13000: Loss = -11209.26334183765
1
Iteration 13100: Loss = -11209.238649380797
Iteration 13200: Loss = -11209.242642306343
1
Iteration 13300: Loss = -11209.222112197182
Iteration 13400: Loss = -11209.22288874714
1
Iteration 13500: Loss = -11209.222239432005
2
Iteration 13600: Loss = -11209.222076944934
Iteration 13700: Loss = -11209.245389718679
1
Iteration 13800: Loss = -11209.220635747495
Iteration 13900: Loss = -11209.223992606761
1
Iteration 14000: Loss = -11209.220597348141
Iteration 14100: Loss = -11209.221571653386
1
Iteration 14200: Loss = -11209.220544014628
Iteration 14300: Loss = -11209.250200153849
1
Iteration 14400: Loss = -11209.220515357663
Iteration 14500: Loss = -11209.220587757312
1
Iteration 14600: Loss = -11209.22043998925
Iteration 14700: Loss = -11209.220342081408
Iteration 14800: Loss = -11209.237033242318
1
Iteration 14900: Loss = -11209.22026722261
Iteration 15000: Loss = -11209.22025194024
Iteration 15100: Loss = -11209.220668877671
1
Iteration 15200: Loss = -11209.220152405296
Iteration 15300: Loss = -11209.248001217733
1
Iteration 15400: Loss = -11209.22001393015
Iteration 15500: Loss = -11209.22002533701
1
Iteration 15600: Loss = -11209.219713080896
Iteration 15700: Loss = -11209.216433889786
Iteration 15800: Loss = -11209.22534913017
1
Iteration 15900: Loss = -11209.21630507806
Iteration 16000: Loss = -11209.21641959425
1
Iteration 16100: Loss = -11209.216323894165
2
Iteration 16200: Loss = -11209.216293001047
Iteration 16300: Loss = -11209.29434700117
1
Iteration 16400: Loss = -11209.216283549868
Iteration 16500: Loss = -11209.216275570117
Iteration 16600: Loss = -11209.218175463251
1
Iteration 16700: Loss = -11209.21624447524
Iteration 16800: Loss = -11209.391331217212
1
Iteration 16900: Loss = -11209.215939096373
Iteration 17000: Loss = -11209.214801493736
Iteration 17100: Loss = -11209.278534460811
1
Iteration 17200: Loss = -11209.35830858177
2
Iteration 17300: Loss = -11209.213997017683
Iteration 17400: Loss = -11209.2137810017
Iteration 17500: Loss = -11209.22081299157
1
Iteration 17600: Loss = -11209.213664934296
Iteration 17700: Loss = -11209.213752210897
1
Iteration 17800: Loss = -11209.212815701858
Iteration 17900: Loss = -11209.211512481621
Iteration 18000: Loss = -11209.210583682989
Iteration 18100: Loss = -11209.247698720921
1
Iteration 18200: Loss = -11209.210553777444
Iteration 18300: Loss = -11209.210613432275
1
Iteration 18400: Loss = -11209.237489015904
2
Iteration 18500: Loss = -11209.210593134756
3
Iteration 18600: Loss = -11209.210580738807
4
Iteration 18700: Loss = -11209.221931873728
5
Iteration 18800: Loss = -11209.210564500025
6
Iteration 18900: Loss = -11209.210526828529
Iteration 19000: Loss = -11209.210194914633
Iteration 19100: Loss = -11209.20919490758
Iteration 19200: Loss = -11209.210958941805
1
Iteration 19300: Loss = -11209.21402539525
2
Iteration 19400: Loss = -11209.209493093593
3
Iteration 19500: Loss = -11209.20904392924
Iteration 19600: Loss = -11209.408236445768
1
Iteration 19700: Loss = -11209.209033995807
Iteration 19800: Loss = -11209.22584767219
1
Iteration 19900: Loss = -11209.25668759228
2
tensor([[-0.2235, -4.3917],
        [ 1.3296, -2.7160],
        [-7.5543,  5.4342],
        [-3.6263,  2.1573],
        [ 1.9130, -3.4366],
        [-8.8145,  4.9199],
        [-0.4798, -2.2243],
        [ 0.4748, -2.2488],
        [-5.1427,  3.7486],
        [-7.1470,  5.5476],
        [ 0.6363, -2.1514],
        [-3.4265,  0.9350],
        [ 4.3104, -5.7265],
        [-4.9475,  3.2288],
        [-9.6337,  6.9649],
        [-5.1559,  3.0471],
        [-4.0185,  1.3615],
        [-6.0019,  3.9905],
        [ 1.9475, -3.7867],
        [-7.1035,  5.7168],
        [-2.3507, -1.1174],
        [ 4.1477, -5.5576],
        [ 0.0106, -2.1771],
        [ 0.5802, -2.6820],
        [-2.6631,  0.4947],
        [-3.5853,  1.1723],
        [-2.4587,  0.6503],
        [ 4.0219, -5.4090],
        [ 0.8012, -2.6701],
        [-2.9048,  1.1471],
        [ 5.4322, -6.8474],
        [ 0.8175, -4.3412],
        [ 1.5374, -3.6769],
        [-9.3196,  7.0914],
        [-1.4646,  0.0677],
        [-9.4117,  7.0862],
        [ 3.7699, -5.4081],
        [-5.3802,  3.8382],
        [-8.5204,  6.4745],
        [-4.9062,  2.8436],
        [-2.1395,  0.6916],
        [-1.8470,  0.4285],
        [-2.7071,  1.3165],
        [ 1.2102, -3.3532],
        [ 2.0583, -3.8711],
        [-6.2887,  4.8025],
        [ 1.3869, -3.0678],
        [-0.5871, -0.8950],
        [-8.8207,  7.1932],
        [-3.4473,  1.6133],
        [ 1.7623, -3.1487],
        [-9.8224,  8.2597],
        [-4.9930,  3.1070],
        [-4.0349,  1.9919],
        [-0.9951, -3.0243],
        [ 4.3989, -5.8593],
        [-3.3296,  1.2235],
        [-4.3345, -0.2808],
        [-0.0822, -1.4314],
        [-5.3160,  3.3835],
        [ 0.8335, -2.3521],
        [-4.7691,  2.4511],
        [ 1.8343, -3.5099],
        [-7.5566,  5.7976],
        [-3.9498,  2.5404],
        [-5.1975,  3.2196],
        [ 1.3550, -2.7412],
        [ 0.8260, -2.2320],
        [-4.2421,  0.5666],
        [ 2.8800, -4.6018],
        [-2.8249,  1.4065],
        [-3.9213,  0.8240],
        [-5.7391,  3.3751],
        [ 0.0434, -2.0168],
        [-3.1515,  1.4339],
        [-4.7100,  3.2343],
        [-4.0729,  2.6573],
        [-9.4551,  7.5398],
        [-5.0724,  0.7812],
        [-2.7750,  1.3490],
        [-6.3210,  4.9159],
        [-2.5060,  1.1090],
        [ 1.9188, -4.4586],
        [-6.1217,  4.0298],
        [ 1.5241, -2.9250],
        [-3.5348,  2.1272],
        [-8.7952,  6.6195],
        [-4.9962,  3.5585],
        [-5.0449,  3.4928],
        [-4.9710,  3.5707],
        [ 0.5065, -1.9242],
        [-6.6864,  5.1931],
        [-9.7141,  7.6006],
        [ 4.1663, -5.8496],
        [ 2.7507, -5.6997],
        [-8.4335,  6.3226],
        [-2.5722,  1.0991],
        [-9.2278,  7.7292],
        [-5.5553,  4.1021],
        [ 4.6097, -6.1218]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.4676, 0.5324],
        [0.5517, 0.4483]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3640, 0.6360], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2770, 0.0962],
         [0.2085, 0.2144]],

        [[0.6733, 0.1023],
         [0.8500, 0.9876]],

        [[0.0705, 0.1006],
         [0.9676, 0.9580]],

        [[0.2585, 0.0958],
         [0.8522, 0.1623]],

        [[0.4791, 0.1056],
         [0.2203, 0.2678]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 82
Adjusted Rand Index: 0.4036363636363636
time is 1
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8448387581811067
time is 2
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 88
Adjusted Rand Index: 0.5733333333333334
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.08343080424116947
Average Adjusted Rand Index: 0.740521817749147
11154.820126537514
new:  [0.5706913820725206, 0.5706913820725206, 0.9214428106396227, 0.08343080424116947] [0.7280729139720148, 0.7280729139720148, 0.9214497464974624, 0.740521817749147] [11175.306255560889, 11175.618534750933, 11127.011451655693, 11209.217258836316]
prior:  [0.9214428106396227, -7.67562268970562e-05, 0.9214428106396227, 0.0] [0.9211251262542455, -0.004558265739409537, 0.9211251262542455, 0.0] [11131.049568984043, 11381.729396246108, 11131.0495519502, nan]
-----------------------------------------------------------------------------------------
This iteration is 16
True Objective function: Loss = -11014.755591317185
Iteration 0: Loss = -30561.212447060636
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.6141,    nan]],

        [[0.1785,    nan],
         [0.2798, 0.9198]],

        [[0.3034,    nan],
         [0.5689, 0.5836]],

        [[0.5086,    nan],
         [0.9263, 0.6156]],

        [[0.9053,    nan],
         [0.9108, 0.8737]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -30552.815986943046
Iteration 100: Loss = -11260.990034656845
Iteration 200: Loss = -11257.971019281436
Iteration 300: Loss = -11256.956100922818
Iteration 400: Loss = -11256.49287274955
Iteration 500: Loss = -11256.228249667445
Iteration 600: Loss = -11256.053702383148
Iteration 700: Loss = -11255.928876193992
Iteration 800: Loss = -11255.834866318199
Iteration 900: Loss = -11255.761350783683
Iteration 1000: Loss = -11255.70200802381
Iteration 1100: Loss = -11255.652705304985
Iteration 1200: Loss = -11255.61022498993
Iteration 1300: Loss = -11255.57160186163
Iteration 1400: Loss = -11255.531906122593
Iteration 1500: Loss = -11255.481088411869
Iteration 1600: Loss = -11254.590131348656
Iteration 1700: Loss = -11252.780070213217
Iteration 1800: Loss = -11251.623143369
Iteration 1900: Loss = -11251.37220928761
Iteration 2000: Loss = -11251.161164325653
Iteration 2100: Loss = -11250.964895862344
Iteration 2200: Loss = -11250.838829392012
Iteration 2300: Loss = -11250.755442081054
Iteration 2400: Loss = -11250.69627395269
Iteration 2500: Loss = -11250.650817799735
Iteration 2600: Loss = -11250.614819929686
Iteration 2700: Loss = -11250.585667959096
Iteration 2800: Loss = -11250.5616693989
Iteration 2900: Loss = -11250.635262841075
1
Iteration 3000: Loss = -11250.52453251192
Iteration 3100: Loss = -11250.509916905363
Iteration 3200: Loss = -11250.497253036636
Iteration 3300: Loss = -11250.48636045703
Iteration 3400: Loss = -11250.476489321924
Iteration 3500: Loss = -11250.467914142006
Iteration 3600: Loss = -11250.464512673254
Iteration 3700: Loss = -11250.453507440101
Iteration 3800: Loss = -11250.447351222485
Iteration 3900: Loss = -11250.448636665875
1
Iteration 4000: Loss = -11250.436866096197
Iteration 4100: Loss = -11250.432383370571
Iteration 4200: Loss = -11250.42825563697
Iteration 4300: Loss = -11250.426727006856
Iteration 4400: Loss = -11250.421043406284
Iteration 4500: Loss = -11250.417915012902
Iteration 4600: Loss = -11250.414976970222
Iteration 4700: Loss = -11250.412638695008
Iteration 4800: Loss = -11250.409894456412
Iteration 4900: Loss = -11250.407571990458
Iteration 5000: Loss = -11250.407891278786
1
Iteration 5100: Loss = -11250.40350241671
Iteration 5200: Loss = -11250.40164580426
Iteration 5300: Loss = -11250.39992483624
Iteration 5400: Loss = -11250.400167684731
1
Iteration 5500: Loss = -11250.396819915124
Iteration 5600: Loss = -11250.395474489085
Iteration 5700: Loss = -11250.509545778246
1
Iteration 5800: Loss = -11250.392944694964
Iteration 5900: Loss = -11250.391809278372
Iteration 6000: Loss = -11250.39071286412
Iteration 6100: Loss = -11250.389744155751
Iteration 6200: Loss = -11250.388759511281
Iteration 6300: Loss = -11250.38786830913
Iteration 6400: Loss = -11250.39371783975
1
Iteration 6500: Loss = -11250.386219579541
Iteration 6600: Loss = -11250.38547100984
Iteration 6700: Loss = -11250.384685134959
Iteration 6800: Loss = -11250.383961797384
Iteration 6900: Loss = -11250.383263070005
Iteration 7000: Loss = -11250.382565271766
Iteration 7100: Loss = -11251.05376245759
1
Iteration 7200: Loss = -11250.381170857443
Iteration 7300: Loss = -11250.380400671778
Iteration 7400: Loss = -11250.379623950028
Iteration 7500: Loss = -11250.398886941024
1
Iteration 7600: Loss = -11250.377832044946
Iteration 7700: Loss = -11250.376770569128
Iteration 7800: Loss = -11250.375543988157
Iteration 7900: Loss = -11250.379033274521
1
Iteration 8000: Loss = -11250.372594721917
Iteration 8100: Loss = -11250.37074408417
Iteration 8200: Loss = -11250.640857774992
1
Iteration 8300: Loss = -11250.366198805377
Iteration 8400: Loss = -11250.363417695535
Iteration 8500: Loss = -11250.360300817507
Iteration 8600: Loss = -11250.359700969484
Iteration 8700: Loss = -11250.35322086934
Iteration 8800: Loss = -11250.349262771046
Iteration 8900: Loss = -11250.395677650931
1
Iteration 9000: Loss = -11250.3409970834
Iteration 9100: Loss = -11250.336774300857
Iteration 9200: Loss = -11250.332553280252
Iteration 9300: Loss = -11250.328484856527
Iteration 9400: Loss = -11250.324492053951
Iteration 9500: Loss = -11250.320670037483
Iteration 9600: Loss = -11250.317918032211
Iteration 9700: Loss = -11250.313642376343
Iteration 9800: Loss = -11250.317257576173
1
Iteration 9900: Loss = -11250.307410600795
Iteration 10000: Loss = -11250.30463205558
Iteration 10100: Loss = -11250.340588954625
1
Iteration 10200: Loss = -11250.2996033111
Iteration 10300: Loss = -11250.297379253312
Iteration 10400: Loss = -11250.2953262441
Iteration 10500: Loss = -11250.294122801582
Iteration 10600: Loss = -11250.29164413181
Iteration 10700: Loss = -11250.290403988278
Iteration 10800: Loss = -11250.302652561357
1
Iteration 10900: Loss = -11250.28720876962
Iteration 11000: Loss = -11250.285890627132
Iteration 11100: Loss = -11250.284739557075
Iteration 11200: Loss = -11250.321478057904
1
Iteration 11300: Loss = -11250.282645277104
Iteration 11400: Loss = -11250.281658148317
Iteration 11500: Loss = -11250.28085688592
Iteration 11600: Loss = -11250.28013183316
Iteration 11700: Loss = -11250.279312988807
Iteration 11800: Loss = -11250.278631046309
Iteration 11900: Loss = -11250.278059081616
Iteration 12000: Loss = -11250.277430074344
Iteration 12100: Loss = -11250.276800214877
Iteration 12200: Loss = -11250.353137909555
1
Iteration 12300: Loss = -11250.275808141512
Iteration 12400: Loss = -11250.275379058476
Iteration 12500: Loss = -11250.437444573023
1
Iteration 12600: Loss = -11250.274572754157
Iteration 12700: Loss = -11250.27421650894
Iteration 12800: Loss = -11250.27769611144
1
Iteration 12900: Loss = -11250.273579462399
Iteration 13000: Loss = -11250.273244667316
Iteration 13100: Loss = -11250.27295794738
Iteration 13200: Loss = -11250.273096243629
1
Iteration 13300: Loss = -11250.272496888601
Iteration 13400: Loss = -11250.27227620884
Iteration 13500: Loss = -11250.273324434693
1
Iteration 13600: Loss = -11250.271817843473
Iteration 13700: Loss = -11250.271794696204
Iteration 13800: Loss = -11250.271447155965
Iteration 13900: Loss = -11250.271310574397
Iteration 14000: Loss = -11250.271304538797
Iteration 14100: Loss = -11250.27099100659
Iteration 14200: Loss = -11250.270893889414
Iteration 14300: Loss = -11250.27730845675
1
Iteration 14400: Loss = -11250.270615856936
Iteration 14500: Loss = -11250.270742957138
1
Iteration 14600: Loss = -11250.563121649948
2
Iteration 14700: Loss = -11250.270307788614
Iteration 14800: Loss = -11250.270221457395
Iteration 14900: Loss = -11250.27136705715
1
Iteration 15000: Loss = -11250.270161094877
Iteration 15100: Loss = -11250.269887448754
Iteration 15200: Loss = -11250.269791837163
Iteration 15300: Loss = -11250.316899916723
1
Iteration 15400: Loss = -11250.269642620491
Iteration 15500: Loss = -11250.269627202302
Iteration 15600: Loss = -11250.30386842749
1
Iteration 15700: Loss = -11250.269463537814
Iteration 15800: Loss = -11250.269395473897
Iteration 15900: Loss = -11250.269355457782
Iteration 16000: Loss = -11250.272805803928
1
Iteration 16100: Loss = -11250.269305400881
Iteration 16200: Loss = -11250.269356863735
1
Iteration 16300: Loss = -11250.269266777877
Iteration 16400: Loss = -11250.269326266065
1
Iteration 16500: Loss = -11250.26923094537
Iteration 16600: Loss = -11250.269206113257
Iteration 16700: Loss = -11250.272693433511
1
Iteration 16800: Loss = -11250.26917227843
Iteration 16900: Loss = -11250.269151437838
Iteration 17000: Loss = -11250.329405827917
1
Iteration 17100: Loss = -11250.269082205825
Iteration 17200: Loss = -11250.269056294212
Iteration 17300: Loss = -11250.269752790413
1
Iteration 17400: Loss = -11250.2689703557
Iteration 17500: Loss = -11250.268956416485
Iteration 17600: Loss = -11250.269597780856
1
Iteration 17700: Loss = -11250.268935830612
Iteration 17800: Loss = -11250.268873622603
Iteration 17900: Loss = -11250.277502171539
1
Iteration 18000: Loss = -11250.269715952561
2
Iteration 18100: Loss = -11250.268818489481
Iteration 18200: Loss = -11250.268866277465
1
Iteration 18300: Loss = -11250.268832788624
2
Iteration 18400: Loss = -11250.268763059345
Iteration 18500: Loss = -11250.26935650535
1
Iteration 18600: Loss = -11250.55383901533
2
Iteration 18700: Loss = -11250.268690320778
Iteration 18800: Loss = -11250.268747817912
1
Iteration 18900: Loss = -11250.26926038515
2
Iteration 19000: Loss = -11250.269605463678
3
Iteration 19100: Loss = -11250.268633003305
Iteration 19200: Loss = -11250.268630219727
Iteration 19300: Loss = -11250.864422229424
1
Iteration 19400: Loss = -11250.268610609977
Iteration 19500: Loss = -11250.269768910193
1
Iteration 19600: Loss = -11250.268592264001
Iteration 19700: Loss = -11250.279236918335
1
Iteration 19800: Loss = -11250.268593002493
2
Iteration 19900: Loss = -11250.268578287652
tensor([[  6.3622, -10.9774],
        [  6.6216, -11.2369],
        [  7.4859, -12.1011],
        [ -3.1328,  -1.4825],
        [  7.7345, -12.3498],
        [  7.5914, -12.2066],
        [  7.8033, -12.4185],
        [  6.2835, -10.8987],
        [  7.5224, -12.1376],
        [  6.3561, -10.9713],
        [  7.4201, -12.0353],
        [  7.6545, -12.2697],
        [  7.2859, -11.9011],
        [  7.5964, -12.2116],
        [  7.1672, -11.7824],
        [  7.3656, -11.9808],
        [  6.6839, -11.2991],
        [  7.5229, -12.1381],
        [  7.4290, -12.0442],
        [  6.0122, -10.6274],
        [  6.1023, -10.7175],
        [  7.2973, -11.9126],
        [  6.2902, -10.9054],
        [  7.3714, -11.9867],
        [  5.7159, -10.3311],
        [  7.4316, -12.0468],
        [  6.3377, -10.9529],
        [  7.7039, -12.3191],
        [  5.6597, -10.2749],
        [  6.3933, -11.0086],
        [  7.2262, -11.8414],
        [  6.3849, -11.0001],
        [  5.4072, -10.0224],
        [  7.1147, -11.7299],
        [  7.0147, -11.6299],
        [  6.1354, -10.7507],
        [  7.1170, -11.7322],
        [  7.4807, -12.0959],
        [  6.5528, -11.1681],
        [  5.8672, -10.4824],
        [  7.5834, -12.1986],
        [  7.2014, -11.8166],
        [  6.0264, -10.6416],
        [  6.5269, -11.1421],
        [  6.2097, -10.8249],
        [  6.6848, -11.3000],
        [  7.3348, -11.9501],
        [  6.5143, -11.1295],
        [  6.4906, -11.1058],
        [  5.9958, -10.6111],
        [  7.6300, -12.2452],
        [  6.3790, -10.9943],
        [  6.5536, -11.1688],
        [  5.4461, -10.0613],
        [  7.4365, -12.0517],
        [  5.7061, -10.3214],
        [  6.3762, -10.9914],
        [  5.2297,  -9.8449],
        [  5.9878, -10.6030],
        [  6.0683, -10.6836],
        [  6.4477, -11.0629],
        [  7.5370, -12.1522],
        [  6.9532, -11.5684],
        [  7.5138, -12.1290],
        [  7.8862, -12.5014],
        [  6.4584, -11.0737],
        [  7.2182, -11.8334],
        [  5.6042, -10.2195],
        [  7.4610, -12.0762],
        [  7.1095, -11.7247],
        [  7.4112, -12.0264],
        [  6.8139, -11.4291],
        [  6.6640, -11.2792],
        [  6.9947, -11.6099],
        [  6.1430, -10.7582],
        [  4.7201,  -9.3353],
        [  6.3019, -10.9171],
        [  7.3729, -11.9881],
        [  6.2495, -10.8648],
        [  7.4443, -12.0595],
        [  7.5126, -12.1278],
        [  7.1956, -11.8108],
        [  6.4888, -11.1040],
        [  6.4992, -11.1144],
        [  6.7830, -11.3982],
        [  6.2618, -10.8771],
        [  7.4648, -12.0801],
        [  6.4699, -11.0852],
        [  5.7870, -10.4023],
        [  6.3543, -10.9696],
        [  6.1089, -10.7242],
        [  6.8547, -11.4699],
        [ -6.6925,   2.0773],
        [  7.4773, -12.0925],
        [  6.2711, -10.8863],
        [  6.5855, -11.2007],
        [  6.3163, -10.9316],
        [  5.8681, -10.4833],
        [  7.5820, -12.1973],
        [  7.3434, -11.9586]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 6.6895e-09],
        [3.3043e-01, 6.6957e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9816, 0.0184], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1702, 0.0501],
         [0.6141, 0.9997]],

        [[0.1785, 0.1317],
         [0.2798, 0.9198]],

        [[0.3034, 0.1313],
         [0.5689, 0.5836]],

        [[0.5086, 0.2222],
         [0.9263, 0.6156]],

        [[0.9053, 0.1818],
         [0.9108, 0.8737]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0020851996200235952
Average Adjusted Rand Index: -0.0017740222066543014
Iteration 0: Loss = -20957.10806529546
Iteration 10: Loss = -11255.644943575931
Iteration 20: Loss = -11255.644947081997
1
Iteration 30: Loss = -11254.568118696823
Iteration 40: Loss = -11253.633103209357
Iteration 50: Loss = -11252.02947655449
Iteration 60: Loss = -11249.394299621144
Iteration 70: Loss = -11249.45979624513
1
Iteration 80: Loss = -11249.648916583004
2
Iteration 90: Loss = -11249.774942943452
3
Stopping early at iteration 89 due to no improvement.
pi: tensor([[9.5442e-01, 4.5581e-02],
        [1.0000e+00, 7.7475e-13]], dtype=torch.float64)
alpha: tensor([0.9607, 0.0393])
beta: tensor([[[0.1648, 0.0562],
         [0.1287, 0.3205]],

        [[0.8044, 0.2040],
         [0.8187, 0.7974]],

        [[0.2537, 0.2480],
         [0.6088, 0.1529]],

        [[0.3457, 0.2107],
         [0.8402, 0.6454]],

        [[0.7786, 0.1977],
         [0.6470, 0.4516]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.002427096100870114
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0006136303313408698
Average Adjusted Rand Index: 3.1348228818088316e-05
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20956.46556788836
Iteration 100: Loss = -11273.740412787478
Iteration 200: Loss = -11261.74881553552
Iteration 300: Loss = -11256.774471840354
Iteration 400: Loss = -11255.177960333416
Iteration 500: Loss = -11254.19939094491
Iteration 600: Loss = -11253.608839271934
Iteration 700: Loss = -11253.1481912463
Iteration 800: Loss = -11252.690771025234
Iteration 900: Loss = -11252.289134823957
Iteration 1000: Loss = -11251.665344961704
Iteration 1100: Loss = -11249.874264598362
Iteration 1200: Loss = -11207.096037167219
Iteration 1300: Loss = -11190.777794041298
Iteration 1400: Loss = -11121.433679924128
Iteration 1500: Loss = -11084.72194878621
Iteration 1600: Loss = -11063.766226621185
Iteration 1700: Loss = -11053.185558234358
Iteration 1800: Loss = -11052.620957124245
Iteration 1900: Loss = -11050.857195285755
Iteration 2000: Loss = -11050.7455357351
Iteration 2100: Loss = -11050.629904001471
Iteration 2200: Loss = -11048.33671493471
Iteration 2300: Loss = -11046.443534270555
Iteration 2400: Loss = -11046.334974818747
Iteration 2500: Loss = -11044.397853957764
Iteration 2600: Loss = -11043.711158180276
Iteration 2700: Loss = -11043.663660385739
Iteration 2800: Loss = -11043.637903344606
Iteration 2900: Loss = -11043.620298190895
Iteration 3000: Loss = -11043.604125201957
Iteration 3100: Loss = -11043.591669835741
Iteration 3200: Loss = -11043.599042316975
1
Iteration 3300: Loss = -11043.570588838438
Iteration 3400: Loss = -11043.55910703454
Iteration 3500: Loss = -11043.542760008511
Iteration 3600: Loss = -11043.534670333613
Iteration 3700: Loss = -11043.528571547055
Iteration 3800: Loss = -11043.524116396575
Iteration 3900: Loss = -11043.519423365266
Iteration 4000: Loss = -11042.45099393589
Iteration 4100: Loss = -11042.21036897904
Iteration 4200: Loss = -11042.206911719753
Iteration 4300: Loss = -11042.203630317004
Iteration 4400: Loss = -11042.199826417562
Iteration 4500: Loss = -11042.193701039836
Iteration 4600: Loss = -11042.169347304143
Iteration 4700: Loss = -11042.02679838831
Iteration 4800: Loss = -11042.018829565104
Iteration 4900: Loss = -11042.017044001044
Iteration 5000: Loss = -11042.015580726202
Iteration 5100: Loss = -11042.01416277821
Iteration 5200: Loss = -11042.012696743745
Iteration 5300: Loss = -11042.011030755633
Iteration 5400: Loss = -11042.008583827936
Iteration 5500: Loss = -11041.981585854324
Iteration 5600: Loss = -11041.599023365208
Iteration 5700: Loss = -11041.597722839058
Iteration 5800: Loss = -11041.622832131805
1
Iteration 5900: Loss = -11041.596026938982
Iteration 6000: Loss = -11041.595226091351
Iteration 6100: Loss = -11041.594195598622
Iteration 6200: Loss = -11041.593753689143
Iteration 6300: Loss = -11041.588539325003
Iteration 6400: Loss = -11041.595875562469
1
Iteration 6500: Loss = -11041.583266106005
Iteration 6600: Loss = -11041.582960162657
Iteration 6700: Loss = -11041.582423798185
Iteration 6800: Loss = -11041.582278936385
Iteration 6900: Loss = -11041.581389943529
Iteration 7000: Loss = -11041.581987196978
1
Iteration 7100: Loss = -11041.580647294162
Iteration 7200: Loss = -11041.593161109582
1
Iteration 7300: Loss = -11041.580062177218
Iteration 7400: Loss = -11041.587160232602
1
Iteration 7500: Loss = -11041.587270255535
2
Iteration 7600: Loss = -11041.635610002513
3
Iteration 7700: Loss = -11041.590028788722
4
Iteration 7800: Loss = -11041.583532050192
5
Iteration 7900: Loss = -11041.57805005955
Iteration 8000: Loss = -11041.57918729553
1
Iteration 8100: Loss = -11041.583730500179
2
Iteration 8200: Loss = -11041.57865572203
3
Iteration 8300: Loss = -11041.579301006748
4
Iteration 8400: Loss = -11041.577146445125
Iteration 8500: Loss = -11041.579799445431
1
Iteration 8600: Loss = -11041.588123589643
2
Iteration 8700: Loss = -11041.572210003062
Iteration 8800: Loss = -11041.572111581027
Iteration 8900: Loss = -11041.576145613548
1
Iteration 9000: Loss = -11041.571692721815
Iteration 9100: Loss = -11041.57215181822
1
Iteration 9200: Loss = -11041.570838353235
Iteration 9300: Loss = -11041.571009386644
1
Iteration 9400: Loss = -11041.577242407111
2
Iteration 9500: Loss = -11041.570269081549
Iteration 9600: Loss = -11041.570309020977
1
Iteration 9700: Loss = -11041.56984218071
Iteration 9800: Loss = -11041.574241840668
1
Iteration 9900: Loss = -11041.569322096655
Iteration 10000: Loss = -11041.620982581751
1
Iteration 10100: Loss = -11041.559540911096
Iteration 10200: Loss = -11041.545215321088
Iteration 10300: Loss = -11041.544009279409
Iteration 10400: Loss = -11041.531109545811
Iteration 10500: Loss = -11041.527638657642
Iteration 10600: Loss = -11041.53293640771
1
Iteration 10700: Loss = -11041.514870357267
Iteration 10800: Loss = -11041.521316404029
1
Iteration 10900: Loss = -11041.599928064546
2
Iteration 11000: Loss = -11041.515216393853
3
Iteration 11100: Loss = -11041.617915007302
4
Iteration 11200: Loss = -11041.514509934897
Iteration 11300: Loss = -11041.521500307947
1
Iteration 11400: Loss = -11041.565362081985
2
Iteration 11500: Loss = -11041.522849997518
3
Iteration 11600: Loss = -11041.536378592436
4
Iteration 11700: Loss = -11041.514236136232
Iteration 11800: Loss = -11041.514932944023
1
Iteration 11900: Loss = -11041.514182282901
Iteration 12000: Loss = -11041.517233166285
1
Iteration 12100: Loss = -11041.50933525518
Iteration 12200: Loss = -11041.509430218613
1
Iteration 12300: Loss = -11041.512177312077
2
Iteration 12400: Loss = -11041.515190597434
3
Iteration 12500: Loss = -11041.509107255475
Iteration 12600: Loss = -11041.511847990916
1
Iteration 12700: Loss = -11041.51000132411
2
Iteration 12800: Loss = -11041.50906439648
Iteration 12900: Loss = -11041.567812731699
1
Iteration 13000: Loss = -11041.508794253321
Iteration 13100: Loss = -11041.509069342355
1
Iteration 13200: Loss = -11041.509161815065
2
Iteration 13300: Loss = -11041.508703556752
Iteration 13400: Loss = -11041.535226894113
1
Iteration 13500: Loss = -11041.52156616389
2
Iteration 13600: Loss = -11041.511442653155
3
Iteration 13700: Loss = -11041.508664991381
Iteration 13800: Loss = -11041.524609852517
1
Iteration 13900: Loss = -11041.50722417236
Iteration 14000: Loss = -11041.519742443434
1
Iteration 14100: Loss = -11041.506868255581
Iteration 14200: Loss = -11041.518656761868
1
Iteration 14300: Loss = -11041.501526056976
Iteration 14400: Loss = -11041.763427725245
1
Iteration 14500: Loss = -11041.501453726138
Iteration 14600: Loss = -11041.51344731254
1
Iteration 14700: Loss = -11041.502256756523
2
Iteration 14800: Loss = -11041.501485249903
3
Iteration 14900: Loss = -11041.571914591723
4
Iteration 15000: Loss = -11041.501421154975
Iteration 15100: Loss = -11041.515151348794
1
Iteration 15200: Loss = -11041.504440408584
2
Iteration 15300: Loss = -11041.503675957925
3
Iteration 15400: Loss = -11041.502597235167
4
Iteration 15500: Loss = -11041.501542337945
5
Iteration 15600: Loss = -11041.544581399461
6
Iteration 15700: Loss = -11041.5063599811
7
Iteration 15800: Loss = -11041.50142908916
8
Iteration 15900: Loss = -11041.505080002044
9
Iteration 16000: Loss = -11041.501392616763
Iteration 16100: Loss = -11041.50788102628
1
Iteration 16200: Loss = -11041.504540730444
2
Iteration 16300: Loss = -11041.936415401646
3
Iteration 16400: Loss = -11041.503978175584
4
Iteration 16500: Loss = -11041.502796058656
5
Iteration 16600: Loss = -11041.501533764134
6
Iteration 16700: Loss = -11041.501466843252
7
Iteration 16800: Loss = -11041.515789527346
8
Iteration 16900: Loss = -11041.50131819369
Iteration 17000: Loss = -11041.511905961068
1
Iteration 17100: Loss = -11041.501320135361
2
Iteration 17200: Loss = -11041.50318125157
3
Iteration 17300: Loss = -11041.49772561171
Iteration 17400: Loss = -11041.50110512237
1
Iteration 17500: Loss = -11041.496904109215
Iteration 17600: Loss = -11041.49718850316
1
Iteration 17700: Loss = -11041.672883332038
2
Iteration 17800: Loss = -11041.502731784827
3
Iteration 17900: Loss = -11041.49785685571
4
Iteration 18000: Loss = -11041.486293616517
Iteration 18100: Loss = -11041.485601445498
Iteration 18200: Loss = -11041.485769946561
1
Iteration 18300: Loss = -11041.485608006209
2
Iteration 18400: Loss = -11041.485575168703
Iteration 18500: Loss = -11041.555454572599
1
Iteration 18600: Loss = -11041.485470387577
Iteration 18700: Loss = -11041.490744282963
1
Iteration 18800: Loss = -11041.485405959926
Iteration 18900: Loss = -11041.487148808374
1
Iteration 19000: Loss = -11041.493861795603
2
Iteration 19100: Loss = -11041.484861486206
Iteration 19200: Loss = -11041.484611622505
Iteration 19300: Loss = -11041.48326136739
Iteration 19400: Loss = -11041.487490597416
1
Iteration 19500: Loss = -11041.4832316847
Iteration 19600: Loss = -11041.628812886134
1
Iteration 19700: Loss = -11041.483139793578
Iteration 19800: Loss = -11041.511149398455
1
Iteration 19900: Loss = -11041.483001078854
tensor([[  3.0689,  -7.4647],
        [  7.9706,  -9.3593],
        [  7.2708, -11.2216],
        [ -1.8246,   0.1840],
        [  9.2212, -10.6121],
        [  6.7877,  -8.3964],
        [  8.1943, -11.9064],
        [  2.2426,  -6.7634],
        [  5.4030,  -7.3979],
        [  4.8159,  -7.0008],
        [  4.4087,  -6.1245],
        [  7.5924,  -9.0540],
        [  2.8312,  -6.1205],
        [  7.7041,  -9.1080],
        [  6.6881,  -9.4827],
        [  4.2732,  -5.7785],
        [  7.9469,  -9.7981],
        [  8.7791, -10.8157],
        [  4.7796,  -6.7429],
        [  1.7668,  -3.5621],
        [  2.4956,  -4.1467],
        [  4.3419,  -6.0263],
        [  3.7257,  -5.5393],
        [  7.8458,  -9.3025],
        [  3.6966,  -5.2033],
        [  5.0670,  -6.4534],
        [  4.1942,  -6.3767],
        [  8.6592, -10.8104],
        [ -0.1141,  -1.2722],
        [  4.5355,  -5.9677],
        [  3.5323,  -5.7284],
        [  4.4358,  -7.1650],
        [  0.7565,  -5.3717],
        [  2.5083,  -3.8967],
        [  2.4275,  -3.9637],
        [  1.3867,  -2.7779],
        [  3.2776,  -4.6696],
        [  4.6384,  -8.1443],
        [  7.0401,  -8.4672],
        [  1.2884,  -2.7806],
        [  8.3969, -11.4557],
        [  3.5018,  -4.9379],
        [  1.9338,  -3.5346],
        [  4.4533,  -8.6126],
        [  3.2962,  -4.6938],
        [  8.4191, -10.7053],
        [  7.6608,  -9.5617],
        [  4.5605,  -8.1749],
        [  5.1993,  -7.6021],
        [  0.9887,  -2.8116],
        [  7.4029,  -9.0763],
        [  4.0267,  -6.3438],
        [  5.7293,  -7.1762],
        [  3.1524,  -4.5528],
        [  7.4001,  -9.3469],
        [ -0.0849,  -2.9663],
        [  4.4990,  -6.0364],
        [  3.5776,  -6.3138],
        [  1.8210,  -3.5099],
        [  1.0763,  -4.0098],
        [  4.6523,  -6.3643],
        [  8.3612,  -9.7477],
        [  5.0911,  -7.2260],
        [  9.0193, -10.6344],
        [  8.5156, -10.0179],
        [  5.0597,  -6.4734],
        [  3.5331,  -5.6709],
        [  4.3191,  -5.8289],
        [  4.3361,  -7.1772],
        [  2.7887,  -5.0960],
        [  6.8076,  -8.8449],
        [  8.1792,  -9.9008],
        [  8.6497, -10.0578],
        [  1.1497,  -3.2122],
        [  2.1355,  -3.6208],
        [  1.8747,  -3.2924],
        [  3.8566,  -5.3990],
        [  4.9775,  -8.0983],
        [  3.8450,  -5.2348],
        [  6.5126,  -7.9003],
        [  9.0737, -10.4764],
        [  2.3630,  -5.4444],
        [  5.5913,  -8.8470],
        [  7.7046, -10.4047],
        [  8.2881,  -9.7413],
        [  3.5775,  -5.7096],
        [  8.9193, -10.6687],
        [  6.4944,  -7.9053],
        [  1.1422,  -4.2544],
        [  5.1746,  -6.6761],
        [  1.6693,  -3.3920],
        [  5.6888,  -7.1622],
        [ -4.2906,   2.4902],
        [  7.4768,  -9.1260],
        [  3.3863,  -4.9178],
        [  5.7210,  -9.9619],
        [  3.6535,  -5.2908],
        [  1.3086,  -2.8730],
        [  6.9622,  -8.3640],
        [  5.2123,  -6.6429]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6797, 0.3203],
        [0.2972, 0.7028]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9766, 0.0234], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1861, 0.0599],
         [0.1287, 0.3066]],

        [[0.8044, 0.0992],
         [0.8187, 0.7974]],

        [[0.2537, 0.0988],
         [0.6088, 0.1529]],

        [[0.3457, 0.1016],
         [0.8402, 0.6454]],

        [[0.7786, 0.0937],
         [0.6470, 0.4516]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 1
tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824283882000855
time is 2
tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824165642894751
time is 3
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 4
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
Global Adjusted Rand Index: 0.5645673055368091
Average Adjusted Rand Index: 0.7056462326378693
Iteration 0: Loss = -25226.593904756453
Iteration 10: Loss = -11255.644943566353
Iteration 20: Loss = -11255.644943566975
1
Iteration 30: Loss = -11255.644471465785
Iteration 40: Loss = -11253.921473927838
Iteration 50: Loss = -11253.445476094219
Iteration 60: Loss = -11253.260479903782
Iteration 70: Loss = -11250.085509963961
Iteration 80: Loss = -11249.36169458724
Iteration 90: Loss = -11249.556902147055
1
Iteration 100: Loss = -11249.716006847932
2
Iteration 110: Loss = -11249.816868116926
3
Stopping early at iteration 109 due to no improvement.
pi: tensor([[2.9047e-27, 1.0000e+00],
        [4.6592e-02, 9.5341e-01]], dtype=torch.float64)
alpha: tensor([0.0400, 0.9600])
beta: tensor([[[0.3196, 0.0564],
         [0.9624, 0.1648]],

        [[0.3881, 0.2038],
         [0.6026, 0.1339]],

        [[0.6882, 0.2470],
         [0.4644, 0.1243]],

        [[0.9519, 0.2105],
         [0.7617, 0.4921]],

        [[0.0438, 0.1977],
         [0.6696, 0.2949]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.00485601903559462
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.001192365071694833
Average Adjusted Rand Index: 0.0005171328157629895
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25228.033588593506
Iteration 100: Loss = -11262.508335103425
Iteration 200: Loss = -11257.89545118114
Iteration 300: Loss = -11257.006936832931
Iteration 400: Loss = -11256.294347809979
Iteration 500: Loss = -11255.16354426624
Iteration 600: Loss = -11254.677402592431
Iteration 700: Loss = -11254.368711260626
Iteration 800: Loss = -11254.129787919774
Iteration 900: Loss = -11253.907738229746
Iteration 1000: Loss = -11253.668992033517
Iteration 1100: Loss = -11253.391889802191
Iteration 1200: Loss = -11253.082541094112
Iteration 1300: Loss = -11252.767772979127
Iteration 1400: Loss = -11252.468625856636
Iteration 1500: Loss = -11252.01418865377
Iteration 1600: Loss = -11230.166563403101
Iteration 1700: Loss = -11204.204112717141
Iteration 1800: Loss = -11182.899353955836
Iteration 1900: Loss = -11104.484972046383
Iteration 2000: Loss = -11093.14095268961
Iteration 2100: Loss = -11075.827666538069
Iteration 2200: Loss = -11075.405842309552
Iteration 2300: Loss = -11071.138571638083
Iteration 2400: Loss = -11069.617426519895
Iteration 2500: Loss = -11069.400864485171
Iteration 2600: Loss = -11064.896652948804
Iteration 2700: Loss = -11057.614577358961
Iteration 2800: Loss = -11056.996323458858
Iteration 2900: Loss = -11055.187332238862
Iteration 3000: Loss = -11055.09743133157
Iteration 3100: Loss = -11055.068050655367
Iteration 3200: Loss = -11055.044056591552
Iteration 3300: Loss = -11055.016496470307
Iteration 3400: Loss = -11054.701206873235
Iteration 3500: Loss = -11054.42490001128
Iteration 3600: Loss = -11054.411693450631
Iteration 3700: Loss = -11054.401697555268
Iteration 3800: Loss = -11054.39308701802
Iteration 3900: Loss = -11054.38557918502
Iteration 4000: Loss = -11054.379461499775
Iteration 4100: Loss = -11054.372501021797
Iteration 4200: Loss = -11054.36703518447
Iteration 4300: Loss = -11054.40876108575
1
Iteration 4400: Loss = -11054.356644712285
Iteration 4500: Loss = -11054.349032036742
Iteration 4600: Loss = -11054.304530937723
Iteration 4700: Loss = -11054.185664097222
Iteration 4800: Loss = -11054.179876381295
Iteration 4900: Loss = -11054.17617743137
Iteration 5000: Loss = -11054.173439033202
Iteration 5100: Loss = -11054.170574757196
Iteration 5200: Loss = -11054.16862699902
Iteration 5300: Loss = -11054.166842306102
Iteration 5400: Loss = -11054.165173539113
Iteration 5500: Loss = -11054.163185086569
Iteration 5600: Loss = -11054.159256463186
Iteration 5700: Loss = -11053.12404316739
Iteration 5800: Loss = -11053.083199414425
Iteration 5900: Loss = -11053.08160537639
Iteration 6000: Loss = -11053.080350575514
Iteration 6100: Loss = -11053.07928707812
Iteration 6200: Loss = -11053.07840643785
Iteration 6300: Loss = -11053.077662282789
Iteration 6400: Loss = -11053.076800623698
Iteration 6500: Loss = -11053.086087684631
1
Iteration 6600: Loss = -11053.078921285352
2
Iteration 6700: Loss = -11053.075915431255
Iteration 6800: Loss = -11053.07359797104
Iteration 6900: Loss = -11053.073199528513
Iteration 7000: Loss = -11053.083913985902
1
Iteration 7100: Loss = -11053.071966190468
Iteration 7200: Loss = -11053.100564078928
1
Iteration 7300: Loss = -11053.071066116008
Iteration 7400: Loss = -11053.071005451873
Iteration 7500: Loss = -11053.132837203178
1
Iteration 7600: Loss = -11053.069184991366
Iteration 7700: Loss = -11053.067446307523
Iteration 7800: Loss = -11049.229780645155
Iteration 7900: Loss = -11049.207300962315
Iteration 8000: Loss = -11049.206384287734
Iteration 8100: Loss = -11049.20715277239
1
Iteration 8200: Loss = -11049.198962930714
Iteration 8300: Loss = -11049.198058468237
Iteration 8400: Loss = -11049.199371198118
1
Iteration 8500: Loss = -11049.201625909602
2
Iteration 8600: Loss = -11049.19656291514
Iteration 8700: Loss = -11049.239202059804
1
Iteration 8800: Loss = -11049.19474507954
Iteration 8900: Loss = -11049.199119299285
1
Iteration 9000: Loss = -11049.194083201948
Iteration 9100: Loss = -11049.228238975897
1
Iteration 9200: Loss = -11049.193103869617
Iteration 9300: Loss = -11049.19248843102
Iteration 9400: Loss = -11049.035274464315
Iteration 9500: Loss = -11049.021052387212
Iteration 9600: Loss = -11049.018463442579
Iteration 9700: Loss = -11049.01825272286
Iteration 9800: Loss = -11049.238642710145
1
Iteration 9900: Loss = -11049.017992302972
Iteration 10000: Loss = -11049.025257701122
1
Iteration 10100: Loss = -11049.017739437339
Iteration 10200: Loss = -11049.017669114355
Iteration 10300: Loss = -11049.01811960034
1
Iteration 10400: Loss = -11049.025570410457
2
Iteration 10500: Loss = -11049.023117873438
3
Iteration 10600: Loss = -11049.016680985485
Iteration 10700: Loss = -11049.015469736602
Iteration 10800: Loss = -11049.015633502062
1
Iteration 10900: Loss = -11049.016827984733
2
Iteration 11000: Loss = -11049.182605028105
3
Iteration 11100: Loss = -11049.013205215744
Iteration 11200: Loss = -11049.013786152638
1
Iteration 11300: Loss = -11049.021592174275
2
Iteration 11400: Loss = -11049.079422080575
3
Iteration 11500: Loss = -11049.008866590637
Iteration 11600: Loss = -11049.014395400056
1
Iteration 11700: Loss = -11049.008339800164
Iteration 11800: Loss = -11048.990864608317
Iteration 11900: Loss = -11048.982582548466
Iteration 12000: Loss = -11048.982496734936
Iteration 12100: Loss = -11048.984371709043
1
Iteration 12200: Loss = -11048.982272765374
Iteration 12300: Loss = -11048.982193625727
Iteration 12400: Loss = -11048.98695619789
1
Iteration 12500: Loss = -11048.981987577019
Iteration 12600: Loss = -11048.982502558738
1
Iteration 12700: Loss = -11048.981773118387
Iteration 12800: Loss = -11049.004619836462
1
Iteration 12900: Loss = -11048.98120086194
Iteration 13000: Loss = -11048.982377221817
1
Iteration 13100: Loss = -11048.973458494498
Iteration 13200: Loss = -11049.027059060687
1
Iteration 13300: Loss = -11048.98140171958
2
Iteration 13400: Loss = -11048.970017831733
Iteration 13500: Loss = -11048.970035810598
1
Iteration 13600: Loss = -11048.974496807321
2
Iteration 13700: Loss = -11048.902323253536
Iteration 13800: Loss = -11048.889362158205
Iteration 13900: Loss = -11048.8765248408
Iteration 14000: Loss = -11048.875339755825
Iteration 14100: Loss = -11048.875394452178
1
Iteration 14200: Loss = -11048.875368990426
2
Iteration 14300: Loss = -11048.875429854961
3
Iteration 14400: Loss = -11048.875453990582
4
Iteration 14500: Loss = -11048.89423892216
5
Iteration 14600: Loss = -11048.875746225467
6
Iteration 14700: Loss = -11048.875503049492
7
Iteration 14800: Loss = -11048.878735457076
8
Iteration 14900: Loss = -11048.87523950153
Iteration 15000: Loss = -11048.910971361698
1
Iteration 15100: Loss = -11048.877194693725
2
Iteration 15200: Loss = -11048.870998936432
Iteration 15300: Loss = -11048.871323346015
1
Iteration 15400: Loss = -11048.88012474855
2
Iteration 15500: Loss = -11041.508231598138
Iteration 15600: Loss = -11041.651591669433
1
Iteration 15700: Loss = -11041.507448535976
Iteration 15800: Loss = -11041.74278279021
1
Iteration 15900: Loss = -11041.502693017586
Iteration 16000: Loss = -11041.504035251068
1
Iteration 16100: Loss = -11041.502731379325
2
Iteration 16200: Loss = -11041.50339512838
3
Iteration 16300: Loss = -11041.50111187786
Iteration 16400: Loss = -11041.500698827842
Iteration 16500: Loss = -11041.501016217593
1
Iteration 16600: Loss = -11041.542323213811
2
Iteration 16700: Loss = -11041.500532954455
Iteration 16800: Loss = -11041.50804563705
1
Iteration 16900: Loss = -11041.518434422584
2
Iteration 17000: Loss = -11041.50256490758
3
Iteration 17100: Loss = -11041.500417457999
Iteration 17200: Loss = -11041.530612134655
1
Iteration 17300: Loss = -11041.500387890113
Iteration 17400: Loss = -11041.504256404913
1
Iteration 17500: Loss = -11041.511999725679
2
Iteration 17600: Loss = -11041.54183708508
3
Iteration 17700: Loss = -11041.500593280001
4
Iteration 17800: Loss = -11041.499846463976
Iteration 17900: Loss = -11041.4997598763
Iteration 18000: Loss = -11041.486166110208
Iteration 18100: Loss = -11041.546123963864
1
Iteration 18200: Loss = -11041.48807083999
2
Iteration 18300: Loss = -11041.4897104248
3
Iteration 18400: Loss = -11041.486050888183
Iteration 18500: Loss = -11041.486047567003
Iteration 18600: Loss = -11041.491829713656
1
Iteration 18700: Loss = -11041.486021094355
Iteration 18800: Loss = -11041.485900234535
Iteration 18900: Loss = -11041.485559520497
Iteration 19000: Loss = -11041.48563020947
1
Iteration 19100: Loss = -11041.491951150654
2
Iteration 19200: Loss = -11041.4851402963
Iteration 19300: Loss = -11041.487299428243
1
Iteration 19400: Loss = -11041.491138396115
2
Iteration 19500: Loss = -11041.485042429971
Iteration 19600: Loss = -11041.510779044766
1
Iteration 19700: Loss = -11041.485218644339
2
Iteration 19800: Loss = -11041.488433411372
3
Iteration 19900: Loss = -11041.485025734231
tensor([[ -5.9608,   4.5605],
        [ -9.6819,   7.6406],
        [ -6.1792,   4.5414],
        [ -1.2365,  -3.2562],
        [-10.7108,   8.7616],
        [ -8.3194,   6.9170],
        [ -9.4357,   7.9813],
        [ -5.3831,   3.6104],
        [ -7.1386,   5.6941],
        [ -7.8025,   4.0044],
        [ -7.5722,   2.9570],
        [ -9.1227,   7.5842],
        [ -5.1635,   3.7750],
        [ -8.9573,   7.3705],
        [ -8.5052,   7.0820],
        [-10.1926,   8.5833],
        [ -8.9578,   7.5646],
        [ -7.0856,   5.6595],
        [ -7.3157,   4.2043],
        [ -3.6388,   1.6761],
        [ -5.4593,   1.1707],
        [ -6.1249,   4.2399],
        [ -5.3287,   3.9321],
        [ -5.9188,   4.2484],
        [ -5.5936,   3.3055],
        [ -6.4532,   5.0649],
        [ -5.9861,   4.5821],
        [ -9.6490,   7.9132],
        [ -1.5045,  -0.3606],
        [ -7.4570,   3.0415],
        [ -5.5240,   3.7319],
        [ -6.5757,   5.0215],
        [ -4.4425,   1.6722],
        [ -3.8927,   2.4993],
        [ -4.1769,   2.1988],
        [ -3.1221,   1.0295],
        [ -4.8157,   3.1243],
        [ -7.1884,   5.5910],
        [ -8.4664,   7.0314],
        [ -3.0739,   0.9817],
        [ -8.5529,   6.8177],
        [ -4.9350,   3.5007],
        [ -3.6208,   1.8341],
        [ -7.2930,   5.7769],
        [ -4.8186,   3.1614],
        [-10.8038,   7.6950],
        [ -6.4170,   3.8254],
        [ -7.6908,   5.0416],
        [ -9.9254,   7.3629],
        [ -2.6097,   1.1753],
        [-10.0333,   7.7428],
        [ -5.9912,   4.3739],
        [ -7.4806,   5.4200],
        [ -5.1588,   2.5383],
        [ -9.1292,   7.1782],
        [ -2.1662,   0.7028],
        [ -6.0582,   4.4735],
        [ -6.3532,   3.5287],
        [ -3.5250,   1.7922],
        [ -3.2433,   1.8271],
        [ -6.3479,   4.6641],
        [ -8.1218,   5.9690],
        [ -7.2672,   5.1534],
        [ -7.0947,   5.7077],
        [-10.5031,   9.1136],
        [ -6.7823,   4.7475],
        [ -5.7769,   3.4248],
        [ -5.7652,   4.3789],
        [ -6.6158,   4.8956],
        [ -4.7572,   3.1091],
        [ -6.4843,   5.0899],
        [-10.1427,   8.5251],
        [ -9.5256,   8.1383],
        [ -3.3661,   0.9795],
        [ -3.6096,   2.1327],
        [ -3.5243,   1.6273],
        [ -6.8084,   2.4389],
        [ -7.6617,   5.4118],
        [ -5.4570,   3.6177],
        [ -7.9125,   6.4959],
        [ -8.2100,   5.6936],
        [ -4.8347,   2.9528],
        [ -7.9128,   6.4921],
        [ -7.1788,   5.6109],
        [ -5.6426,   4.2525],
        [ -5.8238,   3.4532],
        [ -7.4781,   5.3099],
        [ -7.9463,   6.4770],
        [ -4.4506,   0.9324],
        [ -6.6177,   5.2277],
        [ -3.6136,   1.4319],
        [ -7.9224,   4.9185],
        [  2.2228,  -4.5643],
        [ -7.2740,   5.4314],
        [ -6.3146,   1.9842],
        [ -8.7719,   6.7801],
        [ -5.5677,   3.3640],
        [ -2.8119,   1.3567],
        [ -8.7654,   6.5469],
        [ -6.7198,   5.1270]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7008, 0.2992],
        [0.3195, 0.6805]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0231, 0.9769], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3074, 0.0596],
         [0.9624, 0.1852]],

        [[0.3881, 0.0993],
         [0.6026, 0.1339]],

        [[0.6882, 0.0990],
         [0.4644, 0.1243]],

        [[0.9519, 0.1019],
         [0.7617, 0.4921]],

        [[0.0438, 0.0939],
         [0.6696, 0.2949]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 1
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824283882000855
time is 2
tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824165642894751
time is 3
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 4
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
Global Adjusted Rand Index: 0.5645673055368091
Average Adjusted Rand Index: 0.7056462326378693
Iteration 0: Loss = -21454.59817847054
Iteration 10: Loss = -11253.813424527289
Iteration 20: Loss = -11249.215330527177
Iteration 30: Loss = -11249.33276694647
1
Iteration 40: Loss = -11249.493504865808
2
Iteration 50: Loss = -11249.623581165419
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.1105, 0.8895],
        [0.0482, 0.9518]], dtype=torch.float64)
alpha: tensor([0.0451, 0.9549])
beta: tensor([[[0.3153, 0.0576],
         [0.0854, 0.1643]],

        [[0.6722, 0.2024],
         [0.1088, 0.2675]],

        [[0.7372, 0.2443],
         [0.9730, 0.1559]],

        [[0.4372, 0.2090],
         [0.4054, 0.3219]],

        [[0.4887, 0.1996],
         [0.1366, 0.1276]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.00485601903559462
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.001192365071694833
Average Adjusted Rand Index: 0.0005171328157629895
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21454.113833731008
Iteration 100: Loss = -11266.668309616698
Iteration 200: Loss = -11257.110293229398
Iteration 300: Loss = -11256.041711034559
Iteration 400: Loss = -11255.642457853191
Iteration 500: Loss = -11255.454185632736
Iteration 600: Loss = -11255.253349696979
Iteration 700: Loss = -11254.469698471416
Iteration 800: Loss = -11253.673787549977
Iteration 900: Loss = -11253.266719768468
Iteration 1000: Loss = -11252.901392190854
Iteration 1100: Loss = -11252.426627061841
Iteration 1200: Loss = -11251.717331390071
Iteration 1300: Loss = -11250.041921196022
Iteration 1400: Loss = -11185.219938197737
Iteration 1500: Loss = -11088.37432577298
Iteration 1600: Loss = -11064.698262651786
Iteration 1700: Loss = -11063.884512145336
Iteration 1800: Loss = -11059.886261711323
Iteration 1900: Loss = -11059.109012229625
Iteration 2000: Loss = -11058.917790879064
Iteration 2100: Loss = -11058.735408360437
Iteration 2200: Loss = -11056.406064185605
Iteration 2300: Loss = -11056.323563132199
Iteration 2400: Loss = -11056.30270038573
Iteration 2500: Loss = -11056.286397984908
Iteration 2600: Loss = -11056.274119188722
Iteration 2700: Loss = -11056.265129158211
Iteration 2800: Loss = -11056.25773917854
Iteration 2900: Loss = -11056.251478795295
Iteration 3000: Loss = -11056.246081725623
Iteration 3100: Loss = -11056.241023155235
Iteration 3200: Loss = -11056.13720297662
Iteration 3300: Loss = -11055.25357350988
Iteration 3400: Loss = -11055.24743705352
Iteration 3500: Loss = -11055.239917623476
Iteration 3600: Loss = -11055.219059405332
Iteration 3700: Loss = -11055.117921791263
Iteration 3800: Loss = -11055.113778533854
Iteration 3900: Loss = -11055.111189206886
Iteration 4000: Loss = -11055.109224954384
Iteration 4100: Loss = -11055.108201979012
Iteration 4200: Loss = -11055.106135470967
Iteration 4300: Loss = -11055.104672901021
Iteration 4400: Loss = -11055.104069342264
Iteration 4500: Loss = -11055.101594934125
Iteration 4600: Loss = -11055.099783367335
Iteration 4700: Loss = -11055.0982203063
Iteration 4800: Loss = -11055.097299154875
Iteration 4900: Loss = -11055.096192677462
Iteration 5000: Loss = -11055.095188724994
Iteration 5100: Loss = -11055.093696098427
Iteration 5200: Loss = -11050.283616494378
Iteration 5300: Loss = -11050.273312456922
Iteration 5400: Loss = -11050.50510653602
1
Iteration 5500: Loss = -11050.272096492503
Iteration 5600: Loss = -11050.273449184562
1
Iteration 5700: Loss = -11050.282985626685
2
Iteration 5800: Loss = -11050.272265022088
3
Iteration 5900: Loss = -11050.32200051013
4
Iteration 6000: Loss = -11050.274735218996
5
Iteration 6100: Loss = -11049.163555913095
Iteration 6200: Loss = -11049.100280588162
Iteration 6300: Loss = -11049.09987012467
Iteration 6400: Loss = -11049.099731396182
Iteration 6500: Loss = -11049.095550075448
Iteration 6600: Loss = -11049.093975843069
Iteration 6700: Loss = -11049.094090369781
1
Iteration 6800: Loss = -11049.098110181363
2
Iteration 6900: Loss = -11049.093087728317
Iteration 7000: Loss = -11049.093026751978
Iteration 7100: Loss = -11049.093066432806
1
Iteration 7200: Loss = -11049.09884271781
2
Iteration 7300: Loss = -11049.104563439245
3
Iteration 7400: Loss = -11049.092185416459
Iteration 7500: Loss = -11049.092368511041
1
Iteration 7600: Loss = -11049.091903411228
Iteration 7700: Loss = -11049.091887692428
Iteration 7800: Loss = -11049.09169431273
Iteration 7900: Loss = -11049.093308134616
1
Iteration 8000: Loss = -11049.091032635044
Iteration 8100: Loss = -11049.092219180753
1
Iteration 8200: Loss = -11049.090661131166
Iteration 8300: Loss = -11049.383069464322
1
Iteration 8400: Loss = -11049.090538265871
Iteration 8500: Loss = -11049.090449391568
Iteration 8600: Loss = -11049.113480207749
1
Iteration 8700: Loss = -11049.047235168344
Iteration 8800: Loss = -11049.046954097426
Iteration 8900: Loss = -11049.097736956439
1
Iteration 9000: Loss = -11049.04684734761
Iteration 9100: Loss = -11049.046776563668
Iteration 9200: Loss = -11049.064708398279
1
Iteration 9300: Loss = -11049.050848514556
2
Iteration 9400: Loss = -11049.046491873294
Iteration 9500: Loss = -11049.04666272292
1
Iteration 9600: Loss = -11049.046413729382
Iteration 9700: Loss = -11049.046441171377
1
Iteration 9800: Loss = -11049.046302733477
Iteration 9900: Loss = -11049.047228551783
1
Iteration 10000: Loss = -11049.0461627849
Iteration 10100: Loss = -11049.049230482951
1
Iteration 10200: Loss = -11049.04610354514
Iteration 10300: Loss = -11049.046580465096
1
Iteration 10400: Loss = -11049.056919696373
2
Iteration 10500: Loss = -11049.044752336671
Iteration 10600: Loss = -11049.04472912973
Iteration 10700: Loss = -11049.03529598079
Iteration 10800: Loss = -11049.033954605819
Iteration 10900: Loss = -11049.032667129935
Iteration 11000: Loss = -11049.032442884314
Iteration 11100: Loss = -11048.887399288047
Iteration 11200: Loss = -11048.851712783255
Iteration 11300: Loss = -11048.88395674419
1
Iteration 11400: Loss = -11048.851609135794
Iteration 11500: Loss = -11048.851569300574
Iteration 11600: Loss = -11048.852925113544
1
Iteration 11700: Loss = -11048.851512772211
Iteration 11800: Loss = -11048.851508672471
Iteration 11900: Loss = -11048.8512098312
Iteration 12000: Loss = -11048.85113518601
Iteration 12100: Loss = -11048.968069274499
1
Iteration 12200: Loss = -11048.851110046797
Iteration 12300: Loss = -11048.851099352943
Iteration 12400: Loss = -11048.851191559697
1
Iteration 12500: Loss = -11048.851047056867
Iteration 12600: Loss = -11048.851516379762
1
Iteration 12700: Loss = -11048.923503100325
2
Iteration 12800: Loss = -11048.851717765305
3
Iteration 12900: Loss = -11048.89172805483
4
Iteration 13000: Loss = -11048.850367106066
Iteration 13100: Loss = -11048.852579288083
1
Iteration 13200: Loss = -11048.850354164499
Iteration 13300: Loss = -11048.850469491454
1
Iteration 13400: Loss = -11048.86354208494
2
Iteration 13500: Loss = -11048.852309279739
3
Iteration 13600: Loss = -11048.850297812538
Iteration 13700: Loss = -11048.84779346469
Iteration 13800: Loss = -11048.843700574249
Iteration 13900: Loss = -11048.844921185237
1
Iteration 14000: Loss = -11048.843722417929
2
Iteration 14100: Loss = -11048.957285668663
3
Iteration 14200: Loss = -11048.843722240097
4
Iteration 14300: Loss = -11048.843691316199
Iteration 14400: Loss = -11048.850282125379
1
Iteration 14500: Loss = -11048.843657786389
Iteration 14600: Loss = -11048.852785236897
1
Iteration 14700: Loss = -11048.843567664988
Iteration 14800: Loss = -11048.85124221071
1
Iteration 14900: Loss = -11048.843590596376
2
Iteration 15000: Loss = -11048.843679815109
3
Iteration 15100: Loss = -11048.85417638659
4
Iteration 15200: Loss = -11048.843530076703
Iteration 15300: Loss = -11048.844384369764
1
Iteration 15400: Loss = -11048.84434418074
2
Iteration 15500: Loss = -11048.84337641525
Iteration 15600: Loss = -11048.843919354842
1
Iteration 15700: Loss = -11048.84554346971
2
Iteration 15800: Loss = -11048.975455260115
3
Iteration 15900: Loss = -11048.844466090743
4
Iteration 16000: Loss = -11048.843056455356
Iteration 16100: Loss = -11048.843428420987
1
Iteration 16200: Loss = -11048.853034668005
2
Iteration 16300: Loss = -11048.843034822912
Iteration 16400: Loss = -11048.846307846425
1
Iteration 16500: Loss = -11048.842721293333
Iteration 16600: Loss = -11048.86748333967
1
Iteration 16700: Loss = -11048.842589327403
Iteration 16800: Loss = -11048.842629158604
1
Iteration 16900: Loss = -11048.842646876885
2
Iteration 17000: Loss = -11048.842599349458
3
Iteration 17100: Loss = -11048.852469436142
4
Iteration 17200: Loss = -11048.842623911389
5
Iteration 17300: Loss = -11048.845377582686
6
Iteration 17400: Loss = -11048.843041222619
7
Iteration 17500: Loss = -11048.842489056487
Iteration 17600: Loss = -11048.968121074078
1
Iteration 17700: Loss = -11048.845797457965
2
Iteration 17800: Loss = -11048.842709911998
3
Iteration 17900: Loss = -11048.843224390543
4
Iteration 18000: Loss = -11048.848905438883
5
Iteration 18100: Loss = -11048.89050756695
6
Iteration 18200: Loss = -11048.842475607658
Iteration 18300: Loss = -11049.075610903787
1
Iteration 18400: Loss = -11048.856413611173
2
Iteration 18500: Loss = -11048.864222820415
3
Iteration 18600: Loss = -11048.842915698731
4
Iteration 18700: Loss = -11048.844571047326
5
Iteration 18800: Loss = -11048.842481769227
6
Iteration 18900: Loss = -11048.843452402301
7
Iteration 19000: Loss = -11048.842460757933
Iteration 19100: Loss = -11048.84322001045
1
Iteration 19200: Loss = -11048.842430121882
Iteration 19300: Loss = -11048.842562447862
1
Iteration 19400: Loss = -11048.878100501082
2
Iteration 19500: Loss = -11048.842414790448
Iteration 19600: Loss = -11048.893967722955
1
Iteration 19700: Loss = -11048.851218679822
2
Iteration 19800: Loss = -11048.842585981998
3
Iteration 19900: Loss = -11048.842601731121
4
tensor([[ -6.2763,   4.3517],
        [ -9.8560,   7.5198],
        [-11.4986,   8.2660],
        [ -1.3072,  -3.3081],
        [-11.9493,   9.6890],
        [ -8.4187,   6.9665],
        [-11.1257,   9.1781],
        [ -5.2736,   3.8871],
        [ -7.3513,   5.6413],
        [ -6.7052,   5.2183],
        [-10.4680,   8.7987],
        [ -9.6060,   7.7185],
        [ -9.7992,   8.4129],
        [ -9.1489,   7.6806],
        [ -9.0934,   7.2477],
        [-10.4813,   8.9757],
        [-10.5091,   7.6506],
        [-10.0917,   8.4123],
        [ -6.7325,   4.9724],
        [ -3.6546,   1.7405],
        [ -4.0517,   2.6595],
        [ -6.1030,   4.3796],
        [ -5.9876,   3.3687],
        [-11.0834,   8.8214],
        [ -5.2248,   3.7791],
        [-10.1420,   8.7346],
        [ -6.0316,   4.6367],
        [-10.1844,   8.7610],
        [ -1.7483,  -0.4683],
        [ -6.3267,   4.2792],
        [ -5.4565,   3.8943],
        [ -7.0669,   4.6674],
        [ -3.8480,   2.4224],
        [ -4.4433,   2.1025],
        [ -3.9632,   2.5670],
        [ -3.1400,   1.0725],
        [ -5.1534,   2.8803],
        [-10.9332,   7.5353],
        [ -8.6345,   7.0294],
        [ -2.7917,   1.3275],
        [-10.4525,   9.0083],
        [ -5.1254,   3.4837],
        [ -3.4643,   2.0576],
        [ -8.1043,   5.0815],
        [ -5.7868,   2.2862],
        [ -9.7776,   7.4960],
        [ -6.5578,   3.8632],
        [ -7.2038,   5.7274],
        [-10.8272,   9.1750],
        [ -2.6798,   1.2456],
        [-10.4966,   7.3270],
        [ -5.9693,   4.5058],
        [ -7.2483,   5.7757],
        [ -5.0425,   2.7430],
        [ -6.6192,   5.0785],
        [ -2.7264,   0.1940],
        [ -6.1049,   4.5302],
        [ -6.3415,   3.6379],
        [ -4.5659,   0.8314],
        [ -3.4507,   1.7664],
        [ -6.3321,   4.8671],
        [ -8.0347,   6.2497],
        [ -8.4452,   4.0133],
        [-11.7463,   9.3706],
        [-11.0955,   9.7048],
        [ -6.8031,   4.9223],
        [ -5.7692,   3.5321],
        [ -5.8251,   4.4376],
        [ -6.9644,   4.7571],
        [ -4.8500,   3.1146],
        [ -6.5961,   5.1670],
        [-10.4209,   8.8604],
        [-10.8881,   8.5547],
        [ -2.9738,   1.5875],
        [ -3.6481,   2.2529],
        [ -3.7683,   1.4562],
        [ -5.3681,   3.9799],
        [ -7.3687,   5.8323],
        [ -6.1694,   3.0163],
        [ -7.9728,   6.5519],
        [-10.4600,   9.0659],
        [ -4.7493,   3.2087],
        [ -7.9843,   6.5452],
        [ -9.8273,   8.3322],
        [ -6.5994,   3.4841],
        [ -5.5984,   3.7694],
        [-10.4159,   9.0288],
        [ -8.2172,   6.3055],
        [ -3.4866,   1.9721],
        [ -6.6972,   5.2593],
        [ -3.3338,   1.8652],
        [ -7.2742,   5.7737],
        [  2.6364,  -4.1624],
        [-12.0038,   7.7281],
        [ -4.9315,   3.5310],
        [ -8.6089,   7.2149],
        [ -5.5645,   3.5454],
        [ -2.8178,   1.4093],
        [-11.2528,   8.8927],
        [ -7.5158,   4.4397]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6904, 0.3096],
        [0.3238, 0.6762]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0227, 0.9773], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3077, 0.0588],
         [0.0854, 0.1846]],

        [[0.6722, 0.0994],
         [0.1088, 0.2675]],

        [[0.7372, 0.1024],
         [0.9730, 0.1559]],

        [[0.4372, 0.1019],
         [0.4054, 0.3219]],

        [[0.4887, 0.0938],
         [0.1366, 0.1276]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 1
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 3
Adjusted Rand Index: 0.8824283882000855
time is 2
tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448427857772554
time is 3
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 4
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
Global Adjusted Rand Index: 0.5585523339795135
Average Adjusted Rand Index: 0.6981314769354253
Iteration 0: Loss = -14494.869849861201
Iteration 10: Loss = -11248.598495250802
Iteration 20: Loss = -11248.31598741683
Iteration 30: Loss = -11248.268831479323
Iteration 40: Loss = -11248.235167012032
Iteration 50: Loss = -11248.19618907218
Iteration 60: Loss = -11248.146141969082
Iteration 70: Loss = -11248.077923530247
Iteration 80: Loss = -11247.978243554386
Iteration 90: Loss = -11247.816881374307
Iteration 100: Loss = -11247.506188684287
Iteration 110: Loss = -11246.607905882713
Iteration 120: Loss = -11214.512682078765
Iteration 130: Loss = -10992.741632604415
Iteration 140: Loss = -10990.156331582952
Iteration 150: Loss = -10990.159207685112
1
Iteration 160: Loss = -10990.159349762751
2
Iteration 170: Loss = -10990.159360355456
3
Stopping early at iteration 169 due to no improvement.
pi: tensor([[0.6934, 0.3066],
        [0.2530, 0.7470]], dtype=torch.float64)
alpha: tensor([0.4596, 0.5404])
beta: tensor([[[0.2938, 0.0983],
         [0.6138, 0.1942]],

        [[0.6112, 0.0988],
         [0.4372, 0.5605]],

        [[0.3789, 0.0989],
         [0.0175, 0.5528]],

        [[0.3063, 0.1008],
         [0.3465, 0.9359]],

        [[0.8056, 0.0938],
         [0.5578, 0.9481]]], dtype=torch.float64)
time is 0
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448436976567872
time is 1
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208093606567974
time is 2
tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448351863643042
time is 3
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 5
Adjusted Rand Index: 0.8080808080808081
time is 4
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.8758465458042177
Average Adjusted Rand Index: 0.8757138105517394
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -14494.83059791882
Iteration 100: Loss = -11255.762448562466
Iteration 200: Loss = -11253.703860494792
Iteration 300: Loss = -11248.581271049457
Iteration 400: Loss = -11049.892412106965
Iteration 500: Loss = -11046.487408761246
Iteration 600: Loss = -11046.171010031625
Iteration 700: Loss = -11045.965895081426
Iteration 800: Loss = -11045.550098002688
Iteration 900: Loss = -11043.13522558391
Iteration 1000: Loss = -11042.173704266555
Iteration 1100: Loss = -11041.932312672836
Iteration 1200: Loss = -11041.796431959132
Iteration 1300: Loss = -11041.706767479518
Iteration 1400: Loss = -11041.646228391302
Iteration 1500: Loss = -11041.605493249754
Iteration 1600: Loss = -11041.577721096493
Iteration 1700: Loss = -11041.558320995468
Iteration 1800: Loss = -11041.54431302643
Iteration 1900: Loss = -11041.534340654875
Iteration 2000: Loss = -11041.527255096194
Iteration 2100: Loss = -11041.521900235795
Iteration 2200: Loss = -11041.517685300865
Iteration 2300: Loss = -11041.51433000055
Iteration 2400: Loss = -11041.511572665615
Iteration 2500: Loss = -11041.509360123098
Iteration 2600: Loss = -11041.507440600286
Iteration 2700: Loss = -11041.505862619806
Iteration 2800: Loss = -11041.504687438432
Iteration 2900: Loss = -11041.503382222232
Iteration 3000: Loss = -11041.502396030024
Iteration 3100: Loss = -11041.501811521444
Iteration 3200: Loss = -11041.502774651919
1
Iteration 3300: Loss = -11041.500152109507
Iteration 3400: Loss = -11041.499525624604
Iteration 3500: Loss = -11041.499094383456
Iteration 3600: Loss = -11041.498487347828
Iteration 3700: Loss = -11041.600788022213
1
Iteration 3800: Loss = -11041.497639118312
Iteration 3900: Loss = -11041.497221608324
Iteration 4000: Loss = -11041.507597930622
1
Iteration 4100: Loss = -11041.4965478099
Iteration 4200: Loss = -11041.496274740422
Iteration 4300: Loss = -11041.531146319632
1
Iteration 4400: Loss = -11041.495826840344
Iteration 4500: Loss = -11041.495582482064
Iteration 4600: Loss = -11041.495460079279
Iteration 4700: Loss = -11041.495267015423
Iteration 4800: Loss = -11041.495086515186
Iteration 4900: Loss = -11041.605554732443
1
Iteration 5000: Loss = -11041.490148862802
Iteration 5100: Loss = -11041.489874190738
Iteration 5200: Loss = -11041.491201945893
1
Iteration 5300: Loss = -11041.48949389227
Iteration 5400: Loss = -11041.48937540974
Iteration 5500: Loss = -11041.489342020457
Iteration 5600: Loss = -11041.489078585826
Iteration 5700: Loss = -11041.558206746626
1
Iteration 5800: Loss = -11041.484258128532
Iteration 5900: Loss = -11041.48413962028
Iteration 6000: Loss = -11041.485155212848
1
Iteration 6100: Loss = -11041.483947038274
Iteration 6200: Loss = -11041.732744606528
1
Iteration 6300: Loss = -11041.483870241529
Iteration 6400: Loss = -11041.48380527143
Iteration 6500: Loss = -11041.484527806115
1
Iteration 6600: Loss = -11041.483671003374
Iteration 6700: Loss = -11041.513401227894
1
Iteration 6800: Loss = -11041.48360429386
Iteration 6900: Loss = -11041.483667352988
1
Iteration 7000: Loss = -11041.48359344272
Iteration 7100: Loss = -11041.483536169491
Iteration 7200: Loss = -11041.483568556227
1
Iteration 7300: Loss = -11041.483426283787
Iteration 7400: Loss = -11041.510789415346
1
Iteration 7500: Loss = -11041.483408564824
Iteration 7600: Loss = -11041.483384977051
Iteration 7700: Loss = -11041.48917872316
1
Iteration 7800: Loss = -11041.48334201352
Iteration 7900: Loss = -11041.483178883107
Iteration 8000: Loss = -11041.560193709209
1
Iteration 8100: Loss = -11041.483091077562
Iteration 8200: Loss = -11041.483069650933
Iteration 8300: Loss = -11041.483109445186
1
Iteration 8400: Loss = -11041.495878102776
2
Iteration 8500: Loss = -11041.48420159554
3
Iteration 8600: Loss = -11041.483196863108
4
Iteration 8700: Loss = -11041.483548412865
5
Iteration 8800: Loss = -11041.483572302477
6
Iteration 8900: Loss = -11041.491727120125
7
Iteration 9000: Loss = -11041.482966584164
Iteration 9100: Loss = -11041.486788909457
1
Iteration 9200: Loss = -11041.485394246623
2
Iteration 9300: Loss = -11041.482943764398
Iteration 9400: Loss = -11041.48584064163
1
Iteration 9500: Loss = -11041.482928464797
Iteration 9600: Loss = -11041.482919821872
Iteration 9700: Loss = -11041.483749174897
1
Iteration 9800: Loss = -11041.482910365268
Iteration 9900: Loss = -11041.67275316791
1
Iteration 10000: Loss = -11041.482908396889
Iteration 10100: Loss = -11041.482909319398
1
Iteration 10200: Loss = -11041.4832453269
2
Iteration 10300: Loss = -11041.48355084534
3
Iteration 10400: Loss = -11041.48407354347
4
Iteration 10500: Loss = -11041.482910101291
5
Iteration 10600: Loss = -11041.482995720706
6
Iteration 10700: Loss = -11041.490838999105
7
Iteration 10800: Loss = -11041.483151676355
8
Iteration 10900: Loss = -11041.483242445256
9
Iteration 11000: Loss = -11041.4828418594
Iteration 11100: Loss = -11041.498055852007
1
Iteration 11200: Loss = -11041.482845574215
2
Iteration 11300: Loss = -11041.48587691149
3
Iteration 11400: Loss = -11041.48408041363
4
Iteration 11500: Loss = -11041.482860639058
5
Iteration 11600: Loss = -11041.490178285021
6
Iteration 11700: Loss = -11041.48989372153
7
Iteration 11800: Loss = -11041.483952284201
8
Iteration 11900: Loss = -11041.48590116391
9
Iteration 12000: Loss = -11041.483162117456
10
Stopping early at iteration 12000 due to no improvement.
tensor([[  4.5642,  -5.9561],
        [  6.9920,  -8.5876],
        [  4.6158,  -6.1128],
        [ -2.4879,  -0.4697],
        [  6.9581,  -8.4239],
        [  6.1477,  -8.9964],
        [  6.7320,  -9.2660],
        [  3.7935,  -5.1992],
        [  5.2932,  -7.4751],
        [  5.2027,  -6.6014],
        [  4.3875,  -6.1395],
        [  6.7033,  -8.1968],
        [  3.6866,  -5.2504],
        [  6.4593, -10.3485],
        [  6.6252,  -8.7605],
        [  4.1805,  -5.8675],
        [  6.9866,  -8.5834],
        [  5.6319,  -7.0413],
        [  4.6508,  -6.8674],
        [  1.9176,  -3.3974],
        [  2.5533,  -4.0765],
        [  4.4424,  -5.9237],
        [  3.4437,  -5.8161],
        [  4.2978,  -5.8692],
        [  3.5223,  -5.3719],
        [  4.8518,  -6.6600],
        [  4.5880,  -5.9791],
        [  7.3962,  -9.3489],
        [ -0.2175,  -1.3621],
        [  3.9904,  -6.5081],
        [  3.9323,  -5.3216],
        [  3.4901,  -8.1053],
        [  2.0056,  -4.1097],
        [  2.4593,  -3.9318],
        [  2.4448,  -3.9305],
        [  1.3017,  -2.8500],
        [  2.4593,  -5.4809],
        [  5.6470,  -7.0457],
        [  5.9248,  -8.3132],
        [  1.0401,  -3.0164],
        [  6.2633,  -7.6512],
        [  3.1313,  -5.3009],
        [  2.0273,  -3.4277],
        [  5.3505,  -7.7103],
        [  3.0772,  -4.9072],
        [  6.8621,  -8.3640],
        [  4.2969,  -5.9457],
        [  5.5682,  -7.0934],
        [  5.0805,  -7.6093],
        [  1.1927,  -2.5928],
        [  6.7980,  -8.2511],
        [  4.3967,  -5.9672],
        [  5.4533,  -7.3000],
        [  2.8551,  -4.8398],
        [  4.8040,  -6.7048],
        [  0.7406,  -2.1291],
        [  4.5205,  -6.0105],
        [  4.2460,  -5.6371],
        [  1.9329,  -3.3845],
        [  1.7081,  -3.3621],
        [  4.6118,  -6.4027],
        [  6.1960,  -7.6000],
        [  5.2294,  -7.0823],
        [  5.3752,  -7.4171],
        [  7.2570,  -9.7882],
        [  4.3786,  -7.1499],
        [  3.7990,  -5.3982],
        [  4.3771,  -5.7691],
        [  3.9009,  -7.6071],
        [  3.1215,  -4.7497],
        [  4.5102,  -7.0612],
        [  6.8924,  -8.7958],
        [  6.6221,  -8.1801],
        [  1.0658,  -3.2800],
        [  2.0878,  -3.6535],
        [  1.8760,  -3.2771],
        [  3.8721,  -5.3783],
        [  5.2246,  -7.6787],
        [  3.7873,  -5.2877],
        [  6.2585,  -7.7524],
        [  6.1151,  -7.5053],
        [  2.3480,  -5.4424],
        [  5.8285,  -8.5226],
        [  5.2809,  -7.4076],
        [  4.0594,  -5.8349],
        [  3.8585,  -5.4144],
        [  5.6183,  -7.0130],
        [  6.1581,  -8.0683],
        [  1.9942,  -3.3885],
        [  5.1144,  -6.7257],
        [  1.7874,  -3.2580],
        [  5.5240,  -7.3139],
        [ -5.5181,   1.2689],
        [  5.5795,  -6.9885],
        [  3.4141,  -4.8819],
        [  6.4868,  -8.4331],
        [  3.7263,  -5.2102],
        [  1.1627,  -3.0066],
        [  6.4827,  -7.8717],
        [  5.2221,  -6.6132]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6803, 0.3197],
        [0.2996, 0.7004]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9769, 0.0231], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1853, 0.0596],
         [0.6138, 0.3073]],

        [[0.6112, 0.0994],
         [0.4372, 0.5605]],

        [[0.3789, 0.0990],
         [0.0175, 0.5528]],

        [[0.3063, 0.1018],
         [0.3465, 0.9359]],

        [[0.8056, 0.0938],
         [0.5578, 0.9481]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0022703549567796725
time is 1
tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824283882000855
time is 2
tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824165642894751
time is 3
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 4
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
Global Adjusted Rand Index: 0.5645673055368091
Average Adjusted Rand Index: 0.7056462326378693
11014.755591317185
new:  [0.5645673055368091, 0.5645673055368091, 0.5585523339795135, 0.5645673055368091] [0.7056462326378693, 0.7056462326378693, 0.6981314769354253, 0.7056462326378693] [11041.505338025432, 11041.485605761667, 11048.842473547926, 11041.483162117456]
prior:  [0.0006136303313408698, 0.001192365071694833, 0.001192365071694833, 0.8758465458042177] [3.1348228818088316e-05, 0.0005171328157629895, 0.0005171328157629895, 0.8757138105517394] [11249.774942943452, 11249.816868116926, 11249.623581165419, 10990.159360355456]
-----------------------------------------------------------------------------------------
This iteration is 17
True Objective function: Loss = -11411.009477043353
Iteration 0: Loss = -14069.006125116332
Iteration 10: Loss = -11712.983960878377
Iteration 20: Loss = -11710.238880408082
Iteration 30: Loss = -11673.465050316907
Iteration 40: Loss = -11477.901710442104
Iteration 50: Loss = -11474.67040835918
Iteration 60: Loss = -11474.653691561354
Iteration 70: Loss = -11474.650043349144
Iteration 80: Loss = -11474.64923604482
Iteration 90: Loss = -11474.649057217925
Iteration 100: Loss = -11474.649005037034
Iteration 110: Loss = -11474.648973392457
Iteration 120: Loss = -11474.648998677703
1
Iteration 130: Loss = -11474.648992134818
2
Iteration 140: Loss = -11474.648993656408
3
Stopping early at iteration 139 due to no improvement.
pi: tensor([[0.6391, 0.3609],
        [0.3769, 0.6231]], dtype=torch.float64)
alpha: tensor([0.5330, 0.4670])
beta: tensor([[[0.2833, 0.0969],
         [0.8444, 0.2138]],

        [[0.3771, 0.1050],
         [0.0178, 0.4586]],

        [[0.2278, 0.1134],
         [0.2346, 0.9196]],

        [[0.9402, 0.1016],
         [0.7168, 0.9969]],

        [[0.7751, 0.0942],
         [0.8029, 0.6197]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 6
Adjusted Rand Index: 0.7721248949511927
time is 1
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
time is 2
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 3
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 94
Adjusted Rand Index: 0.7720725057296194
Global Adjusted Rand Index: 0.3350547755677957
Average Adjusted Rand Index: 0.8698063238081792
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -14354.943765647973
Iteration 100: Loss = -11733.444837922385
Iteration 200: Loss = -11729.973675043142
Iteration 300: Loss = -11721.790710675108
Iteration 400: Loss = -11712.24966339486
Iteration 500: Loss = -11609.609558728082
Iteration 600: Loss = -11482.901762519641
Iteration 700: Loss = -11470.36740025846
Iteration 800: Loss = -11469.612116589364
Iteration 900: Loss = -11468.79638793966
Iteration 1000: Loss = -11451.2404502789
Iteration 1100: Loss = -11421.925058001698
Iteration 1200: Loss = -11402.856245552362
Iteration 1300: Loss = -11402.819190174263
Iteration 1400: Loss = -11402.795422444855
Iteration 1500: Loss = -11392.972681844947
Iteration 1600: Loss = -11392.959234105301
Iteration 1700: Loss = -11392.954165516401
Iteration 1800: Loss = -11392.950148406393
Iteration 1900: Loss = -11392.947577060897
Iteration 2000: Loss = -11392.94567362537
Iteration 2100: Loss = -11393.04444771905
1
Iteration 2200: Loss = -11392.942994530542
Iteration 2300: Loss = -11392.941894960812
Iteration 2400: Loss = -11393.225927674326
1
Iteration 2500: Loss = -11392.9400377384
Iteration 2600: Loss = -11392.939141222478
Iteration 2700: Loss = -11392.938186415033
Iteration 2800: Loss = -11392.93729732549
Iteration 2900: Loss = -11392.936254851851
Iteration 3000: Loss = -11392.934143934119
Iteration 3100: Loss = -11392.934522018435
1
Iteration 3200: Loss = -11392.932054697907
Iteration 3300: Loss = -11392.931800204758
Iteration 3400: Loss = -11392.946668580991
1
Iteration 3500: Loss = -11392.931404868774
Iteration 3600: Loss = -11392.931202103948
Iteration 3700: Loss = -11392.965290668903
1
Iteration 3800: Loss = -11392.930838240994
Iteration 3900: Loss = -11392.930693457716
Iteration 4000: Loss = -11392.93413916315
1
Iteration 4100: Loss = -11392.93044682728
Iteration 4200: Loss = -11392.930477141123
1
Iteration 4300: Loss = -11392.930204953125
Iteration 4400: Loss = -11392.930115345382
Iteration 4500: Loss = -11392.930007358345
Iteration 4600: Loss = -11392.932020588967
1
Iteration 4700: Loss = -11392.929871980017
Iteration 4800: Loss = -11393.017018949193
1
Iteration 4900: Loss = -11392.92973784763
Iteration 5000: Loss = -11392.92971427865
Iteration 5100: Loss = -11392.930079851716
1
Iteration 5200: Loss = -11392.929593107825
Iteration 5300: Loss = -11392.929568763857
Iteration 5400: Loss = -11392.929592885968
1
Iteration 5500: Loss = -11392.929490548686
Iteration 5600: Loss = -11392.935696829656
1
Iteration 5700: Loss = -11392.929580810489
2
Iteration 5800: Loss = -11392.92948206427
Iteration 5900: Loss = -11392.951786039017
1
Iteration 6000: Loss = -11392.929959469215
2
Iteration 6100: Loss = -11392.929362324641
Iteration 6200: Loss = -11392.93023429445
1
Iteration 6300: Loss = -11392.930548455846
2
Iteration 6400: Loss = -11392.931298356398
3
Iteration 6500: Loss = -11392.930291631192
4
Iteration 6600: Loss = -11392.929343082273
Iteration 6700: Loss = -11392.929313682725
Iteration 6800: Loss = -11392.930483600512
1
Iteration 6900: Loss = -11392.929167420894
Iteration 7000: Loss = -11392.92911854517
Iteration 7100: Loss = -11392.929125533827
1
Iteration 7200: Loss = -11392.929221605773
2
Iteration 7300: Loss = -11392.929122855336
3
Iteration 7400: Loss = -11392.929158516801
4
Iteration 7500: Loss = -11392.929713208232
5
Iteration 7600: Loss = -11392.933803877353
6
Iteration 7700: Loss = -11392.929085465783
Iteration 7800: Loss = -11392.929042565462
Iteration 7900: Loss = -11392.93018059329
1
Iteration 8000: Loss = -11392.929551109379
2
Iteration 8100: Loss = -11392.92951466258
3
Iteration 8200: Loss = -11392.945853188707
4
Iteration 8300: Loss = -11393.032171932737
5
Iteration 8400: Loss = -11392.932314080594
6
Iteration 8500: Loss = -11392.929027489921
Iteration 8600: Loss = -11392.932198351264
1
Iteration 8700: Loss = -11392.928950628751
Iteration 8800: Loss = -11392.929004849884
1
Iteration 8900: Loss = -11392.928973642163
2
Iteration 9000: Loss = -11392.929627429163
3
Iteration 9100: Loss = -11392.928959493462
4
Iteration 9200: Loss = -11392.937841980294
5
Iteration 9300: Loss = -11392.92893823359
Iteration 9400: Loss = -11392.928893555938
Iteration 9500: Loss = -11392.929027516207
1
Iteration 9600: Loss = -11392.928887755088
Iteration 9700: Loss = -11392.929325241987
1
Iteration 9800: Loss = -11392.928866791843
Iteration 9900: Loss = -11392.926034397677
Iteration 10000: Loss = -11392.925980899248
Iteration 10100: Loss = -11392.960636515674
1
Iteration 10200: Loss = -11392.925628980063
Iteration 10300: Loss = -11392.925876532749
1
Iteration 10400: Loss = -11392.932133875214
2
Iteration 10500: Loss = -11392.930319979681
3
Iteration 10600: Loss = -11392.925673112108
4
Iteration 10700: Loss = -11392.925690171625
5
Iteration 10800: Loss = -11392.949375938568
6
Iteration 10900: Loss = -11392.925596567122
Iteration 11000: Loss = -11392.936500826918
1
Iteration 11100: Loss = -11392.925593009433
Iteration 11200: Loss = -11392.926704864021
1
Iteration 11300: Loss = -11392.927588498147
2
Iteration 11400: Loss = -11392.940635752555
3
Iteration 11500: Loss = -11392.925568270477
Iteration 11600: Loss = -11392.926048058833
1
Iteration 11700: Loss = -11392.95911846934
2
Iteration 11800: Loss = -11393.014336704577
3
Iteration 11900: Loss = -11392.92568094432
4
Iteration 12000: Loss = -11392.925705909756
5
Iteration 12100: Loss = -11392.939920658713
6
Iteration 12200: Loss = -11392.927818702494
7
Iteration 12300: Loss = -11392.926555923112
8
Iteration 12400: Loss = -11392.925587998321
9
Iteration 12500: Loss = -11392.925729102037
10
Stopping early at iteration 12500 due to no improvement.
tensor([[-10.1522,   5.5370],
        [  3.6595,  -8.2747],
        [  4.2191,  -8.8343],
        [  4.8454,  -9.4606],
        [  1.2993,  -5.9145],
        [ -6.1293,   1.5140],
        [  2.2470,  -6.8623],
        [ -5.8877,   1.2725],
        [  4.6871,  -9.3023],
        [ -5.7457,   1.1305],
        [  4.5410,  -9.1562],
        [ -6.0434,   1.4282],
        [  0.8280,  -5.4432],
        [  3.7957,  -8.4109],
        [  4.3572,  -8.9724],
        [ -7.1822,   2.5670],
        [  3.3515,  -7.9667],
        [ -4.7843,   0.1691],
        [ -3.0212,  -1.5940],
        [ -8.0094,   3.3941],
        [  1.3651,  -5.9803],
        [ -6.7733,   2.1580],
        [  2.2237,  -6.8389],
        [ -4.5273,  -0.0879],
        [  1.9925,  -6.6077],
        [ -6.9242,   2.3090],
        [  2.3185,  -6.9337],
        [  1.6467,  -6.2619],
        [ -8.1224,   3.5072],
        [ -6.9290,   2.3138],
        [ -6.7635,   2.1483],
        [  0.6653,  -5.2805],
        [  2.8658,  -7.4810],
        [  3.9922,  -8.6074],
        [ -7.4293,   2.8141],
        [  2.7863,  -7.4015],
        [ -7.0389,   2.4237],
        [  0.4374,  -5.0526],
        [ -7.7469,   3.1317],
        [ -9.0120,   4.3968],
        [  3.3046,  -7.9198],
        [  1.4270,  -6.0422],
        [ -3.1509,  -1.4643],
        [  6.0120, -10.6272],
        [ -6.0773,   1.4620],
        [ -1.7811,  -2.8342],
        [ -6.5988,   1.9836],
        [ -3.4465,  -1.1688],
        [  3.2820,  -7.8972],
        [ -8.9326,   4.3174],
        [  3.3191,  -7.9343],
        [  4.7568,  -9.3720],
        [ -7.3542,   2.7390],
        [  4.3860,  -9.0012],
        [  2.8185,  -7.4337],
        [  1.0515,  -5.6667],
        [  4.6734,  -9.2886],
        [  0.8876,  -5.5028],
        [ -5.9847,   1.3695],
        [  1.7532,  -6.3684],
        [ -8.7457,   4.1305],
        [ -2.4687,  -2.1465],
        [ -6.2678,   1.6525],
        [  2.0172,  -6.6324],
        [  3.0647,  -7.6800],
        [ -3.3879,  -1.2273],
        [ -7.8951,   3.2799],
        [ -0.9919,  -3.6233],
        [  5.3673,  -9.9825],
        [ -4.7756,   0.1604],
        [  5.6293, -10.2445],
        [ -6.5731,   1.9579],
        [ -3.1742,  -1.4411],
        [ -3.4100,  -1.2052],
        [ -0.5924,  -4.0228],
        [ -6.8641,   2.2488],
        [ -7.5559,   2.9406],
        [ -1.5056,  -3.1096],
        [  4.3146,  -8.9298],
        [ -7.0970,   2.4818],
        [  3.3452,  -7.9604],
        [  5.5168, -10.1321],
        [ -3.5780,  -1.0372],
        [ -0.2903,  -4.3249],
        [ -4.8176,   0.2024],
        [  2.4436,  -7.0589],
        [  3.5413,  -8.1565],
        [ -4.5894,  -0.0258],
        [ -4.2325,  -0.3827],
        [  3.1403,  -7.7555],
        [ -8.6151,   3.9999],
        [  0.3546,  -4.9698],
        [  2.2514,  -6.8667],
        [ -9.8849,   5.2697],
        [  3.5417,  -8.1569],
        [  2.3485,  -6.9637],
        [ -3.8203,  -0.7950],
        [ -7.3071,   2.6919],
        [ -8.1542,   3.5389],
        [ -8.7598,   4.1446]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7847, 0.2153],
        [0.2988, 0.7012]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5284, 0.4716], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2996, 0.0993],
         [0.8444, 0.1906]],

        [[0.3771, 0.1053],
         [0.0178, 0.4586]],

        [[0.2278, 0.1134],
         [0.2346, 0.9196]],

        [[0.9402, 0.1019],
         [0.7168, 0.9969]],

        [[0.7751, 0.0955],
         [0.8029, 0.6197]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 1
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9207702484198148
time is 2
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 3
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9524710887648254
Average Adjusted Rand Index: 0.9524767933147095
Iteration 0: Loss = -24868.0061483013
Iteration 10: Loss = -11655.94372982145
Iteration 20: Loss = -11395.629477300121
Iteration 30: Loss = -11395.963792048015
1
Iteration 40: Loss = -11395.985719542117
2
Iteration 50: Loss = -11395.99015079526
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.6864, 0.3136],
        [0.2338, 0.7662]], dtype=torch.float64)
alpha: tensor([0.4372, 0.5628])
beta: tensor([[[0.1853, 0.0991],
         [0.7294, 0.2950]],

        [[0.6065, 0.1055],
         [0.5349, 0.1285]],

        [[0.3661, 0.1148],
         [0.3400, 0.6027]],

        [[0.6548, 0.1025],
         [0.7956, 0.8776]],

        [[0.2889, 0.0955],
         [0.2784, 0.9473]]], dtype=torch.float64)
time is 0
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.844846433231073
time is 1
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599857335115991
time is 2
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448484848484848
time is 3
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9291406205478928
Average Adjusted Rand Index: 0.9299361303182314
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24867.737290607944
Iteration 100: Loss = -11756.365704335616
Iteration 200: Loss = -11729.825438981945
Iteration 300: Loss = -11715.34041865559
Iteration 400: Loss = -11696.023597832806
Iteration 500: Loss = -11650.476099834601
Iteration 600: Loss = -11549.337322026686
Iteration 700: Loss = -11492.14817614919
Iteration 800: Loss = -11461.134630259197
Iteration 900: Loss = -11437.0303368166
Iteration 1000: Loss = -11432.399722550825
Iteration 1100: Loss = -11432.271081230818
Iteration 1200: Loss = -11432.190577345395
Iteration 1300: Loss = -11432.100837156275
Iteration 1400: Loss = -11422.524262063698
Iteration 1500: Loss = -11416.882879094335
Iteration 1600: Loss = -11416.110795188397
Iteration 1700: Loss = -11416.080655294963
Iteration 1800: Loss = -11416.03626313325
Iteration 1900: Loss = -11416.003718992972
Iteration 2000: Loss = -11415.987781821841
Iteration 2100: Loss = -11415.979551945742
Iteration 2200: Loss = -11415.96934727394
Iteration 2300: Loss = -11415.533363053182
Iteration 2400: Loss = -11415.517163528804
Iteration 2500: Loss = -11414.446828673012
Iteration 2600: Loss = -11414.418466346078
Iteration 2700: Loss = -11414.41482496327
Iteration 2800: Loss = -11414.41169127073
Iteration 2900: Loss = -11414.40950956201
Iteration 3000: Loss = -11414.406649303277
Iteration 3100: Loss = -11414.40697271242
1
Iteration 3200: Loss = -11414.402727890407
Iteration 3300: Loss = -11414.401297989625
Iteration 3400: Loss = -11414.399544656026
Iteration 3500: Loss = -11414.398155912299
Iteration 3600: Loss = -11414.39694353092
Iteration 3700: Loss = -11414.395667063523
Iteration 3800: Loss = -11414.397812460631
1
Iteration 3900: Loss = -11414.396431902334
2
Iteration 4000: Loss = -11414.392360652193
Iteration 4100: Loss = -11414.38111483908
Iteration 4200: Loss = -11414.381277680555
1
Iteration 4300: Loss = -11414.375270400868
Iteration 4400: Loss = -11414.372321848014
Iteration 4500: Loss = -11414.368503061432
Iteration 4600: Loss = -11403.600972718432
Iteration 4700: Loss = -11403.599904209817
Iteration 4800: Loss = -11403.599475898885
Iteration 4900: Loss = -11403.598744562027
Iteration 5000: Loss = -11403.598364888323
Iteration 5100: Loss = -11403.597728032353
Iteration 5200: Loss = -11403.59775779726
1
Iteration 5300: Loss = -11403.59690315354
Iteration 5400: Loss = -11403.610168549285
1
Iteration 5500: Loss = -11403.596056938788
Iteration 5600: Loss = -11403.595646505913
Iteration 5700: Loss = -11403.595919033123
1
Iteration 5800: Loss = -11403.594807079156
Iteration 5900: Loss = -11403.594351828939
Iteration 6000: Loss = -11403.593873716505
Iteration 6100: Loss = -11403.592385046351
Iteration 6200: Loss = -11403.590143294028
Iteration 6300: Loss = -11403.552357379485
Iteration 6400: Loss = -11397.098297416911
Iteration 6500: Loss = -11397.070308869042
Iteration 6600: Loss = -11397.069693026997
Iteration 6700: Loss = -11397.06979829304
1
Iteration 6800: Loss = -11397.083608342668
2
Iteration 6900: Loss = -11397.068815724477
Iteration 7000: Loss = -11397.068524614215
Iteration 7100: Loss = -11397.069372689542
1
Iteration 7200: Loss = -11397.068017511221
Iteration 7300: Loss = -11397.067837799095
Iteration 7400: Loss = -11397.067663582338
Iteration 7500: Loss = -11397.067582574418
Iteration 7600: Loss = -11397.067348915743
Iteration 7700: Loss = -11397.178368042076
1
Iteration 7800: Loss = -11397.06701656529
Iteration 7900: Loss = -11397.067984382767
1
Iteration 8000: Loss = -11397.017599815752
Iteration 8100: Loss = -11397.018823009188
1
Iteration 8200: Loss = -11397.017141919674
Iteration 8300: Loss = -11397.017537530091
1
Iteration 8400: Loss = -11397.015151345073
Iteration 8500: Loss = -11397.052062981758
1
Iteration 8600: Loss = -11397.015038886118
Iteration 8700: Loss = -11397.014946139929
Iteration 8800: Loss = -11397.036676442873
1
Iteration 8900: Loss = -11397.014284708088
Iteration 9000: Loss = -11397.014217729215
Iteration 9100: Loss = -11397.015985417294
1
Iteration 9200: Loss = -11397.0141545664
Iteration 9300: Loss = -11397.0141066332
Iteration 9400: Loss = -11397.014122330547
1
Iteration 9500: Loss = -11397.013995175083
Iteration 9600: Loss = -11397.01523851244
1
Iteration 9700: Loss = -11397.04601710605
2
Iteration 9800: Loss = -11397.036082386112
3
Iteration 9900: Loss = -11397.013833803558
Iteration 10000: Loss = -11397.013934598614
1
Iteration 10100: Loss = -11397.025882753014
2
Iteration 10200: Loss = -11397.013714907142
Iteration 10300: Loss = -11397.01506105311
1
Iteration 10400: Loss = -11397.023975126667
2
Iteration 10500: Loss = -11397.01445578939
3
Iteration 10600: Loss = -11397.014047701196
4
Iteration 10700: Loss = -11397.031515062861
5
Iteration 10800: Loss = -11397.013619623322
Iteration 10900: Loss = -11397.014721296759
1
Iteration 11000: Loss = -11397.014241998504
2
Iteration 11100: Loss = -11394.915642244727
Iteration 11200: Loss = -11394.917273169307
1
Iteration 11300: Loss = -11394.913850592504
Iteration 11400: Loss = -11394.91263427913
Iteration 11500: Loss = -11394.914131244843
1
Iteration 11600: Loss = -11394.950919868706
2
Iteration 11700: Loss = -11394.912476155207
Iteration 11800: Loss = -11394.913365336997
1
Iteration 11900: Loss = -11394.894049162958
Iteration 12000: Loss = -11394.89432076138
1
Iteration 12100: Loss = -11394.916720481213
2
Iteration 12200: Loss = -11394.893464248109
Iteration 12300: Loss = -11394.922307888668
1
Iteration 12400: Loss = -11394.805679978906
Iteration 12500: Loss = -11394.807519966535
1
Iteration 12600: Loss = -11394.809624260417
2
Iteration 12700: Loss = -11394.805960485483
3
Iteration 12800: Loss = -11394.804150317867
Iteration 12900: Loss = -11394.83440957381
1
Iteration 13000: Loss = -11394.810512766138
2
Iteration 13100: Loss = -11394.79758245228
Iteration 13200: Loss = -11394.818684865055
1
Iteration 13300: Loss = -11394.801315413375
2
Iteration 13400: Loss = -11394.797524042977
Iteration 13500: Loss = -11394.829878157816
1
Iteration 13600: Loss = -11394.797495314704
Iteration 13700: Loss = -11394.805394071334
1
Iteration 13800: Loss = -11394.799856936374
2
Iteration 13900: Loss = -11394.798787248528
3
Iteration 14000: Loss = -11394.81188272731
4
Iteration 14100: Loss = -11394.995251551418
5
Iteration 14200: Loss = -11394.800970689012
6
Iteration 14300: Loss = -11394.795798090387
Iteration 14400: Loss = -11394.797397159586
1
Iteration 14500: Loss = -11394.807353141974
2
Iteration 14600: Loss = -11394.895377856179
3
Iteration 14700: Loss = -11394.79798799969
4
Iteration 14800: Loss = -11394.795006723607
Iteration 14900: Loss = -11394.793531160223
Iteration 15000: Loss = -11394.79843852413
1
Iteration 15100: Loss = -11394.839301107386
2
Iteration 15200: Loss = -11394.790074939297
Iteration 15300: Loss = -11394.791082563579
1
Iteration 15400: Loss = -11394.793732066382
2
Iteration 15500: Loss = -11394.7900304359
Iteration 15600: Loss = -11394.811082601973
1
Iteration 15700: Loss = -11394.793029835544
2
Iteration 15800: Loss = -11394.798328149927
3
Iteration 15900: Loss = -11394.7953295506
4
Iteration 16000: Loss = -11394.790097910043
5
Iteration 16100: Loss = -11394.789661866018
Iteration 16200: Loss = -11394.789781205496
1
Iteration 16300: Loss = -11394.799875142753
2
Iteration 16400: Loss = -11394.78995253092
3
Iteration 16500: Loss = -11394.789757846092
4
Iteration 16600: Loss = -11394.792187781612
5
Iteration 16700: Loss = -11394.820091336995
6
Iteration 16800: Loss = -11394.79417819205
7
Iteration 16900: Loss = -11394.790862900727
8
Iteration 17000: Loss = -11394.789734779166
9
Iteration 17100: Loss = -11394.801219013381
10
Stopping early at iteration 17100 due to no improvement.
tensor([[ 7.4293e+00, -8.9790e+00],
        [-6.7207e+00,  5.1243e+00],
        [-7.4948e+00,  5.9472e+00],
        [-7.9627e+00,  6.4104e+00],
        [-4.5993e+00,  3.0352e+00],
        [ 2.6134e+00, -5.1079e+00],
        [-5.4799e+00,  3.5103e+00],
        [ 2.6833e+00, -4.6554e+00],
        [-8.8026e+00,  5.7638e+00],
        [ 2.8233e+00, -4.2838e+00],
        [-7.8054e+00,  5.9196e+00],
        [ 1.5495e+00, -6.1648e+00],
        [-4.1627e+00,  2.0549e+00],
        [-6.7100e+00,  5.3226e+00],
        [-8.5195e+00,  4.8507e+00],
        [ 4.3811e+00, -5.8525e+00],
        [-6.3524e+00,  4.9064e+00],
        [ 1.7296e+00, -3.4471e+00],
        [-2.2671e-01, -1.2409e+00],
        [ 5.1042e+00, -6.4905e+00],
        [-4.2881e+00,  2.7448e+00],
        [ 3.6392e+00, -5.5681e+00],
        [-5.1713e+00,  3.7834e+00],
        [ 1.5955e+00, -3.1250e+00],
        [-5.1347e+00,  3.3950e+00],
        [ 3.9392e+00, -5.3303e+00],
        [-5.2598e+00,  3.7033e+00],
        [-4.6443e+00,  3.1286e+00],
        [ 4.9165e+00, -6.9337e+00],
        [ 3.9535e+00, -5.5195e+00],
        [ 2.2881e+00, -6.9033e+00],
        [-3.5564e+00,  2.1578e+00],
        [-6.7763e+00,  3.6392e+00],
        [-7.3477e+00,  5.8294e+00],
        [ 4.2242e+00, -5.7439e+00],
        [-7.0186e+00,  3.8593e+00],
        [ 3.9655e+00, -5.6570e+00],
        [-3.3807e+00,  1.9654e+00],
        [ 3.3906e+00, -7.1946e+00],
        [ 5.3196e+00, -8.2401e+00],
        [-6.1757e+00,  4.7873e+00],
        [-4.5233e+00,  2.9033e+00],
        [ 2.0995e-01, -1.6709e+00],
        [-1.0103e+01,  7.9027e+00],
        [ 3.1238e+00, -4.6333e+00],
        [-7.2735e+00,  4.9541e+00],
        [ 3.6933e+00, -5.1531e+00],
        [-2.3566e-01, -2.4820e+00],
        [-6.8193e+00,  4.2049e+00],
        [ 5.4966e+00, -8.0319e+00],
        [-9.9929e+00,  8.5780e+00],
        [-9.2105e+00,  5.6178e+00],
        [ 4.0961e+00, -5.5172e+00],
        [-7.3546e+00,  5.9404e+00],
        [-5.8388e+00,  4.3101e+00],
        [-3.9816e+00,  2.5335e+00],
        [-8.5025e+00,  6.7115e+00],
        [-4.0942e+00,  2.6911e+00],
        [ 3.1094e+00, -4.5042e+00],
        [-4.9046e+00,  3.1585e+00],
        [ 5.8299e+00, -7.2394e+00],
        [-4.9262e-01, -1.1651e+00],
        [ 2.1463e+00, -5.8040e+00],
        [-5.1243e+00,  3.5475e+00],
        [-6.0568e+00,  4.5142e+00],
        [ 1.1602e-02, -2.2867e+00],
        [ 4.6572e+00, -6.5451e+00],
        [-2.2464e+00,  7.6221e-01],
        [-8.6878e+00,  7.2174e+00],
        [ 1.1670e+00, -3.8830e+00],
        [-1.2114e+01,  8.8752e+00],
        [ 3.3249e+00, -5.3995e+00],
        [ 1.8043e-01, -1.7715e+00],
        [ 3.5067e-01, -2.3094e+00],
        [-2.3320e+00,  8.6639e-01],
        [ 3.7496e+00, -5.5404e+00],
        [ 4.3660e+00, -6.2082e+00],
        [-2.3033e+00, -8.3709e-01],
        [-9.7800e+00,  5.8294e+00],
        [ 4.0671e+00, -5.5666e+00],
        [-8.2153e+00,  3.6000e+00],
        [-9.1742e+00,  7.2181e+00],
        [-5.6665e-01, -2.8012e+00],
        [-2.6623e+00,  1.1857e+00],
        [ 7.7811e-01, -4.3796e+00],
        [-5.4614e+00,  4.0331e+00],
        [-7.3872e+00,  4.2989e+00],
        [ 1.7076e+00, -3.1945e+00],
        [ 1.2950e+00, -2.9335e+00],
        [-8.9376e+00,  7.5286e+00],
        [ 5.5138e+00, -7.2623e+00],
        [-3.2550e+00,  1.8263e+00],
        [-5.1304e+00,  3.7107e+00],
        [ 7.0848e+00, -8.5592e+00],
        [-7.0270e+00,  4.6110e+00],
        [-5.4159e+00,  3.8717e+00],
        [ 2.1437e-01, -3.1121e+00],
        [ 4.3752e+00, -5.9400e+00],
        [ 6.0091e+00, -7.4146e+00],
        [ 4.7019e+00, -8.3609e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6995, 0.3005],
        [0.2248, 0.7752]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4690, 0.5310], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1900, 0.0988],
         [0.7294, 0.3020]],

        [[0.6065, 0.1052],
         [0.5349, 0.1285]],

        [[0.3661, 0.1139],
         [0.3400, 0.6027]],

        [[0.6548, 0.1042],
         [0.7956, 0.8776]],

        [[0.2889, 0.0953],
         [0.2784, 0.9473]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 1
tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599857335115991
time is 2
tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 95
Adjusted Rand Index: 0.8080808080808081
time is 3
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599857335115991
time is 4
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9291444196810927
Average Adjusted Rand Index: 0.9297715824899317
Iteration 0: Loss = -24843.505340339107
Iteration 10: Loss = -11733.720197495084
Iteration 20: Loss = -11732.63376607581
Iteration 30: Loss = -11695.846904037335
Iteration 40: Loss = -11679.532357096194
Iteration 50: Loss = -11396.228358912444
Iteration 60: Loss = -11395.912440911214
Iteration 70: Loss = -11395.9521894078
1
Iteration 80: Loss = -11395.954453802215
2
Iteration 90: Loss = -11395.955180035544
3
Stopping early at iteration 89 due to no improvement.
pi: tensor([[0.7663, 0.2337],
        [0.3135, 0.6865]], dtype=torch.float64)
alpha: tensor([0.5628, 0.4372])
beta: tensor([[[0.2950, 0.0991],
         [0.2803, 0.1853]],

        [[0.9327, 0.1055],
         [0.8689, 0.5755]],

        [[0.1819, 0.1148],
         [0.0106, 0.4605]],

        [[0.9425, 0.1025],
         [0.2818, 0.3262]],

        [[0.9665, 0.0955],
         [0.7261, 0.1032]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 5
Adjusted Rand Index: 0.8080808080808081
time is 1
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
time is 2
tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448484848484848
time is 3
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9214265637475184
Average Adjusted Rand Index: 0.9225830052881783
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24841.767618951522
Iteration 100: Loss = -11730.594033926242
Iteration 200: Loss = -11713.701108516
Iteration 300: Loss = -11689.08465376325
Iteration 400: Loss = -11650.833165486392
Iteration 500: Loss = -11602.6258907486
Iteration 600: Loss = -11561.008495182668
Iteration 700: Loss = -11540.44181859821
Iteration 800: Loss = -11528.953867810053
Iteration 900: Loss = -11521.786097134382
Iteration 1000: Loss = -11510.149049756179
Iteration 1100: Loss = -11482.361887039673
Iteration 1200: Loss = -11459.526209118936
Iteration 1300: Loss = -11449.1320130418
Iteration 1400: Loss = -11448.621311371073
Iteration 1500: Loss = -11447.529106729062
Iteration 1600: Loss = -11445.203928675628
Iteration 1700: Loss = -11445.110348974422
Iteration 1800: Loss = -11445.034995568934
Iteration 1900: Loss = -11436.800001337186
Iteration 2000: Loss = -11436.739271499187
Iteration 2100: Loss = -11433.897957200334
Iteration 2200: Loss = -11433.675148761984
Iteration 2300: Loss = -11429.279773973358
Iteration 2400: Loss = -11429.169217107945
Iteration 2500: Loss = -11429.014893034908
Iteration 2600: Loss = -11428.898469338295
Iteration 2700: Loss = -11428.876260457246
Iteration 2800: Loss = -11428.86257027845
Iteration 2900: Loss = -11428.852216828473
Iteration 3000: Loss = -11428.843162874415
Iteration 3100: Loss = -11428.835627725357
Iteration 3200: Loss = -11428.834982049133
Iteration 3300: Loss = -11428.822726550557
Iteration 3400: Loss = -11428.816630669566
Iteration 3500: Loss = -11428.805135849194
Iteration 3600: Loss = -11422.65798358152
Iteration 3700: Loss = -11416.110482735248
Iteration 3800: Loss = -11416.10476850713
Iteration 3900: Loss = -11416.097607899987
Iteration 4000: Loss = -11416.086899108548
Iteration 4100: Loss = -11416.08154810711
Iteration 4200: Loss = -11416.076395654038
Iteration 4300: Loss = -11416.072409376107
Iteration 4400: Loss = -11416.066404664372
Iteration 4500: Loss = -11416.061086212681
Iteration 4600: Loss = -11403.246396313085
Iteration 4700: Loss = -11403.224199114238
Iteration 4800: Loss = -11403.21861436393
Iteration 4900: Loss = -11403.206723001873
Iteration 5000: Loss = -11391.345118467145
Iteration 5100: Loss = -11391.30862366188
Iteration 5200: Loss = -11391.305626069696
Iteration 5300: Loss = -11391.303582481247
Iteration 5400: Loss = -11391.301899996864
Iteration 5500: Loss = -11391.301624874019
Iteration 5600: Loss = -11391.299211697331
Iteration 5700: Loss = -11391.298016854724
Iteration 5800: Loss = -11391.299774724064
1
Iteration 5900: Loss = -11391.295476898886
Iteration 6000: Loss = -11391.293892549886
Iteration 6100: Loss = -11391.294885100247
1
Iteration 6200: Loss = -11391.288778866436
Iteration 6300: Loss = -11391.28712071114
Iteration 6400: Loss = -11391.286166464122
Iteration 6500: Loss = -11391.283788933903
Iteration 6600: Loss = -11391.277477670235
Iteration 6700: Loss = -11391.275888088741
Iteration 6800: Loss = -11391.284167715728
1
Iteration 6900: Loss = -11391.276190130899
2
Iteration 7000: Loss = -11391.27693912194
3
Iteration 7100: Loss = -11391.269970259134
Iteration 7200: Loss = -11391.266670085968
Iteration 7300: Loss = -11391.253885714154
Iteration 7400: Loss = -11391.25339720942
Iteration 7500: Loss = -11391.256358553246
1
Iteration 7600: Loss = -11391.25275214657
Iteration 7700: Loss = -11391.259758757622
1
Iteration 7800: Loss = -11391.256203677367
2
Iteration 7900: Loss = -11391.253432602438
3
Iteration 8000: Loss = -11391.252530780112
Iteration 8100: Loss = -11391.257472789093
1
Iteration 8200: Loss = -11391.256696581162
2
Iteration 8300: Loss = -11391.255912180424
3
Iteration 8400: Loss = -11391.240548512638
Iteration 8500: Loss = -11391.206981714653
Iteration 8600: Loss = -11391.194171947123
Iteration 8700: Loss = -11391.203019715393
1
Iteration 8800: Loss = -11391.194476315157
2
Iteration 8900: Loss = -11391.201803212061
3
Iteration 9000: Loss = -11391.212182518233
4
Iteration 9100: Loss = -11391.289239759331
5
Iteration 9200: Loss = -11391.193066302838
Iteration 9300: Loss = -11391.19284662913
Iteration 9400: Loss = -11391.193147425332
1
Iteration 9500: Loss = -11391.195785561613
2
Iteration 9600: Loss = -11391.05582066479
Iteration 9700: Loss = -11391.054734276588
Iteration 9800: Loss = -11391.185032713498
1
Iteration 9900: Loss = -11391.035321990972
Iteration 10000: Loss = -11391.035432267408
1
Iteration 10100: Loss = -11391.034930269172
Iteration 10200: Loss = -11391.034738928734
Iteration 10300: Loss = -11391.034565884844
Iteration 10400: Loss = -11391.038211571868
1
Iteration 10500: Loss = -11391.034370579435
Iteration 10600: Loss = -11391.034643962288
1
Iteration 10700: Loss = -11391.021634472061
Iteration 10800: Loss = -11391.018323335516
Iteration 10900: Loss = -11391.022380251672
1
Iteration 11000: Loss = -11391.022662683994
2
Iteration 11100: Loss = -11391.018875852551
3
Iteration 11200: Loss = -11391.019087242017
4
Iteration 11300: Loss = -11391.0215106148
5
Iteration 11400: Loss = -11391.026430236829
6
Iteration 11500: Loss = -11391.017919443237
Iteration 11600: Loss = -11391.021416434827
1
Iteration 11700: Loss = -11391.018036916283
2
Iteration 11800: Loss = -11391.024955758261
3
Iteration 11900: Loss = -11391.01750719574
Iteration 12000: Loss = -11391.017959510462
1
Iteration 12100: Loss = -11391.0316140511
2
Iteration 12200: Loss = -11391.017439477573
Iteration 12300: Loss = -11391.048269251798
1
Iteration 12400: Loss = -11391.017469261713
2
Iteration 12500: Loss = -11391.017861795674
3
Iteration 12600: Loss = -11391.017981209652
4
Iteration 12700: Loss = -11391.079795610622
5
Iteration 12800: Loss = -11391.017261234854
Iteration 12900: Loss = -11391.01899610257
1
Iteration 13000: Loss = -11391.02139917832
2
Iteration 13100: Loss = -11391.021871309873
3
Iteration 13200: Loss = -11391.023193489236
4
Iteration 13300: Loss = -11391.017169487977
Iteration 13400: Loss = -11391.017704650958
1
Iteration 13500: Loss = -11391.018802837501
2
Iteration 13600: Loss = -11391.016811504222
Iteration 13700: Loss = -11391.015969563556
Iteration 13800: Loss = -11390.957976926347
Iteration 13900: Loss = -11390.957549176212
Iteration 14000: Loss = -11390.945489125246
Iteration 14100: Loss = -11390.948698707156
1
Iteration 14200: Loss = -11391.015817802578
2
Iteration 14300: Loss = -11390.950222299645
3
Iteration 14400: Loss = -11390.945336204051
Iteration 14500: Loss = -11390.946271833742
1
Iteration 14600: Loss = -11390.948751559175
2
Iteration 14700: Loss = -11390.977987015518
3
Iteration 14800: Loss = -11390.94525351514
Iteration 14900: Loss = -11390.945533159516
1
Iteration 15000: Loss = -11390.94524549129
Iteration 15100: Loss = -11390.94456728399
Iteration 15200: Loss = -11390.947081185568
1
Iteration 15300: Loss = -11390.968159515796
2
Iteration 15400: Loss = -11390.945050273946
3
Iteration 15500: Loss = -11390.944493429904
Iteration 15600: Loss = -11390.963272656387
1
Iteration 15700: Loss = -11390.944524964028
2
Iteration 15800: Loss = -11390.946702059937
3
Iteration 15900: Loss = -11390.975434048138
4
Iteration 16000: Loss = -11390.944401212037
Iteration 16100: Loss = -11390.946639635473
1
Iteration 16200: Loss = -11390.944311350422
Iteration 16300: Loss = -11390.944974923586
1
Iteration 16400: Loss = -11390.96777751619
2
Iteration 16500: Loss = -11390.944154861423
Iteration 16600: Loss = -11390.946500357655
1
Iteration 16700: Loss = -11390.944144477364
Iteration 16800: Loss = -11390.944865657431
1
Iteration 16900: Loss = -11390.944165940784
2
Iteration 17000: Loss = -11390.944491992072
3
Iteration 17100: Loss = -11390.947842501308
4
Iteration 17200: Loss = -11391.03168116465
5
Iteration 17300: Loss = -11390.944160770547
6
Iteration 17400: Loss = -11390.94533055901
7
Iteration 17500: Loss = -11390.944169679007
8
Iteration 17600: Loss = -11390.9443882871
9
Iteration 17700: Loss = -11390.944096513025
Iteration 17800: Loss = -11390.944196467586
1
Iteration 17900: Loss = -11390.944081202211
Iteration 18000: Loss = -11390.944132159155
1
Iteration 18100: Loss = -11390.944053337675
Iteration 18200: Loss = -11390.945040732171
1
Iteration 18300: Loss = -11390.944076566411
2
Iteration 18400: Loss = -11390.944829923264
3
Iteration 18500: Loss = -11390.960342990902
4
Iteration 18600: Loss = -11390.936886821239
Iteration 18700: Loss = -11390.936577539129
Iteration 18800: Loss = -11390.94662840334
1
Iteration 18900: Loss = -11391.055873198122
2
Iteration 19000: Loss = -11391.045517096925
3
Iteration 19100: Loss = -11391.022428416007
4
Iteration 19200: Loss = -11390.936443354303
Iteration 19300: Loss = -11390.936973528465
1
Iteration 19400: Loss = -11390.937058694642
2
Iteration 19500: Loss = -11390.946367437056
3
Iteration 19600: Loss = -11390.979376934876
4
Iteration 19700: Loss = -11390.936703000414
5
Iteration 19800: Loss = -11390.94080204844
6
Iteration 19900: Loss = -11390.941021774308
7
tensor([[-8.8391,  7.2494],
        [ 5.2872, -6.6748],
        [ 5.9092, -7.5954],
        [ 6.1503, -8.2120],
        [ 2.4873, -4.7317],
        [-4.5320,  3.1320],
        [ 3.8716, -5.2706],
        [-4.3186,  2.8562],
        [ 6.0072, -8.0747],
        [-4.5094,  2.3854],
        [ 5.9011, -7.8853],
        [-4.8836,  2.6023],
        [ 1.9165, -4.3788],
        [ 5.3915, -6.7795],
        [ 6.0473, -7.4462],
        [-5.7167,  4.0913],
        [ 4.9900, -6.3922],
        [-3.1980,  1.7691],
        [-1.4298,  0.0196],
        [-6.6311,  4.7938],
        [ 2.5769, -4.7497],
        [-5.2692,  3.7242],
        [ 2.7590, -6.3221],
        [-3.0342,  1.4704],
        [ 1.9998, -6.6150],
        [-5.3573,  3.9040],
        [ 3.9248, -5.3251],
        [ 2.8930, -5.0095],
        [-6.7730,  4.9010],
        [-5.4530,  3.8275],
        [-5.3101,  3.6395],
        [ 2.0571, -3.8570],
        [ 4.3004, -6.0780],
        [ 5.5919, -6.9806],
        [-6.2008,  4.1102],
        [ 2.9291, -7.2798],
        [-5.5139,  3.9695],
        [ 2.0524, -3.4498],
        [-6.1868,  4.7233],
        [-7.6582,  5.8406],
        [ 4.4578, -6.7675],
        [ 3.0240, -4.4133],
        [-2.6622, -0.9712],
        [ 8.0413, -9.9428],
        [-4.4886,  3.0781],
        [-0.1723, -1.2141],
        [-5.1804,  3.4219],
        [-2.0355,  0.2923],
        [ 6.0130, -7.7289],
        [-7.9793,  5.3268],
        [ 3.3404, -7.8922],
        [ 6.5509, -8.5284],
        [-5.8784,  4.2924],
        [ 5.9747, -7.4472],
        [ 4.3740, -5.9069],
        [ 2.3043, -4.3759],
        [ 6.8220, -8.3014],
        [ 1.7214, -4.6866],
        [-4.6596,  2.7705],
        [ 3.3259, -4.8077],
        [-8.6298,  4.2745],
        [-0.8975, -0.5330],
        [-6.2837,  1.6685],
        [ 3.0108, -5.4340],
        [ 4.1913, -6.5317],
        [-1.9362,  0.2328],
        [-6.4162,  4.7932],
        [ 0.4621, -2.1617],
        [ 6.7068, -9.3448],
        [-3.2040,  1.7451],
        [ 7.6939, -9.7843],
        [-4.9692,  3.5797],
        [-1.5670,  0.1747],
        [-2.0802,  0.3663],
        [ 0.9990, -2.4349],
        [-5.3124,  3.8275],
        [-5.9970,  4.5735],
        [ 0.1025, -1.4998],
        [ 5.9243, -7.3123],
        [-5.5401,  4.1109],
        [ 4.9584, -6.3578],
        [ 7.3768, -8.9475],
        [-1.9827,  0.5762],
        [ 1.2718, -2.7607],
        [-3.3591,  1.6786],
        [ 3.7201, -5.7519],
        [ 5.1235, -6.5985],
        [-3.3816,  1.2406],
        [-2.7098,  1.1743],
        [ 4.2908, -6.6225],
        [-7.4941,  5.0682],
        [ 1.4014, -3.8942],
        [ 3.6957, -5.3115],
        [-9.7707,  5.7222],
        [ 4.8984, -6.8282],
        [ 3.5588, -5.8662],
        [-2.3368,  0.6148],
        [-5.8082,  4.2304],
        [-7.3026,  5.8962],
        [-7.3890,  5.5561]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7832, 0.2168],
        [0.2977, 0.7023]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5279, 0.4721], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3003, 0.0993],
         [0.2803, 0.1909]],

        [[0.9327, 0.1054],
         [0.8689, 0.5755]],

        [[0.1819, 0.1135],
         [0.0106, 0.4605]],

        [[0.9425, 0.1019],
         [0.2818, 0.3262]],

        [[0.9665, 0.0954],
         [0.7261, 0.1032]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208056373456518
time is 1
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
time is 2
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 3
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9603129748486161
Average Adjusted Rand Index: 0.9603198903330664
Iteration 0: Loss = -36810.88216534049
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.1429,    nan]],

        [[0.1537,    nan],
         [0.0692, 0.0704]],

        [[0.5144,    nan],
         [0.4960, 0.3929]],

        [[0.7918,    nan],
         [0.9842, 0.8132]],

        [[0.8209,    nan],
         [0.7777, 0.8475]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -36810.50525746307
Iteration 100: Loss = -11792.698998265543
Iteration 200: Loss = -11758.86655665954
Iteration 300: Loss = -11745.927261098794
Iteration 400: Loss = -11739.146598766865
Iteration 500: Loss = -11737.160931040915
Iteration 600: Loss = -11736.08918616918
Iteration 700: Loss = -11735.364919908678
Iteration 800: Loss = -11734.827591061614
Iteration 900: Loss = -11734.384915924667
Iteration 1000: Loss = -11733.939059870943
Iteration 1100: Loss = -11733.499630337334
Iteration 1200: Loss = -11733.145355678473
Iteration 1300: Loss = -11732.624778182877
Iteration 1400: Loss = -11731.678533111812
Iteration 1500: Loss = -11720.36648390166
Iteration 1600: Loss = -11715.523526384228
Iteration 1700: Loss = -11711.32186880564
Iteration 1800: Loss = -11708.199318512645
Iteration 1900: Loss = -11706.342543871986
Iteration 2000: Loss = -11703.77290294876
Iteration 2100: Loss = -11701.346772679068
Iteration 2200: Loss = -11696.746940655521
Iteration 2300: Loss = -11685.227761755596
Iteration 2400: Loss = -11660.351146666753
Iteration 2500: Loss = -11630.014822629391
Iteration 2600: Loss = -11599.041683191559
Iteration 2700: Loss = -11583.542582393304
Iteration 2800: Loss = -11576.994107491675
Iteration 2900: Loss = -11565.044375477748
Iteration 3000: Loss = -11564.303866524428
Iteration 3100: Loss = -11561.13069871953
Iteration 3200: Loss = -11551.94905171147
Iteration 3300: Loss = -11548.97661410177
Iteration 3400: Loss = -11548.6639665898
Iteration 3500: Loss = -11539.320262930782
Iteration 3600: Loss = -11538.801650612748
Iteration 3700: Loss = -11535.027410700412
Iteration 3800: Loss = -11534.96779293919
Iteration 3900: Loss = -11534.86223554366
Iteration 4000: Loss = -11533.807032615043
Iteration 4100: Loss = -11531.778058621441
Iteration 4200: Loss = -11531.44601319147
Iteration 4300: Loss = -11530.939019405767
Iteration 4400: Loss = -11530.914304935537
Iteration 4500: Loss = -11530.896147756464
Iteration 4600: Loss = -11526.509642235611
Iteration 4700: Loss = -11526.439565487366
Iteration 4800: Loss = -11526.353895552595
Iteration 4900: Loss = -11521.303198626132
Iteration 5000: Loss = -11521.279296109291
Iteration 5100: Loss = -11521.260906396426
Iteration 5200: Loss = -11521.241833626647
Iteration 5300: Loss = -11521.220761085166
Iteration 5400: Loss = -11521.198560567442
Iteration 5500: Loss = -11521.18222311563
Iteration 5600: Loss = -11521.173726986864
Iteration 5700: Loss = -11521.16770537972
Iteration 5800: Loss = -11517.856659065592
Iteration 5900: Loss = -11517.753804751072
Iteration 6000: Loss = -11517.751047410125
Iteration 6100: Loss = -11517.74941470236
Iteration 6200: Loss = -11517.738288505609
Iteration 6300: Loss = -11517.711297597973
Iteration 6400: Loss = -11507.237384053367
Iteration 6500: Loss = -11507.22436061021
Iteration 6600: Loss = -11507.18775475028
Iteration 6700: Loss = -11507.177848934956
Iteration 6800: Loss = -11507.17434902067
Iteration 6900: Loss = -11507.135686810334
Iteration 7000: Loss = -11507.117931933119
Iteration 7100: Loss = -11507.119267257105
1
Iteration 7200: Loss = -11501.613850661857
Iteration 7300: Loss = -11501.457725346574
Iteration 7400: Loss = -11501.4468871014
Iteration 7500: Loss = -11501.44396903194
Iteration 7600: Loss = -11501.43746312471
Iteration 7700: Loss = -11501.446934772202
1
Iteration 7800: Loss = -11501.434335250153
Iteration 7900: Loss = -11501.434041787405
Iteration 8000: Loss = -11501.432226423292
Iteration 8100: Loss = -11501.428993151956
Iteration 8200: Loss = -11501.418255295659
Iteration 8300: Loss = -11501.418306320376
1
Iteration 8400: Loss = -11501.39930362502
Iteration 8500: Loss = -11501.394943201301
Iteration 8600: Loss = -11500.756319601916
Iteration 8700: Loss = -11500.760895199459
1
Iteration 8800: Loss = -11500.74278470699
Iteration 8900: Loss = -11500.741868508321
Iteration 9000: Loss = -11500.742195004053
1
Iteration 9100: Loss = -11500.737614416435
Iteration 9200: Loss = -11500.72654771926
Iteration 9300: Loss = -11500.741477707708
1
Iteration 9400: Loss = -11493.690838606024
Iteration 9500: Loss = -11493.652730262333
Iteration 9600: Loss = -11493.653667594748
1
Iteration 9700: Loss = -11493.658839263846
2
Iteration 9800: Loss = -11493.858932771154
3
Iteration 9900: Loss = -11493.626047943935
Iteration 10000: Loss = -11493.624144811944
Iteration 10100: Loss = -11493.631997523242
1
Iteration 10200: Loss = -11493.621847067214
Iteration 10300: Loss = -11493.616563440828
Iteration 10400: Loss = -11493.605075264008
Iteration 10500: Loss = -11493.627781591575
1
Iteration 10600: Loss = -11493.597537382282
Iteration 10700: Loss = -11493.598722244786
1
Iteration 10800: Loss = -11493.59714446925
Iteration 10900: Loss = -11493.597263286088
1
Iteration 11000: Loss = -11493.595424590781
Iteration 11100: Loss = -11493.554894094252
Iteration 11200: Loss = -11493.55469842461
Iteration 11300: Loss = -11493.556872693043
1
Iteration 11400: Loss = -11493.613580074367
2
Iteration 11500: Loss = -11486.890325830796
Iteration 11600: Loss = -11486.874940097628
Iteration 11700: Loss = -11486.867317356342
Iteration 11800: Loss = -11486.869155600863
1
Iteration 11900: Loss = -11486.884832787371
2
Iteration 12000: Loss = -11486.863785559812
Iteration 12100: Loss = -11486.854157873358
Iteration 12200: Loss = -11486.850656325922
Iteration 12300: Loss = -11486.849258454315
Iteration 12400: Loss = -11486.850149439202
1
Iteration 12500: Loss = -11486.848992901589
Iteration 12600: Loss = -11486.847067078303
Iteration 12700: Loss = -11481.270900109015
Iteration 12800: Loss = -11481.27165485617
1
Iteration 12900: Loss = -11481.231627783694
Iteration 13000: Loss = -11481.245197493377
1
Iteration 13100: Loss = -11481.225432030078
Iteration 13200: Loss = -11481.225098478748
Iteration 13300: Loss = -11481.227610418315
1
Iteration 13400: Loss = -11481.223985851237
Iteration 13500: Loss = -11481.223755198533
Iteration 13600: Loss = -11481.418899028538
1
Iteration 13700: Loss = -11481.19830803186
Iteration 13800: Loss = -11474.35928855501
Iteration 13900: Loss = -11474.360744967138
1
Iteration 14000: Loss = -11474.36524164368
2
Iteration 14100: Loss = -11474.35377484891
Iteration 14200: Loss = -11474.35365783413
Iteration 14300: Loss = -11474.354484071682
1
Iteration 14400: Loss = -11474.355609425862
2
Iteration 14500: Loss = -11473.629430821002
Iteration 14600: Loss = -11465.20442749469
Iteration 14700: Loss = -11465.203339396263
Iteration 14800: Loss = -11465.194930635518
Iteration 14900: Loss = -11465.15670407004
Iteration 15000: Loss = -11465.154652717896
Iteration 15100: Loss = -11465.158349779984
1
Iteration 15200: Loss = -11465.144137131363
Iteration 15300: Loss = -11465.115614238992
Iteration 15400: Loss = -11465.11450351657
Iteration 15500: Loss = -11465.055505289507
Iteration 15600: Loss = -11465.048926616997
Iteration 15700: Loss = -11465.147835110944
1
Iteration 15800: Loss = -11465.042439716446
Iteration 15900: Loss = -11465.041329233827
Iteration 16000: Loss = -11465.042634096428
1
Iteration 16100: Loss = -11465.040991083037
Iteration 16200: Loss = -11465.040943599633
Iteration 16300: Loss = -11465.041437297738
1
Iteration 16400: Loss = -11465.040883364198
Iteration 16500: Loss = -11465.040881764004
Iteration 16600: Loss = -11465.009265863324
Iteration 16700: Loss = -11465.00629440416
Iteration 16800: Loss = -11465.006293590734
Iteration 16900: Loss = -11465.009147073417
1
Iteration 17000: Loss = -11465.00673738235
2
Iteration 17100: Loss = -11465.00660080314
3
Iteration 17200: Loss = -11464.956493017375
Iteration 17300: Loss = -11464.932315059015
Iteration 17400: Loss = -11465.052554207441
1
Iteration 17500: Loss = -11464.90272857223
Iteration 17600: Loss = -11464.903828145207
1
Iteration 17700: Loss = -11464.90267327458
Iteration 17800: Loss = -11464.782425157766
Iteration 17900: Loss = -11464.777118471668
Iteration 18000: Loss = -11464.778789092048
1
Iteration 18100: Loss = -11464.776100265066
Iteration 18200: Loss = -11464.874038218051
1
Iteration 18300: Loss = -11464.899089086144
2
Iteration 18400: Loss = -11464.787450927546
3
Iteration 18500: Loss = -11464.885462124
4
Iteration 18600: Loss = -11464.793724068735
5
Iteration 18700: Loss = -11464.775810918727
Iteration 18800: Loss = -11464.781623950837
1
Iteration 18900: Loss = -11464.904345102175
2
Iteration 19000: Loss = -11464.812882163224
3
Iteration 19100: Loss = -11464.775666744255
Iteration 19200: Loss = -11464.776319594368
1
Iteration 19300: Loss = -11464.840843549766
2
Iteration 19400: Loss = -11464.78621502793
3
Iteration 19500: Loss = -11464.83303662071
4
Iteration 19600: Loss = -11462.476788572169
Iteration 19700: Loss = -11462.40152001904
Iteration 19800: Loss = -11462.400819697803
Iteration 19900: Loss = -11462.401877222263
1
tensor([[  6.9082,  -8.2989],
        [ -5.0786,   3.4466],
        [-10.3455,   8.4419],
        [-11.9791,   9.1151],
        [ -6.3495,   3.3962],
        [ -3.1305,   1.7409],
        [ -7.7186,   4.4151],
        [  0.4849,  -2.3649],
        [ -7.2757,   4.4941],
        [  2.0783,  -3.8453],
        [ -8.2409,   6.7668],
        [ -1.9457,   0.4417],
        [ -3.1025,   1.7129],
        [ -9.8106,   6.0367],
        [ -6.0707,   4.6768],
        [  0.7058,  -2.0961],
        [ -6.6306,   4.7594],
        [ -4.2666,   2.8525],
        [ -9.2757,   7.7585],
        [ -2.0291,   0.0180],
        [-10.1718,   8.0378],
        [  0.3180,  -2.2015],
        [ -4.2284,   2.8217],
        [ -1.4787,  -0.2762],
        [ -5.1370,   3.1387],
        [ -3.3838,   1.7314],
        [ -8.7838,   7.3805],
        [ -6.1606,   3.1470],
        [ -4.2086,   1.3220],
        [ -1.6951,  -0.4797],
        [ -2.1103,   0.6654],
        [-10.1048,   8.6769],
        [ -7.4176,   5.8847],
        [-10.7619,   8.9127],
        [ -0.2064,  -2.8105],
        [ -6.3434,   4.8851],
        [ -2.7120,   1.1071],
        [ -9.2878,   7.9008],
        [  3.2938,  -4.6851],
        [  0.8945,  -2.4267],
        [ -9.6376,   7.7140],
        [ -8.8467,   7.4430],
        [ -2.5714,   1.1435],
        [ -9.8144,   8.4137],
        [  1.0829,  -3.1558],
        [ -1.6815,   0.2485],
        [  2.2847,  -3.9106],
        [ -6.0118,   3.3620],
        [ -4.6814,   2.7526],
        [  0.6337,  -3.9094],
        [-10.1348,   8.5061],
        [ -9.6691,   8.1117],
        [ -3.3794,   0.2915],
        [ -5.2137,   3.7287],
        [ -6.4153,   4.9705],
        [ -8.5661,   7.1167],
        [ -9.4099,   7.9027],
        [ -4.5215,   2.2339],
        [ -0.4754,  -1.1724],
        [ -3.9106,   2.4637],
        [  0.6896,  -2.0769],
        [ -9.9792,   8.3836],
        [ -3.6991,   1.4466],
        [ -5.7905,   1.1753],
        [-10.8327,   7.9149],
        [ -3.9695,   2.5564],
        [ -3.3342,   1.6737],
        [ -9.9314,   8.4536],
        [ -9.5741,   7.1317],
        [ -3.0995,   1.4227],
        [-10.6252,   8.4353],
        [ -2.5048,   0.5008],
        [ -1.7848,  -0.2382],
        [ -5.3252,   0.7100],
        [ -3.7812,   2.2945],
        [  1.2006,  -3.5597],
        [ -1.5572,   0.0850],
        [ -9.6493,   8.2626],
        [ -8.1533,   6.5296],
        [ -2.4501,   0.4585],
        [ -6.8310,   3.0854],
        [ -9.6798,   8.2197],
        [ -2.0874,  -0.5939],
        [-10.5191,   8.8150],
        [  0.6560,  -2.3909],
        [ -9.5572,   8.0597],
        [ -9.2155,   7.8186],
        [  0.1253,  -1.6826],
        [ -0.9618,  -0.4551],
        [ -5.9295,   3.8060],
        [  0.1578,  -1.5832],
        [ -8.3953,   6.7212],
        [ -6.3699,   4.8112],
        [ -2.0679,   0.3480],
        [ -9.4468,   8.0173],
        [ -6.3786,   4.6305],
        [ -1.5566,   0.0295],
        [ -1.2604,  -0.3090],
        [ -4.1833,   2.6551],
        [  1.0735,  -2.5559]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7176, 0.2824],
        [0.4319, 0.5681]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1947, 0.8053], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3016, 0.0975],
         [0.1429, 0.1978]],

        [[0.1537, 0.1065],
         [0.0692, 0.0704]],

        [[0.5144, 0.1132],
         [0.4960, 0.3929]],

        [[0.7918, 0.1017],
         [0.9842, 0.8132]],

        [[0.8209, 0.0955],
         [0.7777, 0.8475]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 72
Adjusted Rand Index: 0.18664835745220237
time is 1
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
time is 2
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 5
Adjusted Rand Index: 0.8080808080808081
time is 3
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.47229942931322816
Average Adjusted Rand Index: 0.7909429798089219
Iteration 0: Loss = -30576.666797534577
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.5512,    nan]],

        [[0.1669,    nan],
         [0.5591, 0.6988]],

        [[0.9725,    nan],
         [0.2356, 0.2070]],

        [[0.2403,    nan],
         [0.2722, 0.6722]],

        [[0.8111,    nan],
         [0.8490, 0.7345]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -30575.797253236044
Iteration 100: Loss = -11751.30360728859
Iteration 200: Loss = -11742.988219313524
Iteration 300: Loss = -11737.977623690229
Iteration 400: Loss = -11735.550053295343
Iteration 500: Loss = -11734.174228546546
Iteration 600: Loss = -11733.26038854713
Iteration 700: Loss = -11729.537051110656
Iteration 800: Loss = -11720.944679896902
Iteration 900: Loss = -11714.538068738031
Iteration 1000: Loss = -11708.73302411458
Iteration 1100: Loss = -11703.478175666005
Iteration 1200: Loss = -11697.229483730232
Iteration 1300: Loss = -11692.097734537832
Iteration 1400: Loss = -11683.97257132495
Iteration 1500: Loss = -11664.67048647904
Iteration 1600: Loss = -11649.890562937282
Iteration 1700: Loss = -11630.623628321933
Iteration 1800: Loss = -11603.829993520809
Iteration 1900: Loss = -11584.190006819592
Iteration 2000: Loss = -11577.411587892855
Iteration 2100: Loss = -11565.384639966316
Iteration 2200: Loss = -11553.200532015766
Iteration 2300: Loss = -11545.05226228353
Iteration 2400: Loss = -11544.348341453293
Iteration 2500: Loss = -11542.490167378925
Iteration 2600: Loss = -11536.033551354114
Iteration 2700: Loss = -11534.50272970551
Iteration 2800: Loss = -11534.440024951911
Iteration 2900: Loss = -11534.39822772346
Iteration 3000: Loss = -11534.364545900216
Iteration 3100: Loss = -11534.332552265161
Iteration 3200: Loss = -11534.273367320997
Iteration 3300: Loss = -11529.98190544484
Iteration 3400: Loss = -11522.864339014868
Iteration 3500: Loss = -11522.789639081004
Iteration 3600: Loss = -11522.767022353322
Iteration 3700: Loss = -11522.750841015153
Iteration 3800: Loss = -11522.73755262544
Iteration 3900: Loss = -11522.725878411771
Iteration 4000: Loss = -11522.714604239265
Iteration 4100: Loss = -11522.702441146961
Iteration 4200: Loss = -11522.680489831184
Iteration 4300: Loss = -11522.665786033707
Iteration 4400: Loss = -11522.657674031387
Iteration 4500: Loss = -11522.541814250726
Iteration 4600: Loss = -11522.51691647354
Iteration 4700: Loss = -11522.512010037255
Iteration 4800: Loss = -11522.507732997741
Iteration 4900: Loss = -11522.504796120234
Iteration 5000: Loss = -11522.500013970728
Iteration 5100: Loss = -11522.496707697017
Iteration 5200: Loss = -11522.49459332202
Iteration 5300: Loss = -11522.49097643917
Iteration 5400: Loss = -11522.488393061794
Iteration 5500: Loss = -11522.48619935696
Iteration 5600: Loss = -11522.483500425918
Iteration 5700: Loss = -11522.481089336956
Iteration 5800: Loss = -11522.478075292947
Iteration 5900: Loss = -11522.473959568062
Iteration 6000: Loss = -11522.421392345535
Iteration 6100: Loss = -11522.313565870765
Iteration 6200: Loss = -11522.309856300775
Iteration 6300: Loss = -11522.30699016668
Iteration 6400: Loss = -11522.304464574441
Iteration 6500: Loss = -11522.30358821021
Iteration 6600: Loss = -11522.29683116309
Iteration 6700: Loss = -11522.290645978872
Iteration 6800: Loss = -11522.281838312478
Iteration 6900: Loss = -11522.271961482702
Iteration 7000: Loss = -11522.25946998283
Iteration 7100: Loss = -11522.250538334183
Iteration 7200: Loss = -11522.245161747898
Iteration 7300: Loss = -11522.239395027047
Iteration 7400: Loss = -11522.23295974608
Iteration 7500: Loss = -11522.25377972543
1
Iteration 7600: Loss = -11520.46211443162
Iteration 7700: Loss = -11520.239841051709
Iteration 7800: Loss = -11520.122821371326
Iteration 7900: Loss = -11520.123596266667
1
Iteration 8000: Loss = -11519.82053761369
Iteration 8100: Loss = -11514.80979030058
Iteration 8200: Loss = -11514.803318161757
Iteration 8300: Loss = -11514.79786761262
Iteration 8400: Loss = -11514.761885600621
Iteration 8500: Loss = -11514.279062293786
Iteration 8600: Loss = -11514.147021091465
Iteration 8700: Loss = -11514.169422005896
1
Iteration 8800: Loss = -11514.188929178734
2
Iteration 8900: Loss = -11514.14133292857
Iteration 9000: Loss = -11514.130304747598
Iteration 9100: Loss = -11514.110604261385
Iteration 9200: Loss = -11514.11202802195
1
Iteration 9300: Loss = -11514.097277303756
Iteration 9400: Loss = -11513.907411046626
Iteration 9500: Loss = -11514.030650309187
1
Iteration 9600: Loss = -11513.878638511807
Iteration 9700: Loss = -11513.881873347362
1
Iteration 9800: Loss = -11513.875475514498
Iteration 9900: Loss = -11513.89655841401
1
Iteration 10000: Loss = -11513.874155680485
Iteration 10100: Loss = -11513.897807136318
1
Iteration 10200: Loss = -11510.437339628508
Iteration 10300: Loss = -11510.486284723556
1
Iteration 10400: Loss = -11510.471909564676
2
Iteration 10500: Loss = -11510.420326668222
Iteration 10600: Loss = -11510.40770095931
Iteration 10700: Loss = -11510.505247962265
1
Iteration 10800: Loss = -11510.383400355888
Iteration 10900: Loss = -11508.01017534692
Iteration 11000: Loss = -11508.164534907684
1
Iteration 11100: Loss = -11507.725625573597
Iteration 11200: Loss = -11507.78625210767
1
Iteration 11300: Loss = -11507.709190622678
Iteration 11400: Loss = -11507.699824949499
Iteration 11500: Loss = -11507.6888715266
Iteration 11600: Loss = -11507.636833608418
Iteration 11700: Loss = -11507.641086008041
1
Iteration 11800: Loss = -11507.626800639207
Iteration 11900: Loss = -11507.62780761995
1
Iteration 12000: Loss = -11507.626422077294
Iteration 12100: Loss = -11507.626974975985
1
Iteration 12200: Loss = -11507.625202394733
Iteration 12300: Loss = -11507.655040211625
1
Iteration 12400: Loss = -11507.62359993308
Iteration 12500: Loss = -11507.483385538942
Iteration 12600: Loss = -11507.44730013912
Iteration 12700: Loss = -11507.4472826425
Iteration 12800: Loss = -11506.204103663253
Iteration 12900: Loss = -11506.051293922988
Iteration 13000: Loss = -11505.995145169794
Iteration 13100: Loss = -11505.988110189952
Iteration 13200: Loss = -11505.987795305624
Iteration 13300: Loss = -11505.995908814873
1
Iteration 13400: Loss = -11506.009335114106
2
Iteration 13500: Loss = -11505.987476018436
Iteration 13600: Loss = -11505.98713253678
Iteration 13700: Loss = -11506.212215053605
1
Iteration 13800: Loss = -11505.98637314599
Iteration 13900: Loss = -11506.039778065877
1
Iteration 14000: Loss = -11505.98568904429
Iteration 14100: Loss = -11505.917245846456
Iteration 14200: Loss = -11505.883964254012
Iteration 14300: Loss = -11505.88410735016
1
Iteration 14400: Loss = -11505.884395081765
2
Iteration 14500: Loss = -11506.015946017536
3
Iteration 14600: Loss = -11505.87764880181
Iteration 14700: Loss = -11505.873607763551
Iteration 14800: Loss = -11505.874231791639
1
Iteration 14900: Loss = -11505.872822828373
Iteration 15000: Loss = -11488.954651257447
Iteration 15100: Loss = -11478.910600529865
Iteration 15200: Loss = -11478.879972529594
Iteration 15300: Loss = -11477.276180241444
Iteration 15400: Loss = -11477.244242816287
Iteration 15500: Loss = -11477.177946384078
Iteration 15600: Loss = -11469.850367574963
Iteration 15700: Loss = -11469.78930738963
Iteration 15800: Loss = -11469.766422346582
Iteration 15900: Loss = -11469.76314227133
Iteration 16000: Loss = -11469.777999247704
1
Iteration 16100: Loss = -11469.856227681217
2
Iteration 16200: Loss = -11469.76760601669
3
Iteration 16300: Loss = -11469.766924869777
4
Iteration 16400: Loss = -11469.764912743318
5
Iteration 16500: Loss = -11469.755880919149
Iteration 16600: Loss = -11468.75074530951
Iteration 16700: Loss = -11468.745580739218
Iteration 16800: Loss = -11468.796682320204
1
Iteration 16900: Loss = -11468.74955444393
2
Iteration 17000: Loss = -11468.742731720675
Iteration 17100: Loss = -11468.76510083043
1
Iteration 17200: Loss = -11468.750881290402
2
Iteration 17300: Loss = -11468.742991245013
3
Iteration 17400: Loss = -11468.813684880326
4
Iteration 17500: Loss = -11468.742270554083
Iteration 17600: Loss = -11468.7430670727
1
Iteration 17700: Loss = -11468.743409989538
2
Iteration 17800: Loss = -11468.738380100727
Iteration 17900: Loss = -11468.828245738061
1
Iteration 18000: Loss = -11468.745831931896
2
Iteration 18100: Loss = -11468.740532056074
3
Iteration 18200: Loss = -11468.738373577842
Iteration 18300: Loss = -11468.739345238686
1
Iteration 18400: Loss = -11468.846095446916
2
Iteration 18500: Loss = -11462.905104655707
Iteration 18600: Loss = -11462.897766717766
Iteration 18700: Loss = -11462.898146435911
1
Iteration 18800: Loss = -11462.897835892183
2
Iteration 18900: Loss = -11462.901220177964
3
Iteration 19000: Loss = -11462.944217234584
4
Iteration 19100: Loss = -11462.890435503004
Iteration 19200: Loss = -11462.88754549608
Iteration 19300: Loss = -11462.887644544438
1
Iteration 19400: Loss = -11462.892205829761
2
Iteration 19500: Loss = -11462.915677166555
3
Iteration 19600: Loss = -11462.909764870707
4
Iteration 19700: Loss = -11462.954588459468
5
Iteration 19800: Loss = -11462.901416964482
6
Iteration 19900: Loss = -11462.883798547527
tensor([[  5.9899,  -9.5002],
        [ -4.3404,   2.7282],
        [ -6.9428,   5.4920],
        [-11.2993,   7.2772],
        [ -6.7090,   4.7863],
        [  1.8166,  -3.7063],
        [ -6.7922,   4.6130],
        [ -2.7946,  -0.6542],
        [ -4.9401,   3.3331],
        [ -1.6429,  -0.4759],
        [ -6.2038,   4.7229],
        [  2.1023,  -3.4897],
        [ -5.6902,   3.0544],
        [ -7.6449,   5.6038],
        [ -5.9184,   4.2981],
        [-10.6354,   7.9820],
        [ -7.1411,   5.4335],
        [ -4.7898,   3.3695],
        [ -4.3003,   2.0618],
        [  0.8275,  -3.2691],
        [ -7.2459,   5.0283],
        [  1.9309,  -4.5484],
        [ -6.6996,   5.2895],
        [ -4.8365,   3.3123],
        [ -4.0292,   2.6351],
        [ -2.8799,   1.4728],
        [ -5.1698,   3.1755],
        [ -6.9294,   5.5045],
        [ -3.5781,   2.1896],
        [  3.2601,  -4.6467],
        [  2.6199,  -4.3064],
        [ -6.4684,   4.5200],
        [ -8.5051,   5.7463],
        [ -6.9807,   4.5211],
        [  1.6851,  -3.5660],
        [ -5.8162,   3.6376],
        [ -1.1959,  -0.5015],
        [ -1.9981,   0.5421],
        [  1.7408,  -4.1367],
        [ -2.1219,   0.6752],
        [ -4.0425,   2.3866],
        [ -5.9626,   3.2739],
        [  1.3554,  -3.0578],
        [ -7.7255,   6.3392],
        [  2.1492,  -3.5404],
        [  6.3301,  -8.0686],
        [  0.9668,  -2.3935],
        [ -4.9211,   3.5348],
        [ -6.9439,   5.3539],
        [ -1.3124,  -1.0251],
        [ -6.1625,   4.7007],
        [ -9.9998,   8.4722],
        [ -2.7926,   0.0976],
        [ -5.5453,   4.0029],
        [ -7.5204,   4.9609],
        [ -6.3768,   3.4054],
        [ -6.0148,   4.1914],
        [-10.1162,   6.9693],
        [ -4.2750,   2.4789],
        [ -4.1661,   1.7476],
        [ -1.8994,   0.5126],
        [ -3.9560,   2.1567],
        [ -0.4349,  -1.0854],
        [ -5.1679,   3.2690],
        [ -9.7087,   8.3206],
        [ -5.8314,   2.7330],
        [ -4.4886,   1.1058],
        [ -2.1287,   0.3603],
        [ -8.3767,   6.6848],
        [ -2.8618,   1.3680],
        [ -6.9677,   4.8070],
        [  0.2998,  -2.4234],
        [ -3.0765,   0.9418],
        [ -7.0243,   4.7796],
        [ -5.2691,   2.2737],
        [  1.6412,  -3.1038],
        [ -0.9720,  -0.6536],
        [ -3.8976,   2.4203],
        [ -7.5629,   6.0749],
        [ -3.6320,   1.2779],
        [ -5.2994,   3.8656],
        [ -7.7065,   6.3155],
        [ -2.8499,   0.6501],
        [ -5.2512,   3.5903],
        [ -4.0353,   2.5944],
        [ -9.5558,   7.9819],
        [ -7.8104,   6.1581],
        [ -2.7847,   1.3741],
        [ -2.5321,   1.0714],
        [ -6.6926,   3.7069],
        [ -1.5410,   0.0867],
        [ -4.7703,   2.3354],
        [ -6.8055,   4.9016],
        [ -2.2854,   0.8681],
        [ -5.3193,   3.6453],
        [ -4.4213,   3.0272],
        [ -2.3390,   0.9386],
        [  2.0926,  -3.4949],
        [ -0.9711,  -2.4774],
        [  1.8159,  -4.2384]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6990, 0.3010],
        [0.4400, 0.5600]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2058, 0.7942], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3036, 0.0943],
         [0.5512, 0.1979]],

        [[0.1669, 0.1066],
         [0.5591, 0.6988]],

        [[0.9725, 0.1157],
         [0.2356, 0.2070]],

        [[0.2403, 0.1018],
         [0.2722, 0.6722]],

        [[0.8111, 0.0956],
         [0.8490, 0.7345]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 71
Adjusted Rand Index: 0.16922329769455705
time is 1
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
time is 2
tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 6
Adjusted Rand Index: 0.7721212121212121
time is 3
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.47229942931322816
Average Adjusted Rand Index: 0.7802660486654737
11411.009477043353
new:  [0.9291444196810927, 0.9603129748486161, 0.47229942931322816, 0.47229942931322816] [0.9297715824899317, 0.9603198903330664, 0.7909429798089219, 0.7802660486654737] [11394.801219013381, 11390.93589717854, 11462.399552643315, 11462.881540325508]
prior:  [0.9291406205478928, 0.9214265637475184, 0.0, 0.0] [0.9299361303182314, 0.9225830052881783, 0.0, 0.0] [11395.99015079526, 11395.955180035544, nan, nan]
-----------------------------------------------------------------------------------------
This iteration is 18
True Objective function: Loss = -11010.59352298985
Iteration 0: Loss = -18621.899377382524
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.6507,    nan]],

        [[0.4131,    nan],
         [0.5231, 0.1511]],

        [[0.2092,    nan],
         [0.2752, 0.1979]],

        [[0.2383,    nan],
         [0.0331, 0.0574]],

        [[0.1325,    nan],
         [0.7338, 0.6303]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18729.55202592032
Iteration 100: Loss = -11225.13704409422
Iteration 200: Loss = -11161.395106985976
Iteration 300: Loss = -11107.939405013562
Iteration 400: Loss = -11107.382177274061
Iteration 500: Loss = -11107.230518153618
Iteration 600: Loss = -11107.14341900805
Iteration 700: Loss = -11105.58612842937
Iteration 800: Loss = -11095.453373530578
Iteration 900: Loss = -11095.418049637246
Iteration 1000: Loss = -11095.403579554573
Iteration 1100: Loss = -11094.432206207213
Iteration 1200: Loss = -11091.38584748987
Iteration 1300: Loss = -11086.530128527866
Iteration 1400: Loss = -11079.331514707295
Iteration 1500: Loss = -11071.669126646144
Iteration 1600: Loss = -11051.890436000962
Iteration 1700: Loss = -11045.414022454514
Iteration 1800: Loss = -11036.189032584347
Iteration 1900: Loss = -11027.816295912868
Iteration 2000: Loss = -11027.591586628443
Iteration 2100: Loss = -11017.93336336988
Iteration 2200: Loss = -11017.640544985417
Iteration 2300: Loss = -11017.164414360592
Iteration 2400: Loss = -11017.096965510491
Iteration 2500: Loss = -11017.126587644867
1
Iteration 2600: Loss = -11016.547466160124
Iteration 2700: Loss = -11016.491073530377
Iteration 2800: Loss = -11016.462523782659
Iteration 2900: Loss = -11016.448104744255
Iteration 3000: Loss = -11016.329836336929
Iteration 3100: Loss = -11015.295596960581
Iteration 3200: Loss = -11015.241190949651
Iteration 3300: Loss = -11013.69563346706
Iteration 3400: Loss = -11000.298185871905
Iteration 3500: Loss = -11000.269840946883
Iteration 3600: Loss = -11000.256185454013
Iteration 3700: Loss = -11000.251805508527
Iteration 3800: Loss = -11000.237847043605
Iteration 3900: Loss = -11000.241494796868
1
Iteration 4000: Loss = -11000.246027596037
2
Iteration 4100: Loss = -11000.232449190346
Iteration 4200: Loss = -11000.238028323127
1
Iteration 4300: Loss = -11000.231036643561
Iteration 4400: Loss = -11000.230158401597
Iteration 4500: Loss = -11000.263004771232
1
Iteration 4600: Loss = -11000.206433844767
Iteration 4700: Loss = -11000.20525893934
Iteration 4800: Loss = -11000.209287362874
1
Iteration 4900: Loss = -11000.202185804581
Iteration 5000: Loss = -11000.20329791074
1
Iteration 5100: Loss = -11000.200691897677
Iteration 5200: Loss = -11000.178848086189
Iteration 5300: Loss = -11000.156871890907
Iteration 5400: Loss = -11000.168621303364
1
Iteration 5500: Loss = -11000.156637221826
Iteration 5600: Loss = -11000.156560269434
Iteration 5700: Loss = -11000.15645515343
Iteration 5800: Loss = -11000.156296963583
Iteration 5900: Loss = -11000.187967342255
1
Iteration 6000: Loss = -11000.155886820134
Iteration 6100: Loss = -11000.15568877961
Iteration 6200: Loss = -11000.15490235755
Iteration 6300: Loss = -11000.15401616169
Iteration 6400: Loss = -11000.158450551982
1
Iteration 6500: Loss = -11000.15846125996
2
Iteration 6600: Loss = -11000.156031791928
3
Iteration 6700: Loss = -11000.154191675234
4
Iteration 6800: Loss = -11000.153485894947
Iteration 6900: Loss = -11000.15365896955
1
Iteration 7000: Loss = -11000.174396923243
2
Iteration 7100: Loss = -11000.153443933397
Iteration 7200: Loss = -11000.15369825572
1
Iteration 7300: Loss = -11000.15339805671
Iteration 7400: Loss = -11000.153604416852
1
Iteration 7500: Loss = -11000.16631793545
2
Iteration 7600: Loss = -11000.153416082307
3
Iteration 7700: Loss = -11000.16602706035
4
Iteration 7800: Loss = -11000.153923489512
5
Iteration 7900: Loss = -11000.15371867776
6
Iteration 8000: Loss = -11000.159729989864
7
Iteration 8100: Loss = -11000.153982650369
8
Iteration 8200: Loss = -11000.154856864463
9
Iteration 8300: Loss = -11000.153765877005
10
Stopping early at iteration 8300 due to no improvement.
tensor([[-9.3670,  4.7517],
        [-2.0583, -2.5569],
        [ 2.3045, -6.9197],
        [-5.6093,  0.9941],
        [-5.4956,  0.8804],
        [ 0.9800, -5.5952],
        [ 0.7843, -5.3995],
        [ 1.5843, -6.1996],
        [-6.3304,  1.7152],
        [-3.9851, -0.6301],
        [-3.9081, -0.7071],
        [-6.7569,  2.1417],
        [-4.2239, -0.3913],
        [-7.8874,  3.2722],
        [-4.7207,  0.1055],
        [-6.3760,  1.7608],
        [ 1.0180, -5.6332],
        [-4.3160, -0.2993],
        [-5.2033,  0.5881],
        [-3.5652, -1.0500],
        [-5.4353,  0.8201],
        [-1.2765, -3.3387],
        [-4.8247,  0.2095],
        [-1.0125, -3.6028],
        [-5.3650,  0.7498],
        [-5.0168,  0.4015],
        [-0.7911, -3.8241],
        [ 1.7143, -6.3295],
        [-5.0252,  0.4100],
        [-2.2172, -2.3980],
        [-6.3832,  1.7680],
        [-5.5424,  0.9272],
        [ 1.8310, -6.4462],
        [-4.4235, -0.1917],
        [-5.1834,  0.5682],
        [-0.2110, -4.4042],
        [-5.5969,  0.9816],
        [-4.2585, -0.3567],
        [-3.7280, -0.8872],
        [-0.1717, -4.4435],
        [-6.3192,  1.7040],
        [-1.1728, -3.4425],
        [-6.3238,  1.7086],
        [-7.3730,  2.7578],
        [-7.2400,  2.6248],
        [-6.2406,  1.6253],
        [-1.3771, -3.2381],
        [-8.0013,  3.3861],
        [-5.7559,  1.1407],
        [ 2.6767, -7.2919],
        [-4.4197, -0.1956],
        [-1.3139, -3.3014],
        [-2.4434, -2.1718],
        [-1.5787, -3.0365],
        [-5.3831,  0.7679],
        [-5.5876,  0.9723],
        [-3.1097, -1.5055],
        [-4.2353, -0.3799],
        [-4.5258, -0.0894],
        [-5.7135,  1.0983],
        [ 3.8723, -8.4876],
        [-7.4030,  2.7877],
        [-5.5218,  0.9066],
        [-0.3889, -4.2263],
        [-0.8987, -3.7165],
        [-5.9489,  1.3337],
        [-0.0125, -4.6027],
        [-8.8254,  4.2102],
        [-5.4979,  0.8827],
        [-6.2931,  1.6778],
        [-7.4775,  2.8623],
        [-5.5090,  0.8938],
        [ 1.6879, -6.3031],
        [-2.9105, -1.7047],
        [-4.3565, -0.2588],
        [-8.7030,  4.0877],
        [-5.6797,  1.0644],
        [-5.1247,  0.5095],
        [-5.9878,  1.3726],
        [-7.0095,  2.3943],
        [-4.4693, -0.1459],
        [-5.2471,  0.6318],
        [ 3.0651, -7.6803],
        [-0.8383, -3.7770],
        [-8.6961,  4.0808],
        [-6.1914,  1.5762],
        [-0.6436, -3.9716],
        [-2.5660, -2.0492],
        [ 0.8164, -5.4316],
        [-2.8227, -1.7925],
        [-8.7881,  4.1729],
        [-7.9635,  3.3483],
        [-7.5336,  2.9184],
        [-6.0836,  1.4684],
        [ 0.0757, -4.6909],
        [-5.0450,  0.4298],
        [-6.4439,  1.8287],
        [-0.2906, -4.3246],
        [-6.8058,  2.1906],
        [-4.2260, -0.3893]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7488, 0.2512],
        [0.2862, 0.7138]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3008, 0.6992], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2878, 0.0993],
         [0.6507, 0.1972]],

        [[0.4131, 0.0960],
         [0.5231, 0.1511]],

        [[0.2092, 0.0964],
         [0.2752, 0.1979]],

        [[0.2383, 0.1010],
         [0.0331, 0.0574]],

        [[0.1325, 0.1011],
         [0.7338, 0.6303]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 75
Adjusted Rand Index: 0.24242424242424243
time is 1
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208047711084835
time is 2
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824214784112464
Global Adjusted Rand Index: 0.45048693373721027
Average Adjusted Rand Index: 0.7931290012796475
Iteration 0: Loss = -20800.94802805685
Iteration 10: Loss = -11218.13158277109
Iteration 20: Loss = -11214.058966100247
Iteration 30: Loss = -11212.494673553074
Iteration 40: Loss = -11210.052525801857
Iteration 50: Loss = -11049.701798481196
Iteration 60: Loss = -10979.358417018837
Iteration 70: Loss = -10979.182770959786
Iteration 80: Loss = -10979.17959317874
Iteration 90: Loss = -10979.179693258608
1
Iteration 100: Loss = -10979.179754871253
2
Iteration 110: Loss = -10979.179779984874
3
Stopping early at iteration 109 due to no improvement.
pi: tensor([[0.7352, 0.2648],
        [0.2580, 0.7420]], dtype=torch.float64)
alpha: tensor([0.5153, 0.4847])
beta: tensor([[[0.1944, 0.0963],
         [0.9292, 0.2802]],

        [[0.6093, 0.0953],
         [0.6236, 0.8641]],

        [[0.1157, 0.0969],
         [0.9971, 0.0960]],

        [[0.3936, 0.1012],
         [0.8737, 0.4588]],

        [[0.5396, 0.1015],
         [0.5732, 0.7626]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 91
Adjusted Rand Index: 0.6690909090909091
time is 1
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824177928584268
time is 4
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824214784112464
Global Adjusted Rand Index: 0.868360588945443
Average Adjusted Rand Index: 0.8707849389629694
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20801.040218679325
Iteration 100: Loss = -11226.126102279408
Iteration 200: Loss = -11224.198101359005
Iteration 300: Loss = -11222.886616864433
Iteration 400: Loss = -11219.227125361456
Iteration 500: Loss = -11215.631579416235
Iteration 600: Loss = -11210.790981876486
Iteration 700: Loss = -11191.11156181433
Iteration 800: Loss = -11120.45613989149
Iteration 900: Loss = -11078.334446910723
Iteration 1000: Loss = -11061.835418947168
Iteration 1100: Loss = -11037.358372184923
Iteration 1200: Loss = -11027.97170176857
Iteration 1300: Loss = -11026.915767972425
Iteration 1400: Loss = -11025.730508148303
Iteration 1500: Loss = -11025.585495712043
Iteration 1600: Loss = -11025.535522586144
Iteration 1700: Loss = -11025.496197679015
Iteration 1800: Loss = -11020.662068721162
Iteration 1900: Loss = -11019.60169450482
Iteration 2000: Loss = -11019.582309047322
Iteration 2100: Loss = -11019.566705489533
Iteration 2200: Loss = -11019.552381892645
Iteration 2300: Loss = -11019.496181458155
Iteration 2400: Loss = -11019.484229940825
Iteration 2500: Loss = -11019.477288272294
Iteration 2600: Loss = -11019.471162148531
Iteration 2700: Loss = -11019.465505423557
Iteration 2800: Loss = -11019.46052653975
Iteration 2900: Loss = -11019.456308786672
Iteration 3000: Loss = -11019.452741859262
Iteration 3100: Loss = -11019.449578708654
Iteration 3200: Loss = -11019.44662933198
Iteration 3300: Loss = -11019.443806625632
Iteration 3400: Loss = -11019.44114227736
Iteration 3500: Loss = -11019.43822646796
Iteration 3600: Loss = -11019.42479762681
Iteration 3700: Loss = -11019.349722655412
Iteration 3800: Loss = -11019.347617952228
Iteration 3900: Loss = -11019.345276337854
Iteration 4000: Loss = -11019.341969256911
Iteration 4100: Loss = -11019.33532125013
Iteration 4200: Loss = -11019.30408613237
Iteration 4300: Loss = -11019.302826745707
Iteration 4400: Loss = -11019.301905672157
Iteration 4500: Loss = -11019.301055502876
Iteration 4600: Loss = -11019.300185837468
Iteration 4700: Loss = -11019.299298675167
Iteration 4800: Loss = -11019.298397222638
Iteration 4900: Loss = -11019.297426664009
Iteration 5000: Loss = -11019.296540878135
Iteration 5100: Loss = -11019.295736180276
Iteration 5200: Loss = -11019.295080727568
Iteration 5300: Loss = -11019.294325322971
Iteration 5400: Loss = -11019.293289630223
Iteration 5500: Loss = -11019.292417722789
Iteration 5600: Loss = -11019.291816395895
Iteration 5700: Loss = -11019.294307699285
1
Iteration 5800: Loss = -11019.290782746395
Iteration 5900: Loss = -11018.669304524356
Iteration 6000: Loss = -11018.662493199352
Iteration 6100: Loss = -11018.659477030915
Iteration 6200: Loss = -11017.397702420001
Iteration 6300: Loss = -11017.38034938706
Iteration 6400: Loss = -11017.379996952619
Iteration 6500: Loss = -11017.37963329548
Iteration 6600: Loss = -11017.379564499148
Iteration 6700: Loss = -11017.378963213301
Iteration 6800: Loss = -11017.37855117401
Iteration 6900: Loss = -11017.378036319418
Iteration 7000: Loss = -11017.377098305178
Iteration 7100: Loss = -11017.373134591706
Iteration 7200: Loss = -11017.368885303234
Iteration 7300: Loss = -11017.364183840664
Iteration 7400: Loss = -11017.355419192672
Iteration 7500: Loss = -11017.320170990222
Iteration 7600: Loss = -11017.309355847457
Iteration 7700: Loss = -11017.284115553928
Iteration 7800: Loss = -11017.255511939295
Iteration 7900: Loss = -11017.25560557555
1
Iteration 8000: Loss = -11017.259739144258
2
Iteration 8100: Loss = -11017.250109680142
Iteration 8200: Loss = -11017.251414065682
1
Iteration 8300: Loss = -11017.224462935405
Iteration 8400: Loss = -11017.220221758744
Iteration 8500: Loss = -11017.218078043476
Iteration 8600: Loss = -11017.244201889498
1
Iteration 8700: Loss = -11017.21349443218
Iteration 8800: Loss = -11017.21091120126
Iteration 8900: Loss = -11017.223201425937
1
Iteration 9000: Loss = -11017.133480461001
Iteration 9100: Loss = -11017.087990403554
Iteration 9200: Loss = -11017.084716600326
Iteration 9300: Loss = -11017.084750020646
1
Iteration 9400: Loss = -11017.084584430098
Iteration 9500: Loss = -11017.084375842238
Iteration 9600: Loss = -11017.140296970445
1
Iteration 9700: Loss = -11013.778540696116
Iteration 9800: Loss = -11013.735715682446
Iteration 9900: Loss = -11011.066279513545
Iteration 10000: Loss = -11010.75706967746
Iteration 10100: Loss = -11001.99274586673
Iteration 10200: Loss = -11001.932703471508
Iteration 10300: Loss = -11001.930584594116
Iteration 10400: Loss = -11001.933213378401
1
Iteration 10500: Loss = -11001.93024805111
Iteration 10600: Loss = -11001.9301173635
Iteration 10700: Loss = -11002.071520959702
1
Iteration 10800: Loss = -11001.929119527009
Iteration 10900: Loss = -11001.973769548847
1
Iteration 11000: Loss = -11001.92798361083
Iteration 11100: Loss = -11001.977893864214
1
Iteration 11200: Loss = -11001.926897728412
Iteration 11300: Loss = -11002.315613123286
1
Iteration 11400: Loss = -11001.91754891154
Iteration 11500: Loss = -11001.923093986445
1
Iteration 11600: Loss = -11000.3263440334
Iteration 11700: Loss = -11000.315052930213
Iteration 11800: Loss = -11000.314282090043
Iteration 11900: Loss = -11000.313072508112
Iteration 12000: Loss = -11000.193987477383
Iteration 12100: Loss = -11000.192522446581
Iteration 12200: Loss = -11000.273567618415
1
Iteration 12300: Loss = -11000.192180461108
Iteration 12400: Loss = -11000.22725781591
1
Iteration 12500: Loss = -11000.317204587915
2
Iteration 12600: Loss = -11000.187682411268
Iteration 12700: Loss = -11000.196081121636
1
Iteration 12800: Loss = -11000.187880378713
2
Iteration 12900: Loss = -11000.188380802072
3
Iteration 13000: Loss = -11000.191946237595
4
Iteration 13100: Loss = -11000.318606086694
5
Iteration 13200: Loss = -11000.187948445626
6
Iteration 13300: Loss = -11000.189883262985
7
Iteration 13400: Loss = -11000.193128743897
8
Iteration 13500: Loss = -11000.187983816691
9
Iteration 13600: Loss = -11000.23742226219
10
Stopping early at iteration 13600 due to no improvement.
tensor([[ 6.1929, -8.5175],
        [-1.0373, -0.5397],
        [-6.9243,  2.3091],
        [ 2.5910, -4.0039],
        [ 2.4303, -3.9626],
        [-5.1187,  1.4517],
        [-3.9584,  2.2801],
        [-4.6475,  3.1500],
        [ 2.9669, -5.0901],
        [ 0.8295, -2.5370],
        [ 0.9096, -2.3022],
        [ 3.7587, -5.1532],
        [ 1.1023, -2.7053],
        [ 4.8958, -6.2826],
        [ 1.6785, -3.1596],
        [ 2.9492, -5.1863],
        [-4.4725,  2.1766],
        [ 1.2755, -2.7588],
        [ 2.1684, -3.6368],
        [ 0.3457, -2.1815],
        [ 2.2818, -3.9909],
        [-2.8770, -0.8225],
        [ 1.4531, -3.5959],
        [-2.1189,  0.4813],
        [ 2.3145, -3.7556],
        [ 1.9435, -3.4636],
        [-2.3475,  0.6814],
        [-4.7157,  3.3277],
        [ 1.8187, -3.6145],
        [-2.2678, -2.0912],
        [ 3.3571, -4.8101],
        [ 2.1702, -4.2683],
        [-5.5090,  2.7479],
        [ 0.9315, -3.2950],
        [ 0.9006, -4.8675],
        [-2.7854,  1.3991],
        [ 1.3179, -5.2766],
        [ 0.9182, -2.9965],
        [-0.2848, -3.1376],
        [-3.0612,  1.1928],
        [ 3.1942, -4.8431],
        [-1.8419,  0.4189],
        [ 3.2363, -4.8097],
        [ 3.6677, -6.4828],
        [ 4.1722, -5.7022],
        [ 6.5415, -9.8643],
        [-1.6429,  0.2541],
        [ 4.8662, -6.5507],
        [ 2.5665, -4.3248],
        [-6.1016,  3.9227],
        [ 1.3053, -2.9271],
        [-1.9093,  0.0679],
        [-0.6473, -0.9156],
        [-2.3599, -0.8608],
        [ 2.1531, -3.9658],
        [ 2.0851, -4.4818],
        [-0.0808, -1.7062],
        [ 1.1411, -2.7356],
        [ 1.5071, -2.9007],
        [ 6.5369, -8.4924],
        [-4.6981,  3.3083],
        [ 4.1670, -7.0366],
        [ 2.4258, -3.9953],
        [-4.4770,  3.0559],
        [-2.3345,  0.4686],
        [ 1.7225, -5.5742],
        [-3.0393,  1.5352],
        [ 5.4231, -7.9001],
        [ 2.4553, -3.9408],
        [ 6.6928, -8.1409],
        [ 4.3728, -5.9782],
        [ 2.4481, -3.9712],
        [-4.7526,  3.2413],
        [-0.4573, -1.6859],
        [ 0.9535, -3.1565],
        [ 5.3538, -7.8165],
        [ 2.5053, -4.2187],
        [ 2.1153, -3.5296],
        [ 1.3506, -5.9658],
        [ 3.7798, -5.6270],
        [ 0.5700, -3.7632],
        [ 2.1758, -3.7107],
        [-9.0450,  5.7968],
        [-2.3243,  0.6033],
        [ 5.8647, -7.3976],
        [ 2.3279, -5.4573],
        [-2.3833,  0.9340],
        [-0.9550, -1.4857],
        [-3.9475,  2.2977],
        [-0.1767, -1.2151],
        [ 4.5633, -6.5829],
        [ 7.4115, -9.0754],
        [ 4.5176, -5.9119],
        [ 3.0732, -4.4955],
        [-3.1532,  1.6371],
        [ 2.0078, -3.4695],
        [ 3.4383, -4.8510],
        [-2.7777,  1.2935],
        [ 2.6104, -6.3961],
        [ 1.1215, -2.7264]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7119, 0.2881],
        [0.2508, 0.7492]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6975, 0.3025], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1977, 0.0988],
         [0.9292, 0.2870]],

        [[0.6093, 0.0955],
         [0.6236, 0.8641]],

        [[0.1157, 0.0960],
         [0.9971, 0.0960]],

        [[0.3936, 0.1005],
         [0.8737, 0.4588]],

        [[0.5396, 0.1007],
         [0.5732, 0.7626]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 25
Adjusted Rand Index: 0.24242424242424243
time is 1
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208047711084835
time is 2
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824214784112464
Global Adjusted Rand Index: 0.45048693373721027
Average Adjusted Rand Index: 0.7931290012796475
Iteration 0: Loss = -24420.78022394994
Iteration 10: Loss = -11223.7730282822
Iteration 20: Loss = -11223.11473268639
Iteration 30: Loss = -11215.574528272038
Iteration 40: Loss = -11212.84291448056
Iteration 50: Loss = -11213.113239726563
1
Iteration 60: Loss = -11213.95912256251
2
Iteration 70: Loss = -11201.231642883873
Iteration 80: Loss = -10988.224638459114
Iteration 90: Loss = -10979.22740030362
Iteration 100: Loss = -10979.180742029412
Iteration 110: Loss = -10979.179766410723
Iteration 120: Loss = -10979.179773172491
1
Iteration 130: Loss = -10979.179778488857
2
Iteration 140: Loss = -10979.179794378497
3
Stopping early at iteration 139 due to no improvement.
pi: tensor([[0.7352, 0.2648],
        [0.2580, 0.7420]], dtype=torch.float64)
alpha: tensor([0.5153, 0.4847])
beta: tensor([[[0.1944, 0.0963],
         [0.5441, 0.2802]],

        [[0.4690, 0.0953],
         [0.5917, 0.2774]],

        [[0.4449, 0.0969],
         [0.3771, 0.0621]],

        [[0.9153, 0.1012],
         [0.9735, 0.5044]],

        [[0.6629, 0.1015],
         [0.2846, 0.4221]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 91
Adjusted Rand Index: 0.6690909090909091
time is 1
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824177928584268
time is 4
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824214784112464
Global Adjusted Rand Index: 0.868360588945443
Average Adjusted Rand Index: 0.8707849389629694
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24265.510230027583
Iteration 100: Loss = -11227.018205705794
Iteration 200: Loss = -11224.578277397424
Iteration 300: Loss = -11223.401128243357
Iteration 400: Loss = -11220.406680742517
Iteration 500: Loss = -11219.015254572512
Iteration 600: Loss = -11216.831167348997
Iteration 700: Loss = -11207.83399290498
Iteration 800: Loss = -11185.85081795911
Iteration 900: Loss = -11129.04540106281
Iteration 1000: Loss = -11093.896658175609
Iteration 1100: Loss = -11081.781569328485
Iteration 1200: Loss = -11066.101569000859
Iteration 1300: Loss = -11059.365995689177
Iteration 1400: Loss = -11055.357537003916
Iteration 1500: Loss = -11051.30968588615
Iteration 1600: Loss = -11051.135835459394
Iteration 1700: Loss = -11049.541369446102
Iteration 1800: Loss = -11042.912836971997
Iteration 1900: Loss = -11039.85013790899
Iteration 2000: Loss = -11039.725638789132
Iteration 2100: Loss = -11037.414310933544
Iteration 2200: Loss = -11037.167618525076
Iteration 2300: Loss = -11036.470112145691
Iteration 2400: Loss = -11036.441434008022
Iteration 2500: Loss = -11036.418339510197
Iteration 2600: Loss = -11036.398659602364
Iteration 2700: Loss = -11036.384192153444
Iteration 2800: Loss = -11032.853869193299
Iteration 2900: Loss = -11032.666289189168
Iteration 3000: Loss = -11032.633312521375
Iteration 3100: Loss = -11028.42963155739
Iteration 3200: Loss = -11028.273669823908
Iteration 3300: Loss = -11028.26761058999
Iteration 3400: Loss = -11028.262308884592
Iteration 3500: Loss = -11028.258294444197
Iteration 3600: Loss = -11028.254255177735
Iteration 3700: Loss = -11028.250788100924
Iteration 3800: Loss = -11028.247190222712
Iteration 3900: Loss = -11028.242812541226
Iteration 4000: Loss = -11028.235572756867
Iteration 4100: Loss = -11028.215650354667
Iteration 4200: Loss = -11028.230701147118
1
Iteration 4300: Loss = -11028.189567893689
Iteration 4400: Loss = -11025.556795983244
Iteration 4500: Loss = -11025.452923598748
Iteration 4600: Loss = -11025.430258334478
Iteration 4700: Loss = -11025.422242046197
Iteration 4800: Loss = -11025.38901239607
Iteration 4900: Loss = -11025.387551606958
Iteration 5000: Loss = -11025.38647252145
Iteration 5100: Loss = -11025.385516695194
Iteration 5200: Loss = -11025.386646888208
1
Iteration 5300: Loss = -11025.383912953595
Iteration 5400: Loss = -11025.383219494239
Iteration 5500: Loss = -11025.382547630446
Iteration 5600: Loss = -11025.381959372566
Iteration 5700: Loss = -11025.381316083587
Iteration 5800: Loss = -11025.384643553905
1
Iteration 5900: Loss = -11025.38010700098
Iteration 6000: Loss = -11025.379534175465
Iteration 6100: Loss = -11025.381749993385
1
Iteration 6200: Loss = -11025.37435426849
Iteration 6300: Loss = -11025.367628512537
Iteration 6400: Loss = -11025.366874070281
Iteration 6500: Loss = -11025.3666286226
Iteration 6600: Loss = -11017.662629913546
Iteration 6700: Loss = -11017.659898102505
Iteration 6800: Loss = -11017.641677720914
Iteration 6900: Loss = -11017.635182631251
Iteration 7000: Loss = -11017.627008549043
Iteration 7100: Loss = -11017.624290998308
Iteration 7200: Loss = -11017.623862361563
Iteration 7300: Loss = -11017.62753505857
1
Iteration 7400: Loss = -11017.617486808045
Iteration 7500: Loss = -11017.61797705121
1
Iteration 7600: Loss = -11017.618925837003
2
Iteration 7700: Loss = -11017.61774090267
3
Iteration 7800: Loss = -11017.616727455801
Iteration 7900: Loss = -11017.616093258253
Iteration 8000: Loss = -11017.60580361002
Iteration 8100: Loss = -11017.605407058103
Iteration 8200: Loss = -11017.610210528124
1
Iteration 8300: Loss = -11017.614142400897
2
Iteration 8400: Loss = -11016.390333373989
Iteration 8500: Loss = -11016.381511353797
Iteration 8600: Loss = -11016.363965801234
Iteration 8700: Loss = -11016.366077207274
1
Iteration 8800: Loss = -11016.375931420771
2
Iteration 8900: Loss = -11016.359552182308
Iteration 9000: Loss = -11016.358701225048
Iteration 9100: Loss = -11016.259054864715
Iteration 9200: Loss = -11016.237850026751
Iteration 9300: Loss = -11016.237835220083
Iteration 9400: Loss = -11016.237716660724
Iteration 9500: Loss = -11016.23761369659
Iteration 9600: Loss = -11016.241315331428
1
Iteration 9700: Loss = -11016.237479893096
Iteration 9800: Loss = -11016.237421084585
Iteration 9900: Loss = -11016.237545050202
1
Iteration 10000: Loss = -11016.237248681267
Iteration 10100: Loss = -11016.220081396412
Iteration 10200: Loss = -11013.513012698493
Iteration 10300: Loss = -11013.430517487392
Iteration 10400: Loss = -11013.407289581683
Iteration 10500: Loss = -11013.388199594161
Iteration 10600: Loss = -11013.403598339566
1
Iteration 10700: Loss = -11013.380436200427
Iteration 10800: Loss = -11013.38275316982
1
Iteration 10900: Loss = -11013.379749687612
Iteration 11000: Loss = -11013.321157817205
Iteration 11100: Loss = -11013.320775031858
Iteration 11200: Loss = -11013.32417969751
1
Iteration 11300: Loss = -11013.320569234418
Iteration 11400: Loss = -11013.295501272529
Iteration 11500: Loss = -11013.294718014444
Iteration 11600: Loss = -11013.29595706866
1
Iteration 11700: Loss = -11013.293649543224
Iteration 11800: Loss = -11013.31997548708
1
Iteration 11900: Loss = -11013.286920321765
Iteration 12000: Loss = -11013.284471993582
Iteration 12100: Loss = -11013.283441743377
Iteration 12200: Loss = -11013.282780940383
Iteration 12300: Loss = -11013.289171722412
1
Iteration 12400: Loss = -11013.282711039172
Iteration 12500: Loss = -11013.283293028053
1
Iteration 12600: Loss = -11013.282902775207
2
Iteration 12700: Loss = -11013.27783363448
Iteration 12800: Loss = -11013.262310233184
Iteration 12900: Loss = -11013.261607613476
Iteration 13000: Loss = -11013.262929823908
1
Iteration 13100: Loss = -11013.261902348399
2
Iteration 13200: Loss = -11013.262748972385
3
Iteration 13300: Loss = -11013.262622030172
4
Iteration 13400: Loss = -11013.26270364545
5
Iteration 13500: Loss = -11013.261638788292
6
Iteration 13600: Loss = -11013.263898275192
7
Iteration 13700: Loss = -11013.268893730754
8
Iteration 13800: Loss = -11013.105751098235
Iteration 13900: Loss = -11000.466447566236
Iteration 14000: Loss = -11000.459923840779
Iteration 14100: Loss = -11000.446127892475
Iteration 14200: Loss = -11000.399427395565
Iteration 14300: Loss = -11000.39947168687
1
Iteration 14400: Loss = -11000.398572446124
Iteration 14500: Loss = -11000.280293445961
Iteration 14600: Loss = -11000.234488456937
Iteration 14700: Loss = -11000.232412983125
Iteration 14800: Loss = -11000.246234275644
1
Iteration 14900: Loss = -11000.232116192214
Iteration 15000: Loss = -11000.23772800873
1
Iteration 15100: Loss = -11000.224260079043
Iteration 15200: Loss = -11000.318315963666
1
Iteration 15300: Loss = -11000.223640240412
Iteration 15400: Loss = -11000.22253820289
Iteration 15500: Loss = -11000.22224078592
Iteration 15600: Loss = -11000.22621403377
1
Iteration 15700: Loss = -11000.218657547332
Iteration 15800: Loss = -11000.193956769803
Iteration 15900: Loss = -11000.1880399753
Iteration 16000: Loss = -11000.188403714252
1
Iteration 16100: Loss = -11000.190056166994
2
Iteration 16200: Loss = -11000.186520309097
Iteration 16300: Loss = -11000.18640517913
Iteration 16400: Loss = -11000.185950935418
Iteration 16500: Loss = -11000.184489584832
Iteration 16600: Loss = -11000.184593039026
1
Iteration 16700: Loss = -11000.18443088161
Iteration 16800: Loss = -11000.266534993438
1
Iteration 16900: Loss = -11000.184419076078
Iteration 17000: Loss = -11000.189658328853
1
Iteration 17100: Loss = -11000.184431292386
2
Iteration 17200: Loss = -11000.185895250721
3
Iteration 17300: Loss = -11000.184979571177
4
Iteration 17400: Loss = -11000.184453149514
5
Iteration 17500: Loss = -11000.18851204235
6
Iteration 17600: Loss = -11000.18532967498
7
Iteration 17700: Loss = -11000.213765891767
8
Iteration 17800: Loss = -11000.184416455768
Iteration 17900: Loss = -11000.184333616464
Iteration 18000: Loss = -11000.183304089054
Iteration 18100: Loss = -11000.16783828022
Iteration 18200: Loss = -11000.167415464755
Iteration 18300: Loss = -11000.169670793824
1
Iteration 18400: Loss = -11000.167115557726
Iteration 18500: Loss = -11000.169994396769
1
Iteration 18600: Loss = -11000.16814865002
2
Iteration 18700: Loss = -11000.1669723372
Iteration 18800: Loss = -11000.162650425551
Iteration 18900: Loss = -11000.162117072023
Iteration 19000: Loss = -11000.214287908366
1
Iteration 19100: Loss = -11000.163376923772
2
Iteration 19200: Loss = -11000.16521843955
3
Iteration 19300: Loss = -11000.161969502162
Iteration 19400: Loss = -11000.162104580462
1
Iteration 19500: Loss = -11000.16189296526
Iteration 19600: Loss = -11000.161315499463
Iteration 19700: Loss = -11000.165385994682
1
Iteration 19800: Loss = -11000.232098741144
2
Iteration 19900: Loss = -11000.160687140915
tensor([[  7.6054,  -9.0194],
        [ -1.5864,  -1.0811],
        [ -6.3022,   2.9218],
        [  2.0694,  -4.5204],
        [  8.4073, -11.4928],
        [ -4.6546,   1.9277],
        [ -4.8156,   1.3690],
        [ -5.3833,   2.4045],
        [  3.1471,  -4.8952],
        [  0.9551,  -2.3867],
        [  0.8963,  -2.2877],
        [  2.7687,  -6.1273],
        [  1.1623,  -2.6548],
        [  3.9999,  -7.1604],
        [  1.5676,  -3.2415],
        [  3.2249,  -4.9095],
        [ -4.5260,   2.1266],
        [  1.2012,  -2.8002],
        [  2.1992,  -3.5871],
        [  0.1846,  -2.3164],
        [  2.0973,  -4.1494],
        [ -1.8831,   0.1871],
        [  1.5760,  -3.4511],
        [ -2.3048,   0.2936],
        [  2.2300,  -3.8652],
        [  1.7994,  -3.5995],
        [ -2.4587,   0.5776],
        [ -4.9112,   3.1315],
        [  1.9291,  -3.4884],
        [ -0.8313,  -0.6390],
        [  3.3143,  -4.8279],
        [  1.7499,  -4.7013],
        [ -6.0465,   2.2201],
        [  1.3975,  -2.8146],
        [  1.8868,  -3.8417],
        [ -3.9068,   0.2940],
        [  2.5213,  -4.0487],
        [  0.8265,  -3.0621],
        [  0.7156,  -2.1071],
        [ -4.1914,   0.0853],
        [  3.3154,  -4.7043],
        [ -2.7845,  -0.5125],
        [  2.7879,  -5.2440],
        [  4.1487,  -5.9838],
        [  8.2643, -10.5506],
        [  1.7484,  -6.1128],
        [ -1.7769,   0.0851],
        [  4.9489,  -6.4512],
        [  2.7055,  -4.1824],
        [ -8.9355,   6.7837],
        [  1.1571,  -3.0476],
        [ -1.7048,   0.2936],
        [ -1.1840,  -1.4425],
        [ -2.0998,  -0.6330],
        [  2.3049,  -3.8251],
        [  2.3083,  -4.2381],
        [  0.0598,  -1.5322],
        [  0.5881,  -3.2456],
        [  1.4092,  -3.0161],
        [  2.5567,  -4.2534],
        [ -5.1407,   2.8218],
        [  4.8279,  -6.3586],
        [  2.4575,  -3.9538],
        [ -2.9372,   0.9022],
        [ -2.1079,   0.7194],
        [  2.8719,  -4.3975],
        [ -4.6028,  -0.0124],
        [  6.0481,  -7.4656],
        [  2.4012,  -3.9770],
        [  3.2835,  -4.6840],
        [  4.4756,  -5.8619],
        [  1.8998,  -4.4978],
        [ -4.9805,   3.0088],
        [ -1.0541,  -2.2529],
        [  0.5918,  -3.4914],
        [  5.5722,  -7.6167],
        [  2.5811,  -4.1521],
        [  2.1214,  -3.5100],
        [  2.0403,  -5.3132],
        [  8.4068, -10.2133],
        [  1.3460,  -2.9677],
        [  2.1586,  -3.7048],
        [ -7.3241,   3.4628],
        [ -2.1653,   0.7789],
        [  5.5404,  -7.7064],
        [  2.7474,  -5.0233],
        [ -2.4243,   0.9103],
        [ -0.6537,  -1.1618],
        [ -4.1803,   2.0671],
        [ -0.2848,  -1.3093],
        [  4.8554,  -6.2740],
        [  4.5775,  -6.7332],
        [  4.5205,  -5.9350],
        [  2.8156,  -4.7336],
        [ -3.5925,   1.1794],
        [  1.9425,  -3.5182],
        [  3.2252,  -5.0458],
        [ -2.7154,   1.3213],
        [  3.7771,  -5.2176],
        [  0.5035,  -3.3138]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7165, 0.2835],
        [0.2498, 0.7502]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7014, 0.2986], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1965, 0.0988],
         [0.5441, 0.2888]],

        [[0.4690, 0.0967],
         [0.5917, 0.2774]],

        [[0.4449, 0.0968],
         [0.3771, 0.0621]],

        [[0.9153, 0.1009],
         [0.9735, 0.5044]],

        [[0.6629, 0.1016],
         [0.2846, 0.4221]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 25
Adjusted Rand Index: 0.24242424242424243
time is 1
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208047711084835
time is 2
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824214784112464
Global Adjusted Rand Index: 0.45048693373721027
Average Adjusted Rand Index: 0.7931290012796475
Iteration 0: Loss = -15415.800497163951
Iteration 10: Loss = -11018.560967266852
Iteration 20: Loss = -10979.376603937662
Iteration 30: Loss = -10979.20596765387
Iteration 40: Loss = -10979.185149961846
Iteration 50: Loss = -10979.181191645554
Iteration 60: Loss = -10979.180213606165
Iteration 70: Loss = -10979.17990184081
Iteration 80: Loss = -10979.179828375802
Iteration 90: Loss = -10979.179793669067
Iteration 100: Loss = -10979.17979187494
Iteration 110: Loss = -10979.179785057891
Iteration 120: Loss = -10979.179785392786
1
Iteration 130: Loss = -10979.17979242756
2
Iteration 140: Loss = -10979.179793556406
3
Stopping early at iteration 139 due to no improvement.
pi: tensor([[0.7420, 0.2580],
        [0.2648, 0.7352]], dtype=torch.float64)
alpha: tensor([0.4847, 0.5153])
beta: tensor([[[0.2802, 0.0963],
         [0.9628, 0.1944]],

        [[0.3655, 0.0953],
         [0.8365, 0.8560]],

        [[0.0951, 0.0969],
         [0.3732, 0.2939]],

        [[0.0468, 0.1012],
         [0.1343, 0.3464]],

        [[0.8175, 0.1015],
         [0.6245, 0.2384]]], dtype=torch.float64)
time is 0
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 9
Adjusted Rand Index: 0.6690909090909091
time is 1
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824177928584268
time is 4
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824214784112464
Global Adjusted Rand Index: 0.868360588945443
Average Adjusted Rand Index: 0.8707849389629694
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -15415.17607181551
Iteration 100: Loss = -11219.728028129639
Iteration 200: Loss = -11212.237623224331
Iteration 300: Loss = -11169.11342462971
Iteration 400: Loss = -11041.526612976184
Iteration 500: Loss = -11033.85634676396
Iteration 600: Loss = -11031.641500484422
Iteration 700: Loss = -11024.915784221748
Iteration 800: Loss = -11024.83179788928
Iteration 900: Loss = -11024.791067237176
Iteration 1000: Loss = -11024.746945596753
Iteration 1100: Loss = -11024.644187008074
Iteration 1200: Loss = -11023.926483776902
Iteration 1300: Loss = -11019.762738491834
Iteration 1400: Loss = -11019.579138159243
Iteration 1500: Loss = -11019.562238460569
Iteration 1600: Loss = -11019.555330746853
Iteration 1700: Loss = -11019.547724386519
Iteration 1800: Loss = -11019.538929988656
Iteration 1900: Loss = -11019.507178814989
Iteration 2000: Loss = -11019.28023917289
Iteration 2100: Loss = -11018.95823802366
Iteration 2200: Loss = -11018.955924266462
Iteration 2300: Loss = -11018.954038888645
Iteration 2400: Loss = -11018.952412911982
Iteration 2500: Loss = -11018.95090860699
Iteration 2600: Loss = -11018.94882481078
Iteration 2700: Loss = -11018.937028996479
Iteration 2800: Loss = -11018.93502832978
Iteration 2900: Loss = -11018.934171262741
Iteration 3000: Loss = -11018.933236255438
Iteration 3100: Loss = -11018.915094429938
Iteration 3200: Loss = -11018.87714904988
Iteration 3300: Loss = -11018.876126867906
Iteration 3400: Loss = -11018.87461579524
Iteration 3500: Loss = -11018.87266323071
Iteration 3600: Loss = -11018.870591838831
Iteration 3700: Loss = -11018.860943331767
Iteration 3800: Loss = -11000.470073561468
Iteration 3900: Loss = -11000.234484166498
Iteration 4000: Loss = -11000.226790712017
Iteration 4100: Loss = -11000.211611487572
Iteration 4200: Loss = -11000.20380190736
Iteration 4300: Loss = -11000.203227472588
Iteration 4400: Loss = -11000.202866787356
Iteration 4500: Loss = -11000.202888024525
1
Iteration 4600: Loss = -11000.203333428673
2
Iteration 4700: Loss = -11000.201705330706
Iteration 4800: Loss = -11000.201040551972
Iteration 4900: Loss = -11000.199334101297
Iteration 5000: Loss = -11000.199330608513
Iteration 5100: Loss = -11000.198397602477
Iteration 5200: Loss = -11000.191455013828
Iteration 5300: Loss = -11000.18440275155
Iteration 5400: Loss = -11000.169808193748
Iteration 5500: Loss = -11000.168549111526
Iteration 5600: Loss = -11000.16830263727
Iteration 5700: Loss = -11000.16842591467
1
Iteration 5800: Loss = -11000.168354601137
2
Iteration 5900: Loss = -11000.167917217692
Iteration 6000: Loss = -11000.16773151772
Iteration 6100: Loss = -11000.167059340554
Iteration 6200: Loss = -11000.167584733854
1
Iteration 6300: Loss = -11000.166460943607
Iteration 6400: Loss = -11000.17791132078
1
Iteration 6500: Loss = -11000.166362458205
Iteration 6600: Loss = -11000.167873036917
1
Iteration 6700: Loss = -11000.166184740712
Iteration 6800: Loss = -11000.165919249837
Iteration 6900: Loss = -11000.16651565424
1
Iteration 7000: Loss = -11000.165798821765
Iteration 7100: Loss = -11000.165861896456
1
Iteration 7200: Loss = -11000.165917737195
2
Iteration 7300: Loss = -11000.165754865771
Iteration 7400: Loss = -11000.17020804473
1
Iteration 7500: Loss = -11000.168768054853
2
Iteration 7600: Loss = -11000.165733783966
Iteration 7700: Loss = -11000.223877832179
1
Iteration 7800: Loss = -11000.165671880248
Iteration 7900: Loss = -11000.166141418838
1
Iteration 8000: Loss = -11000.161622950667
Iteration 8100: Loss = -11000.160471372948
Iteration 8200: Loss = -11000.160453910412
Iteration 8300: Loss = -11000.161872805511
1
Iteration 8400: Loss = -11000.160541954701
2
Iteration 8500: Loss = -11000.160788317758
3
Iteration 8600: Loss = -11000.160575156617
4
Iteration 8700: Loss = -11000.161185735285
5
Iteration 8800: Loss = -11000.162660646754
6
Iteration 8900: Loss = -11000.163573341932
7
Iteration 9000: Loss = -11000.160886963975
8
Iteration 9100: Loss = -11000.21509567509
9
Iteration 9200: Loss = -11000.160304286213
Iteration 9300: Loss = -11000.184935499958
1
Iteration 9400: Loss = -11000.1602483368
Iteration 9500: Loss = -11000.160691018653
1
Iteration 9600: Loss = -11000.160240463423
Iteration 9700: Loss = -11000.226310716691
1
Iteration 9800: Loss = -11000.160120787225
Iteration 9900: Loss = -11000.160261651708
1
Iteration 10000: Loss = -11000.160631416407
2
Iteration 10100: Loss = -11000.160055123772
Iteration 10200: Loss = -11000.16007719656
1
Iteration 10300: Loss = -11000.184213270259
2
Iteration 10400: Loss = -11000.15996842063
Iteration 10500: Loss = -11000.162314258605
1
Iteration 10600: Loss = -11000.2314157748
2
Iteration 10700: Loss = -11000.165194071145
3
Iteration 10800: Loss = -11000.166430095427
4
Iteration 10900: Loss = -11000.160008298482
5
Iteration 11000: Loss = -11000.163010522678
6
Iteration 11100: Loss = -11000.352344407473
7
Iteration 11200: Loss = -11000.159913725634
Iteration 11300: Loss = -11000.16272103038
1
Iteration 11400: Loss = -11000.23522290013
2
Iteration 11500: Loss = -11000.160054437747
3
Iteration 11600: Loss = -11000.16144387802
4
Iteration 11700: Loss = -11000.164977054266
5
Iteration 11800: Loss = -11000.201911306673
6
Iteration 11900: Loss = -11000.159900489049
Iteration 12000: Loss = -11000.157617740113
Iteration 12100: Loss = -11000.174767074779
1
Iteration 12200: Loss = -11000.165383352967
2
Iteration 12300: Loss = -11000.19055176588
3
Iteration 12400: Loss = -11000.160608102356
4
Iteration 12500: Loss = -11000.15780216205
5
Iteration 12600: Loss = -11000.181128403794
6
Iteration 12700: Loss = -11000.153620716479
Iteration 12800: Loss = -11000.154059604634
1
Iteration 12900: Loss = -11000.160501034765
2
Iteration 13000: Loss = -11000.152805034799
Iteration 13100: Loss = -11000.155585645869
1
Iteration 13200: Loss = -11000.169583464014
2
Iteration 13300: Loss = -11000.15246998428
Iteration 13400: Loss = -11000.153830670464
1
Iteration 13500: Loss = -11000.15471074131
2
Iteration 13600: Loss = -11000.154717115247
3
Iteration 13700: Loss = -11000.152473955051
4
Iteration 13800: Loss = -11000.15946589195
5
Iteration 13900: Loss = -11000.156651313206
6
Iteration 14000: Loss = -11000.15456581677
7
Iteration 14100: Loss = -11000.15525425595
8
Iteration 14200: Loss = -11000.163669635827
9
Iteration 14300: Loss = -11000.15247562704
10
Stopping early at iteration 14300 due to no improvement.
tensor([[ -9.3386,   6.8272],
        [ -0.9720,  -1.4699],
        [  3.8998,  -5.3246],
        [ -4.0316,   2.5716],
        [ -5.4954,   0.8802],
        [  2.4218,  -4.1529],
        [  1.9424,  -4.2406],
        [  2.8246,  -4.9590],
        [ -4.8359,   3.2094],
        [ -2.4085,   0.9459],
        [ -2.5133,   0.6881],
        [ -5.1927,   3.7054],
        [ -2.6334,   1.1988],
        [ -6.9355,   4.2239],
        [ -3.6839,   1.1420],
        [ -5.8929,   2.2442],
        [  2.3721,  -4.2782],
        [ -2.7567,   1.2597],
        [ -3.6553,   2.1358],
        [ -2.7250,  -0.2091],
        [ -4.0134,   2.2419],
        [ -1.2759,  -3.3394],
        [ -3.6351,   1.3990],
        [  0.5856,  -2.0052],
        [ -3.9660,   2.1488],
        [ -3.5279,   1.8903],
        [  0.0742,  -2.9600],
        [  3.3194,  -4.7245],
        [ -3.4779,   1.9572],
        [ -0.6231,  -0.8052],
        [ -5.2512,   2.8997],
        [ -3.9386,   2.5310],
        [  3.3414,  -4.9252],
        [ -2.8236,   1.4084],
        [ -3.8254,   1.9261],
        [  1.3407,  -2.8531],
        [ -3.9822,   2.5959],
        [ -2.6466,   1.2551],
        [ -2.4191,   0.4210],
        [  1.0903,  -3.1814],
        [ -5.5149,   2.5079],
        [ -0.5168,  -2.7878],
        [ -4.8471,   3.1849],
        [ -6.4407,   3.6905],
        [-10.3260,   6.8098],
        [ -4.7397,   3.1261],
        [ -1.3776,  -3.2376],
        [ -6.4536,   4.9453],
        [ -4.7042,   2.1921],
        [  3.4504,  -6.5165],
        [ -2.9562,   1.2680],
        [  0.0986,  -1.8889],
        [ -1.0876,  -0.8171],
        [ -0.4080,  -1.8661],
        [ -4.2131,   1.9290],
        [ -4.3479,   2.2114],
        [ -2.1597,  -0.5533],
        [ -2.6214,   1.2341],
        [ -3.4062,   1.0301],
        [ -4.2600,   2.5513],
        [  2.8573,  -5.1025],
        [ -7.8962,   3.2810],
        [ -3.9398,   2.4893],
        [  1.2144,  -2.6224],
        [  0.7132,  -2.1040],
        [ -4.3821,   2.9003],
        [  1.4915,  -3.0988],
        [ -7.6468,   5.8626],
        [ -4.1698,   2.2107],
        [ -4.7328,   3.2375],
        [ -6.7920,   3.5459],
        [ -4.0320,   2.3707],
        [  2.4513,  -5.5394],
        [ -1.4191,  -0.2134],
        [ -3.7355,   0.3623],
        [ -7.7891,   5.3972],
        [ -4.0655,   2.6784],
        [ -3.6454,   1.9886],
        [ -4.3846,   2.9757],
        [ -5.4348,   3.9680],
        [ -2.8566,   1.4671],
        [ -3.6330,   2.2456],
        [  4.4286,  -6.3116],
        [ -0.0180,  -2.9571],
        [ -8.6345,   4.6268],
        [ -4.5783,   3.1892],
        [  0.9102,  -2.4180],
        [ -1.1824,  -0.6658],
        [  2.1488,  -4.1010],
        [ -1.2155,  -0.1864],
        [ -6.2583,   4.8717],
        [ -7.1146,   4.2005],
        [ -5.9544,   4.4970],
        [ -4.6416,   2.9106],
        [  0.6734,  -4.0926],
        [ -3.6826,   1.7920],
        [ -4.9075,   3.3654],
        [  0.9914,  -3.0429],
        [ -5.3877,   3.6082],
        [ -2.7007,   1.1361]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7494, 0.2506],
        [0.2861, 0.7139]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3007, 0.6993], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2878, 0.0993],
         [0.9628, 0.1972]],

        [[0.3655, 0.0960],
         [0.8365, 0.8560]],

        [[0.0951, 0.0964],
         [0.3732, 0.2939]],

        [[0.0468, 0.1010],
         [0.1343, 0.3464]],

        [[0.8175, 0.1011],
         [0.6245, 0.2384]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 75
Adjusted Rand Index: 0.24242424242424243
time is 1
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208047711084835
time is 2
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824214784112464
Global Adjusted Rand Index: 0.45048693373721027
Average Adjusted Rand Index: 0.7931290012796475
Iteration 0: Loss = -21868.64685443276
Iteration 10: Loss = -11210.026033707236
Iteration 20: Loss = -11068.113101671237
Iteration 30: Loss = -10979.423916793032
Iteration 40: Loss = -10979.184586285879
Iteration 50: Loss = -10979.17986608124
Iteration 60: Loss = -10979.179782778801
Iteration 70: Loss = -10979.179801552607
1
Iteration 80: Loss = -10979.179793572783
2
Iteration 90: Loss = -10979.179789595893
3
Stopping early at iteration 89 due to no improvement.
pi: tensor([[0.7352, 0.2648],
        [0.2580, 0.7420]], dtype=torch.float64)
alpha: tensor([0.5153, 0.4847])
beta: tensor([[[0.1944, 0.0963],
         [0.4255, 0.2802]],

        [[0.8779, 0.0953],
         [0.4062, 0.2266]],

        [[0.2369, 0.0969],
         [0.6596, 0.9098]],

        [[0.9937, 0.1012],
         [0.0604, 0.3754]],

        [[0.0210, 0.1015],
         [0.7624, 0.5705]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 91
Adjusted Rand Index: 0.6690909090909091
time is 1
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824177928584268
time is 4
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824214784112464
Global Adjusted Rand Index: 0.868360588945443
Average Adjusted Rand Index: 0.8707849389629694
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21868.690737038774
Iteration 100: Loss = -11102.69204700048
Iteration 200: Loss = -11037.47301140455
Iteration 300: Loss = -11019.056988345053
Iteration 400: Loss = -11018.426194218106
Iteration 500: Loss = -11018.140448489663
Iteration 600: Loss = -11017.983154150255
Iteration 700: Loss = -11017.886186300153
Iteration 800: Loss = -11017.822704344582
Iteration 900: Loss = -11017.777561324636
Iteration 1000: Loss = -11017.743657385472
Iteration 1100: Loss = -11017.717077429603
Iteration 1200: Loss = -11017.695770359991
Iteration 1300: Loss = -11017.67774125924
Iteration 1400: Loss = -11017.661298701274
Iteration 1500: Loss = -11017.64368791471
Iteration 1600: Loss = -11017.616494369915
Iteration 1700: Loss = -11017.331524760542
Iteration 1800: Loss = -11017.31970592975
Iteration 1900: Loss = -11017.307712938486
Iteration 2000: Loss = -11017.291422871043
Iteration 2100: Loss = -11017.263298670123
Iteration 2200: Loss = -11017.235983433458
Iteration 2300: Loss = -11017.197013016763
Iteration 2400: Loss = -11017.136881684879
Iteration 2500: Loss = -11017.040978599849
Iteration 2600: Loss = -11016.899944156587
Iteration 2700: Loss = -11016.756219976045
Iteration 2800: Loss = -11016.656306139133
Iteration 2900: Loss = -11015.545533362982
Iteration 3000: Loss = -11013.587707484432
Iteration 3100: Loss = -11012.349022100145
Iteration 3200: Loss = -11007.20789031774
Iteration 3300: Loss = -10988.63205658922
Iteration 3400: Loss = -10985.030694946456
Iteration 3500: Loss = -10984.969145330007
Iteration 3600: Loss = -10976.498414218038
Iteration 3700: Loss = -10976.485548665267
Iteration 3800: Loss = -10976.477016123874
Iteration 3900: Loss = -10976.473950794014
Iteration 4000: Loss = -10976.473019134864
Iteration 4100: Loss = -10976.470263222414
Iteration 4200: Loss = -10976.469024849548
Iteration 4300: Loss = -10976.46902284537
Iteration 4400: Loss = -10976.467044025601
Iteration 4500: Loss = -10976.466167381108
Iteration 4600: Loss = -10976.46747496238
1
Iteration 4700: Loss = -10976.464000892802
Iteration 4800: Loss = -10976.462755506249
Iteration 4900: Loss = -10976.462778962456
1
Iteration 5000: Loss = -10976.46178046308
Iteration 5100: Loss = -10976.461420561875
Iteration 5200: Loss = -10976.46252806687
1
Iteration 5300: Loss = -10976.46217196693
2
Iteration 5400: Loss = -10976.460016559144
Iteration 5500: Loss = -10976.458479062396
Iteration 5600: Loss = -10976.402877867471
Iteration 5700: Loss = -10975.862918022607
Iteration 5800: Loss = -10975.862119140325
Iteration 5900: Loss = -10975.861498855784
Iteration 6000: Loss = -10975.667387031292
Iteration 6100: Loss = -10974.316947394784
Iteration 6200: Loss = -10974.31525264122
Iteration 6300: Loss = -10974.31245671326
Iteration 6400: Loss = -10974.311436986245
Iteration 6500: Loss = -10974.317405988448
1
Iteration 6600: Loss = -10974.310928803996
Iteration 6700: Loss = -10974.311020190007
1
Iteration 6800: Loss = -10974.31048549703
Iteration 6900: Loss = -10974.3100681514
Iteration 7000: Loss = -10974.307478433431
Iteration 7100: Loss = -10974.292975466322
Iteration 7200: Loss = -10974.30123410967
1
Iteration 7300: Loss = -10974.29320521002
2
Iteration 7400: Loss = -10974.29623109599
3
Iteration 7500: Loss = -10974.293468157137
4
Iteration 7600: Loss = -10974.292840612843
Iteration 7700: Loss = -10974.295238791663
1
Iteration 7800: Loss = -10974.299485620253
2
Iteration 7900: Loss = -10974.301071337753
3
Iteration 8000: Loss = -10974.331955873531
4
Iteration 8100: Loss = -10974.292365542367
Iteration 8200: Loss = -10974.292453461701
1
Iteration 8300: Loss = -10974.297690134166
2
Iteration 8400: Loss = -10974.292283085279
Iteration 8500: Loss = -10974.292309462002
1
Iteration 8600: Loss = -10974.292222076294
Iteration 8700: Loss = -10974.292343827337
1
Iteration 8800: Loss = -10974.292227005732
2
Iteration 8900: Loss = -10974.39349050265
3
Iteration 9000: Loss = -10974.292191106784
Iteration 9100: Loss = -10974.2921667533
Iteration 9200: Loss = -10974.33461307693
1
Iteration 9300: Loss = -10974.292146045245
Iteration 9400: Loss = -10974.292140873771
Iteration 9500: Loss = -10974.292376092588
1
Iteration 9600: Loss = -10974.29215076044
2
Iteration 9700: Loss = -10974.29212251527
Iteration 9800: Loss = -10974.357390114572
1
Iteration 9900: Loss = -10974.292117040011
Iteration 10000: Loss = -10974.292086895133
Iteration 10100: Loss = -10974.29567427154
1
Iteration 10200: Loss = -10974.292091449903
2
Iteration 10300: Loss = -10974.292088562685
3
Iteration 10400: Loss = -10974.292070848616
Iteration 10500: Loss = -10974.291989305
Iteration 10600: Loss = -10974.300192176397
1
Iteration 10700: Loss = -10974.291958960954
Iteration 10800: Loss = -10974.291891418436
Iteration 10900: Loss = -10974.292611775027
1
Iteration 11000: Loss = -10974.291920797355
2
Iteration 11100: Loss = -10974.292157621016
3
Iteration 11200: Loss = -10974.291916321255
4
Iteration 11300: Loss = -10974.291897749283
5
Iteration 11400: Loss = -10974.368021403123
6
Iteration 11500: Loss = -10974.29190514849
7
Iteration 11600: Loss = -10974.29189234828
8
Iteration 11700: Loss = -10974.295764189845
9
Iteration 11800: Loss = -10974.291841402326
Iteration 11900: Loss = -10974.291867962122
1
Iteration 12000: Loss = -10974.29224261286
2
Iteration 12100: Loss = -10974.291867987404
3
Iteration 12200: Loss = -10974.745019273525
4
Iteration 12300: Loss = -10974.291864422612
5
Iteration 12400: Loss = -10974.291871797966
6
Iteration 12500: Loss = -10974.738417430439
7
Iteration 12600: Loss = -10974.291899783402
8
Iteration 12700: Loss = -10974.291241297906
Iteration 12800: Loss = -10974.296500674616
1
Iteration 12900: Loss = -10974.291284908686
2
Iteration 13000: Loss = -10974.369852245072
3
Iteration 13100: Loss = -10974.291272375718
4
Iteration 13200: Loss = -10974.291766110307
5
Iteration 13300: Loss = -10974.291279909201
6
Iteration 13400: Loss = -10974.291211201707
Iteration 13500: Loss = -10974.291291350773
1
Iteration 13600: Loss = -10974.296074403821
2
Iteration 13700: Loss = -10974.291362925058
3
Iteration 13800: Loss = -10974.291225440496
4
Iteration 13900: Loss = -10974.323377181636
5
Iteration 14000: Loss = -10974.289954115004
Iteration 14100: Loss = -10974.335703292474
1
Iteration 14200: Loss = -10974.289945305276
Iteration 14300: Loss = -10974.315245634005
1
Iteration 14400: Loss = -10974.2899612774
2
Iteration 14500: Loss = -10974.401450644504
3
Iteration 14600: Loss = -10974.289952808926
4
Iteration 14700: Loss = -10974.298005315104
5
Iteration 14800: Loss = -10974.289943796422
Iteration 14900: Loss = -10974.289957638617
1
Iteration 15000: Loss = -10974.289949040769
2
Iteration 15100: Loss = -10974.289948710222
3
Iteration 15200: Loss = -10974.290058212471
4
Iteration 15300: Loss = -10974.289923750051
Iteration 15400: Loss = -10974.29001668466
1
Iteration 15500: Loss = -10974.29013804107
2
Iteration 15600: Loss = -10974.288856301042
Iteration 15700: Loss = -10974.289245812266
1
Iteration 15800: Loss = -10974.288854158076
Iteration 15900: Loss = -10974.290097920262
1
Iteration 16000: Loss = -10974.288772629521
Iteration 16100: Loss = -10974.348288116595
1
Iteration 16200: Loss = -10974.288793467795
2
Iteration 16300: Loss = -10974.29102590475
3
Iteration 16400: Loss = -10974.288770443423
Iteration 16500: Loss = -10974.288845112173
1
Iteration 16600: Loss = -10974.290414534065
2
Iteration 16700: Loss = -10974.288820905616
3
Iteration 16800: Loss = -10974.326534509188
4
Iteration 16900: Loss = -10974.288783138063
5
Iteration 17000: Loss = -10974.290887393227
6
Iteration 17100: Loss = -10974.2887803131
7
Iteration 17200: Loss = -10974.289302180377
8
Iteration 17300: Loss = -10974.288788219093
9
Iteration 17400: Loss = -10974.288872070278
10
Stopping early at iteration 17400 due to no improvement.
tensor([[-7.9124,  5.1575],
        [ 1.4322, -5.4528],
        [ 4.5116, -6.6240],
        [-3.3133,  1.7770],
        [-2.9716,  1.5414],
        [ 2.0616, -3.5380],
        [ 3.0165, -5.2656],
        [ 4.8014, -8.2975],
        [-0.6592, -1.5831],
        [ 2.9599, -5.4202],
        [-5.1398,  3.3946],
        [-2.0256,  0.5508],
        [ 4.0999, -6.4131],
        [-5.9229,  3.0540],
        [-4.1818,  2.3064],
        [-5.9892,  2.6353],
        [ 3.2508, -7.8660],
        [ 4.1928, -6.0754],
        [-5.9508,  4.5465],
        [-0.6368, -1.7698],
        [-3.2749,  1.4670],
        [ 2.9252, -4.5271],
        [-0.2947, -1.1053],
        [ 1.6949, -6.3101],
        [ 2.6182, -6.7177],
        [-0.7742, -0.7068],
        [ 4.3450, -6.3913],
        [ 1.6587, -6.0942],
        [ 1.9342, -4.0373],
        [ 3.2681, -4.7655],
        [ 3.0468, -4.5238],
        [-0.2848, -1.2488],
        [ 2.1915, -5.6142],
        [-1.6995,  0.0982],
        [ 0.8557, -2.5181],
        [ 2.3953, -5.0966],
        [-3.0618,  1.1145],
        [ 3.8111, -5.2056],
        [ 3.1832, -4.7952],
        [ 5.7674, -7.3176],
        [-3.5383,  1.8079],
        [ 5.4618, -7.4368],
        [-5.6440,  4.2554],
        [-4.6054,  2.6998],
        [-5.5150,  3.9570],
        [-5.9560,  1.3408],
        [ 1.2621, -2.7242],
        [ 0.2799, -1.8690],
        [ 2.6298, -4.3293],
        [ 3.9109, -5.6063],
        [ 2.0497, -3.4524],
        [ 1.7794, -4.8607],
        [ 5.3363, -7.2266],
        [ 2.2964, -4.7003],
        [-3.5182,  1.0503],
        [ 3.6936, -5.8532],
        [ 0.7081, -2.3518],
        [ 3.0239, -4.8532],
        [ 1.4999, -3.0511],
        [-5.5051,  4.0524],
        [ 3.6706, -5.2690],
        [-2.9547,  1.0032],
        [-3.1566,  1.4869],
        [ 3.0458, -4.4338],
        [ 0.4494, -2.9926],
        [-3.4859,  2.0888],
        [ 2.4427, -3.8415],
        [-7.1561,  4.1364],
        [-4.0430,  1.9248],
        [-3.0429, -0.5309],
        [-4.8373,  3.1259],
        [-5.4350,  3.8836],
        [ 1.4765, -2.8690],
        [ 2.3307, -3.7170],
        [-3.9251,  1.8475],
        [-8.3926,  6.6565],
        [ 2.4822, -3.9576],
        [-5.1118,  2.7118],
        [-4.1033,  2.5330],
        [-7.4340,  4.9112],
        [ 2.9707, -4.3798],
        [ 2.4819, -3.8857],
        [ 5.1580, -7.0113],
        [ 1.9632, -3.9801],
        [-6.0503,  4.6019],
        [-7.0132,  4.7807],
        [ 2.1406, -3.9227],
        [ 1.9826, -4.3925],
        [ 2.0189, -3.6194],
        [ 3.7042, -5.5394],
        [-6.7550,  5.2946],
        [-8.3807,  6.5651],
        [-6.6355,  3.2452],
        [-5.4151,  1.9749],
        [ 3.3289, -4.8008],
        [ 1.5101, -3.0563],
        [-5.1478,  3.2696],
        [ 3.0129, -4.9377],
        [-5.2619,  3.0692],
        [-2.7930,  1.2085]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7475, 0.2525],
        [0.2429, 0.7571]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5748, 0.4252], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1974, 0.0969],
         [0.4255, 0.2873]],

        [[0.8779, 0.0952],
         [0.4062, 0.2266]],

        [[0.2369, 0.0965],
         [0.6596, 0.9098]],

        [[0.9937, 0.1010],
         [0.0604, 0.3754]],

        [[0.0210, 0.1012],
         [0.7624, 0.5705]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 91
Adjusted Rand Index: 0.6690909090909091
time is 1
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 2
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824214784112464
Global Adjusted Rand Index: 0.8909177813915478
Average Adjusted Rand Index: 0.8943015998275572
11010.59352298985
new:  [0.45048693373721027, 0.45048693373721027, 0.45048693373721027, 0.8909177813915478] [0.7931290012796475, 0.7931290012796475, 0.7931290012796475, 0.8943015998275572] [11000.23742226219, 11000.193326174362, 11000.15247562704, 10974.288872070278]
prior:  [0.868360588945443, 0.868360588945443, 0.868360588945443, 0.868360588945443] [0.8707849389629694, 0.8707849389629694, 0.8707849389629694, 0.8707849389629694] [10979.179779984874, 10979.179794378497, 10979.179793556406, 10979.179789595893]
-----------------------------------------------------------------------------------------
This iteration is 19
True Objective function: Loss = -10981.483068808191
Iteration 0: Loss = -21256.359574886363
Iteration 10: Loss = -11117.774810165938
Iteration 20: Loss = -11117.7748209628
1
Iteration 30: Loss = -11117.735766843361
Iteration 40: Loss = -11114.300673028303
Iteration 50: Loss = -11113.531056937552
Iteration 60: Loss = -11113.491698176324
Iteration 70: Loss = -11113.468560926829
Iteration 80: Loss = -11113.40189114771
Iteration 90: Loss = -11113.386788713786
Iteration 100: Loss = -11113.385081019507
Iteration 110: Loss = -11113.384764023534
Iteration 120: Loss = -11113.384697492893
Iteration 130: Loss = -11113.384671115778
Iteration 140: Loss = -11113.384698990943
1
Iteration 150: Loss = -11113.384698158074
2
Iteration 160: Loss = -11113.384673476094
3
Stopping early at iteration 159 due to no improvement.
pi: tensor([[0.9689, 0.0311],
        [0.9489, 0.0511]], dtype=torch.float64)
alpha: tensor([0.9684, 0.0316])
beta: tensor([[[0.1631, 0.0862],
         [0.8043, 0.2389]],

        [[0.9812, 0.2392],
         [0.5738, 0.0090]],

        [[0.1527, 0.0889],
         [0.8449, 0.1428]],

        [[0.1355, 0.2249],
         [0.5147, 0.2149]],

        [[0.7808, 0.2347],
         [0.7211, 0.2968]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: -0.00491035742834304
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 62
Adjusted Rand Index: 0.01126387000386094
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0025538476364862913
Global Adjusted Rand Index: 0.00018409493839990484
Average Adjusted Rand Index: 0.0013285254804579867
tensor([[-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21172.99886673687
Iteration 100: Loss = -11120.049951047231
Iteration 200: Loss = -11118.371300166285
Iteration 300: Loss = -11117.918711729331
Iteration 400: Loss = -11117.594088503569
Iteration 500: Loss = -11116.96358793678
Iteration 600: Loss = -11113.402638138732
Iteration 700: Loss = -11112.561603711456
Iteration 800: Loss = -11112.175299807579
Iteration 900: Loss = -11111.98146082957
Iteration 1000: Loss = -11111.840346463412
Iteration 1100: Loss = -11111.730081669928
Iteration 1200: Loss = -11111.63659210403
Iteration 1300: Loss = -11111.553173041199
Iteration 1400: Loss = -11111.477970998025
Iteration 1500: Loss = -11111.410454859757
Iteration 1600: Loss = -11111.350675261638
Iteration 1700: Loss = -11111.298581143705
Iteration 1800: Loss = -11111.254061399892
Iteration 1900: Loss = -11111.216639629938
Iteration 2000: Loss = -11111.185357210517
Iteration 2100: Loss = -11111.159394035027
Iteration 2200: Loss = -11111.137898201023
Iteration 2300: Loss = -11111.119630594441
Iteration 2400: Loss = -11111.10300360917
Iteration 2500: Loss = -11110.966010901842
Iteration 2600: Loss = -11109.586126632754
Iteration 2700: Loss = -11109.488947115125
Iteration 2800: Loss = -11109.459738318585
Iteration 2900: Loss = -11109.444219586007
Iteration 3000: Loss = -11109.43315527575
Iteration 3100: Loss = -11109.41763837301
Iteration 3200: Loss = -11109.41039291076
Iteration 3300: Loss = -11109.405423142794
Iteration 3400: Loss = -11109.399311456502
Iteration 3500: Loss = -11109.393656317197
Iteration 3600: Loss = -11109.326738618807
Iteration 3700: Loss = -11053.213697008114
Iteration 3800: Loss = -11029.49318165165
Iteration 3900: Loss = -11005.086012990789
Iteration 4000: Loss = -10994.177973196485
Iteration 4100: Loss = -10993.383050189825
Iteration 4200: Loss = -10987.492564029786
Iteration 4300: Loss = -10987.256042799929
Iteration 4400: Loss = -10987.25182068424
Iteration 4500: Loss = -10987.248783673267
Iteration 4600: Loss = -10987.244372083194
Iteration 4700: Loss = -10987.245336931437
1
Iteration 4800: Loss = -10987.242009134188
Iteration 4900: Loss = -10987.240891863701
Iteration 5000: Loss = -10987.239752346977
Iteration 5100: Loss = -10987.236013935963
Iteration 5200: Loss = -10987.229999681484
Iteration 5300: Loss = -10987.23517497319
1
Iteration 5400: Loss = -10987.22600778632
Iteration 5500: Loss = -10987.224440674243
Iteration 5600: Loss = -10987.222639016623
Iteration 5700: Loss = -10987.223949130403
1
Iteration 5800: Loss = -10987.206351372155
Iteration 5900: Loss = -10986.52286964045
Iteration 6000: Loss = -10986.044416303526
Iteration 6100: Loss = -10984.940981887388
Iteration 6200: Loss = -10984.599893374216
Iteration 6300: Loss = -10984.590822003951
Iteration 6400: Loss = -10984.511371795554
Iteration 6500: Loss = -10984.364883963925
Iteration 6600: Loss = -10984.329642004266
Iteration 6700: Loss = -10984.29848857082
Iteration 6800: Loss = -10984.29795944371
Iteration 6900: Loss = -10984.290384189035
Iteration 7000: Loss = -10984.284785344247
Iteration 7100: Loss = -10984.276893980648
Iteration 7200: Loss = -10984.274482859328
Iteration 7300: Loss = -10984.27929635683
1
Iteration 7400: Loss = -10984.27264147917
Iteration 7500: Loss = -10984.260763039767
Iteration 7600: Loss = -10984.28315941458
1
Iteration 7700: Loss = -10984.254008818043
Iteration 7800: Loss = -10984.23548094551
Iteration 7900: Loss = -10984.263378310288
1
Iteration 8000: Loss = -10984.23435375247
Iteration 8100: Loss = -10984.236925543526
1
Iteration 8200: Loss = -10984.252413189664
2
Iteration 8300: Loss = -10984.23143014863
Iteration 8400: Loss = -10984.23129029488
Iteration 8500: Loss = -10984.25425877815
1
Iteration 8600: Loss = -10984.231067340448
Iteration 8700: Loss = -10984.230746874267
Iteration 8800: Loss = -10984.238368539029
1
Iteration 8900: Loss = -10984.23299095042
2
Iteration 9000: Loss = -10984.229093229811
Iteration 9100: Loss = -10984.243927735966
1
Iteration 9200: Loss = -10984.153090576187
Iteration 9300: Loss = -10984.157186880917
1
Iteration 9400: Loss = -10983.810866248505
Iteration 9500: Loss = -10983.809211072466
Iteration 9600: Loss = -10983.804846259243
Iteration 9700: Loss = -10983.810277806928
1
Iteration 9800: Loss = -10983.803447695167
Iteration 9900: Loss = -10983.800852084634
Iteration 10000: Loss = -10983.801267801524
1
Iteration 10100: Loss = -10983.826033152094
2
Iteration 10200: Loss = -10983.800513279435
Iteration 10300: Loss = -10983.800578271297
1
Iteration 10400: Loss = -10983.811277964785
2
Iteration 10500: Loss = -10983.800454475817
Iteration 10600: Loss = -10983.811179377384
1
Iteration 10700: Loss = -10983.800368947846
Iteration 10800: Loss = -10983.912580328879
1
Iteration 10900: Loss = -10983.800296575866
Iteration 11000: Loss = -10983.961997287412
1
Iteration 11100: Loss = -10983.800298896691
2
Iteration 11200: Loss = -10983.800429901818
3
Iteration 11300: Loss = -10983.800311065002
4
Iteration 11400: Loss = -10983.800247380595
Iteration 11500: Loss = -10983.800839406273
1
Iteration 11600: Loss = -10983.800259533946
2
Iteration 11700: Loss = -10983.807151997029
3
Iteration 11800: Loss = -10983.801698511237
4
Iteration 11900: Loss = -10983.844950659317
5
Iteration 12000: Loss = -10983.800237238976
Iteration 12100: Loss = -10983.800389822356
1
Iteration 12200: Loss = -10983.818622350092
2
Iteration 12300: Loss = -10983.800225016457
Iteration 12400: Loss = -10983.800291652355
1
Iteration 12500: Loss = -10983.804524089512
2
Iteration 12600: Loss = -10983.809545268603
3
Iteration 12700: Loss = -10983.817741908193
4
Iteration 12800: Loss = -10983.80290338145
5
Iteration 12900: Loss = -10983.799986924534
Iteration 13000: Loss = -10983.80730470682
1
Iteration 13100: Loss = -10983.799991555747
2
Iteration 13200: Loss = -10983.80222366181
3
Iteration 13300: Loss = -10983.80256710035
4
Iteration 13400: Loss = -10983.79999815108
5
Iteration 13500: Loss = -10983.80034288853
6
Iteration 13600: Loss = -10983.804875493666
7
Iteration 13700: Loss = -10983.801200372996
8
Iteration 13800: Loss = -10983.799687866192
Iteration 13900: Loss = -10983.801117399904
1
Iteration 14000: Loss = -10983.799513537986
Iteration 14100: Loss = -10983.807022387886
1
Iteration 14200: Loss = -10983.800503058106
2
Iteration 14300: Loss = -10983.805285146866
3
Iteration 14400: Loss = -10983.822279122516
4
Iteration 14500: Loss = -10983.799400346688
Iteration 14600: Loss = -10983.8025952783
1
Iteration 14700: Loss = -10983.799267000952
Iteration 14800: Loss = -10983.799583283775
1
Iteration 14900: Loss = -10983.798519504911
Iteration 15000: Loss = -10983.800980665465
1
Iteration 15100: Loss = -10983.79852354054
2
Iteration 15200: Loss = -10983.815308652469
3
Iteration 15300: Loss = -10983.805691341408
4
Iteration 15400: Loss = -10983.798479257073
Iteration 15500: Loss = -10983.80003539638
1
Iteration 15600: Loss = -10983.798886980574
2
Iteration 15700: Loss = -10983.798515929113
3
Iteration 15800: Loss = -10983.831065853454
4
Iteration 15900: Loss = -10983.798490918993
5
Iteration 16000: Loss = -10983.799188247189
6
Iteration 16100: Loss = -10983.798581868648
7
Iteration 16200: Loss = -10983.850233697018
8
Iteration 16300: Loss = -10983.79833187319
Iteration 16400: Loss = -10983.799437871723
1
Iteration 16500: Loss = -10983.791576749469
Iteration 16600: Loss = -10983.857223230225
1
Iteration 16700: Loss = -10983.79130794086
Iteration 16800: Loss = -10983.791429742338
1
Iteration 16900: Loss = -10983.791328221489
2
Iteration 17000: Loss = -10983.792683494641
3
Iteration 17100: Loss = -10983.791318676049
4
Iteration 17200: Loss = -10983.841969790403
5
Iteration 17300: Loss = -10983.790291161533
Iteration 17400: Loss = -10983.952403365389
1
Iteration 17500: Loss = -10983.790303535208
2
Iteration 17600: Loss = -10983.790410482325
3
Iteration 17700: Loss = -10983.801473543746
4
Iteration 17800: Loss = -10983.790340694013
5
Iteration 17900: Loss = -10983.790363433967
6
Iteration 18000: Loss = -10983.859186639957
7
Iteration 18100: Loss = -10983.790303413478
8
Iteration 18200: Loss = -10983.806249831207
9
Iteration 18300: Loss = -10983.790293998345
10
Stopping early at iteration 18300 due to no improvement.
tensor([[ 6.0693e-01, -5.2221e+00],
        [-3.0039e+00, -1.6113e+00],
        [ 1.8586e+00, -6.4738e+00],
        [ 1.5696e+00, -6.1848e+00],
        [ 7.3112e-01, -5.3463e+00],
        [-1.1833e-02, -4.6034e+00],
        [-2.0433e-01, -4.4109e+00],
        [-1.5823e+00, -3.0329e+00],
        [ 4.1698e+00, -8.7850e+00],
        [-1.0526e+00, -3.5626e+00],
        [-1.9690e-01, -4.4183e+00],
        [ 1.0492e-01, -4.7201e+00],
        [-4.8895e-01, -4.1263e+00],
        [-2.6036e+00, -2.0116e+00],
        [ 5.5056e-01, -5.1658e+00],
        [ 1.7066e-01, -4.7859e+00],
        [-1.2192e+00, -3.3960e+00],
        [ 4.9813e+00, -9.5965e+00],
        [-1.1789e+00, -3.4363e+00],
        [-3.4966e+00, -1.1186e+00],
        [ 9.8014e-01, -5.5954e+00],
        [ 8.3534e-01, -5.4506e+00],
        [ 4.6767e-01, -5.0829e+00],
        [ 2.2428e+00, -6.8580e+00],
        [-3.9908e-01, -4.2161e+00],
        [ 2.7186e+00, -7.3338e+00],
        [ 4.2299e-01, -5.0382e+00],
        [-8.6219e-01, -3.7530e+00],
        [ 1.8483e+00, -6.4635e+00],
        [ 3.3649e+00, -7.9802e+00],
        [-8.4809e-01, -3.7671e+00],
        [ 2.1064e+00, -6.7216e+00],
        [ 2.5073e+00, -7.1225e+00],
        [ 3.6730e-01, -4.9825e+00],
        [-4.6185e-02, -4.5690e+00],
        [-9.9588e-01, -3.6193e+00],
        [-6.5707e-01, -3.9582e+00],
        [-2.8831e+00, -1.7322e+00],
        [-1.5936e+00, -3.0217e+00],
        [-2.3543e+00, -2.2609e+00],
        [ 1.6099e+00, -6.2251e+00],
        [ 2.0681e+00, -6.6833e+00],
        [ 5.0422e-01, -5.1194e+00],
        [ 3.1628e+00, -7.7780e+00],
        [-1.5390e+00, -3.0762e+00],
        [ 8.2309e-01, -5.4383e+00],
        [ 1.9274e+00, -6.5427e+00],
        [ 7.7635e-01, -5.3916e+00],
        [ 9.0399e-01, -5.5192e+00],
        [ 2.9599e+00, -7.5751e+00],
        [ 2.3128e+00, -6.9280e+00],
        [ 9.9829e-01, -5.6135e+00],
        [ 2.1514e+00, -6.7666e+00],
        [ 4.2652e+00, -8.8805e+00],
        [ 1.8648e+00, -6.4800e+00],
        [ 1.3364e+00, -5.9516e+00],
        [ 2.9875e+00, -7.6027e+00],
        [ 7.8862e-01, -5.4038e+00],
        [ 4.1439e-01, -5.0296e+00],
        [ 1.3865e+00, -6.0017e+00],
        [ 1.3131e+00, -5.9283e+00],
        [-9.7438e-01, -3.6408e+00],
        [-4.8651e+00,  2.4990e-01],
        [ 1.0808e+00, -5.6960e+00],
        [ 4.5440e-01, -5.0696e+00],
        [-2.9864e+00, -1.6288e+00],
        [ 3.2460e-01, -4.9398e+00],
        [ 1.8288e+00, -6.4441e+00],
        [-3.6364e-01, -4.2516e+00],
        [ 7.6067e-01, -5.3759e+00],
        [ 2.7726e+00, -7.3879e+00],
        [ 1.8893e+00, -6.5045e+00],
        [ 2.5532e+00, -7.1684e+00],
        [ 3.4887e+00, -8.1039e+00],
        [ 4.2239e-01, -5.0376e+00],
        [ 1.2051e+00, -5.8203e+00],
        [ 8.2705e-01, -5.4423e+00],
        [ 1.4224e+00, -6.0376e+00],
        [ 1.2704e+00, -5.8856e+00],
        [ 4.4110e-03, -4.6196e+00],
        [ 1.8637e+00, -6.4789e+00],
        [-2.9699e+00, -1.6453e+00],
        [ 4.7982e+00, -9.4134e+00],
        [-6.1964e-01, -3.9956e+00],
        [ 8.6797e-01, -5.4832e+00],
        [ 1.2889e+00, -5.9041e+00],
        [ 2.0210e+00, -6.6362e+00],
        [ 1.5700e+00, -6.1852e+00],
        [-2.9902e-01, -4.3162e+00],
        [ 1.2884e+00, -5.9036e+00],
        [-1.5498e+00, -3.0654e+00],
        [ 8.5575e-01, -5.4710e+00],
        [ 2.7054e-02, -4.6423e+00],
        [-4.0310e-01, -4.2121e+00],
        [ 3.2399e+00, -7.8551e+00],
        [-1.6480e-01, -4.4504e+00],
        [ 1.0180e+00, -5.6333e+00],
        [ 5.8849e-01, -5.2037e+00],
        [ 2.2559e+00, -6.8711e+00],
        [ 1.1698e+00, -5.7851e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6866, 0.3134],
        [0.2706, 0.7294]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9218, 0.0782], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1844, 0.0927],
         [0.8043, 0.2820]],

        [[0.9812, 0.1032],
         [0.5738, 0.0090]],

        [[0.1527, 0.0995],
         [0.8449, 0.1428]],

        [[0.1355, 0.1035],
         [0.5147, 0.2149]],

        [[0.7808, 0.1011],
         [0.7211, 0.2968]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.016623577451856865
time is 1
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8822858823962049
time is 2
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.6137764618269351
Average Adjusted Rand Index: 0.757131834085569
Iteration 0: Loss = -20655.1055135712
Iteration 10: Loss = -11117.774074606814
Iteration 20: Loss = -11117.53518218269
Iteration 30: Loss = -11113.717610636311
Iteration 40: Loss = -11113.41467119981
Iteration 50: Loss = -11113.388816090248
Iteration 60: Loss = -11113.385479415067
Iteration 70: Loss = -11113.384836980516
Iteration 80: Loss = -11113.384699741586
Iteration 90: Loss = -11113.384673597939
Iteration 100: Loss = -11113.384694038463
1
Iteration 110: Loss = -11113.384666947448
Iteration 120: Loss = -11113.384690628074
1
Iteration 130: Loss = -11113.384695877778
2
Iteration 140: Loss = -11113.384698460046
3
Stopping early at iteration 139 due to no improvement.
pi: tensor([[0.9689, 0.0311],
        [0.9489, 0.0511]], dtype=torch.float64)
alpha: tensor([0.9684, 0.0316])
beta: tensor([[[1.6308e-01, 8.6195e-02],
         [1.1977e-01, 2.3888e-01]],

        [[2.7977e-01, 2.3919e-01],
         [3.7532e-02, 1.4711e-01]],

        [[8.9015e-01, 8.8879e-02],
         [8.2752e-01, 4.9465e-01]],

        [[7.3486e-01, 2.2487e-01],
         [8.5329e-05, 2.0299e-01]],

        [[4.6477e-02, 2.3471e-01],
         [7.7311e-01, 6.9431e-01]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: -0.00491035742834304
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 62
Adjusted Rand Index: 0.01126387000386094
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0025538476364862913
Global Adjusted Rand Index: 0.00018409493839990484
Average Adjusted Rand Index: 0.0013285254804579867
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20654.62872820054
Iteration 100: Loss = -11118.257616254841
Iteration 200: Loss = -11116.279228084819
Iteration 300: Loss = -11115.552275215865
Iteration 400: Loss = -11115.175295210616
Iteration 500: Loss = -11114.735773243096
Iteration 600: Loss = -11114.07499070284
Iteration 700: Loss = -11113.460275654625
Iteration 800: Loss = -11112.418905938961
Iteration 900: Loss = -11110.892159918316
Iteration 1000: Loss = -11107.146813218187
Iteration 1100: Loss = -11069.47880404343
Iteration 1200: Loss = -11058.979610188968
Iteration 1300: Loss = -10999.038276207146
Iteration 1400: Loss = -10994.856943377234
Iteration 1500: Loss = -10992.806322536602
Iteration 1600: Loss = -10987.410735028268
Iteration 1700: Loss = -10985.402792159328
Iteration 1800: Loss = -10984.981644667296
Iteration 1900: Loss = -10984.895474548779
Iteration 2000: Loss = -10984.869009632297
Iteration 2100: Loss = -10984.84241768534
Iteration 2200: Loss = -10984.814264071903
Iteration 2300: Loss = -10984.800535981647
Iteration 2400: Loss = -10984.770514289257
Iteration 2500: Loss = -10984.328672160307
Iteration 2600: Loss = -10984.318059954883
Iteration 2700: Loss = -10984.313655850632
Iteration 2800: Loss = -10984.309590603054
Iteration 2900: Loss = -10984.304409348455
Iteration 3000: Loss = -10984.296917832926
Iteration 3100: Loss = -10984.253950044185
Iteration 3200: Loss = -10984.124954660196
Iteration 3300: Loss = -10983.984935659906
Iteration 3400: Loss = -10983.96259472986
Iteration 3500: Loss = -10983.956332359809
Iteration 3600: Loss = -10983.943908187883
Iteration 3700: Loss = -10983.93462551156
Iteration 3800: Loss = -10983.932632873577
Iteration 3900: Loss = -10983.929678913626
Iteration 4000: Loss = -10983.919382923019
Iteration 4100: Loss = -10983.916255791855
Iteration 4200: Loss = -10983.89265758038
Iteration 4300: Loss = -10983.888492885826
Iteration 4400: Loss = -10983.88750338183
Iteration 4500: Loss = -10983.886581639432
Iteration 4600: Loss = -10983.885598393395
Iteration 4700: Loss = -10983.884687390384
Iteration 4800: Loss = -10983.883657382572
Iteration 4900: Loss = -10983.88196797905
Iteration 5000: Loss = -10983.876517297394
Iteration 5100: Loss = -10983.876013437293
Iteration 5200: Loss = -10983.871655972844
Iteration 5300: Loss = -10983.86945532127
Iteration 5400: Loss = -10983.876698482567
1
Iteration 5500: Loss = -10983.866213204508
Iteration 5600: Loss = -10983.850571060007
Iteration 5700: Loss = -10983.847101003466
Iteration 5800: Loss = -10983.84640112035
Iteration 5900: Loss = -10983.844899772941
Iteration 6000: Loss = -10983.843868228576
Iteration 6100: Loss = -10983.843288103973
Iteration 6200: Loss = -10983.842830324573
Iteration 6300: Loss = -10983.842575860977
Iteration 6400: Loss = -10983.848496467854
1
Iteration 6500: Loss = -10983.842258372444
Iteration 6600: Loss = -10983.842037591641
Iteration 6700: Loss = -10983.84134207254
Iteration 6800: Loss = -10983.827718289596
Iteration 6900: Loss = -10983.838261495766
1
Iteration 7000: Loss = -10983.824789049922
Iteration 7100: Loss = -10983.82467364822
Iteration 7200: Loss = -10983.825037418834
1
Iteration 7300: Loss = -10983.824472906997
Iteration 7400: Loss = -10983.824497050584
1
Iteration 7500: Loss = -10983.823990838093
Iteration 7600: Loss = -10983.833396768632
1
Iteration 7700: Loss = -10983.823699492918
Iteration 7800: Loss = -10983.823617239492
Iteration 7900: Loss = -10983.823548043789
Iteration 8000: Loss = -10983.823494900156
Iteration 8100: Loss = -10983.823732489229
1
Iteration 8200: Loss = -10983.832578931251
2
Iteration 8300: Loss = -10983.824859181725
3
Iteration 8400: Loss = -10983.82375374476
4
Iteration 8500: Loss = -10983.88476541247
5
Iteration 8600: Loss = -10983.823113025957
Iteration 8700: Loss = -10984.114735935374
1
Iteration 8800: Loss = -10983.822960752563
Iteration 8900: Loss = -10983.823456123806
1
Iteration 9000: Loss = -10983.8229148996
Iteration 9100: Loss = -10983.822852331477
Iteration 9200: Loss = -10983.823570865623
1
Iteration 9300: Loss = -10983.822843018028
Iteration 9400: Loss = -10984.05016725402
1
Iteration 9500: Loss = -10983.822769371827
Iteration 9600: Loss = -10983.822771704386
1
Iteration 9700: Loss = -10983.823084651684
2
Iteration 9800: Loss = -10983.822769504133
3
Iteration 9900: Loss = -10983.824283420157
4
Iteration 10000: Loss = -10983.82276223314
Iteration 10100: Loss = -10983.88467150566
1
Iteration 10200: Loss = -10983.822732797733
Iteration 10300: Loss = -10983.823187265773
1
Iteration 10400: Loss = -10983.881477388188
2
Iteration 10500: Loss = -10983.822665922611
Iteration 10600: Loss = -10983.825018367235
1
Iteration 10700: Loss = -10983.823121846632
2
Iteration 10800: Loss = -10983.822651345723
Iteration 10900: Loss = -10983.849181326847
1
Iteration 11000: Loss = -10983.821883833809
Iteration 11100: Loss = -10983.801342824238
Iteration 11200: Loss = -10983.80350334178
1
Iteration 11300: Loss = -10983.805280242577
2
Iteration 11400: Loss = -10983.801361484959
3
Iteration 11500: Loss = -10983.801214191828
Iteration 11600: Loss = -10983.853985161302
1
Iteration 11700: Loss = -10983.800004732655
Iteration 11800: Loss = -10983.799453827462
Iteration 11900: Loss = -10983.80104406195
1
Iteration 12000: Loss = -10983.799155612363
Iteration 12100: Loss = -10983.799191293305
1
Iteration 12200: Loss = -10983.815395184782
2
Iteration 12300: Loss = -10983.79908561606
Iteration 12400: Loss = -10983.803559925054
1
Iteration 12500: Loss = -10983.799831705812
2
Iteration 12600: Loss = -10983.799171690353
3
Iteration 12700: Loss = -10983.80685080021
4
Iteration 12800: Loss = -10983.799077048932
Iteration 12900: Loss = -10983.800152994689
1
Iteration 13000: Loss = -10983.79906632419
Iteration 13100: Loss = -10983.801845430657
1
Iteration 13200: Loss = -10983.799123674768
2
Iteration 13300: Loss = -10983.799369169394
3
Iteration 13400: Loss = -10983.81107448526
4
Iteration 13500: Loss = -10983.838505227768
5
Iteration 13600: Loss = -10983.819850466636
6
Iteration 13700: Loss = -10983.832713505311
7
Iteration 13800: Loss = -10983.798350497795
Iteration 13900: Loss = -10983.802877433223
1
Iteration 14000: Loss = -10983.798356285084
2
Iteration 14100: Loss = -10983.80638797793
3
Iteration 14200: Loss = -10983.79903997619
4
Iteration 14300: Loss = -10983.929659491609
5
Iteration 14400: Loss = -10983.798367385109
6
Iteration 14500: Loss = -10983.806445169763
7
Iteration 14600: Loss = -10983.799970801676
8
Iteration 14700: Loss = -10983.798310984012
Iteration 14800: Loss = -10983.806675628755
1
Iteration 14900: Loss = -10983.801106426381
2
Iteration 15000: Loss = -10983.798553719536
3
Iteration 15100: Loss = -10983.798570548126
4
Iteration 15200: Loss = -10983.800206443748
5
Iteration 15300: Loss = -10984.0533911938
6
Iteration 15400: Loss = -10983.798264118623
Iteration 15500: Loss = -10983.798706786501
1
Iteration 15600: Loss = -10983.867613623936
2
Iteration 15700: Loss = -10983.78259225274
Iteration 15800: Loss = -10983.784677423004
1
Iteration 15900: Loss = -10983.7775044687
Iteration 16000: Loss = -10983.778825874888
1
Iteration 16100: Loss = -10983.828886581126
2
Iteration 16200: Loss = -10983.775993957157
Iteration 16300: Loss = -10983.77710419631
1
Iteration 16400: Loss = -10983.776384060719
2
Iteration 16500: Loss = -10983.776070492524
3
Iteration 16600: Loss = -10983.779088163885
4
Iteration 16700: Loss = -10983.776584969364
5
Iteration 16800: Loss = -10983.775570640453
Iteration 16900: Loss = -10983.775604986666
1
Iteration 17000: Loss = -10983.775547267305
Iteration 17100: Loss = -10983.77608513071
1
Iteration 17200: Loss = -10983.775544594022
Iteration 17300: Loss = -10983.775695799612
1
Iteration 17400: Loss = -10983.775533940168
Iteration 17500: Loss = -10983.776829968961
1
Iteration 17600: Loss = -10983.775542111904
2
Iteration 17700: Loss = -10983.780407891642
3
Iteration 17800: Loss = -10983.77564633877
4
Iteration 17900: Loss = -10983.775657175685
5
Iteration 18000: Loss = -10983.776801476979
6
Iteration 18100: Loss = -10983.77553461871
7
Iteration 18200: Loss = -10983.775699036496
8
Iteration 18300: Loss = -10983.7755267932
Iteration 18400: Loss = -10983.775568820787
1
Iteration 18500: Loss = -10983.775531698835
2
Iteration 18600: Loss = -10983.77639622099
3
Iteration 18700: Loss = -10983.775566022434
4
Iteration 18800: Loss = -10983.775612026195
5
Iteration 18900: Loss = -10983.795978207429
6
Iteration 19000: Loss = -10983.775516199536
Iteration 19100: Loss = -10983.776325829915
1
Iteration 19200: Loss = -10983.77643241796
2
Iteration 19300: Loss = -10983.820044313157
3
Iteration 19400: Loss = -10983.77551348473
Iteration 19500: Loss = -10983.779209971286
1
Iteration 19600: Loss = -10983.775847597566
2
Iteration 19700: Loss = -10983.775558183193
3
Iteration 19800: Loss = -10983.796953649586
4
Iteration 19900: Loss = -10983.784612257214
5
tensor([[  1.1066,  -4.7153],
        [ -1.8965,  -0.5025],
        [  3.1867,  -5.1407],
        [  2.9394,  -4.8065],
        [  1.3912,  -4.6239],
        [  1.5773,  -2.9741],
        [ -0.2086,  -4.4066],
        [  0.0303,  -1.4179],
        [  5.5897,  -7.3049],
        [  0.5573,  -1.9463],
        [  1.4072,  -2.8045],
        [  0.0993,  -4.7145],
        [  0.0179,  -3.6156],
        [ -1.0111,  -0.3829],
        [  1.9337,  -3.7424],
        [  1.1651,  -3.7697],
        [  0.3010,  -1.8715],
        [  4.0381,  -5.4244],
        [  0.4113,  -1.8396],
        [ -2.0606,   0.3473],
        [  2.4749,  -4.0821],
        [  2.4432,  -3.8308],
        [  2.0565,  -3.4548],
        [  3.4056,  -5.6909],
        [ -0.4022,  -4.2130],
        [  4.1431,  -5.8826],
        [  1.8062,  -3.6474],
        [  0.6826,  -2.1980],
        [  3.0230,  -5.2785],
        [  4.5554,  -6.7847],
        [  0.7075,  -2.2058],
        [  3.5913,  -5.2215],
        [  3.3545,  -6.2692],
        [  1.8263,  -3.5131],
        [  1.5497,  -2.9640],
        [ -0.1628,  -2.7812],
        [  0.5638,  -2.7303],
        [ -2.0917,  -0.9303],
        [ -0.1048,  -1.5286],
        [ -0.8133,  -0.6647],
        [  2.8985,  -4.9140],
        [  3.5101,  -5.2338],
        [  0.9930,  -4.6253],
        [  4.4990,  -6.4026],
        [ -0.3251,  -1.8219],
        [  2.3476,  -3.8732],
        [  3.5335,  -4.9282],
        [  2.3086,  -3.8530],
        [  2.2979,  -4.1154],
        [  2.9473,  -7.5625],
        [  3.8517,  -5.3762],
        [  2.5910,  -3.9853],
        [  3.4574,  -5.4203],
        [  5.7540,  -7.3901],
        [  2.9322,  -5.3591],
        [  2.6293,  -4.6137],
        [  4.5822,  -6.0003],
        [  2.3247,  -3.8568],
        [  1.7184,  -3.6841],
        [  2.9754,  -4.3717],
        [  2.2709,  -4.9589],
        [  0.3179,  -2.3433],
        [ -3.4921,   1.6178],
        [  2.6863,  -4.0805],
        [  2.0665,  -3.4532],
        [ -1.8219,  -0.4575],
        [  1.9348,  -3.3230],
        [  8.0246, -10.8437],
        [  0.7016,  -3.1869],
        [  1.3759,  -4.7225],
        [  3.5891,  -6.5604],
        [  3.4988,  -4.8877],
        [  3.4948,  -6.2093],
        [  5.0942,  -6.4841],
        [  1.7564,  -3.6945],
        [  2.8107,  -4.2012],
        [  1.5130,  -4.7463],
        [  2.3922,  -5.0298],
        [  2.8529,  -4.2642],
        [  1.2253,  -3.3686],
        [  3.2694,  -5.0579],
        [ -1.3669,  -0.0221],
        [  1.2300,  -2.9901],
        [  0.9383,  -2.4302],
        [  2.4715,  -3.8611],
        [  1.7207,  -5.4662],
        [  3.5837,  -5.0672],
        [  2.9480,  -4.8010],
        [  1.3031,  -2.6921],
        [  2.8871,  -4.2947],
        [ -0.0182,  -1.5330],
        [  2.2590,  -4.0583],
        [  1.5061,  -3.1495],
        [ -0.1864,  -3.9866],
        [  4.8390,  -6.2455],
        [  1.4346,  -2.8336],
        [  2.5239,  -4.1068],
        [  2.1236,  -3.6601],
        [  3.6789,  -5.4322],
        [  2.6474,  -4.2970]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6884, 0.3116],
        [0.2727, 0.7273]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9223, 0.0777], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[1.8362e-01, 9.2354e-02],
         [1.1977e-01, 2.8315e-01]],

        [[2.7977e-01, 1.0368e-01],
         [3.7532e-02, 1.4711e-01]],

        [[8.9015e-01, 9.9950e-02],
         [8.2752e-01, 4.9465e-01]],

        [[7.3486e-01, 1.0386e-01],
         [8.5329e-05, 2.0299e-01]],

        [[4.6477e-02, 1.0150e-01],
         [7.7311e-01, 6.9431e-01]]], dtype=torch.float64,
       grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.016623577451856865
time is 1
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8822858823962049
time is 2
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.6137764618269351
Average Adjusted Rand Index: 0.757131834085569
Iteration 0: Loss = -27255.92831690142
Iteration 10: Loss = -11115.236654472206
Iteration 20: Loss = -11113.98143954583
Iteration 30: Loss = -11113.595769918278
Iteration 40: Loss = -11113.347862249686
Iteration 50: Loss = -11113.160391685991
Iteration 60: Loss = -11113.011792727215
Iteration 70: Loss = -11112.889950932335
Iteration 80: Loss = -11112.78668883444
Iteration 90: Loss = -11112.696410575772
Iteration 100: Loss = -11112.615005684465
Iteration 110: Loss = -11112.539641800488
Iteration 120: Loss = -11112.467902022428
Iteration 130: Loss = -11112.397979614914
Iteration 140: Loss = -11112.328291506115
Iteration 150: Loss = -11112.25738627498
Iteration 160: Loss = -11112.183766942493
Iteration 170: Loss = -11112.106004263647
Iteration 180: Loss = -11112.02235649121
Iteration 190: Loss = -11111.930928291185
Iteration 200: Loss = -11111.829420480874
Iteration 210: Loss = -11111.715083403673
Iteration 220: Loss = -11111.584537897486
Iteration 230: Loss = -11111.433718440587
Iteration 240: Loss = -11111.257983396661
Iteration 250: Loss = -11111.052742761241
Iteration 260: Loss = -11110.814692172977
Iteration 270: Loss = -11110.544407115272
Iteration 280: Loss = -11110.249761298206
Iteration 290: Loss = -11109.946011908476
Iteration 300: Loss = -11109.651732078637
Iteration 310: Loss = -11109.384396968964
Iteration 320: Loss = -11109.16001888967
Iteration 330: Loss = -11108.990417041256
Iteration 340: Loss = -11108.877927011623
Iteration 350: Loss = -11108.81493779845
Iteration 360: Loss = -11108.787750705438
Iteration 370: Loss = -11108.782835721426
Iteration 380: Loss = -11108.789721465362
1
Iteration 390: Loss = -11108.801697773157
2
Iteration 400: Loss = -11108.814974655297
3
Stopping early at iteration 399 due to no improvement.
pi: tensor([[0.2243, 0.7757],
        [0.0632, 0.9368]], dtype=torch.float64)
alpha: tensor([0.0777, 0.9223])
beta: tensor([[[0.0820, 0.1034],
         [0.7846, 0.1734]],

        [[0.3886, 0.1389],
         [0.6531, 0.1800]],

        [[0.2670, 0.1087],
         [0.3626, 0.2819]],

        [[0.6171, 0.1059],
         [0.8008, 0.9084]],

        [[0.8740, 0.1049],
         [0.5491, 0.2325]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.014778186472389411
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 39
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.003243945514707375
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.004144175314943718
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.003243945514707375
Global Adjusted Rand Index: -0.0065005738465195575
Average Adjusted Rand Index: -0.003424380437372089
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27256.09879650857
Iteration 100: Loss = -11140.64655920809
Iteration 200: Loss = -11110.877780512765
Iteration 300: Loss = -11089.34232251107
Iteration 400: Loss = -11006.533951022619
Iteration 500: Loss = -10998.589644198655
Iteration 600: Loss = -10992.63805566937
Iteration 700: Loss = -10991.686795562244
Iteration 800: Loss = -10991.223188905275
Iteration 900: Loss = -10991.152092037095
Iteration 1000: Loss = -10991.118604030184
Iteration 1100: Loss = -10991.090422308102
Iteration 1200: Loss = -10991.050017461324
Iteration 1300: Loss = -10991.030047742619
Iteration 1400: Loss = -10991.017624294027
Iteration 1500: Loss = -10990.60431727125
Iteration 1600: Loss = -10983.930736238384
Iteration 1700: Loss = -10983.918655770038
Iteration 1800: Loss = -10983.906028426365
Iteration 1900: Loss = -10983.88689015552
Iteration 2000: Loss = -10983.881566092332
Iteration 2100: Loss = -10983.877007080997
Iteration 2200: Loss = -10983.872687542089
Iteration 2300: Loss = -10983.868958336077
Iteration 2400: Loss = -10983.86592887171
Iteration 2500: Loss = -10983.863561098286
Iteration 2600: Loss = -10983.861823822024
Iteration 2700: Loss = -10983.860418668348
Iteration 2800: Loss = -10983.85923759152
Iteration 2900: Loss = -10983.858110011923
Iteration 3000: Loss = -10983.856683303475
Iteration 3100: Loss = -10983.85362314304
Iteration 3200: Loss = -10983.849770291197
Iteration 3300: Loss = -10983.843019843483
Iteration 3400: Loss = -10983.840128600443
Iteration 3500: Loss = -10983.839117827512
Iteration 3600: Loss = -10983.83830708342
Iteration 3700: Loss = -10983.837437404469
Iteration 3800: Loss = -10983.836620249027
Iteration 3900: Loss = -10983.835743773385
Iteration 4000: Loss = -10983.83336688027
Iteration 4100: Loss = -10983.827837255474
Iteration 4200: Loss = -10983.826762469727
Iteration 4300: Loss = -10983.826258119518
Iteration 4400: Loss = -10983.825882239507
Iteration 4500: Loss = -10983.825616359925
Iteration 4600: Loss = -10983.82534897496
Iteration 4700: Loss = -10983.825097801862
Iteration 4800: Loss = -10983.824895036045
Iteration 4900: Loss = -10983.824685665108
Iteration 5000: Loss = -10983.824515614655
Iteration 5100: Loss = -10983.824320503534
Iteration 5200: Loss = -10983.82417388438
Iteration 5300: Loss = -10983.825587688558
1
Iteration 5400: Loss = -10983.823923654521
Iteration 5500: Loss = -10983.823832684722
Iteration 5600: Loss = -10983.823711862125
Iteration 5700: Loss = -10983.823622857566
Iteration 5800: Loss = -10983.824094748377
1
Iteration 5900: Loss = -10983.82336555425
Iteration 6000: Loss = -10983.823477815831
1
Iteration 6100: Loss = -10983.824025641343
2
Iteration 6200: Loss = -10983.822970760204
Iteration 6300: Loss = -10983.82285785863
Iteration 6400: Loss = -10983.82267421408
Iteration 6500: Loss = -10983.822542262082
Iteration 6600: Loss = -10983.822102196358
Iteration 6700: Loss = -10983.83656185052
1
Iteration 6800: Loss = -10983.8214281021
Iteration 6900: Loss = -10983.821852471754
1
Iteration 7000: Loss = -10983.827162186812
2
Iteration 7100: Loss = -10983.814510011642
Iteration 7200: Loss = -10983.80806981608
Iteration 7300: Loss = -10983.797752051287
Iteration 7400: Loss = -10983.798773380207
1
Iteration 7500: Loss = -10983.803274644326
2
Iteration 7600: Loss = -10983.797712827612
Iteration 7700: Loss = -10983.780674810523
Iteration 7800: Loss = -10983.77990425854
Iteration 7900: Loss = -10983.779795124368
Iteration 8000: Loss = -10983.777054864639
Iteration 8100: Loss = -10983.778537939106
1
Iteration 8200: Loss = -10983.796733917738
2
Iteration 8300: Loss = -10983.78585870764
3
Iteration 8400: Loss = -10983.781536008726
4
Iteration 8500: Loss = -10983.780962270546
5
Iteration 8600: Loss = -10983.790624035635
6
Iteration 8700: Loss = -10983.77925674518
7
Iteration 8800: Loss = -10983.778984334765
8
Iteration 8900: Loss = -10983.792162185444
9
Iteration 9000: Loss = -10983.783380869045
10
Stopping early at iteration 9000 due to no improvement.
tensor([[ 2.2110, -3.6205],
        [-2.3613, -0.9671],
        [ 3.4704, -4.8646],
        [ 2.7973, -4.9556],
        [ 2.1226, -3.9005],
        [ 1.0788, -3.4816],
        [ 1.3876, -2.8205],
        [-0.4209, -1.8867],
        [ 5.5949, -6.9853],
        [ 0.4002, -2.1161],
        [ 0.9159, -3.3030],
        [ 1.2224, -3.6013],
        [-0.1934, -3.8435],
        [-1.0070, -0.3793],
        [ 2.0968, -3.5890],
        [ 1.7635, -3.1812],
        [-0.4158, -2.6034],
        [ 3.9517, -5.5124],
        [ 0.2923, -1.9631],
        [-1.9194,  0.4754],
        [ 1.7400, -4.8261],
        [ 2.2942, -3.9900],
        [ 1.8638, -3.6571],
        [ 3.3415, -5.7590],
        [ 1.0227, -2.7974],
        [ 4.2926, -5.7439],
        [ 1.6616, -3.8015],
        [ 0.6642, -2.2292],
        [ 3.4604, -4.8515],
        [ 5.9548, -7.7466],
        [ 0.6859, -2.2336],
        [ 2.1009, -6.7161],
        [ 3.9135, -5.7157],
        [ 1.8269, -3.5217],
        [ 1.3469, -3.1764],
        [ 0.4071, -2.2128],
        [ 0.9439, -2.3629],
        [-2.1067, -0.9648],
        [-0.0862, -1.5256],
        [-0.7589, -0.6289],
        [ 2.9578, -4.8633],
        [ 3.6805, -5.0671],
        [ 2.1188, -3.5087],
        [ 3.9727, -6.9345],
        [-0.0888, -1.5874],
        [ 2.4110, -3.8187],
        [ 2.4558, -6.0112],
        [ 2.3707, -3.8004],
        [ 2.0650, -4.3580],
        [ 4.3213, -6.1996],
        [ 3.4801, -5.7521],
        [ 1.7031, -4.8818],
        [ 3.7366, -5.1442],
        [ 5.2809, -7.8318],
        [ 3.4369, -4.8626],
        [ 2.8992, -4.3535],
        [ 4.1850, -6.3982],
        [ 2.2497, -3.9415],
        [ 1.9132, -3.4988],
        [ 2.7129, -4.6436],
        [ 2.9214, -4.3182],
        [ 0.5614, -2.1128],
        [-3.6317,  1.4672],
        [ 2.0003, -4.7763],
        [ 2.0091, -3.5196],
        [-2.1349, -0.7715],
        [ 1.9389, -3.3285],
        [ 3.3422, -4.9225],
        [ 1.2116, -2.6874],
        [ 2.3581, -3.7500],
        [ 4.3243, -5.8648],
        [ 3.4938, -4.8986],
        [ 4.1203, -5.5851],
        [ 5.0668, -6.5024],
        [ 1.9036, -3.5565],
        [ 2.8068, -4.2143],
        [ 2.0160, -4.2528],
        [ 2.9327, -4.4984],
        [ 2.3926, -4.7328],
        [ 1.4461, -3.1558],
        [ 3.2999, -5.0331],
        [-1.7918, -0.4529],
        [ 1.4091, -2.8213],
        [ 0.6815, -2.7004],
        [ 2.4723, -3.8692],
        [ 2.6047, -4.5913],
        [ 3.6084, -5.0484],
        [ 3.1840, -4.5738],
        [ 1.2735, -2.7313],
        [ 2.9011, -4.2895],
        [-0.8085, -2.3418],
        [ 1.8501, -4.4765],
        [ 1.6160, -3.0486],
        [ 1.1788, -2.6303],
        [ 4.2051, -6.8845],
        [ 0.6397, -3.6350],
        [ 2.6264, -4.0144],
        [ 2.2028, -3.5904],
        [ 3.7771, -5.3384],
        [ 1.8728, -5.0809]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6830, 0.3170],
        [0.2727, 0.7273]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9215, 0.0785], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1845, 0.0928],
         [0.7846, 0.2819]],

        [[0.3886, 0.1030],
         [0.6531, 0.1800]],

        [[0.2670, 0.0994],
         [0.3626, 0.2819]],

        [[0.6171, 0.1034],
         [0.8008, 0.9084]],

        [[0.8740, 0.1010],
         [0.5491, 0.2325]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.016623577451856865
time is 1
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8822858823962049
time is 2
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.6137764618269351
Average Adjusted Rand Index: 0.757131834085569
Iteration 0: Loss = -36450.50559305159
Iteration 10: Loss = -11117.774590578387
Iteration 20: Loss = -11117.66927555634
Iteration 30: Loss = -11113.7926208247
Iteration 40: Loss = -11113.411112494103
Iteration 50: Loss = -11113.388346122785
Iteration 60: Loss = -11113.385388530514
Iteration 70: Loss = -11113.384854943066
Iteration 80: Loss = -11113.384704572876
Iteration 90: Loss = -11113.384699934724
Iteration 100: Loss = -11113.384715864673
1
Iteration 110: Loss = -11113.384657166416
Iteration 120: Loss = -11113.384692288051
1
Iteration 130: Loss = -11113.384677633134
2
Iteration 140: Loss = -11113.384675346455
3
Stopping early at iteration 139 due to no improvement.
pi: tensor([[0.0511, 0.9489],
        [0.0311, 0.9689]], dtype=torch.float64)
alpha: tensor([0.0316, 0.9684])
beta: tensor([[[0.2389, 0.0862],
         [0.2233, 0.1631]],

        [[0.4476, 0.2392],
         [0.2058, 0.4470]],

        [[0.8663, 0.0889],
         [0.1671, 0.5284]],

        [[0.1914, 0.2249],
         [0.2015, 0.9458]],

        [[0.1442, 0.2347],
         [0.0394, 0.7082]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: -0.00491035742834304
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 38
Adjusted Rand Index: 0.01126387000386094
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
Global Adjusted Rand Index: 0.00018409493839990484
Average Adjusted Rand Index: 0.0013285254804579867
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -36450.08868826085
Iteration 100: Loss = -11177.881498174116
Iteration 200: Loss = -11153.617583771294
Iteration 300: Loss = -11142.758757838537
Iteration 400: Loss = -11133.986691710805
Iteration 500: Loss = -11121.645153223135
Iteration 600: Loss = -11119.193715778001
Iteration 700: Loss = -11118.579158500392
Iteration 800: Loss = -11118.245723557353
Iteration 900: Loss = -11118.039684878297
Iteration 1000: Loss = -11117.898450916427
Iteration 1100: Loss = -11117.795467007223
Iteration 1200: Loss = -11117.71743432977
Iteration 1300: Loss = -11117.65592061965
Iteration 1400: Loss = -11117.605604297167
Iteration 1500: Loss = -11117.562958113185
Iteration 1600: Loss = -11117.526272606314
Iteration 1700: Loss = -11117.494911601978
Iteration 1800: Loss = -11117.468075633324
Iteration 1900: Loss = -11117.444506476257
Iteration 2000: Loss = -11117.422994687871
Iteration 2100: Loss = -11117.402337490352
Iteration 2200: Loss = -11117.380829147294
Iteration 2300: Loss = -11117.356472648771
Iteration 2400: Loss = -11117.330298356375
Iteration 2500: Loss = -11117.294267720976
Iteration 2600: Loss = -11117.212257332661
Iteration 2700: Loss = -11116.32250337757
Iteration 2800: Loss = -11114.389845690865
Iteration 2900: Loss = -11113.707488807868
Iteration 3000: Loss = -11113.30566806668
Iteration 3100: Loss = -11113.050234444001
Iteration 3200: Loss = -11112.923051585454
Iteration 3300: Loss = -11112.750881180535
Iteration 3400: Loss = -11112.692203808117
Iteration 3500: Loss = -11112.655564271694
Iteration 3600: Loss = -11112.62318346686
Iteration 3700: Loss = -11112.596681322235
Iteration 3800: Loss = -11112.580012783357
Iteration 3900: Loss = -11112.564671990394
Iteration 4000: Loss = -11112.55088958097
Iteration 4100: Loss = -11112.533966184405
Iteration 4200: Loss = -11109.884129341288
Iteration 4300: Loss = -11109.843150020923
Iteration 4400: Loss = -11109.833604694575
Iteration 4500: Loss = -11109.82636925619
Iteration 4600: Loss = -11109.82024124078
Iteration 4700: Loss = -11109.814768619375
Iteration 4800: Loss = -11109.808831477885
Iteration 4900: Loss = -11109.802762439987
Iteration 5000: Loss = -11109.799211922089
Iteration 5100: Loss = -11109.796421211682
Iteration 5200: Loss = -11109.793855548653
Iteration 5300: Loss = -11109.79138296171
Iteration 5400: Loss = -11109.788990564246
Iteration 5500: Loss = -11109.786565698503
Iteration 5600: Loss = -11109.784048852367
Iteration 5700: Loss = -11109.730678031276
Iteration 5800: Loss = -11109.725385818481
Iteration 5900: Loss = -11109.723208335026
Iteration 6000: Loss = -11109.720712746208
Iteration 6100: Loss = -11109.700717255531
Iteration 6200: Loss = -11109.682564418781
Iteration 6300: Loss = -11109.663435710243
Iteration 6400: Loss = -11109.634630358894
Iteration 6500: Loss = -11109.628382321162
Iteration 6600: Loss = -11109.61229822103
Iteration 6700: Loss = -11109.604809652637
Iteration 6800: Loss = -11109.552463061038
Iteration 6900: Loss = -11109.551406641887
Iteration 7000: Loss = -11109.548057490312
Iteration 7100: Loss = -11109.545876389828
Iteration 7200: Loss = -11109.545201339874
Iteration 7300: Loss = -11109.544510549504
Iteration 7400: Loss = -11109.543520937576
Iteration 7500: Loss = -11109.537184640669
Iteration 7600: Loss = -11109.536038397377
Iteration 7700: Loss = -11109.53043903308
Iteration 7800: Loss = -11109.526251511343
Iteration 7900: Loss = -11109.520217415647
Iteration 8000: Loss = -11109.506288878722
Iteration 8100: Loss = -11109.501441024975
Iteration 8200: Loss = -11109.498591529444
Iteration 8300: Loss = -11109.4968500703
Iteration 8400: Loss = -11109.496287089341
Iteration 8500: Loss = -11109.494550258141
Iteration 8600: Loss = -11109.491569081581
Iteration 8700: Loss = -11109.483943860283
Iteration 8800: Loss = -11109.480777842724
Iteration 8900: Loss = -11109.478044608275
Iteration 9000: Loss = -11109.47473103749
Iteration 9100: Loss = -11109.47126835765
Iteration 9200: Loss = -11109.471656203837
1
Iteration 9300: Loss = -11109.461860107933
Iteration 9400: Loss = -11109.605909015623
1
Iteration 9500: Loss = -11109.46095753581
Iteration 9600: Loss = -11109.45928820485
Iteration 9700: Loss = -11109.459633541705
1
Iteration 9800: Loss = -11109.458232457708
Iteration 9900: Loss = -11109.45780730827
Iteration 10000: Loss = -11109.458128965696
1
Iteration 10100: Loss = -11109.455939183366
Iteration 10200: Loss = -11109.455088761079
Iteration 10300: Loss = -11109.45641235022
1
Iteration 10400: Loss = -11109.453489331148
Iteration 10500: Loss = -11109.450681377393
Iteration 10600: Loss = -11109.459975425865
1
Iteration 10700: Loss = -11109.443967100902
Iteration 10800: Loss = -11109.429675427084
Iteration 10900: Loss = -11109.500189215474
1
Iteration 11000: Loss = -11109.427618653623
Iteration 11100: Loss = -11109.426911616545
Iteration 11200: Loss = -11109.437781434115
1
Iteration 11300: Loss = -11109.42376271231
Iteration 11400: Loss = -11109.423348199061
Iteration 11500: Loss = -11109.423689579762
1
Iteration 11600: Loss = -11109.419726904192
Iteration 11700: Loss = -11109.418093171476
Iteration 11800: Loss = -11109.41796379353
Iteration 11900: Loss = -11109.416686035758
Iteration 12000: Loss = -11109.415916528746
Iteration 12100: Loss = -11109.413370290933
Iteration 12200: Loss = -11109.41413922033
1
Iteration 12300: Loss = -11109.412326355116
Iteration 12400: Loss = -11109.412136664512
Iteration 12500: Loss = -11109.411230677024
Iteration 12600: Loss = -11109.398150706897
Iteration 12700: Loss = -11109.398050281998
Iteration 12800: Loss = -11109.398290172307
1
Iteration 12900: Loss = -11109.397782794935
Iteration 13000: Loss = -11109.428092283883
1
Iteration 13100: Loss = -11109.39697079231
Iteration 13200: Loss = -11109.396869454169
Iteration 13300: Loss = -11109.399237080292
1
Iteration 13400: Loss = -11109.395715855422
Iteration 13500: Loss = -11109.395059470047
Iteration 13600: Loss = -11109.398345731057
1
Iteration 13700: Loss = -11109.392000421714
Iteration 13800: Loss = -11109.39195987986
Iteration 13900: Loss = -11109.512376916475
1
Iteration 14000: Loss = -11109.391821327392
Iteration 14100: Loss = -11109.391328433336
Iteration 14200: Loss = -11109.390977123076
Iteration 14300: Loss = -11077.30762930081
Iteration 14400: Loss = -11067.073362310635
Iteration 14500: Loss = -11051.454651994098
Iteration 14600: Loss = -11041.547793116433
Iteration 14700: Loss = -11034.110287174644
Iteration 14800: Loss = -11026.502673256995
Iteration 14900: Loss = -11018.232331298948
Iteration 15000: Loss = -11007.13654421739
Iteration 15100: Loss = -11000.846195894172
Iteration 15200: Loss = -10998.322153701592
Iteration 15300: Loss = -10998.17614691726
Iteration 15400: Loss = -10995.782795901092
Iteration 15500: Loss = -10995.778844935383
Iteration 15600: Loss = -10991.365381409165
Iteration 15700: Loss = -10991.394143630154
1
Iteration 15800: Loss = -10991.36279584584
Iteration 15900: Loss = -10991.368196533864
1
Iteration 16000: Loss = -10991.352038651969
Iteration 16100: Loss = -10991.341412246646
Iteration 16200: Loss = -10991.577536042232
1
Iteration 16300: Loss = -10991.243685166282
Iteration 16400: Loss = -10991.313624505046
1
Iteration 16500: Loss = -10991.242717811658
Iteration 16600: Loss = -10991.227572982196
Iteration 16700: Loss = -10991.219426835836
Iteration 16800: Loss = -10991.219270498472
Iteration 16900: Loss = -10991.218619157109
Iteration 17000: Loss = -10991.218156558949
Iteration 17100: Loss = -10991.217165980266
Iteration 17200: Loss = -10991.13374763667
Iteration 17300: Loss = -10991.130524334292
Iteration 17400: Loss = -10991.132159224373
1
Iteration 17500: Loss = -10991.089388686962
Iteration 17600: Loss = -10991.06356757714
Iteration 17700: Loss = -10990.963698457512
Iteration 17800: Loss = -10990.96238304596
Iteration 17900: Loss = -10990.962503402454
1
Iteration 18000: Loss = -10990.960650259987
Iteration 18100: Loss = -10990.94430663452
Iteration 18200: Loss = -10990.941619128178
Iteration 18300: Loss = -10990.941514268687
Iteration 18400: Loss = -10990.941708106877
1
Iteration 18500: Loss = -10990.941312512752
Iteration 18600: Loss = -10990.941266118534
Iteration 18700: Loss = -10990.941250074082
Iteration 18800: Loss = -10990.940442176263
Iteration 18900: Loss = -10990.949599872576
1
Iteration 19000: Loss = -10990.9244695154
Iteration 19100: Loss = -10990.923698211369
Iteration 19200: Loss = -10990.940822631474
1
Iteration 19300: Loss = -10990.906489036624
Iteration 19400: Loss = -10990.906764946008
1
Iteration 19500: Loss = -10990.903940142773
Iteration 19600: Loss = -10990.90384390541
Iteration 19700: Loss = -10990.905757482551
1
Iteration 19800: Loss = -10990.9038135524
Iteration 19900: Loss = -10990.903790085717
tensor([[-10.1872,   8.6597],
        [-10.2530,   8.3689],
        [ -9.2952,   7.7584],
        [ -8.9716,   7.5057],
        [ -9.9162,   8.0660],
        [-10.7231,   8.7184],
        [ -9.3940,   7.9120],
        [ -9.0179,   7.5888],
        [-11.2467,   8.9501],
        [ -9.1909,   7.4587],
        [ -9.7930,   8.4062],
        [ -9.0358,   7.6198],
        [-10.0228,   7.1818],
        [-10.2414,   7.5515],
        [ -9.9477,   7.5631],
        [ -9.8155,   7.9780],
        [ -8.9284,   7.4955],
        [ -9.4158,   7.7855],
        [ -9.6635,   7.0130],
        [ -8.8551,   7.3081],
        [ -9.6352,   8.2162],
        [-10.8327,   7.5975],
        [ -9.3145,   7.7106],
        [ -9.9099,   7.6614],
        [-10.2778,   8.2015],
        [-10.3495,   6.9023],
        [ -9.0379,   7.5766],
        [-10.6393,   6.7678],
        [ -9.9335,   8.5470],
        [ -9.4747,   8.0862],
        [ -9.4251,   7.8445],
        [ -9.9929,   7.5564],
        [ -9.4429,   7.9232],
        [ -9.5978,   8.1931],
        [ -9.1626,   7.7058],
        [ -9.9610,   8.4271],
        [ -8.7298,   7.3419],
        [ -9.0841,   7.4767],
        [ -8.6662,   7.1967],
        [ -9.2061,   7.4589],
        [ -9.4193,   8.0060],
        [-10.8843,   8.5718],
        [ -9.4877,   7.8791],
        [-10.0633,   7.9264],
        [-11.4224,   6.8072],
        [ -9.2554,   7.6871],
        [-10.3920,   8.8071],
        [ -9.2337,   7.8459],
        [ -9.1028,   7.6364],
        [ -9.4391,   7.4928],
        [-10.6529,   7.6192],
        [ -9.7253,   8.2773],
        [-11.8084,   7.1932],
        [ -9.2382,   7.6231],
        [ -9.1512,   7.7140],
        [ -9.8580,   8.4363],
        [ -9.8373,   8.2901],
        [ -9.5697,   8.1830],
        [ -9.5433,   8.1503],
        [-10.0499,   7.8698],
        [ -9.5274,   7.8290],
        [ -9.4677,   7.6065],
        [ -9.2189,   6.7330],
        [ -9.1520,   7.7087],
        [-10.9549,   8.0452],
        [ -9.9431,   8.5407],
        [-10.6467,   8.9156],
        [-10.8343,   8.3575],
        [ -9.0220,   7.6300],
        [ -9.7887,   7.5483],
        [ -9.5482,   8.1347],
        [-10.3618,   8.7295],
        [-11.4475,   7.1421],
        [ -9.8817,   8.0298],
        [-10.4179,   8.9307],
        [ -9.1271,   7.3698],
        [ -8.9847,   7.5971],
        [ -9.8852,   8.3195],
        [ -9.3921,   7.5193],
        [-10.4008,   6.6579],
        [ -9.5558,   8.0439],
        [-10.9291,   8.0400],
        [ -9.3919,   7.9080],
        [ -9.7300,   7.1015],
        [ -9.7894,   7.5494],
        [-10.5644,   7.1767],
        [-11.2826,   7.1925],
        [ -9.6779,   8.1839],
        [ -9.7567,   8.3683],
        [ -9.9125,   7.9966],
        [ -9.7765,   7.1297],
        [ -9.3410,   7.8299],
        [-10.1029,   8.3693],
        [ -9.8164,   8.3934],
        [-10.1745,   7.7388],
        [-10.0327,   8.6318],
        [-10.0054,   8.6066],
        [-10.0426,   7.8093],
        [ -9.6471,   8.2568],
        [-11.0642,   8.0365]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7499, 0.2501],
        [0.3110, 0.6890]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([3.0062e-08, 1.0000e+00], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2823, 0.1493],
         [0.2233, 0.1783]],

        [[0.4476, 0.1048],
         [0.2058, 0.4470]],

        [[0.8663, 0.0996],
         [0.1671, 0.5284]],

        [[0.1914, 0.1036],
         [0.2015, 0.9458]],

        [[0.1442, 0.1012],
         [0.0394, 0.7082]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 6
Adjusted Rand Index: 0.7716057836307171
time is 2
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.6327338734797608
Average Adjusted Rand Index: 0.7383205298228428
Iteration 0: Loss = -27525.105401214274
Iteration 10: Loss = -11117.774804426073
Iteration 20: Loss = -11117.774807442993
1
Iteration 30: Loss = -11117.774754978855
Iteration 40: Loss = -11117.71474872073
Iteration 50: Loss = -11114.140103989428
Iteration 60: Loss = -11113.52497555196
Iteration 70: Loss = -11113.491832458752
Iteration 80: Loss = -11113.487570669253
Iteration 90: Loss = -11113.485405452067
Iteration 100: Loss = -11113.456496863218
Iteration 110: Loss = -11113.39594502075
Iteration 120: Loss = -11113.386125623721
Iteration 130: Loss = -11113.384980824883
Iteration 140: Loss = -11113.3847535599
Iteration 150: Loss = -11113.384709643416
Iteration 160: Loss = -11113.384704449452
Iteration 170: Loss = -11113.384682359203
Iteration 180: Loss = -11113.384679824208
Iteration 190: Loss = -11113.384707740115
1
Iteration 200: Loss = -11113.384702189482
2
Iteration 210: Loss = -11113.38470615549
3
Stopping early at iteration 209 due to no improvement.
pi: tensor([[0.9689, 0.0311],
        [0.9489, 0.0511]], dtype=torch.float64)
alpha: tensor([0.9684, 0.0316])
beta: tensor([[[0.1631, 0.0862],
         [0.3623, 0.2389]],

        [[0.4705, 0.2392],
         [0.5589, 0.4665]],

        [[0.1330, 0.0889],
         [0.2985, 0.7381]],

        [[0.9135, 0.2249],
         [0.5604, 0.8019]],

        [[0.1635, 0.2347],
         [0.2867, 0.0369]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: -0.00491035742834304
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 62
Adjusted Rand Index: 0.01126387000386094
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0025538476364862913
Global Adjusted Rand Index: 0.00018409493839990484
Average Adjusted Rand Index: 0.0013285254804579867
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27524.89637733413
Iteration 100: Loss = -11126.400395971026
Iteration 200: Loss = -11121.318175300312
Iteration 300: Loss = -11119.284411669514
Iteration 400: Loss = -11118.218413943418
Iteration 500: Loss = -11117.34259358812
Iteration 600: Loss = -11116.565898990033
Iteration 700: Loss = -11116.081011661134
Iteration 800: Loss = -11115.631009878778
Iteration 900: Loss = -11115.164531776829
Iteration 1000: Loss = -11114.152552777367
Iteration 1100: Loss = -11112.541408281935
Iteration 1200: Loss = -11111.361145189681
Iteration 1300: Loss = -11110.803447445676
Iteration 1400: Loss = -11110.274225359602
Iteration 1500: Loss = -11108.280622021779
Iteration 1600: Loss = -11098.153327577826
Iteration 1700: Loss = -11088.790148291828
Iteration 1800: Loss = -11083.873754776472
Iteration 1900: Loss = -11071.257736197247
Iteration 2000: Loss = -11028.079821668693
Iteration 2100: Loss = -11019.098160757467
Iteration 2200: Loss = -11010.1617671653
Iteration 2300: Loss = -11010.045286469416
Iteration 2400: Loss = -11004.143011727676
Iteration 2500: Loss = -11004.044969925479
Iteration 2600: Loss = -11004.013441254921
Iteration 2700: Loss = -10997.95956987268
Iteration 2800: Loss = -10997.918337207366
Iteration 2900: Loss = -10997.89813673222
Iteration 3000: Loss = -10997.760431064697
Iteration 3100: Loss = -10992.74412502293
Iteration 3200: Loss = -10992.711199204967
Iteration 3300: Loss = -10992.642532674055
Iteration 3400: Loss = -10992.626653134148
Iteration 3500: Loss = -10992.602460786187
Iteration 3600: Loss = -10992.594056638163
Iteration 3700: Loss = -10992.58774860513
Iteration 3800: Loss = -10992.581664439083
Iteration 3900: Loss = -10992.574796138371
Iteration 4000: Loss = -10992.564005947726
Iteration 4100: Loss = -10988.114301284122
Iteration 4200: Loss = -10988.0954545058
Iteration 4300: Loss = -10988.082691913483
Iteration 4400: Loss = -10988.030956882752
Iteration 4500: Loss = -10987.931553033774
Iteration 4600: Loss = -10987.928634030752
Iteration 4700: Loss = -10987.926233873397
Iteration 4800: Loss = -10987.923913179211
Iteration 4900: Loss = -10987.921833175917
Iteration 5000: Loss = -10987.91967397317
Iteration 5100: Loss = -10987.915987826282
Iteration 5200: Loss = -10987.903702212054
Iteration 5300: Loss = -10987.901972549913
Iteration 5400: Loss = -10987.90044593524
Iteration 5500: Loss = -10987.898970914785
Iteration 5600: Loss = -10987.902218764311
1
Iteration 5700: Loss = -10987.896230962888
Iteration 5800: Loss = -10987.894639129716
Iteration 5900: Loss = -10987.8923210103
Iteration 6000: Loss = -10987.890857393999
Iteration 6100: Loss = -10987.889902958708
Iteration 6200: Loss = -10987.888825511169
Iteration 6300: Loss = -10987.88782370219
Iteration 6400: Loss = -10987.892018716198
1
Iteration 6500: Loss = -10987.8855509125
Iteration 6600: Loss = -10987.888703097906
1
Iteration 6700: Loss = -10987.883772059009
Iteration 6800: Loss = -10987.88009089116
Iteration 6900: Loss = -10987.87600450914
Iteration 7000: Loss = -10987.867611251859
Iteration 7100: Loss = -10987.657414169767
Iteration 7200: Loss = -10986.969184774762
Iteration 7300: Loss = -10986.87170788391
Iteration 7400: Loss = -10986.854285028961
Iteration 7500: Loss = -10986.839275009443
Iteration 7600: Loss = -10986.573250485537
Iteration 7700: Loss = -10986.367558237871
Iteration 7800: Loss = -10985.106709692272
Iteration 7900: Loss = -10985.08358456375
Iteration 8000: Loss = -10985.085386453557
1
Iteration 8100: Loss = -10985.074459199113
Iteration 8200: Loss = -10985.071972380812
Iteration 8300: Loss = -10985.068650654279
Iteration 8400: Loss = -10985.066839790967
Iteration 8500: Loss = -10985.05927680728
Iteration 8600: Loss = -10985.036941329032
Iteration 8700: Loss = -10984.682333042121
Iteration 8800: Loss = -10984.689891968039
1
Iteration 8900: Loss = -10984.722324037533
2
Iteration 9000: Loss = -10984.683445067505
3
Iteration 9100: Loss = -10984.67312660055
Iteration 9200: Loss = -10984.67232926108
Iteration 9300: Loss = -10984.679751121916
1
Iteration 9400: Loss = -10984.671911322748
Iteration 9500: Loss = -10984.672052474449
1
Iteration 9600: Loss = -10984.671593033998
Iteration 9700: Loss = -10984.67162139094
1
Iteration 9800: Loss = -10984.67100708429
Iteration 9900: Loss = -10984.673242630693
1
Iteration 10000: Loss = -10984.670218479638
Iteration 10100: Loss = -10984.670396205767
1
Iteration 10200: Loss = -10984.670127586562
Iteration 10300: Loss = -10984.669994776594
Iteration 10400: Loss = -10984.695429950762
1
Iteration 10500: Loss = -10984.669783236364
Iteration 10600: Loss = -10984.722078577357
1
Iteration 10700: Loss = -10984.674529570728
2
Iteration 10800: Loss = -10984.670921016366
3
Iteration 10900: Loss = -10984.675326417186
4
Iteration 11000: Loss = -10984.669056436187
Iteration 11100: Loss = -10984.679780887007
1
Iteration 11200: Loss = -10984.669030124604
Iteration 11300: Loss = -10984.712165555633
1
Iteration 11400: Loss = -10984.668942382423
Iteration 11500: Loss = -10984.668906494957
Iteration 11600: Loss = -10984.64927313873
Iteration 11700: Loss = -10984.64872180914
Iteration 11800: Loss = -10984.64886137249
1
Iteration 11900: Loss = -10984.656561078553
2
Iteration 12000: Loss = -10984.648661006931
Iteration 12100: Loss = -10984.650780319045
1
Iteration 12200: Loss = -10984.649652370063
2
Iteration 12300: Loss = -10983.863427258657
Iteration 12400: Loss = -10983.862023410613
Iteration 12500: Loss = -10983.861761091082
Iteration 12600: Loss = -10983.860849521458
Iteration 12700: Loss = -10983.86105417458
1
Iteration 12800: Loss = -10983.860853387912
2
Iteration 12900: Loss = -10983.86388148192
3
Iteration 13000: Loss = -10983.861322416782
4
Iteration 13100: Loss = -10983.871712868126
5
Iteration 13200: Loss = -10983.873985093547
6
Iteration 13300: Loss = -10983.86079019574
Iteration 13400: Loss = -10983.861179555828
1
Iteration 13500: Loss = -10983.862213340966
2
Iteration 13600: Loss = -10984.056447636716
3
Iteration 13700: Loss = -10983.860597927045
Iteration 13800: Loss = -10983.861310051147
1
Iteration 13900: Loss = -10983.863893673339
2
Iteration 14000: Loss = -10983.859981248233
Iteration 14100: Loss = -10983.867321504744
1
Iteration 14200: Loss = -10983.860012900153
2
Iteration 14300: Loss = -10983.864944613559
3
Iteration 14400: Loss = -10983.860007290543
4
Iteration 14500: Loss = -10983.86022767062
5
Iteration 14600: Loss = -10983.870899821904
6
Iteration 14700: Loss = -10983.863461408391
7
Iteration 14800: Loss = -10983.860076724206
8
Iteration 14900: Loss = -10983.86007393421
9
Iteration 15000: Loss = -10983.870872477914
10
Stopping early at iteration 15000 due to no improvement.
tensor([[ 2.1387e+00, -3.6964e+00],
        [-1.3683e+00, -3.7289e-02],
        [ 3.3553e+00, -4.9945e+00],
        [ 3.1718e+00, -4.5775e+00],
        [ 1.8418e+00, -4.1529e+00],
        [ 1.4000e+00, -3.1608e+00],
        [ 1.0577e+00, -3.1340e+00],
        [-9.8229e-03, -1.4581e+00],
        [ 5.6342e+00, -7.2944e+00],
        [ 5.4929e-01, -1.9561e+00],
        [ 1.3488e+00, -2.8621e+00],
        [ 7.4361e-01, -4.0586e+00],
        [ 4.8115e-01, -3.1802e+00],
        [-1.9006e+00, -1.2536e+00],
        [ 1.8725e+00, -3.8549e+00],
        [ 1.7837e+00, -3.2073e+00],
        [-8.8861e-01, -3.0597e+00],
        [ 3.5996e+00, -5.8624e+00],
        [-3.0418e-02, -2.2961e+00],
        [-3.5239e+00, -1.0913e+00],
        [ 2.5719e+00, -4.0015e+00],
        [ 2.3711e+00, -3.9152e+00],
        [ 1.8448e+00, -3.6711e+00],
        [ 3.7720e+00, -5.3290e+00],
        [ 1.1350e+00, -2.7038e+00],
        [ 4.3134e+00, -5.7166e+00],
        [ 2.0348e+00, -3.4282e+00],
        [ 4.1427e-01, -2.4632e+00],
        [ 3.1814e+00, -5.1422e+00],
        [ 4.9862e+00, -6.3754e+00],
        [ 7.3922e-01, -2.2022e+00],
        [ 3.1660e+00, -5.6893e+00],
        [ 4.0137e+00, -5.6121e+00],
        [ 1.8328e+00, -3.5130e+00],
        [ 1.5609e+00, -2.9614e+00],
        [ 4.1006e-01, -2.2211e+00],
        [ 4.3112e-01, -2.8632e+00],
        [-1.2238e+00, -1.7908e-01],
        [ 7.7743e-03, -1.4163e+00],
        [-8.1967e-01, -6.4942e-01],
        [ 3.2017e+00, -4.5905e+00],
        [ 3.6718e+00, -5.0780e+00],
        [ 2.0663e+00, -3.5776e+00],
        [ 4.7088e+00, -6.1825e+00],
        [-1.3657e-01, -1.6934e+00],
        [ 2.4119e+00, -3.8113e+00],
        [ 5.0794e+00, -9.4544e+00],
        [ 2.3741e+00, -3.7996e+00],
        [ 9.3157e-01, -5.4822e+00],
        [ 3.9970e+00, -6.5096e+00],
        [ 3.8572e+00, -5.3872e+00],
        [ 2.5688e+00, -4.0056e+00],
        [ 3.3786e+00, -5.5040e+00],
        [ 5.2618e+00, -7.6847e+00],
        [ 2.7894e+00, -5.4936e+00],
        [ 2.9143e+00, -4.3378e+00],
        [ 3.5657e+00, -7.0119e+00],
        [ 2.3116e+00, -3.8844e+00],
        [ 1.8520e+00, -3.5592e+00],
        [ 2.9472e+00, -4.3947e+00],
        [ 2.9201e+00, -4.3149e+00],
        [ 4.6035e-01, -2.2173e+00],
        [-3.3064e+00,  1.7409e+00],
        [ 2.5188e+00, -4.2571e+00],
        [ 2.0686e+00, -3.4571e+00],
        [-1.4379e+00, -7.2035e-02],
        [ 1.7477e+00, -3.5437e+00],
        [ 2.0565e+00, -6.1931e+00],
        [ 1.1529e+00, -2.7532e+00],
        [ 2.2500e+00, -3.8511e+00],
        [ 3.8721e+00, -6.3286e+00],
        [ 3.3736e+00, -5.0280e+00],
        [ 4.1205e+00, -5.5703e+00],
        [ 4.4030e+00, -7.1626e+00],
        [ 1.8870e+00, -3.5786e+00],
        [ 2.7164e+00, -4.2895e+00],
        [ 1.0708e+00, -5.1888e+00],
        [ 2.8745e+00, -4.5733e+00],
        [ 2.6307e+00, -4.4863e+00],
        [ 1.6205e+00, -3.0114e+00],
        [ 3.4483e+00, -4.8654e+00],
        [-1.3866e+00, -2.4299e-02],
        [ 1.3568e+00, -2.8667e+00],
        [ 7.9334e-01, -2.5748e+00],
        [ 2.4619e+00, -3.8659e+00],
        [ 2.8988e+00, -4.2895e+00],
        [ 3.5496e+00, -5.1011e+00],
        [ 3.1399e+00, -4.6240e+00],
        [ 1.1926e+00, -2.7986e+00],
        [ 2.0504e+00, -5.1431e+00],
        [ 6.9357e-02, -1.4656e+00],
        [ 2.2149e+00, -4.1076e+00],
        [ 1.6002e+00, -3.0593e+00],
        [ 8.5057e-01, -2.9603e+00],
        [ 4.7835e+00, -6.3372e+00],
        [ 1.2948e+00, -2.9928e+00],
        [ 5.2596e+00, -8.9710e+00],
        [ 2.1891e+00, -3.6463e+00],
        [ 3.8438e+00, -5.2929e+00],
        [ 2.6657e+00, -4.2964e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6872, 0.3128],
        [0.2701, 0.7299]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9220, 0.0780], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1841, 0.0927],
         [0.3623, 0.2825]],

        [[0.4705, 0.1033],
         [0.5589, 0.4665]],

        [[0.1330, 0.0996],
         [0.2985, 0.7381]],

        [[0.9135, 0.1037],
         [0.5604, 0.8019]],

        [[0.1635, 0.1012],
         [0.2867, 0.0369]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.016623577451856865
time is 1
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 21%|██        | 21/100 [16:23:50<61:23:34, 2797.65s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 22%|██▏       | 22/100 [17:10:30<60:37:33, 2798.12s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 23%|██▎       | 23/100 [17:58:41<60:27:02, 2826.26s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 24%|██▍       | 24/100 [18:43:32<58:48:09, 2785.39s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 25%|██▌       | 25/100 [19:25:24<56:19:12, 2703.36s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 26%|██▌       | 26/100 [20:07:51<54:36:19, 2656.48s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
