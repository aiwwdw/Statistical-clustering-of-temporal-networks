nohup: ignoring input
  0%|          | 0/100 [00:00<?, ?it/s]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/adj_generator.py:108: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1720538439675/work/torch/csrc/utils/tensor_new.cpp:278.)
  Y = torch.tensor(Y)
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  1%|          | 1/100 [48:59<80:49:30, 2939.10s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  2%|         | 2/100 [1:30:48<73:07:41, 2686.34s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  3%|         | 3/100 [2:04:46<64:24:04, 2390.15s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  4%|         | 4/100 [2:49:52<67:03:40, 2514.80s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  5%|         | 5/100 [3:32:25<66:43:36, 2528.60s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  6%|         | 6/100 [4:22:18<70:09:13, 2686.74s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  7%|         | 7/100 [5:03:44<67:42:29, 2620.96s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  8%|         | 8/100 [5:48:30<67:30:36, 2641.70s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  9%|         | 9/100 [6:33:04<67:01:48, 2651.74s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
-----------------------------------------------------------------------------------------
This iteration is 0
True Objective function: Loss = -11800.476253394521
Iteration 0: Loss = -23404.176487753608
Iteration 10: Loss = -11832.276440408066
Iteration 20: Loss = -11832.276922427816
1
Iteration 30: Loss = -11832.276914909911
2
Iteration 40: Loss = -11832.276914909911
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[0.3446, 0.6554],
        [0.6027, 0.3973]], dtype=torch.float64)
alpha: tensor([0.4815, 0.5185])
beta: tensor([[[0.2961, 0.1024],
         [0.0964, 0.2957]],

        [[0.6342, 0.0942],
         [0.1696, 0.4651]],

        [[0.8220, 0.1013],
         [0.7576, 0.1072]],

        [[0.7724, 0.0950],
         [0.3363, 0.2896]],

        [[0.7523, 0.0942],
         [0.4000, 0.1153]]], dtype=torch.float64)
time is 0
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.036488259774944735
Average Adjusted Rand Index: 0.9919993417272899
tensor([[-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23369.86114695491
Iteration 100: Loss = -12354.404069128474
Iteration 200: Loss = -12351.295033839911
Iteration 300: Loss = -12350.23947614894
Iteration 400: Loss = -12349.751823227962
Iteration 500: Loss = -12349.450571675696
Iteration 600: Loss = -12349.236611110076
Iteration 700: Loss = -12349.083606267672
Iteration 800: Loss = -12348.986189109199
Iteration 900: Loss = -12348.936033531045
Iteration 1000: Loss = -12348.913428851714
Iteration 1100: Loss = -12348.900145914607
Iteration 1200: Loss = -12348.890295049901
Iteration 1300: Loss = -12348.88260423724
Iteration 1400: Loss = -12348.876506799083
Iteration 1500: Loss = -12348.871497830018
Iteration 1600: Loss = -12348.867224490394
Iteration 1700: Loss = -12348.863498851493
Iteration 1800: Loss = -12348.860096283102
Iteration 1900: Loss = -12348.856885130772
Iteration 2000: Loss = -12348.85374220291
Iteration 2100: Loss = -12348.850483547818
Iteration 2200: Loss = -12348.84683105503
Iteration 2300: Loss = -12348.842398108322
Iteration 2400: Loss = -12348.836485359734
Iteration 2500: Loss = -12348.82724161865
Iteration 2600: Loss = -12348.81061213372
Iteration 2700: Loss = -12348.776197716134
Iteration 2800: Loss = -12348.720998586607
Iteration 2900: Loss = -12348.686658265018
Iteration 3000: Loss = -12348.674654452681
Iteration 3100: Loss = -12348.668794914483
Iteration 3200: Loss = -12348.66464444725
Iteration 3300: Loss = -12348.661108775386
Iteration 3400: Loss = -12348.657999876394
Iteration 3500: Loss = -12348.65511330365
Iteration 3600: Loss = -12348.652369529249
Iteration 3700: Loss = -12348.649676843741
Iteration 3800: Loss = -12348.647042393004
Iteration 3900: Loss = -12348.644274595132
Iteration 4000: Loss = -12348.641521349298
Iteration 4100: Loss = -12348.638706536814
Iteration 4200: Loss = -12348.635765303019
Iteration 4300: Loss = -12348.632849737467
Iteration 4400: Loss = -12348.630201597663
Iteration 4500: Loss = -12348.627943090716
Iteration 4600: Loss = -12348.625874867044
Iteration 4700: Loss = -12348.691554798052
1
Iteration 4800: Loss = -12348.621829569007
Iteration 4900: Loss = -12348.649642524408
1
Iteration 5000: Loss = -12348.61743895431
Iteration 5100: Loss = -12348.615848124757
Iteration 5200: Loss = -12348.6723237852
1
Iteration 5300: Loss = -12348.613403791345
Iteration 5400: Loss = -12348.612741127285
Iteration 5500: Loss = -12348.611611597022
Iteration 5600: Loss = -12348.610787660245
Iteration 5700: Loss = -12348.616139985465
1
Iteration 5800: Loss = -12348.609226557499
Iteration 5900: Loss = -12348.608514199927
Iteration 6000: Loss = -12348.608295014623
Iteration 6100: Loss = -12348.607252136877
Iteration 6200: Loss = -12348.606658303528
Iteration 6300: Loss = -12348.606295123285
Iteration 6400: Loss = -12348.605794504601
Iteration 6500: Loss = -12348.605478633042
Iteration 6600: Loss = -12348.616933045512
1
Iteration 6700: Loss = -12348.604787943565
Iteration 6800: Loss = -12348.604538007654
Iteration 6900: Loss = -12348.604250489489
Iteration 7000: Loss = -12348.604079482227
Iteration 7100: Loss = -12348.603762150187
Iteration 7200: Loss = -12348.603496280108
Iteration 7300: Loss = -12348.643933744406
1
Iteration 7400: Loss = -12348.60299514972
Iteration 7500: Loss = -12348.60271821167
Iteration 7600: Loss = -12348.602461924187
Iteration 7700: Loss = -12348.602374303935
Iteration 7800: Loss = -12348.601985374014
Iteration 7900: Loss = -12348.601816716959
Iteration 8000: Loss = -12348.61567889808
1
Iteration 8100: Loss = -12348.601452151042
Iteration 8200: Loss = -12348.601278323684
Iteration 8300: Loss = -12349.027674403369
1
Iteration 8400: Loss = -12348.600992256144
Iteration 8500: Loss = -12348.600856839239
Iteration 8600: Loss = -12348.600738268977
Iteration 8700: Loss = -12348.600738965668
1
Iteration 8800: Loss = -12348.600441521345
Iteration 8900: Loss = -12348.60033240571
Iteration 9000: Loss = -12348.619271142416
1
Iteration 9100: Loss = -12348.600053210446
Iteration 9200: Loss = -12348.599919817294
Iteration 9300: Loss = -12348.77729055451
1
Iteration 9400: Loss = -12348.599656873177
Iteration 9500: Loss = -12348.599522389764
Iteration 9600: Loss = -12348.599401587904
Iteration 9700: Loss = -12348.600314259347
1
Iteration 9800: Loss = -12348.599179548459
Iteration 9900: Loss = -12348.599062051919
Iteration 10000: Loss = -12348.600234803891
1
Iteration 10100: Loss = -12348.598900629308
Iteration 10200: Loss = -12348.598746961588
Iteration 10300: Loss = -12348.82521008936
1
Iteration 10400: Loss = -12348.598502047762
Iteration 10500: Loss = -12348.598448862245
Iteration 10600: Loss = -12348.599749265479
1
Iteration 10700: Loss = -12348.598319225075
Iteration 10800: Loss = -12348.598210029275
Iteration 10900: Loss = -12348.598122354731
Iteration 11000: Loss = -12348.598148936906
1
Iteration 11100: Loss = -12348.598027048265
Iteration 11200: Loss = -12348.597956467516
Iteration 11300: Loss = -12348.59810777576
1
Iteration 11400: Loss = -12348.59786763249
Iteration 11500: Loss = -12348.59779909061
Iteration 11600: Loss = -12348.605521201629
1
Iteration 11700: Loss = -12348.597717351764
Iteration 11800: Loss = -12348.597664792329
Iteration 11900: Loss = -12348.628245127278
1
Iteration 12000: Loss = -12348.59759378455
Iteration 12100: Loss = -12348.597560594739
Iteration 12200: Loss = -12349.160699136304
1
Iteration 12300: Loss = -12348.59747764382
Iteration 12400: Loss = -12348.597467950023
Iteration 12500: Loss = -12348.597828025077
1
Iteration 12600: Loss = -12348.597406505201
Iteration 12700: Loss = -12348.597409347294
1
Iteration 12800: Loss = -12348.597364512267
Iteration 12900: Loss = -12348.597441375641
1
Iteration 13000: Loss = -12348.59731450764
Iteration 13100: Loss = -12348.597281059008
Iteration 13200: Loss = -12348.599611864343
1
Iteration 13300: Loss = -12348.597242395424
Iteration 13400: Loss = -12348.597220888209
Iteration 13500: Loss = -12348.621033100146
1
Iteration 13600: Loss = -12348.597209096362
Iteration 13700: Loss = -12348.597180921122
Iteration 13800: Loss = -12348.65710383732
1
Iteration 13900: Loss = -12348.597181332063
2
Iteration 14000: Loss = -12348.597150225654
Iteration 14100: Loss = -12348.630842928615
1
Iteration 14200: Loss = -12348.60044209555
2
Iteration 14300: Loss = -12348.59711565732
Iteration 14400: Loss = -12348.626968618746
1
Iteration 14500: Loss = -12348.5970619613
Iteration 14600: Loss = -12348.645101375212
1
Iteration 14700: Loss = -12348.597055677723
Iteration 14800: Loss = -12348.640845916432
1
Iteration 14900: Loss = -12348.597058782532
2
Iteration 15000: Loss = -12348.629834728303
3
Iteration 15100: Loss = -12348.596989491196
Iteration 15200: Loss = -12348.599153150579
1
Iteration 15300: Loss = -12348.60000842234
2
Iteration 15400: Loss = -12348.597390501567
3
Iteration 15500: Loss = -12348.606291331487
4
Iteration 15600: Loss = -12348.596970340663
Iteration 15700: Loss = -12348.600057562848
1
Iteration 15800: Loss = -12348.597041681918
2
Iteration 15900: Loss = -12348.646551684571
3
Iteration 16000: Loss = -12348.596916837192
Iteration 16100: Loss = -12348.597198544274
1
Iteration 16200: Loss = -12348.596959349396
2
Iteration 16300: Loss = -12348.596962875803
3
Iteration 16400: Loss = -12348.5987426838
4
Iteration 16500: Loss = -12348.59690417467
Iteration 16600: Loss = -12348.627046491409
1
Iteration 16700: Loss = -12348.5988623643
2
Iteration 16800: Loss = -12348.5976079295
3
Iteration 16900: Loss = -12348.596900990657
Iteration 17000: Loss = -12348.597387395454
1
Iteration 17100: Loss = -12348.596972464024
2
Iteration 17200: Loss = -12348.59698642981
3
Iteration 17300: Loss = -12348.597843000785
4
Iteration 17400: Loss = -12348.596891075651
Iteration 17500: Loss = -12348.875486138977
1
Iteration 17600: Loss = -12348.597833606938
2
Iteration 17700: Loss = -12348.731314093078
3
Iteration 17800: Loss = -12348.596869789373
Iteration 17900: Loss = -12348.62538403032
1
Iteration 18000: Loss = -12348.596863644692
Iteration 18100: Loss = -12348.599208543228
1
Iteration 18200: Loss = -12348.596890019704
2
Iteration 18300: Loss = -12348.601018611174
3
Iteration 18400: Loss = -12348.59687680348
4
Iteration 18500: Loss = -12348.597829517888
5
Iteration 18600: Loss = -12348.606880153447
6
Iteration 18700: Loss = -12348.59684323454
Iteration 18800: Loss = -12348.597002509161
1
Iteration 18900: Loss = -12348.596851524533
2
Iteration 19000: Loss = -12348.59795293817
3
Iteration 19100: Loss = -12348.596861293592
4
Iteration 19200: Loss = -12348.613055009047
5
Iteration 19300: Loss = -12348.596825932142
Iteration 19400: Loss = -12348.601236723867
1
Iteration 19500: Loss = -12348.596828257341
2
Iteration 19600: Loss = -12348.597891423786
3
Iteration 19700: Loss = -12348.596837237405
4
Iteration 19800: Loss = -12348.61009267433
5
Iteration 19900: Loss = -12348.596848286386
6
tensor([[-4.8408,  0.2256],
        [-4.7865,  0.1713],
        [-4.8187,  0.2035],
        [-4.7574,  0.1422],
        [-3.4076, -1.2077],
        [-4.2126, -0.4026],
        [-2.5656, -2.0496],
        [-4.2474, -0.3678],
        [-3.4967, -1.1186],
        [-3.9415, -0.6738],
        [-4.0841, -0.5311],
        [-6.0528,  1.4376],
        [-5.3333,  0.7180],
        [-5.2384,  0.6232],
        [-5.5263,  0.9110],
        [-5.4633,  0.8480],
        [-4.0653, -0.5499],
        [-4.2305, -0.3847],
        [-3.6043, -1.0109],
        [-5.5177,  0.9024],
        [-4.5890, -0.0262],
        [-5.6439,  1.0286],
        [-4.5991, -0.0161],
        [-4.9106,  0.2954],
        [-3.6667, -0.9485],
        [-5.4572,  0.8420],
        [-4.8942,  0.2789],
        [-4.6561,  0.0409],
        [-4.7093,  0.0941],
        [-3.8868, -0.7284],
        [-4.2798, -0.3354],
        [-4.9584,  0.3431],
        [-3.8938, -0.7214],
        [-5.3560,  0.7408],
        [-2.7745, -1.8407],
        [-4.4307, -0.1846],
        [-4.6873,  0.0721],
        [-4.9229,  0.3077],
        [-4.1514, -0.4638],
        [-5.3403,  0.7251],
        [-4.5267, -0.0885],
        [-2.9402, -1.6750],
        [-2.9522, -1.6630],
        [-5.2118,  0.5966],
        [-4.2179, -0.3974],
        [-4.2460, -0.3692],
        [-5.0844,  0.4692],
        [-4.1746, -0.4407],
        [-3.8934, -0.7218],
        [-4.7445,  0.1292],
        [-5.0547,  0.4394],
        [-4.7444,  0.1292],
        [-4.6237,  0.0085],
        [-4.4583, -0.1569],
        [-4.7461,  0.1309],
        [-5.5123,  0.8970],
        [-4.1134, -0.5018],
        [-4.2224, -0.3928],
        [-4.9209,  0.3056],
        [-5.1218,  0.5066],
        [-5.7622,  1.1470],
        [-4.0334, -0.5818],
        [-3.9514, -0.6638],
        [-3.3076, -1.3077],
        [-4.4227, -0.1925],
        [-4.7544,  0.1392],
        [-5.1163,  0.5010],
        [-3.7137, -0.9015],
        [-4.7248,  0.1096],
        [-5.3081,  0.6929],
        [-4.9165,  0.3013],
        [-5.2447,  0.6294],
        [-3.7604, -0.8549],
        [-5.3570,  0.7418],
        [-4.7513,  0.1361],
        [-5.3821,  0.7669],
        [-4.9159,  0.3007],
        [-4.6687,  0.0534],
        [-3.6204, -0.9948],
        [-4.0760, -0.5392],
        [-4.3392, -0.2761],
        [-3.5008, -1.1144],
        [-4.4321, -0.1831],
        [-5.4991,  0.8839],
        [-4.7420,  0.1267],
        [-5.4332,  0.8179],
        [-5.0700,  0.4548],
        [-3.5451, -1.0701],
        [-3.9168, -0.6984],
        [-4.5931, -0.0221],
        [-3.2687, -1.3465],
        [-5.2575,  0.6423],
        [-3.9250, -0.6902],
        [-4.7258,  0.1106],
        [-4.6690,  0.0538],
        [-4.0797, -0.5356],
        [-4.5696, -0.0456],
        [-5.3714,  0.7562],
        [-4.8603,  0.2451],
        [-4.4010, -0.2142]], dtype=torch.float64, requires_grad=True)
pi: tensor([[2.5639e-01, 7.4361e-01],
        [1.2986e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0301, 0.9699], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2982, 0.2556],
         [0.0964, 0.1981]],

        [[0.6342, 0.2581],
         [0.1696, 0.4651]],

        [[0.8220, 0.1940],
         [0.7576, 0.1072]],

        [[0.7724, 0.2286],
         [0.3363, 0.2896]],

        [[0.7523, 0.8483],
         [0.4000, 0.1153]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -27457.64041032714
Iteration 10: Loss = -12349.058597370362
Iteration 20: Loss = -12349.031935749123
Iteration 30: Loss = -12349.007632155097
Iteration 40: Loss = -12348.986789267537
Iteration 50: Loss = -12348.970022185124
Iteration 60: Loss = -12348.957250788933
Iteration 70: Loss = -12348.947837917432
Iteration 80: Loss = -12348.940911431288
Iteration 90: Loss = -12348.935950773104
Iteration 100: Loss = -12348.932220661809
Iteration 110: Loss = -12348.929203232039
Iteration 120: Loss = -12348.926472778707
Iteration 130: Loss = -12348.924126665994
Iteration 140: Loss = -12348.922385118942
Iteration 150: Loss = -12348.921176141484
Iteration 160: Loss = -12348.920363387882
Iteration 170: Loss = -12348.91976920581
Iteration 180: Loss = -12348.919319862358
Iteration 190: Loss = -12348.91898455055
Iteration 200: Loss = -12348.918790651374
Iteration 210: Loss = -12348.918594066954
Iteration 220: Loss = -12348.918500370815
Iteration 230: Loss = -12348.918388914506
Iteration 240: Loss = -12348.918320846897
Iteration 250: Loss = -12348.918285066338
Iteration 260: Loss = -12348.918207746932
Iteration 270: Loss = -12348.918153628034
Iteration 280: Loss = -12348.918173211641
1
Iteration 290: Loss = -12348.918173574551
2
Iteration 300: Loss = -12348.918098787062
Iteration 310: Loss = -12348.918107669217
1
Iteration 320: Loss = -12348.918117886853
2
Iteration 330: Loss = -12348.918094108036
Iteration 340: Loss = -12348.91810338542
1
Iteration 350: Loss = -12348.918088930468
Iteration 360: Loss = -12348.91810629891
1
Iteration 370: Loss = -12348.918097055383
2
Iteration 380: Loss = -12348.918096463585
3
Stopping early at iteration 379 due to no improvement.
pi: tensor([[0.0142, 0.9858],
        [0.0106, 0.9894]], dtype=torch.float64)
alpha: tensor([0.0109, 0.9891])
beta: tensor([[[0.2604, 0.2649],
         [0.8230, 0.1959]],

        [[0.9499, 0.2059],
         [0.4084, 0.8417]],

        [[0.5796, 0.2254],
         [0.9818, 0.3484]],

        [[0.8464, 0.2380],
         [0.7022, 0.6079]],

        [[0.5025, 0.2865],
         [0.2419, 0.3359]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27457.302274415015
Iteration 100: Loss = -12426.682056268115
Iteration 200: Loss = -12406.482855057247
Iteration 300: Loss = -12393.559232903997
Iteration 400: Loss = -12381.700958484704
Iteration 500: Loss = -12373.303326033645
Iteration 600: Loss = -12365.01894792322
Iteration 700: Loss = -12360.36736693743
Iteration 800: Loss = -12355.265089789293
Iteration 900: Loss = -12352.54721894632
Iteration 1000: Loss = -12351.798240989321
Iteration 1100: Loss = -12351.189017459868
Iteration 1200: Loss = -12350.221080974332
Iteration 1300: Loss = -12349.841907240785
Iteration 1400: Loss = -12349.661605133348
Iteration 1500: Loss = -12349.529844823874
Iteration 1600: Loss = -12349.427143340967
Iteration 1700: Loss = -12349.344455276021
Iteration 1800: Loss = -12349.276812001046
Iteration 1900: Loss = -12349.223277550538
Iteration 2000: Loss = -12349.172919434079
Iteration 2100: Loss = -12349.132330355242
Iteration 2200: Loss = -12349.098989099202
Iteration 2300: Loss = -12349.067118047866
Iteration 2400: Loss = -12349.040610966982
Iteration 2500: Loss = -12349.020040047923
Iteration 2600: Loss = -12348.996677295976
Iteration 2700: Loss = -12348.978307975298
Iteration 2800: Loss = -12348.961949030525
Iteration 2900: Loss = -12348.947214604297
Iteration 3000: Loss = -12348.933966492576
Iteration 3100: Loss = -12348.921851833536
Iteration 3200: Loss = -12349.127207210371
1
Iteration 3300: Loss = -12348.900699390322
Iteration 3400: Loss = -12348.891280808792
Iteration 3500: Loss = -12348.8824630422
Iteration 3600: Loss = -12348.881604283904
Iteration 3700: Loss = -12348.866321260872
Iteration 3800: Loss = -12348.858770307905
Iteration 3900: Loss = -12348.85167261005
Iteration 4000: Loss = -12348.844921984728
Iteration 4100: Loss = -12348.83845265545
Iteration 4200: Loss = -12348.832347815776
Iteration 4300: Loss = -12348.839015058416
1
Iteration 4400: Loss = -12348.820883321096
Iteration 4500: Loss = -12348.81555862365
Iteration 4600: Loss = -12348.810349935557
Iteration 4700: Loss = -12348.806927215466
Iteration 4800: Loss = -12348.800346159665
Iteration 4900: Loss = -12348.795465672485
Iteration 5000: Loss = -12349.400160057357
1
Iteration 5100: Loss = -12348.78593460132
Iteration 5200: Loss = -12348.781288835035
Iteration 5300: Loss = -12348.776780690767
Iteration 5400: Loss = -12348.773694034642
Iteration 5500: Loss = -12348.7680458999
Iteration 5600: Loss = -12348.763976639351
Iteration 5700: Loss = -12348.760178787703
Iteration 5800: Loss = -12348.756651074758
Iteration 5900: Loss = -12348.753152093706
Iteration 6000: Loss = -12348.75013965304
Iteration 6100: Loss = -12348.748162623408
Iteration 6200: Loss = -12348.817835112812
1
Iteration 6300: Loss = -12348.741875353773
Iteration 6400: Loss = -12348.740821116076
Iteration 6500: Loss = -12348.737350468446
Iteration 6600: Loss = -12348.735827594626
Iteration 6700: Loss = -12348.733338743861
Iteration 6800: Loss = -12348.731503254836
Iteration 6900: Loss = -12348.729759821523
Iteration 7000: Loss = -12348.729351038297
Iteration 7100: Loss = -12348.726532045474
Iteration 7200: Loss = -12348.725017873627
Iteration 7300: Loss = -12348.723742271084
Iteration 7400: Loss = -12348.722132174475
Iteration 7500: Loss = -12348.720777667477
Iteration 7600: Loss = -12348.719709114
Iteration 7700: Loss = -12348.718290430324
Iteration 7800: Loss = -12348.718595877068
1
Iteration 7900: Loss = -12348.715828748462
Iteration 8000: Loss = -12348.716547807911
1
Iteration 8100: Loss = -12348.713629188864
Iteration 8200: Loss = -12348.712594052016
Iteration 8300: Loss = -12348.711676407607
Iteration 8400: Loss = -12348.710630583102
Iteration 8500: Loss = -12348.718067938662
1
Iteration 8600: Loss = -12348.708893045648
Iteration 8700: Loss = -12348.746696635822
1
Iteration 8800: Loss = -12348.709145388864
2
Iteration 8900: Loss = -12348.71122239722
3
Iteration 9000: Loss = -12348.706383771208
Iteration 9100: Loss = -12348.705310056137
Iteration 9200: Loss = -12348.704907925196
Iteration 9300: Loss = -12348.73397902151
1
Iteration 9400: Loss = -12348.703797256454
Iteration 9500: Loss = -12348.712200547192
1
Iteration 9600: Loss = -12348.703990346869
2
Iteration 9700: Loss = -12348.702640894364
Iteration 9800: Loss = -12348.70440859892
1
Iteration 9900: Loss = -12348.703480225071
2
Iteration 10000: Loss = -12348.70145422136
Iteration 10100: Loss = -12348.701067290338
Iteration 10200: Loss = -12348.703881737478
1
Iteration 10300: Loss = -12348.700465111719
Iteration 10400: Loss = -12348.700200672387
Iteration 10500: Loss = -12348.701247527555
1
Iteration 10600: Loss = -12348.699754765425
Iteration 10700: Loss = -12348.699641057665
Iteration 10800: Loss = -12348.731172771324
1
Iteration 10900: Loss = -12348.699243716528
Iteration 11000: Loss = -12348.700255520314
1
Iteration 11100: Loss = -12348.700072973708
2
Iteration 11200: Loss = -12348.698641128558
Iteration 11300: Loss = -12348.735605610404
1
Iteration 11400: Loss = -12348.69854102031
Iteration 11500: Loss = -12348.69819685147
Iteration 11600: Loss = -12348.698348461605
1
Iteration 11700: Loss = -12348.805296856252
2
Iteration 11800: Loss = -12348.697948292998
Iteration 11900: Loss = -12348.715784610997
1
Iteration 12000: Loss = -12348.697789582213
Iteration 12100: Loss = -12348.69776679587
Iteration 12200: Loss = -12348.697443553037
Iteration 12300: Loss = -12348.697565539958
1
Iteration 12400: Loss = -12348.698358067317
2
Iteration 12500: Loss = -12348.94964764904
3
Iteration 12600: Loss = -12348.697313014556
Iteration 12700: Loss = -12348.697095555237
Iteration 12800: Loss = -12348.697208566617
1
Iteration 12900: Loss = -12348.697160291986
2
Iteration 13000: Loss = -12348.69731785712
3
Iteration 13100: Loss = -12348.699343084943
4
Iteration 13200: Loss = -12348.696894802333
Iteration 13300: Loss = -12348.697016699114
1
Iteration 13400: Loss = -12348.725022737697
2
Iteration 13500: Loss = -12348.696848779886
Iteration 13600: Loss = -12348.696822355472
Iteration 13700: Loss = -12349.029022896484
1
Iteration 13800: Loss = -12348.696651194376
Iteration 13900: Loss = -12348.701286153764
1
Iteration 14000: Loss = -12348.696621023662
Iteration 14100: Loss = -12348.70745156875
1
Iteration 14200: Loss = -12348.696477136524
Iteration 14300: Loss = -12348.699130099265
1
Iteration 14400: Loss = -12348.696472151927
Iteration 14500: Loss = -12348.72698473195
1
Iteration 14600: Loss = -12348.697327358443
2
Iteration 14700: Loss = -12348.6963704957
Iteration 14800: Loss = -12348.696509046576
1
Iteration 14900: Loss = -12348.696676899091
2
Iteration 15000: Loss = -12348.696492491039
3
Iteration 15100: Loss = -12348.696764806622
4
Iteration 15200: Loss = -12348.69618352403
Iteration 15300: Loss = -12348.697541117821
1
Iteration 15400: Loss = -12348.740792585118
2
Iteration 15500: Loss = -12348.69762006317
3
Iteration 15600: Loss = -12348.696272144902
4
Iteration 15700: Loss = -12348.696094964382
Iteration 15800: Loss = -12348.696977930755
1
Iteration 15900: Loss = -12348.69614086045
2
Iteration 16000: Loss = -12348.697564711327
3
Iteration 16100: Loss = -12348.696047620468
Iteration 16200: Loss = -12348.730947390188
1
Iteration 16300: Loss = -12348.696074654103
2
Iteration 16400: Loss = -12348.700876670038
3
Iteration 16500: Loss = -12348.696081461843
4
Iteration 16600: Loss = -12348.696043905325
Iteration 16700: Loss = -12348.696170351575
1
Iteration 16800: Loss = -12348.69596136362
Iteration 16900: Loss = -12348.696101421894
1
Iteration 17000: Loss = -12348.697562033793
2
Iteration 17100: Loss = -12348.702452951129
3
Iteration 17200: Loss = -12348.780513816117
4
Iteration 17300: Loss = -12348.696027609862
5
Iteration 17400: Loss = -12348.780764582565
6
Iteration 17500: Loss = -12348.696037209156
7
Iteration 17600: Loss = -12348.695915052755
Iteration 17700: Loss = -12348.77399141763
1
Iteration 17800: Loss = -12348.696010475665
2
Iteration 17900: Loss = -12348.699079918342
3
Iteration 18000: Loss = -12348.69593329876
4
Iteration 18100: Loss = -12348.697479695866
5
Iteration 18200: Loss = -12348.695917864628
6
Iteration 18300: Loss = -12348.696216667064
7
Iteration 18400: Loss = -12348.728867681913
8
Iteration 18500: Loss = -12348.696075278851
9
Iteration 18600: Loss = -12348.696693002888
10
Stopping early at iteration 18600 due to no improvement.
tensor([[ 5.5103, -7.9652],
        [ 5.1383, -8.3764],
        [ 5.6792, -7.7840],
        [ 6.0906, -7.4775],
        [ 4.6781, -9.2933],
        [ 5.8920, -7.4315],
        [ 5.5414, -8.7888],
        [ 6.0302, -7.7420],
        [ 5.4585, -8.5338],
        [ 5.9935, -7.8659],
        [ 5.9258, -7.6506],
        [ 5.7864, -7.2210],
        [ 5.6001, -7.5846],
        [ 5.7507, -7.4422],
        [ 5.8880, -7.2834],
        [ 5.9486, -7.3358],
        [ 6.0453, -7.6812],
        [ 5.6159, -8.0636],
        [ 6.0842, -7.6916],
        [ 5.7420, -7.5541],
        [ 6.1113, -7.5147],
        [ 5.5711, -7.6278],
        [ 5.9544, -7.6563],
        [ 5.7918, -7.7638],
        [ 6.1900, -7.6813],
        [ 5.7725, -7.4428],
        [ 5.3736, -8.1995],
        [ 5.7589, -7.8105],
        [ 6.0878, -7.4871],
        [ 5.8046, -7.9343],
        [ 5.7428, -7.9761],
        [ 5.9782, -7.5020],
        [ 6.1846, -7.6201],
        [ 5.9038, -7.4611],
        [ 4.6934, -9.3086],
        [ 5.5769, -7.9602],
        [ 6.0084, -7.4293],
        [ 5.9989, -7.3996],
        [ 5.7707, -7.9204],
        [ 5.8602, -7.2714],
        [ 5.8474, -7.4525],
        [ 6.2311, -7.7249],
        [ 5.7970, -8.3072],
        [ 5.5776, -7.7119],
        [ 5.3760, -8.1758],
        [ 6.1086, -7.6755],
        [ 4.4537, -9.0689],
        [ 6.0850, -7.4725],
        [ 5.9567, -7.8733],
        [ 5.5510, -7.9143],
        [ 5.7011, -7.6756],
        [ 5.7039, -7.8599],
        [ 5.6959, -7.8782],
        [ 5.5472, -8.1621],
        [ 6.0668, -7.4531],
        [ 5.9224, -7.3847],
        [ 6.1146, -7.6669],
        [ 4.4369, -9.0521],
        [ 5.9015, -7.6493],
        [ 4.9075, -8.2039],
        [ 5.5958, -7.5913],
        [ 5.7256, -8.1077],
        [ 6.0476, -7.7162],
        [ 5.6557, -8.4824],
        [ 5.8712, -7.8719],
        [ 4.5127, -9.1279],
        [ 5.1325, -8.2591],
        [ 6.2553, -7.6886],
        [ 5.8333, -7.7179],
        [ 5.6950, -7.6731],
        [ 6.0176, -7.4076],
        [ 5.2558, -7.8521],
        [ 5.9057, -7.9742],
        [ 5.6054, -7.7448],
        [ 6.0769, -7.4724],
        [ 5.7777, -7.4040],
        [ 5.9191, -7.4082],
        [ 4.5131, -9.1284],
        [ 6.2877, -7.6742],
        [ 6.0789, -7.7415],
        [ 6.0555, -7.6178],
        [ 6.1865, -7.6111],
        [ 6.0552, -7.4831],
        [ 5.3168, -7.9710],
        [ 6.0717, -7.5292],
        [ 5.9113, -7.4355],
        [ 5.6130, -7.8646],
        [ 6.2095, -7.5960],
        [ 5.5651, -7.7221],
        [ 6.1018, -7.5509],
        [ 6.3176, -7.7064],
        [ 5.9628, -7.4059],
        [ 6.2104, -7.6651],
        [ 6.0468, -7.4492],
        [ 5.5818, -7.8661],
        [ 6.1026, -7.5670],
        [ 5.7829, -7.8239],
        [ 5.9053, -7.4411],
        [ 5.9444, -7.4188],
        [ 6.0322, -7.6552]], dtype=torch.float64, requires_grad=True)
pi: tensor([[4.8519e-08, 1.0000e+00],
        [1.2403e-01, 8.7597e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 1.3290e-06], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1995, 0.1901],
         [0.8230, 0.1988]],

        [[0.9499, 0.6462],
         [0.4084, 0.8417]],

        [[0.5796, 0.2030],
         [0.9818, 0.3484]],

        [[0.8464, 0.2033],
         [0.7022, 0.6079]],

        [[0.5025, 0.1910],
         [0.2419, 0.3359]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0009354685621591986
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -16111.066733063246
Iteration 10: Loss = -12348.95600956908
Iteration 20: Loss = -12348.935275626478
Iteration 30: Loss = -12348.930314724164
Iteration 40: Loss = -12348.927029100463
Iteration 50: Loss = -12348.9245150627
Iteration 60: Loss = -12348.922704306184
Iteration 70: Loss = -12348.92142622287
Iteration 80: Loss = -12348.920567657035
Iteration 90: Loss = -12348.919931758279
Iteration 100: Loss = -12348.919437600307
Iteration 110: Loss = -12348.919075274971
Iteration 120: Loss = -12348.918835197153
Iteration 130: Loss = -12348.91867590716
Iteration 140: Loss = -12348.918511717338
Iteration 150: Loss = -12348.918401729949
Iteration 160: Loss = -12348.918372593855
Iteration 170: Loss = -12348.918274194662
Iteration 180: Loss = -12348.918237761316
Iteration 190: Loss = -12348.918203984264
Iteration 200: Loss = -12348.918175294717
Iteration 210: Loss = -12348.91813700165
Iteration 220: Loss = -12348.918113965741
Iteration 230: Loss = -12348.918126182862
1
Iteration 240: Loss = -12348.918125830336
2
Iteration 250: Loss = -12348.918107816344
Iteration 260: Loss = -12348.918115888051
1
Iteration 270: Loss = -12348.918126145743
2
Iteration 280: Loss = -12348.91811874591
3
Stopping early at iteration 279 due to no improvement.
pi: tensor([[0.9894, 0.0106],
        [0.9858, 0.0142]], dtype=torch.float64)
alpha: tensor([0.9891, 0.0109])
beta: tensor([[[0.1959, 0.2649],
         [0.4448, 0.2604]],

        [[0.2493, 0.2059],
         [0.4682, 0.4714]],

        [[0.5589, 0.2254],
         [0.6152, 0.7896]],

        [[0.0824, 0.2380],
         [0.1631, 0.3918]],

        [[0.4751, 0.2865],
         [0.9817, 0.0948]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -16111.145557481925
Iteration 100: Loss = -12349.34925366527
Iteration 200: Loss = -12348.85369118055
Iteration 300: Loss = -12348.768811508353
Iteration 400: Loss = -12348.738249167665
Iteration 500: Loss = -12348.719995047884
Iteration 600: Loss = -12348.70484412033
Iteration 700: Loss = -12348.688124175833
Iteration 800: Loss = -12348.67156174058
Iteration 900: Loss = -12348.65299828612
Iteration 1000: Loss = -12348.632828256743
Iteration 1100: Loss = -12348.612937951066
Iteration 1200: Loss = -12348.59569189248
Iteration 1300: Loss = -12348.58241927529
Iteration 1400: Loss = -12348.573250669084
Iteration 1500: Loss = -12348.56718365828
Iteration 1600: Loss = -12348.56333765596
Iteration 1700: Loss = -12348.560694149604
Iteration 1800: Loss = -12348.558870349061
Iteration 1900: Loss = -12348.557444641076
Iteration 2000: Loss = -12348.556194780685
Iteration 2100: Loss = -12348.555094206467
Iteration 2200: Loss = -12348.553993803956
Iteration 2300: Loss = -12348.553016666081
Iteration 2400: Loss = -12348.55192205593
Iteration 2500: Loss = -12348.550898911917
Iteration 2600: Loss = -12348.54972973771
Iteration 2700: Loss = -12348.548483505549
Iteration 2800: Loss = -12348.54708844355
Iteration 2900: Loss = -12348.545398493292
Iteration 3000: Loss = -12348.543368326187
Iteration 3100: Loss = -12348.540802434401
Iteration 3200: Loss = -12348.537416508589
Iteration 3300: Loss = -12348.53280947885
Iteration 3400: Loss = -12348.526150398744
Iteration 3500: Loss = -12348.515804648687
Iteration 3600: Loss = -12348.49837512517
Iteration 3700: Loss = -12348.465488507965
Iteration 3800: Loss = -12348.397779347593
Iteration 3900: Loss = -12348.282755638953
Iteration 4000: Loss = -12348.176458490201
Iteration 4100: Loss = -12348.105774527661
Iteration 4200: Loss = -12348.050812825682
Iteration 4300: Loss = -12348.00422315374
Iteration 4400: Loss = -12347.97753002274
Iteration 4500: Loss = -12347.9658705464
Iteration 4600: Loss = -12347.958701190613
Iteration 4700: Loss = -12347.953279709638
Iteration 4800: Loss = -12347.94895926736
Iteration 4900: Loss = -12347.945480293507
Iteration 5000: Loss = -12347.94256766617
Iteration 5100: Loss = -12347.940092211938
Iteration 5200: Loss = -12347.938582810108
Iteration 5300: Loss = -12347.936225505046
Iteration 5400: Loss = -12347.934597380872
Iteration 5500: Loss = -12347.933285183684
Iteration 5600: Loss = -12347.932606820785
Iteration 5700: Loss = -12347.937802475153
1
Iteration 5800: Loss = -12347.964381313366
2
Iteration 5900: Loss = -12347.929144997657
Iteration 6000: Loss = -12347.928360771544
Iteration 6100: Loss = -12347.92759068342
Iteration 6200: Loss = -12347.927013497763
Iteration 6300: Loss = -12347.926353499595
Iteration 6400: Loss = -12347.925886805288
Iteration 6500: Loss = -12347.925343041656
Iteration 6600: Loss = -12347.925730303774
1
Iteration 6700: Loss = -12347.92440713336
Iteration 6800: Loss = -12347.926330886312
1
Iteration 6900: Loss = -12347.923695002886
Iteration 7000: Loss = -12347.971858753106
1
Iteration 7100: Loss = -12347.923055193618
Iteration 7200: Loss = -12347.924736034174
1
Iteration 7300: Loss = -12347.922497043786
Iteration 7400: Loss = -12347.93460165446
1
Iteration 7500: Loss = -12347.922151730454
Iteration 7600: Loss = -12347.92221952669
1
Iteration 7700: Loss = -12347.92158791118
Iteration 7800: Loss = -12347.921567266194
Iteration 7900: Loss = -12347.928538197988
1
Iteration 8000: Loss = -12347.92405097
2
Iteration 8100: Loss = -12348.042569355388
3
Iteration 8200: Loss = -12347.920766608744
Iteration 8300: Loss = -12347.924805952858
1
Iteration 8400: Loss = -12347.920624099166
Iteration 8500: Loss = -12347.920501819666
Iteration 8600: Loss = -12347.920287135747
Iteration 8700: Loss = -12347.920824479652
1
Iteration 8800: Loss = -12347.9200413469
Iteration 8900: Loss = -12347.923651746949
1
Iteration 9000: Loss = -12347.919908104997
Iteration 9100: Loss = -12347.919709479598
Iteration 9200: Loss = -12347.91990499549
1
Iteration 9300: Loss = -12347.91962348207
Iteration 9400: Loss = -12347.92506572514
1
Iteration 9500: Loss = -12347.919504057501
Iteration 9600: Loss = -12347.919832436428
1
Iteration 9700: Loss = -12348.199721302588
2
Iteration 9800: Loss = -12347.919324406195
Iteration 9900: Loss = -12347.92263243773
1
Iteration 10000: Loss = -12347.942094444848
2
Iteration 10100: Loss = -12347.919169906118
Iteration 10200: Loss = -12347.926438584256
1
Iteration 10300: Loss = -12347.91906401854
Iteration 10400: Loss = -12347.936091920363
1
Iteration 10500: Loss = -12347.939316217657
2
Iteration 10600: Loss = -12347.918963479719
Iteration 10700: Loss = -12347.959196081854
1
Iteration 10800: Loss = -12347.918849826097
Iteration 10900: Loss = -12347.942162006508
1
Iteration 11000: Loss = -12347.91879036563
Iteration 11100: Loss = -12347.919699287846
1
Iteration 11200: Loss = -12348.1847114849
2
Iteration 11300: Loss = -12347.918711360548
Iteration 11400: Loss = -12348.058817822968
1
Iteration 11500: Loss = -12347.91874200711
2
Iteration 11600: Loss = -12347.922741813165
3
Iteration 11700: Loss = -12347.922998048964
4
Iteration 11800: Loss = -12347.949592687231
5
Iteration 11900: Loss = -12347.98349793324
6
Iteration 12000: Loss = -12347.920597825592
7
Iteration 12100: Loss = -12347.920334878794
8
Iteration 12200: Loss = -12347.942609599091
9
Iteration 12300: Loss = -12347.919674319413
10
Stopping early at iteration 12300 due to no improvement.
tensor([[-7.1676e-01, -1.1988e+00],
        [-9.3403e-01, -1.3108e+00],
        [-5.3103e-01, -1.0849e+00],
        [-1.1884e-01, -1.9186e+00],
        [-8.9687e-01, -1.9487e+00],
        [-7.7297e-01, -1.5391e+00],
        [-8.9641e-01, -1.1981e+00],
        [-1.0481e+00, -2.8411e+00],
        [ 1.6987e-02, -1.4254e+00],
        [ 4.0561e-01, -2.3272e+00],
        [-2.3977e-01, -1.4840e+00],
        [ 1.1777e+00, -2.6729e+00],
        [ 7.4270e-02, -1.6903e+00],
        [ 8.8006e-01, -2.3175e+00],
        [-3.8276e-01, -1.3688e+00],
        [-4.3955e-01, -1.5837e+00],
        [-8.7343e-01, -6.4162e-01],
        [-1.6276e+00, -9.9910e-01],
        [-8.7680e-02, -1.3955e+00],
        [ 1.7631e-01, -1.8678e+00],
        [ 1.8972e-01, -2.2842e+00],
        [ 1.6747e-01, -1.7947e+00],
        [ 8.7469e-01, -2.6053e+00],
        [ 7.0213e-01, -2.4006e+00],
        [ 1.4955e-01, -1.5547e+00],
        [ 5.2625e-01, -1.9978e+00],
        [ 8.3319e-04, -1.4231e+00],
        [-1.1590e-01, -1.2883e+00],
        [ 3.8070e-01, -1.7735e+00],
        [-7.9803e-01, -3.8172e+00],
        [ 9.8208e-02, -1.5299e+00],
        [ 2.5834e-01, -1.6546e+00],
        [-1.6378e+00, -1.4815e+00],
        [-4.9720e-01, -1.4885e+00],
        [-1.9020e+00, -6.9961e-01],
        [-1.8320e+00, -2.3195e+00],
        [-5.2252e-01, -8.9740e-01],
        [-1.9873e-01, -3.4179e+00],
        [-1.0846e+00, -3.4313e-01],
        [ 4.9101e-01, -2.2339e+00],
        [-9.2895e-02, -1.6772e+00],
        [-1.3327e+00, -9.5799e-01],
        [-1.8553e+00, -9.4749e-01],
        [ 9.4484e-01, -2.6592e+00],
        [-5.8684e-01, -1.2990e+00],
        [-1.1125e+00, -2.5053e+00],
        [-5.3760e-01, -2.4652e+00],
        [-8.4327e-01, -1.2724e+00],
        [-4.2391e-01, -1.2870e+00],
        [-4.5091e-01, -2.6435e+00],
        [-3.4479e-01, -2.6095e+00],
        [-1.7708e-01, -2.9373e+00],
        [ 2.1249e-01, -1.6013e+00],
        [ 4.4916e-01, -2.0430e+00],
        [-2.6487e-01, -1.2248e+00],
        [ 2.7519e-01, -1.6635e+00],
        [-5.4912e-02, -1.3597e+00],
        [-2.4618e-01, -1.1489e+00],
        [ 7.3099e-01, -2.1844e+00],
        [-1.9278e+00, -2.6860e+00],
        [ 6.7563e-01, -2.3530e+00],
        [-1.4299e+00, -1.3715e+00],
        [-1.1774e+00, -3.2990e+00],
        [-1.3975e+00, -4.8177e-01],
        [-1.1388e-01, -1.9952e+00],
        [-3.8000e-01, -2.0996e+00],
        [ 3.0014e-01, -1.7171e+00],
        [-1.4373e+00, -4.8588e-01],
        [-5.8297e-01, -2.7901e+00],
        [-9.4267e-03, -1.4008e+00],
        [-4.9863e-01, -3.8088e+00],
        [ 2.0349e-01, -2.6762e+00],
        [-4.9727e-02, -2.2650e+00],
        [-1.1350e+00, -2.7103e+00],
        [ 9.3997e-01, -2.4582e+00],
        [-2.4086e-01, -1.1665e+00],
        [ 9.9778e-01, -2.5236e+00],
        [-1.7151e-01, -1.6657e+00],
        [-4.1925e-01, -1.1090e+00],
        [ 2.6901e-01, -1.7742e+00],
        [-7.9493e-01, -1.1718e+00],
        [-1.2836e+00, -1.1923e+00],
        [ 7.8367e-02, -1.8133e+00],
        [ 5.9676e-01, -2.0224e+00],
        [ 6.3464e-01, -2.0284e+00],
        [ 9.6791e-01, -2.4673e+00],
        [ 7.5166e-01, -2.1394e+00],
        [-1.2838e-01, -1.2897e+00],
        [-2.9581e-01, -1.2777e+00],
        [ 4.4381e-02, -1.5962e+00],
        [-6.1862e-01, -1.0968e+00],
        [-9.7206e-01, -2.9386e+00],
        [ 4.4524e-01, -2.2611e+00],
        [-2.0209e-01, -1.2205e+00],
        [-9.1449e-01, -1.8783e+00],
        [-1.1164e+00, -1.3781e+00],
        [ 7.2254e-01, -2.1524e+00],
        [ 4.5278e-01, -1.8742e+00],
        [-1.4915e-01, -1.6232e+00],
        [ 1.4918e-01, -2.1894e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.9995e-01, 5.3489e-05],
        [2.5246e-04, 9.9975e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7766, 0.2234], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1902, 0.2119],
         [0.4448, 0.2272]],

        [[0.2493, 0.2094],
         [0.4682, 0.4714]],

        [[0.5589, 0.2128],
         [0.6152, 0.7896]],

        [[0.0824, 0.2101],
         [0.1631, 0.3918]],

        [[0.4751, 0.2066],
         [0.9817, 0.0948]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.0026082811587838017
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0042150251991023655
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: -0.0026936560438907487
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.010245671163764708
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.01570657266520905
Global Adjusted Rand Index: 0.0007283691995199969
Average Adjusted Rand Index: 0.0032870562854393688
Iteration 0: Loss = -36119.78011827625
Iteration 10: Loss = -12349.139762347604
Iteration 20: Loss = -12349.139762347604
1
Iteration 30: Loss = -12349.139762347604
2
Iteration 40: Loss = -12349.139762347604
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[3.2276e-31, 1.0000e+00],
        [8.0318e-21, 1.0000e+00]], dtype=torch.float64)
alpha: tensor([8.2576e-21, 1.0000e+00])
beta: tensor([[[0.2514, 0.2715],
         [0.6100, 0.1970]],

        [[0.7104, 0.1945],
         [0.6226, 0.1770]],

        [[0.2788, 0.2234],
         [0.3237, 0.4771]],

        [[0.2719, 0.2405],
         [0.8636, 0.2763]],

        [[0.7801, 0.2964],
         [0.4567, 0.0530]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -36120.04676518189
Iteration 100: Loss = -12384.738036117213
Iteration 200: Loss = -12363.815056526113
Iteration 300: Loss = -12355.380625190928
Iteration 400: Loss = -12353.177777675868
Iteration 500: Loss = -12352.007259082393
Iteration 600: Loss = -12351.27395281883
Iteration 700: Loss = -12350.777177669594
Iteration 800: Loss = -12350.421413277996
Iteration 900: Loss = -12350.156093633137
Iteration 1000: Loss = -12349.951992828643
Iteration 1100: Loss = -12349.791035984928
Iteration 1200: Loss = -12349.661566110899
Iteration 1300: Loss = -12349.555617054762
Iteration 1400: Loss = -12349.467687812341
Iteration 1500: Loss = -12349.394172177577
Iteration 1600: Loss = -12349.33190530586
Iteration 1700: Loss = -12349.27858318777
Iteration 1800: Loss = -12349.23258744334
Iteration 1900: Loss = -12349.19259763356
Iteration 2000: Loss = -12349.157597408135
Iteration 2100: Loss = -12349.126763951019
Iteration 2200: Loss = -12349.099515510712
Iteration 2300: Loss = -12349.075242666542
Iteration 2400: Loss = -12349.0536145962
Iteration 2500: Loss = -12349.034244727522
Iteration 2600: Loss = -12349.016752192338
Iteration 2700: Loss = -12349.000993972175
Iteration 2800: Loss = -12348.986615763775
Iteration 2900: Loss = -12348.973575305223
Iteration 3000: Loss = -12348.961603356884
Iteration 3100: Loss = -12348.95064936169
Iteration 3200: Loss = -12348.940515301805
Iteration 3300: Loss = -12348.931218177693
Iteration 3400: Loss = -12348.92250918242
Iteration 3500: Loss = -12348.914344482588
Iteration 3600: Loss = -12348.906580008412
Iteration 3700: Loss = -12348.899026812303
Iteration 3800: Loss = -12348.89156490268
Iteration 3900: Loss = -12348.883989250668
Iteration 4000: Loss = -12348.877109675945
Iteration 4100: Loss = -12348.872338363135
Iteration 4200: Loss = -12348.86850158543
Iteration 4300: Loss = -12348.864884736397
Iteration 4400: Loss = -12348.861303130641
Iteration 4500: Loss = -12348.857743108481
Iteration 4600: Loss = -12348.854176661356
Iteration 4700: Loss = -12348.851055642
Iteration 4800: Loss = -12348.848181190982
Iteration 4900: Loss = -12348.845285228233
Iteration 5000: Loss = -12348.842078661242
Iteration 5100: Loss = -12348.838163653272
Iteration 5200: Loss = -12348.832630500394
Iteration 5300: Loss = -12348.823181645066
Iteration 5400: Loss = -12348.806894439453
Iteration 5500: Loss = -12348.792987997054
Iteration 5600: Loss = -12348.786647023353
Iteration 5700: Loss = -12348.781943711236
Iteration 5800: Loss = -12348.777458382134
Iteration 5900: Loss = -12348.772946840581
Iteration 6000: Loss = -12348.768214021178
Iteration 6100: Loss = -12348.76311641557
Iteration 6200: Loss = -12348.75748247685
Iteration 6300: Loss = -12348.751157581819
Iteration 6400: Loss = -12348.744088773243
Iteration 6500: Loss = -12348.73597356808
Iteration 6600: Loss = -12348.726558595488
Iteration 6700: Loss = -12348.714030449584
Iteration 6800: Loss = -12348.69478995323
Iteration 6900: Loss = -12348.682426198542
Iteration 7000: Loss = -12348.671552045389
Iteration 7100: Loss = -12348.662201828975
Iteration 7200: Loss = -12348.653828598272
Iteration 7300: Loss = -12348.646678493325
Iteration 7400: Loss = -12348.641075276442
Iteration 7500: Loss = -12348.646641316058
1
Iteration 7600: Loss = -12348.628894878302
Iteration 7700: Loss = -12348.624238684572
Iteration 7800: Loss = -12348.620719091861
Iteration 7900: Loss = -12348.618999453041
Iteration 8000: Loss = -12348.615486981571
Iteration 8100: Loss = -12348.613445307714
Iteration 8200: Loss = -12349.067662934503
1
Iteration 8300: Loss = -12348.608935295684
Iteration 8400: Loss = -12348.605996546867
Iteration 8500: Loss = -12348.600030832891
Iteration 8600: Loss = -12348.56856389184
Iteration 8700: Loss = -12347.604492877566
Iteration 8800: Loss = -12347.471322303996
Iteration 8900: Loss = -12347.819886105484
1
Iteration 9000: Loss = -12347.430907514963
Iteration 9100: Loss = -12347.420911609155
Iteration 9200: Loss = -12347.41372650901
Iteration 9300: Loss = -12347.408297512415
Iteration 9400: Loss = -12347.404066241546
Iteration 9500: Loss = -12347.400635762735
Iteration 9600: Loss = -12347.398111967092
Iteration 9700: Loss = -12347.395522209132
Iteration 9800: Loss = -12347.393563434916
Iteration 9900: Loss = -12347.394538519273
1
Iteration 10000: Loss = -12347.39030460099
Iteration 10100: Loss = -12347.389040326736
Iteration 10200: Loss = -12347.388001221549
Iteration 10300: Loss = -12347.38703765201
Iteration 10400: Loss = -12347.38606357113
Iteration 10500: Loss = -12347.385254200424
Iteration 10600: Loss = -12347.38678568771
1
Iteration 10700: Loss = -12347.383908291456
Iteration 10800: Loss = -12347.383245912275
Iteration 10900: Loss = -12347.393518937899
1
Iteration 11000: Loss = -12347.382103464051
Iteration 11100: Loss = -12347.381317851301
Iteration 11200: Loss = -12347.381334099726
1
Iteration 11300: Loss = -12347.370769675752
Iteration 11400: Loss = -12347.369932607007
Iteration 11500: Loss = -12347.36846293129
Iteration 11600: Loss = -12347.368207171674
Iteration 11700: Loss = -12347.36775324993
Iteration 11800: Loss = -12347.367418320973
Iteration 11900: Loss = -12347.563288765652
1
Iteration 12000: Loss = -12347.366924720493
Iteration 12100: Loss = -12347.366692820318
Iteration 12200: Loss = -12347.366555527027
Iteration 12300: Loss = -12347.366504063259
Iteration 12400: Loss = -12347.366176429592
Iteration 12500: Loss = -12347.366000868453
Iteration 12600: Loss = -12347.367160597001
1
Iteration 12700: Loss = -12347.365697353029
Iteration 12800: Loss = -12347.365462778944
Iteration 12900: Loss = -12347.40597483211
1
Iteration 13000: Loss = -12347.365239702147
Iteration 13100: Loss = -12347.365023995066
Iteration 13200: Loss = -12347.364154436149
Iteration 13300: Loss = -12347.36362597481
Iteration 13400: Loss = -12347.363462197633
Iteration 13500: Loss = -12347.363382951935
Iteration 13600: Loss = -12347.36335737761
Iteration 13700: Loss = -12347.363246500478
Iteration 13800: Loss = -12347.36311965091
Iteration 13900: Loss = -12347.374535870526
1
Iteration 14000: Loss = -12347.363083062966
Iteration 14100: Loss = -12347.362970048174
Iteration 14200: Loss = -12347.379500775687
1
Iteration 14300: Loss = -12347.362899369102
Iteration 14400: Loss = -12347.36276941078
Iteration 14500: Loss = -12347.362405011727
Iteration 14600: Loss = -12347.363618353322
1
Iteration 14700: Loss = -12347.360509108614
Iteration 14800: Loss = -12347.360479566903
Iteration 14900: Loss = -12347.394998295724
1
Iteration 15000: Loss = -12347.36044480017
Iteration 15100: Loss = -12347.360388922076
Iteration 15200: Loss = -12347.36161417484
1
Iteration 15300: Loss = -12347.363997021137
2
Iteration 15400: Loss = -12347.360575226716
3
Iteration 15500: Loss = -12347.363045926177
4
Iteration 15600: Loss = -12347.361083037844
5
Iteration 15700: Loss = -12347.360239455982
Iteration 15800: Loss = -12347.360949786877
1
Iteration 15900: Loss = -12347.360162715522
Iteration 16000: Loss = -12347.360272693746
1
Iteration 16100: Loss = -12347.36013284666
Iteration 16200: Loss = -12347.362026312388
1
Iteration 16300: Loss = -12347.360144200704
2
Iteration 16400: Loss = -12347.405908403274
3
Iteration 16500: Loss = -12347.360087496307
Iteration 16600: Loss = -12347.360550065157
1
Iteration 16700: Loss = -12347.359848747536
Iteration 16800: Loss = -12347.365790324555
1
Iteration 16900: Loss = -12347.35976353088
Iteration 17000: Loss = -12347.360463744162
1
Iteration 17100: Loss = -12347.371277395614
2
Iteration 17200: Loss = -12347.359713049205
Iteration 17300: Loss = -12347.368152194389
1
Iteration 17400: Loss = -12347.359657637304
Iteration 17500: Loss = -12347.453164747576
1
Iteration 17600: Loss = -12347.359705610144
2
Iteration 17700: Loss = -12347.359706647558
3
Iteration 17800: Loss = -12347.373558529338
4
Iteration 17900: Loss = -12347.359654143283
Iteration 18000: Loss = -12347.364417109895
1
Iteration 18100: Loss = -12347.35985761694
2
Iteration 18200: Loss = -12347.371960396917
3
Iteration 18300: Loss = -12347.36295431457
4
Iteration 18400: Loss = -12347.364245244698
5
Iteration 18500: Loss = -12347.360280898787
6
Iteration 18600: Loss = -12347.359635283923
Iteration 18700: Loss = -12347.367575926048
1
Iteration 18800: Loss = -12347.359021914852
Iteration 18900: Loss = -12347.3379255495
Iteration 19000: Loss = -12347.339919768077
1
Iteration 19100: Loss = -12347.33637316125
Iteration 19200: Loss = -12347.350397735234
1
Iteration 19300: Loss = -12347.336330005337
Iteration 19400: Loss = -12347.357641145945
1
Iteration 19500: Loss = -12347.332987296753
Iteration 19600: Loss = -12347.468642193395
1
Iteration 19700: Loss = -12347.332930181852
Iteration 19800: Loss = -12347.333497750198
1
Iteration 19900: Loss = -12347.333118269295
2
tensor([[ -3.6236,   2.0688],
        [ -8.4192,   6.2929],
        [ -8.1776,   6.4515],
        [ -8.6948,   6.2668],
        [ -5.0121,   3.5616],
        [ -8.6504,   6.1502],
        [ -3.4776,   1.2947],
        [ -8.1401,   6.4136],
        [ -4.3478,   2.0971],
        [ -9.8710,   5.2558],
        [ -3.9060,   2.3310],
        [ -8.9914,   6.9853],
        [ -8.4585,   7.0464],
        [ -9.2310,   6.6907],
        [ -8.3103,   6.8810],
        [-10.4200,   5.8717],
        [ -4.0671,   2.3821],
        [  0.4156,  -1.8223],
        [ -4.8563,   3.2609],
        [ -8.9785,   6.6473],
        [ -8.3573,   6.8382],
        [ -8.6777,   6.7208],
        [ -7.7451,   6.3584],
        [ -9.2936,   6.2331],
        [ -7.7571,   6.3391],
        [ -4.8124,   2.8944],
        [ -8.3757,   6.9846],
        [ -8.3793,   6.7919],
        [ -8.3037,   6.9132],
        [ -7.9028,   6.4022],
        [ -8.2471,   6.8603],
        [ -8.9122,   6.5417],
        [ -9.1857,   5.8328],
        [ -8.5455,   7.0269],
        [ -0.6842,  -1.3406],
        [ -8.6163,   6.3692],
        [ -8.8226,   6.2894],
        [ -8.4408,   6.9602],
        [ -2.3881,   0.3900],
        [ -8.5729,   7.0199],
        [ -8.2736,   6.7172],
        [ -4.7882,   2.2616],
        [ -4.4872,   1.7394],
        [ -8.4607,   7.0744],
        [ -1.6821,   0.2714],
        [ -8.0408,   6.6544],
        [ -5.3939,   3.7827],
        [ -8.5572,   5.3011],
        [ -7.9547,   6.4773],
        [ -9.0409,   6.3563],
        [ -8.1821,   6.7872],
        [ -8.3199,   6.8075],
        [ -5.6019,   3.5123],
        [ -8.4406,   6.6673],
        [ -8.3951,   6.9808],
        [ -3.8111,   2.2183],
        [ -8.3932,   6.3245],
        [ -4.9796,   3.2028],
        [ -8.4638,   6.8693],
        [ -8.1643,   6.6683],
        [ -8.7953,   6.9540],
        [ -8.4300,   6.1456],
        [ -3.3155,   1.4738],
        [ -9.1632,   5.1681],
        [ -8.2194,   6.7701],
        [ -8.7898,   6.6810],
        [ -4.9829,   0.3677],
        [ -3.3547,   1.9326],
        [ -8.7516,   6.2285],
        [ -4.3959,   2.0738],
        [ -8.3114,   6.8780],
        [ -8.0826,   6.6497],
        [ -8.1479,   6.5598],
        [ -2.5465,   1.1601],
        [ -8.7303,   6.9221],
        [ -8.0546,   6.5262],
        [ -8.3915,   7.0052],
        [ -8.2813,   6.4634],
        [ -6.7977,   5.0390],
        [ -8.0876,   6.6126],
        [ -8.9113,   5.8635],
        [ -5.2297,   3.6272],
        [ -8.2565,   6.8172],
        [ -9.8951,   5.9513],
        [ -8.6516,   6.8392],
        [ -8.3240,   6.9070],
        [ -8.4323,   6.9004],
        [ -3.2564,   1.7214],
        [ -9.0500,   5.2743],
        [ -8.7534,   6.6997],
        [ -8.2839,   6.3521],
        [ -8.2693,   6.8830],
        [ -8.0333,   6.5300],
        [ -9.2973,   5.5704],
        [ -8.3560,   6.8757],
        [ -8.1817,   6.7901],
        [ -6.0806,   4.4397],
        [ -8.5951,   7.1107],
        [ -9.2431,   6.1157],
        [ -8.4393,   6.5565]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.9999e-01, 9.7372e-06],
        [7.3024e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0183, 0.9817], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3524, 0.2208],
         [0.6100, 0.1969]],

        [[0.7104, 0.2769],
         [0.6226, 0.1770]],

        [[0.2788, 0.1697],
         [0.3237, 0.4771]],

        [[0.2719, 0.2384],
         [0.8636, 0.2763]],

        [[0.7801, 0.2807],
         [0.4567, 0.0530]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.01171303074670571
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
Global Adjusted Rand Index: 0.0001102299408497439
Average Adjusted Rand Index: 0.002047675090720444
Iteration 0: Loss = -16566.734127804863
Iteration 10: Loss = -12349.012124788555
Iteration 20: Loss = -12349.010480114433
Iteration 30: Loss = -12349.009488127316
Iteration 40: Loss = -12349.008912482965
Iteration 50: Loss = -12349.008591559052
Iteration 60: Loss = -12349.008384488328
Iteration 70: Loss = -12349.0082486522
Iteration 80: Loss = -12349.008117467109
Iteration 90: Loss = -12349.008077979168
Iteration 100: Loss = -12349.008049928783
Iteration 110: Loss = -12349.008069937536
1
Iteration 120: Loss = -12349.0080329555
Iteration 130: Loss = -12349.008040820474
1
Iteration 140: Loss = -12349.008020251218
Iteration 150: Loss = -12349.008076336515
1
Iteration 160: Loss = -12349.008017069
Iteration 170: Loss = -12349.007996838589
Iteration 180: Loss = -12349.008003074867
1
Iteration 190: Loss = -12349.007986429302
Iteration 200: Loss = -12349.008009316485
1
Iteration 210: Loss = -12349.008022231765
2
Iteration 220: Loss = -12349.007974472059
Iteration 230: Loss = -12349.008026567464
1
Iteration 240: Loss = -12349.008011501835
2
Iteration 250: Loss = -12349.007997457611
3
Stopping early at iteration 249 due to no improvement.
pi: tensor([[0.6077, 0.3923],
        [0.4091, 0.5909]], dtype=torch.float64)
alpha: tensor([0.5105, 0.4895])
beta: tensor([[[0.1969, 0.1976],
         [0.6983, 0.1970]],

        [[0.8104, 0.1966],
         [0.5619, 0.2489]],

        [[0.3860, 0.1986],
         [0.2666, 0.9666]],

        [[0.4848, 0.1984],
         [0.4097, 0.7144]],

        [[0.3292, 0.1936],
         [0.5410, 0.3550]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -16566.386966061444
Iteration 100: Loss = -12355.903915614375
Iteration 200: Loss = -12351.215565453334
Iteration 300: Loss = -12350.038542772438
Iteration 400: Loss = -12349.545223489164
Iteration 500: Loss = -12349.313119056776
Iteration 600: Loss = -12349.174364580915
Iteration 700: Loss = -12349.081705341328
Iteration 800: Loss = -12349.02467194764
Iteration 900: Loss = -12348.985243546222
Iteration 1000: Loss = -12348.955518424564
Iteration 1100: Loss = -12348.932557411237
Iteration 1200: Loss = -12348.914659875609
Iteration 1300: Loss = -12348.899568547968
Iteration 1400: Loss = -12348.884757277085
Iteration 1500: Loss = -12348.874488800922
Iteration 1600: Loss = -12348.867208545005
Iteration 1700: Loss = -12348.861129008937
Iteration 1800: Loss = -12348.855901919758
Iteration 1900: Loss = -12348.851379595819
Iteration 2000: Loss = -12348.847447053815
Iteration 2100: Loss = -12348.843966048695
Iteration 2200: Loss = -12348.840889589677
Iteration 2300: Loss = -12348.838150429285
Iteration 2400: Loss = -12348.835663112273
Iteration 2500: Loss = -12348.833336501364
Iteration 2600: Loss = -12348.831256415486
Iteration 2700: Loss = -12348.829206045168
Iteration 2800: Loss = -12348.82737435635
Iteration 2900: Loss = -12348.825745493163
Iteration 3000: Loss = -12348.824293405592
Iteration 3100: Loss = -12348.82298082022
Iteration 3200: Loss = -12348.821758956692
Iteration 3300: Loss = -12348.820651957614
Iteration 3400: Loss = -12348.819651659123
Iteration 3500: Loss = -12348.818635703508
Iteration 3600: Loss = -12348.817664780432
Iteration 3700: Loss = -12348.816689914993
Iteration 3800: Loss = -12348.815594346966
Iteration 3900: Loss = -12348.814153426561
Iteration 4000: Loss = -12348.811666392961
Iteration 4100: Loss = -12348.806097587698
Iteration 4200: Loss = -12348.795556545643
Iteration 4300: Loss = -12348.788065623128
Iteration 4400: Loss = -12348.783242614849
Iteration 4500: Loss = -12348.779814788924
Iteration 4600: Loss = -12348.777046944251
Iteration 4700: Loss = -12348.774645046444
Iteration 4800: Loss = -12348.772826966751
Iteration 4900: Loss = -12348.77148967215
Iteration 5000: Loss = -12348.770356602115
Iteration 5100: Loss = -12348.769432663483
Iteration 5200: Loss = -12348.768578052706
Iteration 5300: Loss = -12348.770773455482
1
Iteration 5400: Loss = -12348.76722587859
Iteration 5500: Loss = -12348.766659597824
Iteration 5600: Loss = -12348.766043353784
Iteration 5700: Loss = -12348.777869851841
1
Iteration 5800: Loss = -12348.764930210029
Iteration 5900: Loss = -12348.764320357486
Iteration 6000: Loss = -12348.763130628424
Iteration 6100: Loss = -12348.766403956162
1
Iteration 6200: Loss = -12348.763651523901
2
Iteration 6300: Loss = -12348.761546080921
Iteration 6400: Loss = -12348.786033858078
1
Iteration 6500: Loss = -12348.761232862991
Iteration 6600: Loss = -12348.760600202222
Iteration 6700: Loss = -12348.761277308911
1
Iteration 6800: Loss = -12348.765205435302
2
Iteration 6900: Loss = -12348.760443888725
Iteration 7000: Loss = -12348.871323303803
1
Iteration 7100: Loss = -12348.75984662998
Iteration 7200: Loss = -12348.768503780304
1
Iteration 7300: Loss = -12348.759527257813
Iteration 7400: Loss = -12348.759777886851
1
Iteration 7500: Loss = -12348.759305844924
Iteration 7600: Loss = -12348.75966632991
1
Iteration 7700: Loss = -12348.803534618059
2
Iteration 7800: Loss = -12348.758992761961
Iteration 7900: Loss = -12348.759733768857
1
Iteration 8000: Loss = -12348.758671938593
Iteration 8100: Loss = -12348.757433116962
Iteration 8200: Loss = -12348.756709850632
Iteration 8300: Loss = -12348.761712719746
1
Iteration 8400: Loss = -12348.807487149987
2
Iteration 8500: Loss = -12348.757865023783
3
Iteration 8600: Loss = -12348.757077198192
4
Iteration 8700: Loss = -12348.756444405099
Iteration 8800: Loss = -12348.80577412351
1
Iteration 8900: Loss = -12348.758140183456
2
Iteration 9000: Loss = -12348.768439380452
3
Iteration 9100: Loss = -12348.756491006365
4
Iteration 9200: Loss = -12348.761739453714
5
Iteration 9300: Loss = -12348.756080757545
Iteration 9400: Loss = -12348.761240364805
1
Iteration 9500: Loss = -12348.755459082438
Iteration 9600: Loss = -12348.774894297529
1
Iteration 9700: Loss = -12348.755002551494
Iteration 9800: Loss = -12348.806676073013
1
Iteration 9900: Loss = -12348.75402329516
Iteration 10000: Loss = -12348.844251020746
1
Iteration 10100: Loss = -12348.751853229474
Iteration 10200: Loss = -12348.751789702223
Iteration 10300: Loss = -12348.752364795766
1
Iteration 10400: Loss = -12348.751084880983
Iteration 10500: Loss = -12348.750781101684
Iteration 10600: Loss = -12348.75087828383
1
Iteration 10700: Loss = -12348.750746695065
Iteration 10800: Loss = -12348.750711817438
Iteration 10900: Loss = -12348.75196516899
1
Iteration 11000: Loss = -12348.750672721319
Iteration 11100: Loss = -12348.778684367173
1
Iteration 11200: Loss = -12348.750823094744
2
Iteration 11300: Loss = -12348.75062795479
Iteration 11400: Loss = -12348.752825338743
1
Iteration 11500: Loss = -12348.744403181243
Iteration 11600: Loss = -12348.770918168877
1
Iteration 11700: Loss = -12348.744369768334
Iteration 11800: Loss = -12348.746139210358
1
Iteration 11900: Loss = -12348.744855531348
2
Iteration 12000: Loss = -12348.744516233892
3
Iteration 12100: Loss = -12348.747296745569
4
Iteration 12200: Loss = -12348.74438183177
5
Iteration 12300: Loss = -12348.744394030655
6
Iteration 12400: Loss = -12348.74428574933
Iteration 12500: Loss = -12348.744439703867
1
Iteration 12600: Loss = -12348.744270358731
Iteration 12700: Loss = -12348.980215601978
1
Iteration 12800: Loss = -12348.744253340874
Iteration 12900: Loss = -12348.73861274212
Iteration 13000: Loss = -12348.827738601043
1
Iteration 13100: Loss = -12348.738521604468
Iteration 13200: Loss = -12348.738963891414
1
Iteration 13300: Loss = -12348.740311729129
2
Iteration 13400: Loss = -12348.737839508272
Iteration 13500: Loss = -12348.737770610851
Iteration 13600: Loss = -12348.86252548727
1
Iteration 13700: Loss = -12348.737507237
Iteration 13800: Loss = -12348.775389920007
1
Iteration 13900: Loss = -12348.737533930223
2
Iteration 14000: Loss = -12348.737906704959
3
Iteration 14100: Loss = -12348.745235741848
4
Iteration 14200: Loss = -12348.744023328769
5
Iteration 14300: Loss = -12348.78086668197
6
Iteration 14400: Loss = -12348.73604220629
Iteration 14500: Loss = -12348.742764228537
1
Iteration 14600: Loss = -12348.737764197742
2
Iteration 14700: Loss = -12348.736108757255
3
Iteration 14800: Loss = -12348.997323009171
4
Iteration 14900: Loss = -12348.736075590634
5
Iteration 15000: Loss = -12348.736045232252
6
Iteration 15100: Loss = -12348.738099831948
7
Iteration 15200: Loss = -12348.763320563161
8
Iteration 15300: Loss = -12348.736437747068
9
Iteration 15400: Loss = -12348.880317416786
10
Stopping early at iteration 15400 due to no improvement.
tensor([[ -4.6287,   2.8229],
        [ -4.6884,   2.9944],
        [ -2.6224,   0.6656],
        [-11.0745,   7.2574],
        [-10.0411,   8.4056],
        [-10.2544,   7.7207],
        [ -9.8793,   7.8993],
        [-10.1261,   7.8768],
        [-10.7622,   7.3007],
        [-10.2235,   7.4455],
        [ -9.9228,   7.3137],
        [ -3.5291,   1.2455],
        [ -4.2183,   2.7824],
        [ -9.6438,   8.0802],
        [ -1.4657,  -0.4538],
        [ -3.3531,   0.5965],
        [ -4.1583,   1.6927],
        [ -9.4044,   7.9792],
        [ -9.9596,   8.3967],
        [ -9.1030,   7.6809],
        [-10.1680,   7.5668],
        [ -4.1323,   2.7459],
        [ -9.5893,   8.1431],
        [ -9.4629,   8.0368],
        [ -9.7623,   8.1648],
        [ -4.1176,   2.6749],
        [ -4.0705,   2.3067],
        [ -4.3643,   2.8722],
        [ -9.6889,   8.0967],
        [ -9.7407,   8.0456],
        [ -5.1250,   3.7080],
        [ -9.7559,   7.9905],
        [ -5.2602,   1.9437],
        [ -3.5141,  -1.1011],
        [ -6.7479,   4.3131],
        [ -3.3907,   1.9992],
        [ -4.1782,   1.8828],
        [-10.2267,   7.8498],
        [ -4.8859,   2.9624],
        [ -9.2857,   7.1711],
        [ -9.5463,   8.1591],
        [ -6.3657,   3.9655],
        [ -5.3681,   3.9315],
        [ -9.2380,   7.6288],
        [ -9.4479,   6.5097],
        [ -5.3184,   2.2347],
        [ -9.6321,   8.0540],
        [ -4.4349,   1.6322],
        [-11.2855,   6.9146],
        [ -9.0074,   7.4411],
        [ -5.3289,   3.9124],
        [-10.2073,   7.8233],
        [-10.0597,   7.7629],
        [ -5.8428,   3.8095],
        [ -9.9904,   7.9398],
        [ -9.1889,   7.4041],
        [-10.3763,   8.4199],
        [-10.5976,   8.4085],
        [ -4.1603,   2.4860],
        [-10.0435,   7.4410],
        [ -3.7036,   2.2862],
        [ -9.6478,   7.6343],
        [ -9.5040,   8.1160],
        [ -6.3727,   1.7575],
        [ -9.4867,   8.0588],
        [ -9.1164,   7.7205],
        [ -9.5062,   8.0044],
        [ -4.3096,   2.5327],
        [ -4.3108,   2.7517],
        [ -4.0746,   2.6854],
        [-10.1661,   7.4141],
        [ -9.4014,   8.0124],
        [-10.1759,   8.4205],
        [ -9.5040,   7.3746],
        [ -9.5797,   8.1818],
        [ -5.2610,   2.2009],
        [-10.5864,   7.8340],
        [ -9.8744,   8.1214],
        [-10.9594,   8.6364],
        [-10.2386,   7.8828],
        [ -5.0256,   3.4719],
        [ -4.9706,   3.3054],
        [ -4.2279,   2.7770],
        [ -4.0035,   2.5329],
        [ -9.8843,   8.0634],
        [ -3.6856,   1.3174],
        [ -9.4648,   7.8302],
        [-10.7886,   7.9874],
        [ -4.3232,   2.7611],
        [ -9.6671,   8.0607],
        [-10.7286,   7.1257],
        [ -9.6613,   7.9115],
        [-10.1250,   7.9605],
        [ -4.5534,   1.9787],
        [ -5.2866,   2.8973],
        [ -4.8438,   3.2970],
        [-10.1659,   8.0815],
        [-10.3987,   6.5012],
        [ -4.1998,   2.7961],
        [ -4.8924,   3.1621]], dtype=torch.float64, requires_grad=True)
pi: tensor([[6.0218e-07, 1.0000e+00],
        [1.0000e+00, 3.8767e-07]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0046, 0.9954], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1994, 0.1493],
         [0.6983, 0.1970]],

        [[0.8104, 0.1928],
         [0.5619, 0.2489]],

        [[0.3860, 0.2843],
         [0.2666, 0.9666]],

        [[0.4848, 0.2317],
         [0.4097, 0.7144]],

        [[0.3292, 0.1782],
         [0.5410, 0.3550]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0011475446544096347
Average Adjusted Rand Index: 0.0
11800.476253394521
new:  [-0.0009354685621591986, 0.0007283691995199969, 0.0001102299408497439, -0.0011475446544096347] [0.0, 0.0032870562854393688, 0.002047675090720444, 0.0] [12348.696693002888, 12347.919674319413, 12347.351554420318, 12348.880317416786]
prior:  [0.0, 0.0, 0.0, 0.0] [0.0, 0.0, 0.0, 0.0] [12348.918096463585, 12348.91811874591, 12349.139762347604, 12349.007997457611]
-----------------------------------------------------------------------------------------
This iteration is 1
True Objective function: Loss = -11915.982256436346
Iteration 0: Loss = -35014.32485777404
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.6791,    nan]],

        [[0.0507,    nan],
         [0.7934, 0.4286]],

        [[0.9478,    nan],
         [0.4166, 0.0817]],

        [[0.9958,    nan],
         [0.5633, 0.0811]],

        [[0.2318,    nan],
         [0.9720, 0.6632]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 62
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -34152.87682664314
Iteration 100: Loss = -12434.544191274454
Iteration 200: Loss = -12432.905658132271
Iteration 300: Loss = -12432.166887228797
Iteration 400: Loss = -12431.758042618496
Iteration 500: Loss = -12431.4868570801
Iteration 600: Loss = -12431.294820358025
Iteration 700: Loss = -12431.150558939422
Iteration 800: Loss = -12431.03232633919
Iteration 900: Loss = -12430.911137668098
Iteration 1000: Loss = -12430.784146653046
Iteration 1100: Loss = -12430.670441391676
Iteration 1200: Loss = -12430.554233639026
Iteration 1300: Loss = -12430.413833076118
Iteration 1400: Loss = -12430.016987971434
Iteration 1500: Loss = -12427.992300954491
Iteration 1600: Loss = -12427.591249762883
Iteration 1700: Loss = -12427.290490266047
Iteration 1800: Loss = -12427.03053978787
Iteration 1900: Loss = -12426.498442745882
Iteration 2000: Loss = -12426.12017476412
Iteration 2100: Loss = -12426.000794088772
Iteration 2200: Loss = -12425.933339141307
Iteration 2300: Loss = -12425.880977280287
Iteration 2400: Loss = -12425.828140253874
Iteration 2500: Loss = -12425.779282906136
Iteration 2600: Loss = -12425.746588681952
Iteration 2700: Loss = -12425.720671707393
Iteration 2800: Loss = -12425.697727970715
Iteration 2900: Loss = -12425.677707613226
Iteration 3000: Loss = -12425.659837934742
Iteration 3100: Loss = -12425.643165801108
Iteration 3200: Loss = -12425.62780773192
Iteration 3300: Loss = -12425.626631864097
Iteration 3400: Loss = -12425.603206928985
Iteration 3500: Loss = -12425.593318826099
Iteration 3600: Loss = -12425.58418747975
Iteration 3700: Loss = -12425.577511135827
Iteration 3800: Loss = -12425.566895998601
Iteration 3900: Loss = -12425.559155021387
Iteration 4000: Loss = -12425.551862947163
Iteration 4100: Loss = -12425.545060424096
Iteration 4200: Loss = -12425.541239978524
Iteration 4300: Loss = -12425.53261474131
Iteration 4400: Loss = -12425.527961073452
Iteration 4500: Loss = -12425.522888450318
Iteration 4600: Loss = -12425.5190943044
Iteration 4700: Loss = -12425.515930824335
Iteration 4800: Loss = -12425.514071735683
Iteration 4900: Loss = -12425.516966815176
1
Iteration 5000: Loss = -12425.508580025522
Iteration 5100: Loss = -12425.50672848238
Iteration 5200: Loss = -12425.50530375128
Iteration 5300: Loss = -12425.50362670886
Iteration 5400: Loss = -12425.50219298268
Iteration 5500: Loss = -12425.501239321451
Iteration 5600: Loss = -12425.500116248584
Iteration 5700: Loss = -12425.499018878287
Iteration 5800: Loss = -12425.498294655501
Iteration 5900: Loss = -12425.497451449362
Iteration 6000: Loss = -12425.501283199448
1
Iteration 6100: Loss = -12425.496215017232
Iteration 6200: Loss = -12425.495729363163
Iteration 6300: Loss = -12425.502864764157
1
Iteration 6400: Loss = -12425.494867866671
Iteration 6500: Loss = -12425.494569493349
Iteration 6600: Loss = -12425.516144177425
1
Iteration 6700: Loss = -12425.49405997386
Iteration 6800: Loss = -12425.493836224241
Iteration 6900: Loss = -12425.493603561548
Iteration 7000: Loss = -12425.49381939173
1
Iteration 7100: Loss = -12425.493272415395
Iteration 7200: Loss = -12425.493122194905
Iteration 7300: Loss = -12425.504074415658
1
Iteration 7400: Loss = -12425.492829481414
Iteration 7500: Loss = -12425.492711326719
Iteration 7600: Loss = -12425.492548736633
Iteration 7700: Loss = -12425.492659315029
1
Iteration 7800: Loss = -12425.492344961483
Iteration 7900: Loss = -12425.492247827591
Iteration 8000: Loss = -12425.5365722997
1
Iteration 8100: Loss = -12425.49207185538
Iteration 8200: Loss = -12425.492001847642
Iteration 8300: Loss = -12425.527891157408
1
Iteration 8400: Loss = -12425.491895876838
Iteration 8500: Loss = -12425.491812447866
Iteration 8600: Loss = -12425.491747734224
Iteration 8700: Loss = -12425.491815755373
1
Iteration 8800: Loss = -12425.491652185232
Iteration 8900: Loss = -12425.491612634774
Iteration 9000: Loss = -12425.502168155555
1
Iteration 9100: Loss = -12425.491532668822
Iteration 9200: Loss = -12425.491511468223
Iteration 9300: Loss = -12425.520172911692
1
Iteration 9400: Loss = -12425.491436281753
Iteration 9500: Loss = -12425.491408101852
Iteration 9600: Loss = -12425.491412090762
1
Iteration 9700: Loss = -12425.499002226343
2
Iteration 9800: Loss = -12425.493599692023
3
Iteration 9900: Loss = -12425.49134341554
Iteration 10000: Loss = -12425.495995454394
1
Iteration 10100: Loss = -12425.491318871902
Iteration 10200: Loss = -12425.49129963832
Iteration 10300: Loss = -12425.492223312036
1
Iteration 10400: Loss = -12425.491291007365
Iteration 10500: Loss = -12425.49125707485
Iteration 10600: Loss = -12425.491619210057
1
Iteration 10700: Loss = -12425.491219979436
Iteration 10800: Loss = -12425.491190381763
Iteration 10900: Loss = -12425.491293443927
1
Iteration 11000: Loss = -12425.491182192955
Iteration 11100: Loss = -12425.494013342923
1
Iteration 11200: Loss = -12425.491145428843
Iteration 11300: Loss = -12425.605824024447
1
Iteration 11400: Loss = -12425.491176574666
2
Iteration 11500: Loss = -12425.49614282463
3
Iteration 11600: Loss = -12425.491193953425
4
Iteration 11700: Loss = -12425.495495334415
5
Iteration 11800: Loss = -12425.491151327536
6
Iteration 11900: Loss = -12425.51619367594
7
Iteration 12000: Loss = -12425.49114223673
Iteration 12100: Loss = -12425.608464181365
1
Iteration 12200: Loss = -12425.491106216663
Iteration 12300: Loss = -12425.491141222543
1
Iteration 12400: Loss = -12425.491261860914
2
Iteration 12500: Loss = -12425.491088806377
Iteration 12600: Loss = -12425.491269546657
1
Iteration 12700: Loss = -12425.492736759461
2
Iteration 12800: Loss = -12425.491101332986
3
Iteration 12900: Loss = -12425.523901724915
4
Iteration 13000: Loss = -12425.492377116618
5
Iteration 13100: Loss = -12425.491079963007
Iteration 13200: Loss = -12425.496763225832
1
Iteration 13300: Loss = -12425.49106494933
Iteration 13400: Loss = -12425.491264080207
1
Iteration 13500: Loss = -12425.52319646066
2
Iteration 13600: Loss = -12425.491041394924
Iteration 13700: Loss = -12425.533673025835
1
Iteration 13800: Loss = -12425.491026930864
Iteration 13900: Loss = -12425.491067269098
1
Iteration 14000: Loss = -12425.49110810865
2
Iteration 14100: Loss = -12425.492784441632
3
Iteration 14200: Loss = -12425.75628509544
4
Iteration 14300: Loss = -12425.493587839803
5
Iteration 14400: Loss = -12425.504728446573
6
Iteration 14500: Loss = -12425.491017148986
Iteration 14600: Loss = -12425.494554568759
1
Iteration 14700: Loss = -12425.497931686583
2
Iteration 14800: Loss = -12425.4910461313
3
Iteration 14900: Loss = -12425.493509754297
4
Iteration 15000: Loss = -12425.52291859893
5
Iteration 15100: Loss = -12425.504830965867
6
Iteration 15200: Loss = -12425.64296599349
7
Iteration 15300: Loss = -12425.491344267875
8
Iteration 15400: Loss = -12425.491118373995
9
Iteration 15500: Loss = -12425.49277797482
10
Stopping early at iteration 15500 due to no improvement.
tensor([[-0.0420, -4.5732],
        [ 0.4749, -5.0901],
        [-3.9070, -0.7082],
        [ 0.0813, -4.6965],
        [ 0.2794, -4.8946],
        [ 0.1405, -4.7557],
        [-0.2699, -4.3453],
        [-0.5280, -4.0872],
        [-0.0961, -4.5191],
        [ 0.4633, -5.0785],
        [-0.1931, -4.4221],
        [ 0.6283, -5.2435],
        [ 0.0212, -4.6364],
        [-0.0998, -4.5154],
        [-0.4680, -4.1472],
        [-0.9901, -3.6251],
        [ 0.0484, -4.6636],
        [-0.3503, -4.2649],
        [ 0.3605, -4.9757],
        [ 0.3116, -4.9268],
        [-0.4577, -4.1575],
        [ 0.2686, -4.8839],
        [ 0.1384, -4.7536],
        [-0.2103, -4.4049],
        [ 0.5658, -5.1811],
        [ 0.5111, -5.1264],
        [ 0.2868, -4.9021],
        [-0.9728, -3.6425],
        [-0.9783, -3.6370],
        [ 0.2866, -4.9019],
        [-2.3626, -2.2526],
        [-1.1793, -3.4359],
        [ 0.0658, -4.6810],
        [-0.2863, -4.3289],
        [-0.8688, -3.7465],
        [-1.0696, -3.5457],
        [-0.0215, -4.5937],
        [-2.2681, -2.3471],
        [ 0.1683, -4.7835],
        [-0.3804, -4.2348],
        [-0.4098, -4.2054],
        [-0.1612, -4.4540],
        [-1.7879, -2.8273],
        [ 0.1311, -4.7464],
        [ 0.3418, -4.9570],
        [-0.0385, -4.5767],
        [-0.8800, -3.7352],
        [-0.2814, -4.3338],
        [-1.2335, -3.3817],
        [-1.4319, -3.1833],
        [ 0.3225, -4.9377],
        [-3.8612, -0.7541],
        [ 0.0666, -4.6819],
        [-0.3228, -4.2924],
        [-0.9438, -3.6714],
        [-0.0566, -4.5586],
        [ 0.1441, -4.7593],
        [ 0.0706, -4.6858],
        [-0.7097, -3.9055],
        [ 0.1404, -4.7557],
        [-0.3401, -4.2751],
        [-0.0240, -4.5912],
        [-0.6696, -3.9457],
        [ 0.0843, -4.6995],
        [ 0.2472, -4.8625],
        [-0.3118, -4.3034],
        [-0.0697, -4.5455],
        [-0.1923, -4.4230],
        [ 0.5101, -5.1253],
        [-0.4207, -4.1945],
        [ 0.5683, -5.1835],
        [-0.1611, -4.4541],
        [-0.3119, -4.3033],
        [-0.3252, -4.2900],
        [-0.2408, -4.3745],
        [-2.5620, -2.0532],
        [-0.2814, -4.3338],
        [-1.5204, -3.0949],
        [ 0.0602, -4.6755],
        [-0.7146, -3.9007],
        [-0.1468, -4.4684],
        [ 0.2035, -4.8187],
        [-0.2256, -4.3897],
        [ 0.1914, -4.8066],
        [ 0.0974, -4.7127],
        [ 0.2548, -4.8700],
        [-0.1376, -4.4776],
        [-1.0207, -3.5946],
        [-0.7609, -3.8543],
        [ 0.2531, -4.8684],
        [ 0.2435, -4.8587],
        [ 0.1934, -4.8086],
        [-1.3505, -3.2647],
        [-0.2782, -4.3370],
        [-0.3054, -4.3099],
        [ 0.1115, -4.7267],
        [ 0.2893, -4.9045],
        [-0.4507, -4.1645],
        [-0.1191, -4.4962],
        [-0.0479, -4.5674]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 1.8416e-07],
        [1.4006e-01, 8.5994e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9398, 0.0602], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1986, 0.2152],
         [0.6791, 0.3185]],

        [[0.0507, 0.2726],
         [0.7934, 0.4286]],

        [[0.9478, 0.1585],
         [0.4166, 0.0817]],

        [[0.9958, 0.1992],
         [0.5633, 0.0811]],

        [[0.2318, 0.2873],
         [0.9720, 0.6632]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0014922741295917668
time is 1
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: -0.025916162480371957
time is 2
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 7.13818506546007e-05
time is 4
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.008484848484848486
Global Adjusted Rand Index: -0.0036724477531268423
Average Adjusted Rand Index: -0.0062754917599426325
Iteration 0: Loss = -19826.54665051918
Iteration 10: Loss = -12429.501279479191
Iteration 20: Loss = -12428.679167596896
Iteration 30: Loss = -12428.44346304588
Iteration 40: Loss = -12428.319288506229
Iteration 50: Loss = -12428.263484667343
Iteration 60: Loss = -12428.243489165014
Iteration 70: Loss = -12428.239365791605
Iteration 80: Loss = -12428.24056750942
1
Iteration 90: Loss = -12428.242594551522
2
Iteration 100: Loss = -12428.244349545132
3
Stopping early at iteration 99 due to no improvement.
pi: tensor([[0.0770, 0.9230],
        [0.0634, 0.9366]], dtype=torch.float64)
alpha: tensor([0.0640, 0.9360])
beta: tensor([[[0.2344, 0.2029],
         [0.3160, 0.1958]],

        [[0.3653, 0.2819],
         [0.4719, 0.8912]],

        [[0.3296, 0.1946],
         [0.1250, 0.8717]],

        [[0.4822, 0.1982],
         [0.8988, 0.2140]],

        [[0.6051, 0.2393],
         [0.7026, 0.7603]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 41
Adjusted Rand Index: -0.020678759622205528
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: -0.0026099303253762193
Average Adjusted Rand Index: -0.004863024651713833
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19826.32192655755
Iteration 100: Loss = -12461.383821062998
Iteration 200: Loss = -12437.779941466391
Iteration 300: Loss = -12432.127466993301
Iteration 400: Loss = -12430.703999685149
Iteration 500: Loss = -12429.83144023117
Iteration 600: Loss = -12429.419455441352
Iteration 700: Loss = -12429.153076338813
Iteration 800: Loss = -12428.95671775041
Iteration 900: Loss = -12428.802260508268
Iteration 1000: Loss = -12428.6645076939
Iteration 1100: Loss = -12428.535486721685
Iteration 1200: Loss = -12428.419333920465
Iteration 1300: Loss = -12428.316673478372
Iteration 1400: Loss = -12428.222831404339
Iteration 1500: Loss = -12428.135090122345
Iteration 1600: Loss = -12428.061582269207
Iteration 1700: Loss = -12428.000461476711
Iteration 1800: Loss = -12427.959206797857
Iteration 1900: Loss = -12427.930544613486
Iteration 2000: Loss = -12427.910252200667
Iteration 2100: Loss = -12427.896650817202
Iteration 2200: Loss = -12427.887785407349
Iteration 2300: Loss = -12427.885724235695
Iteration 2400: Loss = -12427.877708581687
Iteration 2500: Loss = -12427.874453585828
Iteration 2600: Loss = -12427.889776224765
1
Iteration 2700: Loss = -12427.869529347425
Iteration 2800: Loss = -12427.867437000368
Iteration 2900: Loss = -12427.893767797323
1
Iteration 3000: Loss = -12427.863775480257
Iteration 3100: Loss = -12427.861988514927
Iteration 3200: Loss = -12428.497221015943
1
Iteration 3300: Loss = -12427.857777615985
Iteration 3400: Loss = -12427.856631090906
Iteration 3500: Loss = -12427.85564213323
Iteration 3600: Loss = -12427.855189037757
Iteration 3700: Loss = -12427.853790790912
Iteration 3800: Loss = -12427.852958614643
Iteration 3900: Loss = -12427.852766640464
Iteration 4000: Loss = -12427.851416114669
Iteration 4100: Loss = -12427.850459859554
Iteration 4200: Loss = -12427.849562952848
Iteration 4300: Loss = -12427.84291130778
Iteration 4400: Loss = -12427.839929713045
Iteration 4500: Loss = -12427.83834449945
Iteration 4600: Loss = -12427.83038432937
Iteration 4700: Loss = -12427.829547851848
Iteration 4800: Loss = -12427.839354491547
1
Iteration 4900: Loss = -12427.828046223358
Iteration 5000: Loss = -12427.827554075086
Iteration 5100: Loss = -12427.827561082804
1
Iteration 5200: Loss = -12427.827267924546
Iteration 5300: Loss = -12427.827188698431
Iteration 5400: Loss = -12428.091452805646
1
Iteration 5500: Loss = -12427.82698433862
Iteration 5600: Loss = -12427.8268461718
Iteration 5700: Loss = -12427.858944152
1
Iteration 5800: Loss = -12427.826712942207
Iteration 5900: Loss = -12427.826584969811
Iteration 6000: Loss = -12427.826532164565
Iteration 6100: Loss = -12427.826724385965
1
Iteration 6200: Loss = -12427.826288708195
Iteration 6300: Loss = -12427.82620055352
Iteration 6400: Loss = -12427.831521497388
1
Iteration 6500: Loss = -12427.826056852615
Iteration 6600: Loss = -12427.825943121346
Iteration 6700: Loss = -12427.825843813827
Iteration 6800: Loss = -12427.825980032329
1
Iteration 6900: Loss = -12427.82565997099
Iteration 7000: Loss = -12427.825578871956
Iteration 7100: Loss = -12427.825602896324
1
Iteration 7200: Loss = -12427.825414943542
Iteration 7300: Loss = -12427.825327432689
Iteration 7400: Loss = -12427.868163172265
1
Iteration 7500: Loss = -12427.82516764298
Iteration 7600: Loss = -12427.825069481096
Iteration 7700: Loss = -12427.834855103376
1
Iteration 7800: Loss = -12427.824901401253
Iteration 7900: Loss = -12427.824813245998
Iteration 8000: Loss = -12427.880526307046
1
Iteration 8100: Loss = -12427.824710237212
Iteration 8200: Loss = -12427.824625013409
Iteration 8300: Loss = -12427.878062261954
1
Iteration 8400: Loss = -12427.824474586672
Iteration 8500: Loss = -12427.824412733034
Iteration 8600: Loss = -12427.876086603852
1
Iteration 8700: Loss = -12427.82426301628
Iteration 8800: Loss = -12427.824158932463
Iteration 8900: Loss = -12427.827837618197
1
Iteration 9000: Loss = -12427.83145012687
2
Iteration 9100: Loss = -12427.87843303877
3
Iteration 9200: Loss = -12427.824001285842
Iteration 9300: Loss = -12427.825310257822
1
Iteration 9400: Loss = -12427.824733417921
2
Iteration 9500: Loss = -12427.824287412837
3
Iteration 9600: Loss = -12427.82487249749
4
Iteration 9700: Loss = -12427.823701598567
Iteration 9800: Loss = -12427.824311180804
1
Iteration 9900: Loss = -12427.82368380159
Iteration 10000: Loss = -12427.823590204915
Iteration 10100: Loss = -12427.823712471281
1
Iteration 10200: Loss = -12427.823511406868
Iteration 10300: Loss = -12427.823997919577
1
Iteration 10400: Loss = -12427.82342826219
Iteration 10500: Loss = -12427.825085257024
1
Iteration 10600: Loss = -12427.824503391677
2
Iteration 10700: Loss = -12427.823730167673
3
Iteration 10800: Loss = -12427.823399134273
Iteration 10900: Loss = -12427.8233743426
Iteration 11000: Loss = -12427.823241778886
Iteration 11100: Loss = -12427.8232124063
Iteration 11200: Loss = -12427.825156460096
1
Iteration 11300: Loss = -12427.823193712215
Iteration 11400: Loss = -12427.825334720637
1
Iteration 11500: Loss = -12427.824735733497
2
Iteration 11600: Loss = -12427.823168699284
Iteration 11700: Loss = -12427.823079288642
Iteration 11800: Loss = -12427.82308684898
1
Iteration 11900: Loss = -12427.824676207429
2
Iteration 12000: Loss = -12427.823529843634
3
Iteration 12100: Loss = -12427.825417300699
4
Iteration 12200: Loss = -12427.826909743664
5
Iteration 12300: Loss = -12428.197649719332
6
Iteration 12400: Loss = -12427.836229341452
7
Iteration 12500: Loss = -12427.8237029419
8
Iteration 12600: Loss = -12427.823301365788
9
Iteration 12700: Loss = -12427.82286493402
Iteration 12800: Loss = -12427.825752059774
1
Iteration 12900: Loss = -12427.822833403456
Iteration 13000: Loss = -12427.825261709344
1
Iteration 13100: Loss = -12427.822823443978
Iteration 13200: Loss = -12427.843491242762
1
Iteration 13300: Loss = -12427.823525179458
2
Iteration 13400: Loss = -12427.82282709671
3
Iteration 13500: Loss = -12427.828714444802
4
Iteration 13600: Loss = -12427.82280049288
Iteration 13700: Loss = -12427.831798118314
1
Iteration 13800: Loss = -12427.822810382784
2
Iteration 13900: Loss = -12427.849251635394
3
Iteration 14000: Loss = -12427.822828842083
4
Iteration 14100: Loss = -12427.825071269821
5
Iteration 14200: Loss = -12427.845990085805
6
Iteration 14300: Loss = -12427.822885497768
7
Iteration 14400: Loss = -12427.824318490553
8
Iteration 14500: Loss = -12427.825070947623
9
Iteration 14600: Loss = -12427.8290176818
10
Stopping early at iteration 14600 due to no improvement.
tensor([[-5.1960,  3.7499],
        [-5.5492,  3.4385],
        [-6.0844,  2.8735],
        [-5.5372,  3.4447],
        [-5.7572,  3.2040],
        [-5.1960,  3.7770],
        [-5.2466,  3.7462],
        [-5.1824,  3.7860],
        [-5.2043,  3.7933],
        [-6.7952,  2.1800],
        [-5.3238,  3.6647],
        [-5.5029,  3.4953],
        [-5.1923,  3.7616],
        [-5.5350,  3.4731],
        [-5.4979,  3.5530],
        [-5.4187,  3.8286],
        [-5.6101,  3.4037],
        [-5.1755,  3.7696],
        [-5.2028,  3.7653],
        [-5.1896,  3.7669],
        [-5.1843,  3.7536],
        [-5.4853,  3.5060],
        [-6.0582,  2.9116],
        [-6.8034,  2.1882],
        [-5.3342,  3.6583],
        [-5.3298,  3.6517],
        [-5.3882,  3.5827],
        [-5.3757,  3.6418],
        [-5.2933,  3.9063],
        [-5.9340,  3.0287],
        [-5.4352,  3.8214],
        [-5.4381,  3.7341],
        [-5.1965,  3.7867],
        [-5.2012,  3.7173],
        [-5.5147,  3.4681],
        [-5.5937,  3.8415],
        [-5.1886,  3.7666],
        [-6.1546,  3.5220],
        [-5.3259,  3.6631],
        [-5.2318,  3.7170],
        [-5.2431,  3.7964],
        [-5.1922,  3.7909],
        [-5.7782,  4.0383],
        [-5.5151,  3.4408],
        [-5.3247,  3.6365],
        [-6.2500,  2.6849],
        [-5.2637,  3.7748],
        [-5.1899,  3.7630],
        [-6.9621,  2.4033],
        [-5.3038,  3.9175],
        [-5.8969,  3.0657],
        [-6.2384,  4.5605],
        [-5.9230,  3.0297],
        [-5.1809,  3.7943],
        [-5.3193,  3.9159],
        [-5.2755,  3.6750],
        [-5.6090,  3.3536],
        [-5.2702,  3.7108],
        [-5.3418,  3.6275],
        [-5.5535,  3.4090],
        [-5.1807,  3.7852],
        [-5.2161,  3.7155],
        [-5.4655,  3.9166],
        [-5.7468,  3.2101],
        [-5.1875,  3.7873],
        [-5.2755,  3.6788],
        [-5.4530,  3.4771],
        [-5.4010,  3.5669],
        [-5.9879,  2.9990],
        [-5.3353,  3.6341],
        [-6.1446,  2.8726],
        [-5.2065,  3.7760],
        [-6.5758,  2.3861],
        [-5.3577,  3.6751],
        [-5.2508,  3.6982],
        [-5.3196,  3.9326],
        [-5.6472,  3.3596],
        [-5.2777,  3.7631],
        [-5.1668,  3.7751],
        [-5.2215,  3.7561],
        [-5.3128,  3.6038],
        [-5.2109,  3.7389],
        [-6.2234,  2.7776],
        [-6.2774,  2.6936],
        [-5.1908,  3.7743],
        [-5.4464,  3.5356],
        [-5.2575,  3.6991],
        [-5.6411,  3.3207],
        [-5.2553,  3.6948],
        [-5.5092,  3.4856],
        [-5.4277,  3.5369],
        [-5.2899,  3.7196],
        [-5.5158,  3.4597],
        [-6.1198,  2.8754],
        [-5.2019,  3.7833],
        [-5.6831,  3.3203],
        [-5.2733,  3.6784],
        [-5.6590,  3.2632],
        [-5.9728,  2.9952],
        [-5.2104,  3.7360]], dtype=torch.float64, requires_grad=True)
pi: tensor([[2.3718e-04, 9.9976e-01],
        [5.9637e-02, 9.4036e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.2104e-04, 9.9988e-01], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2613, 0.1991],
         [0.3160, 0.1976]],

        [[0.3653, 0.2810],
         [0.4719, 0.8912]],

        [[0.3296, 0.1984],
         [0.1250, 0.8717]],

        [[0.4822, 0.2016],
         [0.8988, 0.2140]],

        [[0.6051, 0.2476],
         [0.7026, 0.7603]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 41
Adjusted Rand Index: -0.020678759622205528
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: -0.0026099303253762193
Average Adjusted Rand Index: -0.004863024651713833
Iteration 0: Loss = -20845.913787934718
Iteration 10: Loss = -12429.976435372739
Iteration 20: Loss = -12429.95460170929
Iteration 30: Loss = -12429.95454846399
Iteration 40: Loss = -12429.954624774304
1
Iteration 50: Loss = -12429.9545919264
2
Iteration 60: Loss = -12429.954549885015
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.2217, 0.7783],
        [0.6803, 0.3197]], dtype=torch.float64)
alpha: tensor([0.4664, 0.5336])
beta: tensor([[[0.1995, 0.1974],
         [0.1708, 0.1992]],

        [[0.0159, 0.2050],
         [0.4198, 0.3121]],

        [[0.5586, 0.1936],
         [0.9141, 0.8140]],

        [[0.8535, 0.1954],
         [0.0881, 0.9949]],

        [[0.5935, 0.2052],
         [0.5651, 0.5961]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 38
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20735.826418959383
Iteration 100: Loss = -12542.259298381654
Iteration 200: Loss = -12480.252830761703
Iteration 300: Loss = -12444.693514922386
Iteration 400: Loss = -12433.432505849309
Iteration 500: Loss = -12432.196183351485
Iteration 600: Loss = -12431.544989824371
Iteration 700: Loss = -12431.142589936038
Iteration 800: Loss = -12430.853840399748
Iteration 900: Loss = -12430.627326359385
Iteration 1000: Loss = -12430.432247614312
Iteration 1100: Loss = -12430.262226880819
Iteration 1200: Loss = -12430.131058391626
Iteration 1300: Loss = -12430.042602104033
Iteration 1400: Loss = -12429.981193993248
Iteration 1500: Loss = -12429.935712255316
Iteration 1600: Loss = -12429.900358476843
Iteration 1700: Loss = -12429.87205227869
Iteration 1800: Loss = -12429.848799854888
Iteration 1900: Loss = -12429.82932585207
Iteration 2000: Loss = -12429.812923978378
Iteration 2100: Loss = -12429.798876360997
Iteration 2200: Loss = -12429.786718113557
Iteration 2300: Loss = -12429.776169246941
Iteration 2400: Loss = -12429.766778535803
Iteration 2500: Loss = -12429.758366252992
Iteration 2600: Loss = -12429.750673615572
Iteration 2700: Loss = -12429.743330925858
Iteration 2800: Loss = -12429.73551810855
Iteration 2900: Loss = -12429.723437918019
Iteration 3000: Loss = -12429.691054935249
Iteration 3100: Loss = -12429.629581564266
Iteration 3200: Loss = -12429.569744208275
Iteration 3300: Loss = -12429.525243725398
Iteration 3400: Loss = -12429.494573254322
Iteration 3500: Loss = -12429.473471896652
Iteration 3600: Loss = -12429.458590886408
Iteration 3700: Loss = -12429.447768533706
Iteration 3800: Loss = -12429.439510835895
Iteration 3900: Loss = -12429.434286851776
Iteration 4000: Loss = -12429.428020055791
Iteration 4100: Loss = -12429.423811688173
Iteration 4200: Loss = -12429.420320084708
Iteration 4300: Loss = -12429.419280680271
Iteration 4400: Loss = -12429.414777463648
Iteration 4500: Loss = -12429.412396142983
Iteration 4600: Loss = -12429.410316497708
Iteration 4700: Loss = -12429.408751210603
Iteration 4800: Loss = -12429.406828393501
Iteration 4900: Loss = -12429.525329366605
1
Iteration 5000: Loss = -12429.404006836454
Iteration 5100: Loss = -12429.40275988981
Iteration 5200: Loss = -12429.401944835025
Iteration 5300: Loss = -12429.400607803545
Iteration 5400: Loss = -12429.399621472254
Iteration 5500: Loss = -12429.40762259033
1
Iteration 5600: Loss = -12429.397850712514
Iteration 5700: Loss = -12429.4008773268
1
Iteration 5800: Loss = -12429.396396365788
Iteration 5900: Loss = -12429.400531143952
1
Iteration 6000: Loss = -12429.395126066578
Iteration 6100: Loss = -12429.394463996234
Iteration 6200: Loss = -12429.396124212402
1
Iteration 6300: Loss = -12429.39340074674
Iteration 6400: Loss = -12429.392940697224
Iteration 6500: Loss = -12429.393453863724
1
Iteration 6600: Loss = -12429.392076465969
Iteration 6700: Loss = -12429.39170748162
Iteration 6800: Loss = -12429.391334015285
Iteration 6900: Loss = -12429.541103728961
1
Iteration 7000: Loss = -12429.390604958899
Iteration 7100: Loss = -12429.444340984288
1
Iteration 7200: Loss = -12429.390052078072
Iteration 7300: Loss = -12429.392298213003
1
Iteration 7400: Loss = -12429.43984994195
2
Iteration 7500: Loss = -12429.389227159478
Iteration 7600: Loss = -12429.39187778123
1
Iteration 7700: Loss = -12429.405412878408
2
Iteration 7800: Loss = -12429.388520867258
Iteration 7900: Loss = -12429.388603513924
1
Iteration 8000: Loss = -12429.388111476384
Iteration 8100: Loss = -12429.388389901396
1
Iteration 8200: Loss = -12429.387774515877
Iteration 8300: Loss = -12429.38767214076
Iteration 8400: Loss = -12429.387446324617
Iteration 8500: Loss = -12429.39710520089
1
Iteration 8600: Loss = -12429.387207869748
Iteration 8700: Loss = -12429.387071921605
Iteration 8800: Loss = -12429.387495099667
1
Iteration 8900: Loss = -12429.38679835444
Iteration 9000: Loss = -12429.476899144995
1
Iteration 9100: Loss = -12429.386587454434
Iteration 9200: Loss = -12429.387666915984
1
Iteration 9300: Loss = -12429.386472000517
Iteration 9400: Loss = -12429.439360329994
1
Iteration 9500: Loss = -12429.386262803719
Iteration 9600: Loss = -12429.409100003902
1
Iteration 9700: Loss = -12429.386073635302
Iteration 9800: Loss = -12429.437965834331
1
Iteration 9900: Loss = -12429.385950166981
Iteration 10000: Loss = -12429.3938953455
1
Iteration 10100: Loss = -12429.385860593611
Iteration 10200: Loss = -12429.46412056344
1
Iteration 10300: Loss = -12429.38577329896
Iteration 10400: Loss = -12429.478191045386
1
Iteration 10500: Loss = -12429.389719142493
2
Iteration 10600: Loss = -12429.3896308227
3
Iteration 10700: Loss = -12429.385498014899
Iteration 10800: Loss = -12429.385544173325
1
Iteration 10900: Loss = -12429.466035822223
2
Iteration 11000: Loss = -12429.386139378747
3
Iteration 11100: Loss = -12429.385697957661
4
Iteration 11200: Loss = -12429.391215663754
5
Iteration 11300: Loss = -12429.40976031536
6
Iteration 11400: Loss = -12429.48997543935
7
Iteration 11500: Loss = -12429.385811297354
8
Iteration 11600: Loss = -12429.38539533415
Iteration 11700: Loss = -12429.385145445594
Iteration 11800: Loss = -12429.385268676042
1
Iteration 11900: Loss = -12429.429215313914
2
Iteration 12000: Loss = -12429.3937800038
3
Iteration 12100: Loss = -12429.385134581931
Iteration 12200: Loss = -12429.385341737121
1
Iteration 12300: Loss = -12429.387115712012
2
Iteration 12400: Loss = -12429.387830993468
3
Iteration 12500: Loss = -12429.385975655903
4
Iteration 12600: Loss = -12429.384974519948
Iteration 12700: Loss = -12429.38510504295
1
Iteration 12800: Loss = -12429.43411323441
2
Iteration 12900: Loss = -12429.40648132438
3
Iteration 13000: Loss = -12429.398395243417
4
Iteration 13100: Loss = -12429.385360287954
5
Iteration 13200: Loss = -12429.488659116665
6
Iteration 13300: Loss = -12429.384869773872
Iteration 13400: Loss = -12429.389844405046
1
Iteration 13500: Loss = -12429.394371483766
2
Iteration 13600: Loss = -12429.386137863125
3
Iteration 13700: Loss = -12429.38480375347
Iteration 13800: Loss = -12429.38594033573
1
Iteration 13900: Loss = -12429.403040207757
2
Iteration 14000: Loss = -12429.385252374728
3
Iteration 14100: Loss = -12429.420296278802
4
Iteration 14200: Loss = -12429.384725990034
Iteration 14300: Loss = -12429.385376054339
1
Iteration 14400: Loss = -12429.400088646082
2
Iteration 14500: Loss = -12429.424885104807
3
Iteration 14600: Loss = -12429.463243806264
4
Iteration 14700: Loss = -12429.384743275372
5
Iteration 14800: Loss = -12429.384815229856
6
Iteration 14900: Loss = -12429.453838065634
7
Iteration 15000: Loss = -12429.38772799712
8
Iteration 15100: Loss = -12429.390651605174
9
Iteration 15200: Loss = -12429.40046668407
10
Stopping early at iteration 15200 due to no improvement.
tensor([[ -9.4969,   8.0748],
        [ -9.2451,   7.0601],
        [ -9.0514,   7.2874],
        [ -8.9524,   7.4759],
        [ -8.6130,   7.2153],
        [ -9.6802,   8.0431],
        [ -9.8508,   6.8657],
        [-11.1781,   7.4545],
        [ -8.7191,   7.2537],
        [ -9.7253,   8.1495],
        [ -8.5859,   7.0891],
        [ -9.6304,   7.5610],
        [-10.1648,   6.6145],
        [ -9.3687,   7.5076],
        [ -8.9312,   7.3813],
        [ -8.7990,   7.4120],
        [ -9.9753,   7.7842],
        [ -9.2315,   7.8297],
        [-10.6534,   7.7591],
        [ -8.6455,   7.1069],
        [-10.6560,   6.3001],
        [ -9.8280,   8.0510],
        [ -9.3915,   7.9223],
        [ -9.5418,   6.5723],
        [ -8.8737,   7.3159],
        [ -9.1573,   6.7797],
        [ -9.4580,   7.7375],
        [ -9.0899,   7.6726],
        [-10.6310,   6.6974],
        [ -9.1379,   7.5284],
        [ -9.4456,   8.0561],
        [ -8.9946,   7.6016],
        [ -8.9766,   7.4757],
        [ -8.8703,   7.4836],
        [ -8.7639,   7.3691],
        [ -8.9932,   7.4562],
        [ -9.8673,   7.5997],
        [ -9.1333,   7.7364],
        [ -9.0521,   7.5801],
        [ -9.2862,   7.3704],
        [ -9.3971,   7.6502],
        [ -8.6748,   7.1996],
        [ -8.5837,   7.1974],
        [ -9.4111,   7.9628],
        [ -9.5186,   7.2081],
        [ -8.5445,   7.0486],
        [ -9.1848,   7.3246],
        [ -8.8312,   7.1328],
        [ -9.1283,   6.9264],
        [ -9.0899,   7.2650],
        [ -9.3264,   7.7581],
        [ -9.8496,   7.2434],
        [ -9.6805,   8.1416],
        [-10.1560,   6.9338],
        [ -8.9815,   7.0637],
        [ -9.4266,   7.6466],
        [ -9.7231,   6.5390],
        [ -9.1319,   7.5035],
        [ -9.4461,   7.2082],
        [ -9.7313,   7.8027],
        [ -9.5284,   8.1393],
        [ -8.7809,   7.3946],
        [ -8.6724,   7.2798],
        [ -8.5475,   7.1548],
        [ -9.0171,   6.8762],
        [ -9.1715,   7.7378],
        [ -9.0618,   6.4875],
        [ -9.8215,   6.5397],
        [ -8.7842,   7.3504],
        [ -9.2503,   6.9380],
        [ -9.6082,   7.7342],
        [ -9.1984,   7.7987],
        [ -9.4665,   7.9369],
        [ -9.3822,   7.9119],
        [ -9.1973,   7.5210],
        [ -8.7916,   7.3363],
        [ -9.4152,   7.7328],
        [ -8.4859,   7.0622],
        [ -8.5053,   6.9429],
        [ -8.7483,   7.1597],
        [ -9.4036,   7.3034],
        [ -9.0644,   7.4590],
        [ -9.7237,   8.1913],
        [-10.9579,   7.7082],
        [ -9.3687,   7.8444],
        [ -9.5609,   7.8010],
        [ -8.4359,   6.9427],
        [ -9.1336,   7.6318],
        [ -8.8700,   7.4536],
        [ -9.6168,   7.6491],
        [-10.8302,   6.8718],
        [ -8.9324,   7.3882],
        [ -9.5323,   7.9651],
        [ -9.1331,   7.3739],
        [ -8.6473,   6.9151],
        [ -9.5101,   7.9909],
        [ -8.9501,   7.5423],
        [ -9.5342,   8.1124],
        [ -9.8989,   7.4819],
        [ -8.3949,   6.9353]], dtype=torch.float64, requires_grad=True)
pi: tensor([[7.9136e-03, 9.9209e-01],
        [1.0000e+00, 1.0857e-07]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.1442e-08, 1.0000e+00], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2026, 0.2050],
         [0.1708, 0.1998]],

        [[0.0159, 0.2651],
         [0.4198, 0.3121]],

        [[0.5586, 0.1440],
         [0.9141, 0.8140]],

        [[0.8535, 0.1842],
         [0.0881, 0.9949]],

        [[0.5935, 0.3056],
         [0.5651, 0.5961]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 62
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: -0.0021241119770515314
Average Adjusted Rand Index: 0.000324944210806727
Iteration 0: Loss = -20246.184365769037
Iteration 10: Loss = -12429.685939638759
Iteration 20: Loss = -12429.449064447617
Iteration 30: Loss = -12429.026635046619
Iteration 40: Loss = -12428.576228033498
Iteration 50: Loss = -12428.375404698405
Iteration 60: Loss = -12428.299888980491
Iteration 70: Loss = -12428.27102104565
Iteration 80: Loss = -12428.258910179626
Iteration 90: Loss = -12428.253406269718
Iteration 100: Loss = -12428.250615896095
Iteration 110: Loss = -12428.249203325258
Iteration 120: Loss = -12428.248392086796
Iteration 130: Loss = -12428.24793102634
Iteration 140: Loss = -12428.247721857395
Iteration 150: Loss = -12428.247566690292
Iteration 160: Loss = -12428.24751199271
Iteration 170: Loss = -12428.247447348333
Iteration 180: Loss = -12428.247439465611
Iteration 190: Loss = -12428.2473809324
Iteration 200: Loss = -12428.247420764632
1
Iteration 210: Loss = -12428.247403588392
2
Iteration 220: Loss = -12428.247421881788
3
Stopping early at iteration 219 due to no improvement.
pi: tensor([[0.0790, 0.9210],
        [0.0646, 0.9354]], dtype=torch.float64)
alpha: tensor([0.0653, 0.9347])
beta: tensor([[[0.2341, 0.2030],
         [0.2814, 0.1957]],

        [[0.4878, 0.2816],
         [0.0015, 0.0045]],

        [[0.4373, 0.1949],
         [0.1734, 0.7303]],

        [[0.6475, 0.1983],
         [0.3919, 0.8610]],

        [[0.1728, 0.2385],
         [0.1661, 0.5676]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 41
Adjusted Rand Index: -0.020678759622205528
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: -0.0026099303253762193
Average Adjusted Rand Index: -0.004863024651713833
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20245.76286377074
Iteration 100: Loss = -12444.234934047845
Iteration 200: Loss = -12432.366200006794
Iteration 300: Loss = -12430.37650253264
Iteration 400: Loss = -12429.435054350546
Iteration 500: Loss = -12428.993053163333
Iteration 600: Loss = -12428.75668186798
Iteration 700: Loss = -12428.629186886375
Iteration 800: Loss = -12428.548292975864
Iteration 900: Loss = -12428.489316202951
Iteration 1000: Loss = -12428.437898725932
Iteration 1100: Loss = -12428.393356059369
Iteration 1200: Loss = -12428.362130104179
Iteration 1300: Loss = -12428.335448853386
Iteration 1400: Loss = -12428.312987215551
Iteration 1500: Loss = -12428.292246727135
Iteration 1600: Loss = -12428.272653369782
Iteration 1700: Loss = -12428.254027115283
Iteration 1800: Loss = -12428.23617891667
Iteration 1900: Loss = -12428.21989954654
Iteration 2000: Loss = -12428.205000984615
Iteration 2100: Loss = -12428.190438929718
Iteration 2200: Loss = -12428.175682332889
Iteration 2300: Loss = -12428.158248456455
Iteration 2400: Loss = -12428.141905825694
Iteration 2500: Loss = -12428.126102155218
Iteration 2600: Loss = -12428.111458683683
Iteration 2700: Loss = -12428.099140680775
Iteration 2800: Loss = -12428.089659307841
Iteration 2900: Loss = -12428.0830383142
Iteration 3000: Loss = -12428.078210619185
Iteration 3100: Loss = -12428.074365057657
Iteration 3200: Loss = -12428.07244709976
Iteration 3300: Loss = -12428.07087097155
Iteration 3400: Loss = -12428.069455999863
Iteration 3500: Loss = -12428.068089887649
Iteration 3600: Loss = -12428.066626254973
Iteration 3700: Loss = -12428.065196261501
Iteration 3800: Loss = -12428.06382105983
Iteration 3900: Loss = -12428.062420124243
Iteration 4000: Loss = -12428.06120280498
Iteration 4100: Loss = -12428.060058844534
Iteration 4200: Loss = -12428.05926618727
Iteration 4300: Loss = -12428.058478180914
Iteration 4400: Loss = -12428.057836811238
Iteration 4500: Loss = -12428.058775148564
1
Iteration 4600: Loss = -12428.056611252434
Iteration 4700: Loss = -12428.055995351424
Iteration 4800: Loss = -12428.055364545678
Iteration 4900: Loss = -12428.16561015397
1
Iteration 5000: Loss = -12428.053859229098
Iteration 5100: Loss = -12428.053156870816
Iteration 5200: Loss = -12428.068949131557
1
Iteration 5300: Loss = -12428.052095649578
Iteration 5400: Loss = -12428.051774545884
Iteration 5500: Loss = -12428.052288255281
1
Iteration 5600: Loss = -12428.05125650899
Iteration 5700: Loss = -12428.050950686335
Iteration 5800: Loss = -12428.052110011067
1
Iteration 5900: Loss = -12428.050358785347
Iteration 6000: Loss = -12428.050091584859
Iteration 6100: Loss = -12428.050074793537
Iteration 6200: Loss = -12428.049883758176
Iteration 6300: Loss = -12428.04978433121
Iteration 6400: Loss = -12428.049611114857
Iteration 6500: Loss = -12428.049867194431
1
Iteration 6600: Loss = -12428.066836143154
2
Iteration 6700: Loss = -12428.050125083144
3
Iteration 6800: Loss = -12428.056950036509
4
Iteration 6900: Loss = -12428.09682929233
5
Iteration 7000: Loss = -12428.049024119759
Iteration 7100: Loss = -12428.053146841185
1
Iteration 7200: Loss = -12428.04950524717
2
Iteration 7300: Loss = -12428.052085206711
3
Iteration 7400: Loss = -12428.068886931582
4
Iteration 7500: Loss = -12428.059456626703
5
Iteration 7600: Loss = -12428.048371785035
Iteration 7700: Loss = -12428.089669158213
1
Iteration 7800: Loss = -12428.048164434247
Iteration 7900: Loss = -12428.065053806937
1
Iteration 8000: Loss = -12428.048015291712
Iteration 8100: Loss = -12428.051293850418
1
Iteration 8200: Loss = -12428.047880781021
Iteration 8300: Loss = -12428.047834751074
Iteration 8400: Loss = -12428.081515796937
1
Iteration 8500: Loss = -12428.047728102638
Iteration 8600: Loss = -12428.047709207518
Iteration 8700: Loss = -12428.049872403137
1
Iteration 8800: Loss = -12428.047695853076
Iteration 8900: Loss = -12428.049196150036
1
Iteration 9000: Loss = -12428.070946122385
2
Iteration 9100: Loss = -12428.052917430578
3
Iteration 9200: Loss = -12428.048232598554
4
Iteration 9300: Loss = -12428.048984783469
5
Iteration 9400: Loss = -12428.04717936382
Iteration 9500: Loss = -12428.047280075416
1
Iteration 9600: Loss = -12428.054464609264
2
Iteration 9700: Loss = -12428.047062185817
Iteration 9800: Loss = -12428.047148448852
1
Iteration 9900: Loss = -12428.04697676097
Iteration 10000: Loss = -12428.046992386455
1
Iteration 10100: Loss = -12428.046948309924
Iteration 10200: Loss = -12428.201916483575
1
Iteration 10300: Loss = -12428.046871277476
Iteration 10400: Loss = -12428.0468717449
1
Iteration 10500: Loss = -12428.047845479176
2
Iteration 10600: Loss = -12428.21918035304
3
Iteration 10700: Loss = -12428.051748268927
4
Iteration 10800: Loss = -12428.060405599896
5
Iteration 10900: Loss = -12428.151209184312
6
Iteration 11000: Loss = -12428.047090577764
7
Iteration 11100: Loss = -12428.066344070157
8
Iteration 11200: Loss = -12428.173668908286
9
Iteration 11300: Loss = -12428.049395361451
10
Stopping early at iteration 11300 due to no improvement.
tensor([[ 4.2683, -5.7803],
        [ 4.1551, -5.8078],
        [ 3.6294, -6.5276],
        [ 3.7809, -6.2744],
        [ 4.1986, -5.8306],
        [ 4.2634, -5.6927],
        [ 4.2786, -5.7347],
        [ 3.5789, -6.4420],
        [ 5.7948, -7.5609],
        [ 3.6323, -6.3644],
        [ 4.1709, -5.8343],
        [ 4.2395, -5.6312],
        [ 3.5965, -6.4856],
        [ 4.3043, -5.6906],
        [ 4.3276, -5.7368],
        [ 4.4412, -5.9172],
        [ 4.2948, -5.6811],
        [ 2.7769, -7.3922],
        [ 4.1719, -5.8263],
        [ 4.2455, -5.8011],
        [ 3.8639, -6.2715],
        [ 4.2588, -5.6591],
        [ 4.1971, -5.8066],
        [ 4.1417, -5.9309],
        [ 3.9617, -5.9416],
        [ 4.2472, -5.6704],
        [ 4.2426, -5.7399],
        [ 4.2651, -5.8389],
        [ 4.3495, -5.7618],
        [ 2.7012, -7.3165],
        [ 4.4853, -5.8762],
        [ 4.3741, -5.8397],
        [ 3.8942, -6.0793],
        [ 3.6325, -6.5284],
        [ 4.0807, -6.0604],
        [ 4.1172, -6.2955],
        [ 2.8868, -7.1844],
        [ 4.5182, -6.1187],
        [ 3.9585, -5.9833],
        [ 4.3482, -5.7842],
        [ 3.9910, -6.1113],
        [ 4.0476, -5.9569],
        [ 3.6195, -6.9298],
        [ 3.8780, -6.1861],
        [ 2.7063, -7.3216],
        [ 4.3863, -5.7779],
        [ 3.6546, -6.1813],
        [ 3.9198, -6.1693],
        [ 4.5257, -5.9283],
        [ 3.8833, -6.2291],
        [ 3.4859, -6.5143],
        [ 4.4925, -6.3332],
        [ 4.3246, -5.7446],
        [ 4.1255, -5.9345],
        [ 4.4038, -5.7966],
        [ 4.3073, -5.7545],
        [ 3.9519, -6.0870],
        [ 4.2930, -5.6802],
        [ 3.4724, -6.6794],
        [ 4.1762, -5.8546],
        [ 4.1492, -5.9140],
        [ 2.8364, -7.3341],
        [ 4.3586, -5.8577],
        [ 2.7179, -7.3332],
        [ 4.2846, -5.7150],
        [ 2.7551, -7.3170],
        [ 3.5585, -6.5991],
        [ 4.1281, -5.8678],
        [ 4.2749, -5.6615],
        [ 4.2331, -5.8409],
        [ 3.6979, -6.1599],
        [ 4.3358, -5.7619],
        [ 3.9958, -6.0503],
        [ 3.9754, -5.9920],
        [ 5.8169, -8.2359],
        [ 4.3639, -5.7789],
        [ 3.1828, -6.8659],
        [ 4.4104, -5.8155],
        [ 4.1796, -5.9241],
        [ 4.3605, -5.7546],
        [ 4.3547, -5.8227],
        [ 3.8774, -6.2262],
        [ 4.2971, -5.6851],
        [ 2.8520, -7.1501],
        [ 4.3288, -5.7380],
        [ 4.1960, -5.7279],
        [ 4.2864, -5.7904],
        [ 4.0090, -6.0488],
        [ 4.3231, -5.8671],
        [ 4.3013, -5.6903],
        [ 4.3225, -5.7092],
        [ 3.8775, -6.0626],
        [ 4.1223, -5.9913],
        [ 3.7094, -6.3771],
        [ 6.1480, -7.5533],
        [ 4.1465, -5.7423],
        [ 4.0712, -5.9634],
        [ 4.3974, -5.8141],
        [ 2.7085, -7.3237],
        [ 4.0617, -6.0783]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.0538, 0.9462],
        [0.0171, 0.9829]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9996e-01, 4.1036e-05], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1992, 0.1960],
         [0.2814, 0.1990]],

        [[0.4878, 0.2866],
         [0.0015, 0.0045]],

        [[0.4373, 0.1777],
         [0.1734, 0.7303]],

        [[0.6475, 0.1938],
         [0.3919, 0.8610]],

        [[0.1728, 0.2955],
         [0.1661, 0.5676]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 40
Adjusted Rand Index: -0.014617321874876162
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: 0.0012963872921693505
Average Adjusted Rand Index: -0.00365073710224796
Iteration 0: Loss = -27118.11369706711
Iteration 10: Loss = -12430.883950464076
Iteration 20: Loss = -12430.88395140613
1
Iteration 30: Loss = -12430.884122451676
2
Iteration 40: Loss = -12430.878366570929
Iteration 50: Loss = -12429.71642023705
Iteration 60: Loss = -12428.740897708904
Iteration 70: Loss = -12428.498767492525
Iteration 80: Loss = -12428.409800440204
Iteration 90: Loss = -12428.355654303934
Iteration 100: Loss = -12428.275636396744
Iteration 110: Loss = -12428.244164809523
Iteration 120: Loss = -12428.239987445962
Iteration 130: Loss = -12428.24142061852
1
Iteration 140: Loss = -12428.24335742861
2
Iteration 150: Loss = -12428.244860755356
3
Stopping early at iteration 149 due to no improvement.
pi: tensor([[0.0773, 0.9227],
        [0.0636, 0.9364]], dtype=torch.float64)
alpha: tensor([0.0643, 0.9357])
beta: tensor([[[0.2344, 0.2029],
         [0.5444, 0.1958]],

        [[0.0408, 0.2818],
         [0.5763, 0.5183]],

        [[0.2825, 0.1947],
         [0.6970, 0.9284]],

        [[0.2719, 0.1983],
         [0.5453, 0.2279]],

        [[0.5855, 0.2391],
         [0.0444, 0.4932]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 41
Adjusted Rand Index: -0.020678759622205528
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: -0.0026099303253762193
Average Adjusted Rand Index: -0.004863024651713833
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27118.664495271885
Iteration 100: Loss = -12434.468976210144
Iteration 200: Loss = -12432.038994753631
Iteration 300: Loss = -12431.30765066224
Iteration 400: Loss = -12430.741604125295
Iteration 500: Loss = -12430.319939865545
Iteration 600: Loss = -12430.04386833761
Iteration 700: Loss = -12429.787708470825
Iteration 800: Loss = -12429.394381329217
Iteration 900: Loss = -12429.075314739415
Iteration 1000: Loss = -12428.916900905704
Iteration 1100: Loss = -12428.812713148769
Iteration 1200: Loss = -12428.730316221767
Iteration 1300: Loss = -12428.659154717285
Iteration 1400: Loss = -12428.594026186245
Iteration 1500: Loss = -12428.532242537063
Iteration 1600: Loss = -12428.471914901924
Iteration 1700: Loss = -12428.411842864889
Iteration 1800: Loss = -12428.351605133716
Iteration 1900: Loss = -12428.291112181796
Iteration 2000: Loss = -12428.230975663877
Iteration 2100: Loss = -12428.172243242638
Iteration 2200: Loss = -12428.116396265115
Iteration 2300: Loss = -12428.065201798865
Iteration 2400: Loss = -12428.019962885746
Iteration 2500: Loss = -12427.981166196008
Iteration 2600: Loss = -12427.948138204023
Iteration 2700: Loss = -12427.919245442818
Iteration 2800: Loss = -12427.892933963847
Iteration 2900: Loss = -12427.87373549602
Iteration 3000: Loss = -12427.860790354112
Iteration 3100: Loss = -12427.851728448217
Iteration 3200: Loss = -12427.845735350144
Iteration 3300: Loss = -12427.841909299019
Iteration 3400: Loss = -12427.83948389232
Iteration 3500: Loss = -12427.837718912186
Iteration 3600: Loss = -12427.836496687592
Iteration 3700: Loss = -12427.835600155226
Iteration 3800: Loss = -12427.834898952733
Iteration 3900: Loss = -12427.834410460166
Iteration 4000: Loss = -12427.83398759932
Iteration 4100: Loss = -12427.833632130238
Iteration 4200: Loss = -12427.833342119056
Iteration 4300: Loss = -12427.833068314736
Iteration 4400: Loss = -12427.832801471708
Iteration 4500: Loss = -12427.832605918462
Iteration 4600: Loss = -12427.832389328987
Iteration 4700: Loss = -12427.832220436152
Iteration 4800: Loss = -12427.831996535662
Iteration 4900: Loss = -12427.83180828728
Iteration 5000: Loss = -12427.83161810073
Iteration 5100: Loss = -12427.831448571978
Iteration 5200: Loss = -12427.831246846119
Iteration 5300: Loss = -12427.831053516478
Iteration 5400: Loss = -12427.830842093588
Iteration 5500: Loss = -12427.83068025486
Iteration 5600: Loss = -12427.830494205225
Iteration 5700: Loss = -12427.830322129506
Iteration 5800: Loss = -12427.830133729609
Iteration 5900: Loss = -12427.82992229992
Iteration 6000: Loss = -12427.82975502574
Iteration 6100: Loss = -12427.82958656802
Iteration 6200: Loss = -12427.829413801344
Iteration 6300: Loss = -12427.829254599377
Iteration 6400: Loss = -12427.829068623972
Iteration 6500: Loss = -12427.82886182398
Iteration 6600: Loss = -12427.828685940734
Iteration 6700: Loss = -12427.828547402847
Iteration 6800: Loss = -12427.828337119974
Iteration 6900: Loss = -12427.828197658537
Iteration 7000: Loss = -12427.834522776979
1
Iteration 7100: Loss = -12427.827871948028
Iteration 7200: Loss = -12427.829795720265
1
Iteration 7300: Loss = -12427.827533223004
Iteration 7400: Loss = -12427.827432475755
Iteration 7500: Loss = -12427.840214605574
1
Iteration 7600: Loss = -12428.017971760322
2
Iteration 7700: Loss = -12427.826896378463
Iteration 7800: Loss = -12427.82706422829
1
Iteration 7900: Loss = -12427.82660234842
Iteration 8000: Loss = -12427.826996624633
1
Iteration 8100: Loss = -12427.826334165999
Iteration 8200: Loss = -12427.831326632566
1
Iteration 8300: Loss = -12427.826087612757
Iteration 8400: Loss = -12427.82593189301
Iteration 8500: Loss = -12427.831987087138
1
Iteration 8600: Loss = -12427.825693437726
Iteration 8700: Loss = -12427.82558517938
Iteration 8800: Loss = -12427.828690638402
1
Iteration 8900: Loss = -12427.825347971637
Iteration 9000: Loss = -12427.825266463477
Iteration 9100: Loss = -12427.828115281252
1
Iteration 9200: Loss = -12427.825035662818
Iteration 9300: Loss = -12427.824919467068
Iteration 9400: Loss = -12427.824945870781
1
Iteration 9500: Loss = -12427.824758309813
Iteration 9600: Loss = -12427.82465813273
Iteration 9700: Loss = -12427.826956216717
1
Iteration 9800: Loss = -12427.824490957199
Iteration 9900: Loss = -12427.824402473763
Iteration 10000: Loss = -12427.846141914495
1
Iteration 10100: Loss = -12427.824286242496
Iteration 10200: Loss = -12427.824203995591
Iteration 10300: Loss = -12427.917823629981
1
Iteration 10400: Loss = -12427.8240436735
Iteration 10500: Loss = -12427.823993714524
Iteration 10600: Loss = -12427.82395186292
Iteration 10700: Loss = -12427.823888477838
Iteration 10800: Loss = -12427.823784180644
Iteration 10900: Loss = -12427.82374443807
Iteration 11000: Loss = -12427.903695635823
1
Iteration 11100: Loss = -12427.823664428897
Iteration 11200: Loss = -12427.823575410308
Iteration 11300: Loss = -12427.82440652499
1
Iteration 11400: Loss = -12427.823518980194
Iteration 11500: Loss = -12427.823482310901
Iteration 11600: Loss = -12427.82360704156
1
Iteration 11700: Loss = -12427.823430489889
Iteration 11800: Loss = -12427.823824484603
1
Iteration 11900: Loss = -12427.823347279567
Iteration 12000: Loss = -12427.844051387938
1
Iteration 12100: Loss = -12427.989933569881
2
Iteration 12200: Loss = -12427.823817005501
3
Iteration 12300: Loss = -12427.824101924478
4
Iteration 12400: Loss = -12427.825628701079
5
Iteration 12500: Loss = -12427.82325259234
Iteration 12600: Loss = -12427.823501653855
1
Iteration 12700: Loss = -12427.82406872392
2
Iteration 12800: Loss = -12428.174458394402
3
Iteration 12900: Loss = -12427.823014111325
Iteration 13000: Loss = -12427.82348287957
1
Iteration 13100: Loss = -12427.823053907327
2
Iteration 13200: Loss = -12427.825161392215
3
Iteration 13300: Loss = -12427.822938738916
Iteration 13400: Loss = -12427.825366111694
1
Iteration 13500: Loss = -12427.824925314284
2
Iteration 13600: Loss = -12427.823038928736
3
Iteration 13700: Loss = -12427.85398520852
4
Iteration 13800: Loss = -12427.822848050691
Iteration 13900: Loss = -12427.83128331264
1
Iteration 14000: Loss = -12427.822831603718
Iteration 14100: Loss = -12427.823268866177
1
Iteration 14200: Loss = -12427.824716141167
2
Iteration 14300: Loss = -12427.824314567195
3
Iteration 14400: Loss = -12427.822970258138
4
Iteration 14500: Loss = -12427.825656605943
5
Iteration 14600: Loss = -12427.82536033618
6
Iteration 14700: Loss = -12427.822859733817
7
Iteration 14800: Loss = -12427.823046342657
8
Iteration 14900: Loss = -12427.82280232399
Iteration 15000: Loss = -12427.824692915094
1
Iteration 15100: Loss = -12427.853392742561
2
Iteration 15200: Loss = -12427.822857726409
3
Iteration 15300: Loss = -12427.840013300669
4
Iteration 15400: Loss = -12427.8248717412
5
Iteration 15500: Loss = -12427.874690793973
6
Iteration 15600: Loss = -12427.833480889743
7
Iteration 15700: Loss = -12427.823555630963
8
Iteration 15800: Loss = -12427.822664800062
Iteration 15900: Loss = -12427.8956645387
1
Iteration 16000: Loss = -12427.827000112766
2
Iteration 16100: Loss = -12427.824370661358
3
Iteration 16200: Loss = -12427.82549302221
4
Iteration 16300: Loss = -12427.823152883893
5
Iteration 16400: Loss = -12427.840151842636
6
Iteration 16500: Loss = -12427.822689251692
7
Iteration 16600: Loss = -12427.8226310144
Iteration 16700: Loss = -12427.844615868182
1
Iteration 16800: Loss = -12427.822597363684
Iteration 16900: Loss = -12427.824201663541
1
Iteration 17000: Loss = -12427.822622051372
2
Iteration 17100: Loss = -12427.848874644065
3
Iteration 17200: Loss = -12427.822655828619
4
Iteration 17300: Loss = -12427.867755235213
5
Iteration 17400: Loss = -12427.82266428288
6
Iteration 17500: Loss = -12427.862689493757
7
Iteration 17600: Loss = -12427.82271743882
8
Iteration 17700: Loss = -12427.8263106174
9
Iteration 17800: Loss = -12427.823403151893
10
Stopping early at iteration 17800 due to no improvement.
tensor([[-5.7338,  4.1844],
        [-6.1457,  3.8135],
        [-5.7594,  4.1553],
        [-5.7564,  4.1918],
        [-5.7376,  4.1903],
        [-6.0623,  3.8815],
        [-7.2906,  2.6754],
        [-5.6626,  4.2739],
        [-5.7195,  4.2464],
        [-5.7040,  4.2389],
        [-6.0309,  3.9331],
        [-5.7264,  4.2430],
        [-5.6504,  4.2625],
        [-6.2405,  3.7424],
        [-6.1638,  3.8569],
        [-5.8207,  4.3925],
        [-5.6895,  4.2992],
        [-5.8706,  4.0397],
        [-6.4756,  3.4682],
        [-5.7274,  4.1940],
        [-5.6446,  4.2572],
        [-5.6760,  4.2853],
        [-5.6849,  4.2513],
        [-5.9783,  3.9780],
        [-6.1061,  3.8664],
        [-7.2283,  2.7296],
        [-5.7273,  4.2181],
        [-5.9563,  4.0328],
        [-5.8499,  4.3265],
        [-5.7100,  4.2180],
        [-5.8598,  4.3640],
        [-7.0551,  3.0898],
        [-5.7084,  4.2469],
        [-5.6691,  4.2127],
        [-6.5888,  3.3605],
        [-7.5138,  2.8986],
        [-6.0043,  3.9219],
        [-7.0482,  3.5981],
        [-6.1876,  3.7751],
        [-5.7282,  4.1872],
        [-5.9451,  4.0673],
        [-5.8132,  4.1421],
        [-6.6955,  4.0963],
        [-5.9320,  3.9945],
        [-5.6816,  4.2449],
        [-5.6628,  4.2350],
        [-5.7178,  4.2926],
        [-6.1243,  3.7989],
        [-6.0281,  4.2953],
        [-5.8077,  4.3794],
        [-6.0049,  3.9236],
        [-7.2232,  4.5501],
        [-5.7152,  4.2012],
        [-5.8336,  4.1108],
        [-5.8753,  4.3355],
        [-5.6856,  4.2363],
        [-5.8819,  4.0495],
        [-5.6758,  4.2775],
        [-5.6672,  4.2664],
        [-5.7987,  4.1302],
        [-6.4631,  3.4729],
        [-5.9331,  3.9682],
        [-5.8784,  4.4770],
        [-5.6614,  4.2620],
        [-5.8459,  4.0975],
        [-5.7055,  4.2167],
        [-6.0597,  3.8377],
        [-5.6892,  4.2484],
        [-5.6802,  4.2751],
        [-6.0744,  3.8629],
        [-5.7517,  4.2393],
        [-5.9127,  4.0340],
        [-5.6580,  4.2699],
        [-5.9599,  4.0504],
        [-6.2196,  3.6992],
        [-5.8556,  4.3740],
        [-5.7057,  4.2650],
        [-7.3131,  2.6979],
        [-5.7026,  4.2079],
        [-5.7693,  4.1739],
        [-6.2993,  3.5822],
        [-5.7313,  4.1838],
        [-6.5242,  3.4475],
        [-5.6742,  4.2651],
        [-6.4794,  3.4545],
        [-5.6713,  4.2828],
        [-7.0719,  2.8554],
        [-5.9324,  3.9973],
        [-5.7242,  4.1934],
        [-6.5802,  3.3894],
        [-5.9767,  3.9601],
        [-6.7888,  3.1983],
        [-6.3576,  3.5839],
        [-5.9914,  3.9693],
        [-6.3846,  3.5733],
        [-6.0007,  3.9745],
        [-5.8751,  4.0473],
        [-6.0593,  3.8237],
        [-5.7198,  4.2151],
        [-5.7160,  4.1956]], dtype=torch.float64, requires_grad=True)
pi: tensor([[7.8242e-05, 9.9992e-01],
        [5.9786e-02, 9.4021e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([4.5920e-05, 9.9995e-01], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2610, 0.1991],
         [0.5444, 0.1981]],

        [[0.0408, 0.2808],
         [0.5763, 0.5183]],

        [[0.2825, 0.1979],
         [0.6970, 0.9284]],

        [[0.2719, 0.2011],
         [0.5453, 0.2279]],

        [[0.5855, 0.2461],
         [0.0444, 0.4932]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 41
Adjusted Rand Index: -0.020678759622205528
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: -0.0026099303253762193
Average Adjusted Rand Index: -0.004863024651713833
11915.982256436346
new:  [-0.0026099303253762193, -0.0021241119770515314, 0.0012963872921693505, -0.0026099303253762193] [-0.004863024651713833, 0.000324944210806727, -0.00365073710224796, -0.004863024651713833] [12427.8290176818, 12429.40046668407, 12428.049395361451, 12427.823403151893]
prior:  [-0.0026099303253762193, 0.0, -0.0026099303253762193, -0.0026099303253762193] [-0.004863024651713833, 0.0, -0.004863024651713833, -0.004863024651713833] [12428.244349545132, 12429.954549885015, 12428.247421881788, 12428.244860755356]
-----------------------------------------------------------------------------------------
This iteration is 2
True Objective function: Loss = -11733.446935995691
Iteration 0: Loss = -19135.50984591725
Iteration 10: Loss = -12276.04544391051
Iteration 20: Loss = -12275.911426336625
Iteration 30: Loss = -12275.815530295356
Iteration 40: Loss = -12275.760359810263
Iteration 50: Loss = -12275.720966629666
Iteration 60: Loss = -12275.61197503486
Iteration 70: Loss = -12275.482315801644
Iteration 80: Loss = -12275.528028639463
1
Iteration 90: Loss = -12275.557405579171
2
Iteration 100: Loss = -12275.568983210453
3
Stopping early at iteration 99 due to no improvement.
pi: tensor([[5.9088e-05, 9.9994e-01],
        [1.2326e-02, 9.8767e-01]], dtype=torch.float64)
alpha: tensor([0.0134, 0.9866])
beta: tensor([[[0.1061, 0.3288],
         [0.0695, 0.1935]],

        [[0.9690, 0.2132],
         [0.4047, 0.3904]],

        [[0.1450, 0.1078],
         [0.3187, 0.4274]],

        [[0.7675, 0.3060],
         [0.3205, 0.0827]],

        [[0.8668, 0.2540],
         [0.6493, 0.4429]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.007760604115428168
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.005453571602473584
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00042302607439050034
Average Adjusted Rand Index: -0.00012796095200498238
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19138.389933637904
Iteration 100: Loss = -12279.635960774029
Iteration 200: Loss = -12278.25393150326
Iteration 300: Loss = -12277.849603303614
Iteration 400: Loss = -12277.65270422845
Iteration 500: Loss = -12277.524921474795
Iteration 600: Loss = -12277.409626067454
Iteration 700: Loss = -12277.197266908064
Iteration 800: Loss = -12276.834129484265
Iteration 900: Loss = -12274.966482110718
Iteration 1000: Loss = -12271.417068190443
Iteration 1100: Loss = -12271.151276616847
Iteration 1200: Loss = -12271.022721784548
Iteration 1300: Loss = -12270.937269588954
Iteration 1400: Loss = -12270.874143710753
Iteration 1500: Loss = -12270.830318306776
Iteration 1600: Loss = -12270.786662864733
Iteration 1700: Loss = -12270.755541329487
Iteration 1800: Loss = -12270.945082934604
1
Iteration 1900: Loss = -12270.706865639064
Iteration 2000: Loss = -12270.684692158458
Iteration 2100: Loss = -12270.66402074704
Iteration 2200: Loss = -12270.652805731554
Iteration 2300: Loss = -12270.622443174792
Iteration 2400: Loss = -12270.596382096253
Iteration 2500: Loss = -12270.866758940796
1
Iteration 2600: Loss = -12270.50104791744
Iteration 2700: Loss = -12270.130930700241
Iteration 2800: Loss = -12269.284627118242
Iteration 2900: Loss = -12269.079544731389
Iteration 3000: Loss = -12268.962043183788
Iteration 3100: Loss = -12268.907314314833
Iteration 3200: Loss = -12268.861120943253
Iteration 3300: Loss = -12268.833119214447
Iteration 3400: Loss = -12268.812508157836
Iteration 3500: Loss = -12268.796516538558
Iteration 3600: Loss = -12268.784024000774
Iteration 3700: Loss = -12268.773633020353
Iteration 3800: Loss = -12268.76508168338
Iteration 3900: Loss = -12268.757977953535
Iteration 4000: Loss = -12268.752108103145
Iteration 4100: Loss = -12268.746818787207
Iteration 4200: Loss = -12268.742427599605
Iteration 4300: Loss = -12268.738580521564
Iteration 4400: Loss = -12268.735183795889
Iteration 4500: Loss = -12268.732243568724
Iteration 4600: Loss = -12268.75986002754
1
Iteration 4700: Loss = -12268.72733830605
Iteration 4800: Loss = -12268.725261852587
Iteration 4900: Loss = -12268.95420720824
1
Iteration 5000: Loss = -12268.721826346655
Iteration 5100: Loss = -12268.720305514445
Iteration 5200: Loss = -12268.722684732276
1
Iteration 5300: Loss = -12268.717770998253
Iteration 5400: Loss = -12268.716620734358
Iteration 5500: Loss = -12268.71561877469
Iteration 5600: Loss = -12268.716362321693
1
Iteration 5700: Loss = -12268.71383750078
Iteration 5800: Loss = -12268.713106584511
Iteration 5900: Loss = -12268.827983688205
1
Iteration 6000: Loss = -12268.711862484246
Iteration 6100: Loss = -12268.711298535014
Iteration 6200: Loss = -12268.710944525283
Iteration 6300: Loss = -12268.710429863148
Iteration 6400: Loss = -12268.70995028269
Iteration 6500: Loss = -12268.709582620084
Iteration 6600: Loss = -12268.709467348941
Iteration 6700: Loss = -12268.708928665204
Iteration 6800: Loss = -12268.708701988306
Iteration 6900: Loss = -12268.708588670572
Iteration 7000: Loss = -12268.708108750981
Iteration 7100: Loss = -12268.707918463415
Iteration 7200: Loss = -12268.717864902741
1
Iteration 7300: Loss = -12268.707545370695
Iteration 7400: Loss = -12268.707394312598
Iteration 7500: Loss = -12268.724878830648
1
Iteration 7600: Loss = -12268.707085758773
Iteration 7700: Loss = -12268.706943614417
Iteration 7800: Loss = -12268.713970191739
1
Iteration 7900: Loss = -12268.706736566595
Iteration 8000: Loss = -12268.706644534008
Iteration 8100: Loss = -12268.706537127897
Iteration 8200: Loss = -12268.706521195189
Iteration 8300: Loss = -12268.706415728333
Iteration 8400: Loss = -12268.706332097114
Iteration 8500: Loss = -12268.707285415672
1
Iteration 8600: Loss = -12268.706221454171
Iteration 8700: Loss = -12268.706139538894
Iteration 8800: Loss = -12268.713142295408
1
Iteration 8900: Loss = -12268.70601213334
Iteration 9000: Loss = -12268.705935531647
Iteration 9100: Loss = -12268.818936237658
1
Iteration 9200: Loss = -12268.7064248265
2
Iteration 9300: Loss = -12268.70580640921
Iteration 9400: Loss = -12268.774406407185
1
Iteration 9500: Loss = -12268.705739655134
Iteration 9600: Loss = -12268.827124377909
1
Iteration 9700: Loss = -12268.705705199023
Iteration 9800: Loss = -12268.70568195123
Iteration 9900: Loss = -12268.70840838515
1
Iteration 10000: Loss = -12268.70566389122
Iteration 10100: Loss = -12268.70558012769
Iteration 10200: Loss = -12268.783824639944
1
Iteration 10300: Loss = -12268.705569714672
Iteration 10400: Loss = -12268.705802847835
1
Iteration 10500: Loss = -12268.705523832716
Iteration 10600: Loss = -12268.707674875372
1
Iteration 10700: Loss = -12268.705487049043
Iteration 10800: Loss = -12268.843692421946
1
Iteration 10900: Loss = -12268.705464945535
Iteration 11000: Loss = -12268.705430195481
Iteration 11100: Loss = -12268.710915725846
1
Iteration 11200: Loss = -12268.705434139205
2
Iteration 11300: Loss = -12268.705365917956
Iteration 11400: Loss = -12268.715227713757
1
Iteration 11500: Loss = -12268.705387721506
2
Iteration 11600: Loss = -12268.705370717867
3
Iteration 11700: Loss = -12268.72079616318
4
Iteration 11800: Loss = -12268.705371620106
5
Iteration 11900: Loss = -12268.907998514773
6
Iteration 12000: Loss = -12268.705315264826
Iteration 12100: Loss = -12268.72304752346
1
Iteration 12200: Loss = -12268.705296241447
Iteration 12300: Loss = -12268.70530117944
1
Iteration 12400: Loss = -12268.70685072116
2
Iteration 12500: Loss = -12268.705282329118
Iteration 12600: Loss = -12268.705289193267
1
Iteration 12700: Loss = -12268.705886147352
2
Iteration 12800: Loss = -12268.705259848532
Iteration 12900: Loss = -12268.70525212848
Iteration 13000: Loss = -12268.705457016404
1
Iteration 13100: Loss = -12268.705244431147
Iteration 13200: Loss = -12268.735982188677
1
Iteration 13300: Loss = -12268.705293232048
2
Iteration 13400: Loss = -12268.71055861879
3
Iteration 13500: Loss = -12268.719834682603
4
Iteration 13600: Loss = -12268.736653084956
5
Iteration 13700: Loss = -12268.70563960742
6
Iteration 13800: Loss = -12268.705360538497
7
Iteration 13900: Loss = -12268.708275594761
8
Iteration 14000: Loss = -12268.836579556624
9
Iteration 14100: Loss = -12268.7059419352
10
Stopping early at iteration 14100 due to no improvement.
tensor([[-11.6199,   7.0046],
        [-12.6444,   8.0292],
        [-10.6739,   6.0586],
        [-10.0146,   5.3994],
        [-10.1508,   5.5356],
        [ -5.7939,   1.1787],
        [-11.7689,   7.1537],
        [-12.2784,   7.6632],
        [-11.1302,   6.5150],
        [-10.6163,   6.0011],
        [-10.1432,   5.5280],
        [-11.8217,   7.2065],
        [-11.8456,   7.2304],
        [-10.7738,   6.1586],
        [-12.6773,   8.0620],
        [-11.9675,   7.3523],
        [-11.5237,   6.9085],
        [-11.9019,   7.2867],
        [-10.2435,   5.6283],
        [-10.6823,   6.0671],
        [-10.7241,   6.1088],
        [-11.6056,   6.9903],
        [-11.9019,   7.2867],
        [-10.7339,   6.1186],
        [-10.4469,   5.8317],
        [-11.4267,   6.8115],
        [-10.5030,   5.8878],
        [-11.7541,   7.1389],
        [-10.3345,   5.7193],
        [-11.8574,   7.2422],
        [-11.9526,   7.3374],
        [-11.2775,   6.6623],
        [-10.3468,   5.7315],
        [-10.9925,   6.3773],
        [-10.3858,   5.7706],
        [-10.3830,   5.7678],
        [-10.8136,   6.1984],
        [ -7.8638,   3.2486],
        [-10.8614,   6.2462],
        [-10.9007,   6.2855],
        [-12.3171,   7.7019],
        [ -9.9166,   5.3014],
        [-10.6893,   6.0740],
        [-11.6750,   7.0598],
        [-11.8998,   7.2845],
        [-10.3051,   5.6899],
        [-10.3287,   5.7135],
        [-10.6221,   6.0069],
        [-10.6246,   6.0094],
        [-10.2069,   5.5917],
        [ -9.7673,   5.1520],
        [-10.1919,   5.5767],
        [-11.3144,   6.6991],
        [-10.6155,   6.0003],
        [-10.4667,   5.8515],
        [  0.5894,  -5.2046],
        [-10.2471,   5.6318],
        [-10.5796,   5.9643],
        [-11.7489,   7.1336],
        [ -9.7827,   5.1675],
        [-11.4960,   6.8808],
        [-11.2332,   6.6180],
        [-10.5485,   5.9332],
        [  0.5382,  -5.1534],
        [-11.4748,   6.8596],
        [-10.5896,   5.9743],
        [-10.4345,   5.8193],
        [-11.8784,   7.2632],
        [-10.7698,   6.1546],
        [-11.3601,   6.7449],
        [-11.8094,   7.1942],
        [-11.5947,   6.9795],
        [-10.2801,   5.6649],
        [ -9.9204,   5.3051],
        [-10.7650,   6.1498],
        [-10.9233,   6.3080],
        [-10.1565,   5.5412],
        [-11.3397,   6.7245],
        [-11.5229,   6.9076],
        [-10.3204,   5.7052],
        [-10.5244,   5.9091],
        [-11.5887,   6.9735],
        [-10.5626,   5.9474],
        [-10.1449,   5.5297],
        [-10.3754,   5.7601],
        [-10.6167,   6.0014],
        [-11.6495,   7.0343],
        [ -5.0985,   0.4832],
        [-10.5346,   5.9194],
        [-10.3956,   5.7804],
        [-12.0691,   7.4539],
        [-11.9676,   7.3524],
        [ -5.7629,   1.1477],
        [-10.9964,   6.3812],
        [-11.0748,   6.4596],
        [-10.9729,   6.3577],
        [-11.4471,   6.8319],
        [-12.0861,   7.4708],
        [-11.5769,   6.9617],
        [-10.1559,   5.5407]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 1.8654e-06],
        [1.7960e-02, 9.8204e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0200, 0.9800], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[3.1940e-04, 3.3509e-01],
         [6.9521e-02, 1.9463e-01]],

        [[9.6896e-01, 2.3297e-01],
         [4.0471e-01, 3.9036e-01]],

        [[1.4497e-01, 2.0992e-01],
         [3.1871e-01, 4.2742e-01]],

        [[7.6750e-01, 1.5348e-01],
         [3.2051e-01, 8.2737e-02]],

        [[8.6678e-01, 2.5322e-01],
         [6.4928e-01, 4.4291e-01]]], dtype=torch.float64,
       grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.007760604115428168
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.008484848484848486
time is 2
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.002508217918958077
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0013258012451002998
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.002918749315918129
Global Adjusted Rand Index: -0.0009329607956105976
Average Adjusted Rand Index: -0.004069323718010513
Iteration 0: Loss = -16553.835838795065
Iteration 10: Loss = -12276.913112821287
Iteration 20: Loss = -12276.865879560692
Iteration 30: Loss = -12276.817756542863
Iteration 40: Loss = -12276.753271899053
Iteration 50: Loss = -12276.670422283476
Iteration 60: Loss = -12276.582227381201
Iteration 70: Loss = -12276.506751531162
Iteration 80: Loss = -12276.446014567902
Iteration 90: Loss = -12276.38859580491
Iteration 100: Loss = -12276.321334653425
Iteration 110: Loss = -12276.234104613322
Iteration 120: Loss = -12276.122898373618
Iteration 130: Loss = -12275.996310143812
Iteration 140: Loss = -12275.877604388472
Iteration 150: Loss = -12275.795328385579
Iteration 160: Loss = -12275.748293403007
Iteration 170: Loss = -12275.704279532732
Iteration 180: Loss = -12275.540193690813
Iteration 190: Loss = -12275.494214590568
Iteration 200: Loss = -12275.539596604107
1
Iteration 210: Loss = -12275.56218410281
2
Iteration 220: Loss = -12275.570674180226
3
Stopping early at iteration 219 due to no improvement.
pi: tensor([[4.1134e-05, 9.9996e-01],
        [1.2284e-02, 9.8772e-01]], dtype=torch.float64)
alpha: tensor([0.0133, 0.9867])
beta: tensor([[[0.1056, 0.3288],
         [0.6835, 0.1935]],

        [[0.9950, 0.2132],
         [0.0947, 0.8229]],

        [[0.7433, 0.1077],
         [0.7391, 0.5013]],

        [[0.4311, 0.3061],
         [0.4941, 0.8204]],

        [[0.9541, 0.2541],
         [0.4590, 0.4068]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: -0.007760604115428168
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.005453571602473584
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00042302607439050034
Average Adjusted Rand Index: -0.00012796095200498238
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -16553.510165417796
Iteration 100: Loss = -12277.26156136862
Iteration 200: Loss = -12276.555840759369
Iteration 300: Loss = -12276.383008826613
Iteration 400: Loss = -12276.232497446317
Iteration 500: Loss = -12276.100242211125
Iteration 600: Loss = -12275.943223894807
Iteration 700: Loss = -12275.730251582187
Iteration 800: Loss = -12275.458052618756
Iteration 900: Loss = -12275.21382371815
Iteration 1000: Loss = -12275.080005263135
Iteration 1100: Loss = -12275.021513099195
Iteration 1200: Loss = -12274.99504959333
Iteration 1300: Loss = -12274.981560001026
Iteration 1400: Loss = -12274.973909908005
Iteration 1500: Loss = -12274.969256752653
Iteration 1600: Loss = -12274.966207348161
Iteration 1700: Loss = -12274.964155985994
Iteration 1800: Loss = -12274.962719479778
Iteration 1900: Loss = -12274.961648702123
Iteration 2000: Loss = -12274.960886864434
Iteration 2100: Loss = -12274.96025442796
Iteration 2200: Loss = -12274.959775725201
Iteration 2300: Loss = -12274.959413816649
Iteration 2400: Loss = -12274.959132989514
Iteration 2500: Loss = -12274.958867750836
Iteration 2600: Loss = -12274.95867376146
Iteration 2700: Loss = -12274.958546281114
Iteration 2800: Loss = -12274.958373909369
Iteration 2900: Loss = -12274.958273862925
Iteration 3000: Loss = -12274.95817434013
Iteration 3100: Loss = -12274.95810884053
Iteration 3200: Loss = -12274.958012091953
Iteration 3300: Loss = -12274.957957638568
Iteration 3400: Loss = -12274.957909857976
Iteration 3500: Loss = -12274.957853759672
Iteration 3600: Loss = -12274.95783702681
Iteration 3700: Loss = -12274.957774869592
Iteration 3800: Loss = -12274.957758417006
Iteration 3900: Loss = -12274.957718511107
Iteration 4000: Loss = -12274.957690053501
Iteration 4100: Loss = -12274.957675554895
Iteration 4200: Loss = -12274.95765045384
Iteration 4300: Loss = -12274.957652647843
1
Iteration 4400: Loss = -12275.027496127404
2
Iteration 4500: Loss = -12274.957595602213
Iteration 4600: Loss = -12274.957592686953
Iteration 4700: Loss = -12274.958811238845
1
Iteration 4800: Loss = -12274.957564521734
Iteration 4900: Loss = -12274.9575933619
1
Iteration 5000: Loss = -12274.957736214761
2
Iteration 5100: Loss = -12274.957564907938
3
Iteration 5200: Loss = -12274.957525916003
Iteration 5300: Loss = -12274.957683666938
1
Iteration 5400: Loss = -12274.957516068647
Iteration 5500: Loss = -12274.957518006044
1
Iteration 5600: Loss = -12275.02364949607
2
Iteration 5700: Loss = -12274.957504677042
Iteration 5800: Loss = -12274.957473025097
Iteration 5900: Loss = -12275.021597243076
1
Iteration 6000: Loss = -12274.957475469824
2
Iteration 6100: Loss = -12274.957445625701
Iteration 6200: Loss = -12274.98211177738
1
Iteration 6300: Loss = -12274.957498560456
2
Iteration 6400: Loss = -12274.957464556202
3
Iteration 6500: Loss = -12274.95746715439
4
Iteration 6600: Loss = -12274.957639314563
5
Iteration 6700: Loss = -12274.957492342055
6
Iteration 6800: Loss = -12274.95746933688
7
Iteration 6900: Loss = -12274.958049863524
8
Iteration 7000: Loss = -12274.957460345551
9
Iteration 7100: Loss = -12274.95745606304
10
Stopping early at iteration 7100 due to no improvement.
tensor([[ 3.0561, -5.0514],
        [ 2.4656, -5.1376],
        [ 4.6879, -6.2816],
        [ 2.3784, -3.8044],
        [ 3.6088, -5.0748],
        [ 4.2333, -5.8055],
        [ 3.8199, -5.2108],
        [ 0.5229, -1.9231],
        [ 3.0139, -4.4023],
        [ 1.6787, -3.6336],
        [ 3.0001, -4.5149],
        [ 4.0261, -5.9878],
        [ 3.7229, -6.0769],
        [ 2.3606, -3.9320],
        [ 5.7425, -7.2029],
        [ 4.4042, -7.4437],
        [ 1.9421, -3.4140],
        [ 3.3964, -5.0502],
        [ 2.7886, -4.5885],
        [ 3.2545, -4.9049],
        [ 1.7280, -3.2220],
        [ 3.9854, -5.3819],
        [ 3.2434, -4.6415],
        [ 1.6925, -3.2093],
        [ 5.4214, -6.8556],
        [ 3.0391, -5.7521],
        [ 3.3584, -7.2833],
        [ 3.5083, -6.3403],
        [ 0.9647, -2.3961],
        [ 5.0922, -6.4998],
        [ 1.6961, -3.3727],
        [ 3.7372, -5.6374],
        [ 3.4951, -5.5636],
        [ 3.0136, -4.4035],
        [ 1.8287, -3.4258],
        [ 1.5472, -2.9587],
        [ 2.4031, -3.8691],
        [ 2.6281, -5.4337],
        [ 1.7694, -3.2114],
        [ 2.4277, -5.0944],
        [ 1.2606, -5.8758],
        [ 1.6948, -4.0138],
        [ 1.7516, -3.1413],
        [ 1.8579, -3.4908],
        [ 4.7621, -6.2449],
        [ 1.1312, -2.5507],
        [ 2.6701, -4.4864],
        [ 4.2901, -6.8169],
        [ 4.7545, -6.2171],
        [ 1.9238, -3.4438],
        [ 3.9037, -6.3930],
        [ 3.0196, -4.4862],
        [ 3.5911, -5.1275],
        [ 3.8936, -5.9410],
        [ 3.4162, -5.0170],
        [-1.5947, -0.5269],
        [ 0.7911, -5.4063],
        [ 3.4981, -5.5478],
        [ 3.0181, -4.7437],
        [ 1.4744, -2.8664],
        [ 4.2625, -5.7598],
        [ 2.4454, -4.5041],
        [ 4.4141, -6.8714],
        [-2.3756, -1.1471],
        [ 1.1397, -2.9043],
        [ 2.8426, -4.9122],
        [ 3.7970, -6.4231],
        [ 2.7183, -4.2807],
        [ 1.6962, -3.1952],
        [ 3.8308, -5.5370],
        [ 2.7667, -5.0734],
        [-0.1073, -4.5079],
        [ 2.0770, -3.5002],
        [ 2.7646, -4.1974],
        [ 2.1153, -3.5112],
        [ 4.6424, -6.3137],
        [ 1.4377, -6.0529],
        [ 3.2011, -4.6344],
        [ 2.1276, -3.7734],
        [ 0.7927, -2.5256],
        [ 3.3882, -5.0354],
        [ 5.2620, -6.7004],
        [ 2.5153, -4.0471],
        [ 3.0382, -4.4271],
        [ 0.3354, -3.4990],
        [ 2.9972, -4.5129],
        [ 2.9165, -4.3445],
        [ 0.7426, -3.5261],
        [ 1.2793, -3.5170],
        [ 4.4994, -6.0301],
        [ 4.4573, -5.8436],
        [ 2.7067, -4.4214],
        [ 4.2963, -5.6930],
        [ 2.3653, -3.7946],
        [ 2.8133, -4.8210],
        [ 2.6344, -4.1084],
        [ 3.0180, -4.9000],
        [ 1.9433, -3.6134],
        [ 1.7682, -4.2326],
        [ 3.2055, -5.5235]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.0780, 0.9220],
        [0.0222, 0.9778]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9810, 0.0190], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1977, 0.3195],
         [0.6835, 0.1965]],

        [[0.9950, 0.2073],
         [0.0947, 0.8229]],

        [[0.7433, 0.1209],
         [0.7391, 0.5013]],

        [[0.4311, 0.1384],
         [0.4941, 0.8204]],

        [[0.9541, 0.2206],
         [0.4590, 0.4068]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.007760604115428168
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0014962626821396297
Average Adjusted Rand Index: -0.0012186752724996992
Iteration 0: Loss = -24057.16077885322
Iteration 10: Loss = -12275.981158373093
Iteration 20: Loss = -12275.848667499544
Iteration 30: Loss = -12275.777978845104
Iteration 40: Loss = -12275.734480329173
Iteration 50: Loss = -12275.668098804388
Iteration 60: Loss = -12275.485709541537
Iteration 70: Loss = -12275.512712384796
1
Iteration 80: Loss = -12275.550343429404
2
Iteration 90: Loss = -12275.566359444867
3
Stopping early at iteration 89 due to no improvement.
pi: tensor([[9.8761e-01, 1.2391e-02],
        [9.9991e-01, 8.6287e-05]], dtype=torch.float64)
alpha: tensor([0.9866, 0.0134])
beta: tensor([[[0.1935, 0.3288],
         [0.3157, 0.1068]],

        [[0.2957, 0.2133],
         [0.1443, 0.0653]],

        [[0.2594, 0.1079],
         [0.4757, 0.9917]],

        [[0.0694, 0.3058],
         [0.8718, 0.9585]],

        [[0.7463, 0.2539],
         [0.6092, 0.4476]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.007760604115428168
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.005453571602473584
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00042302607439050034
Average Adjusted Rand Index: -0.00012796095200498238
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24057.014638412926
Iteration 100: Loss = -12167.359625133387
Iteration 200: Loss = -11950.45909995122
Iteration 300: Loss = -11929.392209679165
Iteration 400: Loss = -11911.134271682733
Iteration 500: Loss = -11907.475943431462
Iteration 600: Loss = -11883.879764949157
Iteration 700: Loss = -11797.567438870974
Iteration 800: Loss = -11779.685703695655
Iteration 900: Loss = -11776.739273505254
Iteration 1000: Loss = -11772.177710425392
Iteration 1100: Loss = -11772.114871444468
Iteration 1200: Loss = -11772.07276974891
Iteration 1300: Loss = -11772.041501931606
Iteration 1400: Loss = -11772.008568231773
Iteration 1500: Loss = -11764.078438043192
Iteration 1600: Loss = -11764.056395655205
Iteration 1700: Loss = -11764.04194102238
Iteration 1800: Loss = -11764.029382599701
Iteration 1900: Loss = -11764.018359965376
Iteration 2000: Loss = -11764.010442564026
Iteration 2100: Loss = -11764.00695481756
Iteration 2200: Loss = -11763.998745208515
Iteration 2300: Loss = -11763.994141865687
Iteration 2400: Loss = -11763.990082370945
Iteration 2500: Loss = -11763.986535159316
Iteration 2600: Loss = -11763.983399388917
Iteration 2700: Loss = -11763.980644689205
Iteration 2800: Loss = -11763.978285598441
Iteration 2900: Loss = -11763.975973398472
Iteration 3000: Loss = -11763.973970716579
Iteration 3100: Loss = -11763.987002850447
1
Iteration 3200: Loss = -11763.9705073522
Iteration 3300: Loss = -11763.969009025397
Iteration 3400: Loss = -11763.967642863703
Iteration 3500: Loss = -11763.966470200077
Iteration 3600: Loss = -11763.965255191528
Iteration 3700: Loss = -11763.964248539452
Iteration 3800: Loss = -11763.964344432978
1
Iteration 3900: Loss = -11763.962388819986
Iteration 4000: Loss = -11763.961595106302
Iteration 4100: Loss = -11763.964898001557
1
Iteration 4200: Loss = -11763.960047118782
Iteration 4300: Loss = -11763.959459921438
Iteration 4400: Loss = -11763.9587977981
Iteration 4500: Loss = -11763.95825695768
Iteration 4600: Loss = -11763.957658079025
Iteration 4700: Loss = -11763.957153026366
Iteration 4800: Loss = -11763.961735842246
1
Iteration 4900: Loss = -11763.956178591809
Iteration 5000: Loss = -11763.955759979906
Iteration 5100: Loss = -11763.956914213588
1
Iteration 5200: Loss = -11763.955023570155
Iteration 5300: Loss = -11763.954679075407
Iteration 5400: Loss = -11763.95467079995
Iteration 5500: Loss = -11763.95406486806
Iteration 5600: Loss = -11763.97186000582
1
Iteration 5700: Loss = -11763.953858220844
Iteration 5800: Loss = -11763.961202600918
1
Iteration 5900: Loss = -11763.953045326618
Iteration 6000: Loss = -11763.972145257685
1
Iteration 6100: Loss = -11763.952616189616
Iteration 6200: Loss = -11763.95430609105
1
Iteration 6300: Loss = -11763.952359990471
Iteration 6400: Loss = -11763.952084268729
Iteration 6500: Loss = -11763.95228511922
1
Iteration 6600: Loss = -11763.952326047729
2
Iteration 6700: Loss = -11763.95646781321
3
Iteration 6800: Loss = -11763.952111241655
4
Iteration 6900: Loss = -11763.956794747071
5
Iteration 7000: Loss = -11763.952154917968
6
Iteration 7100: Loss = -11763.951330944645
Iteration 7200: Loss = -11763.951252073486
Iteration 7300: Loss = -11763.952466970766
1
Iteration 7400: Loss = -11763.953483079893
2
Iteration 7500: Loss = -11763.952169383107
3
Iteration 7600: Loss = -11763.955883935496
4
Iteration 7700: Loss = -11763.952839252286
5
Iteration 7800: Loss = -11763.953653272856
6
Iteration 7900: Loss = -11763.960283483919
7
Iteration 8000: Loss = -11763.951348877084
8
Iteration 8100: Loss = -11763.955643792038
9
Iteration 8200: Loss = -11763.954001024993
10
Stopping early at iteration 8200 due to no improvement.
tensor([[  4.3995,  -7.3243],
        [  5.8256,  -8.2069],
        [ -5.5638,   3.8913],
        [  5.1199,  -9.7351],
        [ -7.7976,   6.2038],
        [ -6.5080,   5.1209],
        [  5.4821,  -6.8704],
        [  6.3884, -10.6138],
        [ -4.0526,   2.6555],
        [ -6.8051,   5.3645],
        [  4.9661,  -6.6448],
        [ -6.4926,   5.1049],
        [  4.9524,  -7.5628],
        [  5.8738,  -7.8362],
        [ -4.7522,   3.3658],
        [ -3.6504,   1.7056],
        [  5.7448,  -7.2370],
        [ -5.6895,   3.8283],
        [ -7.3390,   5.9521],
        [  5.6958,  -8.4154],
        [ -7.2334,   5.6545],
        [ -7.1782,   5.5428],
        [ -6.3265,   4.4155],
        [ -5.8890,   4.4316],
        [ -5.4440,   4.0522],
        [ -3.1265,   1.7157],
        [ -3.6725,   1.6909],
        [  5.1333,  -6.8509],
        [  5.4554,  -8.1816],
        [ -7.7932,   5.5954],
        [  6.4771,  -7.9632],
        [ -4.7778,   3.3085],
        [ -8.7278,   5.2736],
        [ -7.2235,   5.7806],
        [  4.5199,  -9.1351],
        [  2.6231,  -5.0120],
        [  5.0890,  -7.1882],
        [ -6.4586,   4.3058],
        [  5.8586,  -7.3122],
        [  4.2937,  -6.0338],
        [  6.2517,  -7.7737],
        [  6.2871,  -7.6874],
        [  5.5483,  -8.4066],
        [  6.3147,  -7.7072],
        [ -4.0709,   2.6845],
        [ -4.6467,   2.9750],
        [  5.9362,  -7.3368],
        [  5.6429,  -7.7333],
        [ -4.5220,   2.2011],
        [  5.5242,  -9.4058],
        [ -8.3915,   3.7763],
        [ -6.7673,   4.8364],
        [ -7.4061,   5.7430],
        [  5.3957,  -7.0117],
        [ -7.4601,   5.5935],
        [  6.2725,  -7.7363],
        [ -8.9177,   6.1256],
        [  5.2469,  -6.8210],
        [  6.0354,  -7.6369],
        [  5.3861,  -6.8972],
        [ -2.7245,   1.2508],
        [  6.3481,  -8.5348],
        [ -6.4318,   5.0347],
        [  5.2367,  -6.7459],
        [  6.1297,  -7.5164],
        [ -3.8138,   2.4221],
        [  3.4962,  -6.0274],
        [  5.8944,  -7.2939],
        [ -8.1337,   6.0994],
        [  5.9969,  -9.8942],
        [  4.8647,  -6.8516],
        [  4.7589,  -6.8955],
        [  5.1797,  -8.2694],
        [  5.8778,  -7.2911],
        [  2.8153,  -5.2809],
        [ -6.6644,   5.0999],
        [  5.1699,  -6.6254],
        [  5.9187,  -7.4125],
        [  6.4576,  -7.8585],
        [  5.0971,  -8.4666],
        [ -6.6516,   4.9888],
        [ -4.7980,   3.3242],
        [  5.6897, -10.3050],
        [  2.6410,  -4.0817],
        [  5.9647,  -7.3720],
        [  4.5373,  -6.1854],
        [  4.0542,  -5.4975],
        [ -5.4786,   3.5221],
        [  6.2155,  -8.8702],
        [ -6.4265,   4.2797],
        [ -7.4275,   5.6940],
        [ -4.2735,   2.4950],
        [ -7.0538,   5.4356],
        [  5.7406,  -7.1307],
        [  6.2003,  -7.8158],
        [ -4.9464,   3.1968],
        [  4.2179,  -6.5559],
        [  5.1589,  -6.8892],
        [  6.4386,  -7.8450],
        [ -7.4002,   5.1898]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6002, 0.3998],
        [0.3903, 0.6097]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5658, 0.4342], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2984, 0.0966],
         [0.3157, 0.2995]],

        [[0.2957, 0.0890],
         [0.1443, 0.0653]],

        [[0.2594, 0.0998],
         [0.4757, 0.9917]],

        [[0.0694, 0.0893],
         [0.8718, 0.9585]],

        [[0.7463, 0.1010],
         [0.6092, 0.4476]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.349161053296719
Average Adjusted Rand Index: 0.9841616161616162
Iteration 0: Loss = -18007.32403293255
Iteration 10: Loss = -12086.641170896548
Iteration 20: Loss = -11811.050396805625
Iteration 30: Loss = -11764.3150175026
Iteration 40: Loss = -11764.315020029728
1
Iteration 50: Loss = -11764.315020029728
2
Iteration 60: Loss = -11764.315020029728
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.6249, 0.3751],
        [0.4189, 0.5811]], dtype=torch.float64)
alpha: tensor([0.5374, 0.4626])
beta: tensor([[[0.2965, 0.0966],
         [0.6503, 0.2882]],

        [[0.3568, 0.0892],
         [0.8607, 0.7141]],

        [[0.5829, 0.0999],
         [0.6138, 0.0642]],

        [[0.4815, 0.0894],
         [0.4108, 0.0719]],

        [[0.2565, 0.1010],
         [0.4649, 0.0870]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 4
tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.03649415378018524
Average Adjusted Rand Index: 0.9761612713656115
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18007.102175808694
Iteration 100: Loss = -12296.912053871023
Iteration 200: Loss = -12277.306469980576
Iteration 300: Loss = -12276.469309844319
Iteration 400: Loss = -12276.364472596835
Iteration 500: Loss = -12276.224856122384
Iteration 600: Loss = -12275.978850985864
Iteration 700: Loss = -12275.787199740817
Iteration 800: Loss = -12275.692227208083
Iteration 900: Loss = -12275.618402096892
Iteration 1000: Loss = -12275.56078906306
Iteration 1100: Loss = -12275.517694246357
Iteration 1200: Loss = -12275.48647973842
Iteration 1300: Loss = -12275.464022937696
Iteration 1400: Loss = -12275.447703676346
Iteration 1500: Loss = -12275.435544352946
Iteration 1600: Loss = -12275.426340516256
Iteration 1700: Loss = -12275.41901637572
Iteration 1800: Loss = -12275.413010474484
Iteration 1900: Loss = -12275.407850341326
Iteration 2000: Loss = -12275.4032863255
Iteration 2100: Loss = -12275.399011338553
Iteration 2200: Loss = -12275.395025069813
Iteration 2300: Loss = -12275.39113717328
Iteration 2400: Loss = -12275.387230629665
Iteration 2500: Loss = -12275.383146278646
Iteration 2600: Loss = -12275.378868199883
Iteration 2700: Loss = -12275.373927076867
Iteration 2800: Loss = -12275.36811227119
Iteration 2900: Loss = -12275.360398688961
Iteration 3000: Loss = -12275.348933265019
Iteration 3100: Loss = -12275.328567539249
Iteration 3200: Loss = -12275.283325002822
Iteration 3300: Loss = -12275.175053517582
Iteration 3400: Loss = -12273.333887475346
Iteration 3500: Loss = -12271.781790022458
Iteration 3600: Loss = -12271.759964050794
Iteration 3700: Loss = -12271.71473051169
Iteration 3800: Loss = -12271.705371783679
Iteration 3900: Loss = -12271.699247046829
Iteration 4000: Loss = -12271.704885207728
1
Iteration 4100: Loss = -12271.69158706913
Iteration 4200: Loss = -12271.689046625119
Iteration 4300: Loss = -12271.686919434009
Iteration 4400: Loss = -12271.685224695622
Iteration 4500: Loss = -12271.683446441482
Iteration 4600: Loss = -12271.681935455803
Iteration 4700: Loss = -12271.755537037474
1
Iteration 4800: Loss = -12271.679241239171
Iteration 4900: Loss = -12271.677932844002
Iteration 5000: Loss = -12271.676586339789
Iteration 5100: Loss = -12271.675322891524
Iteration 5200: Loss = -12271.673959682636
Iteration 5300: Loss = -12271.672656569519
Iteration 5400: Loss = -12271.672351647143
Iteration 5500: Loss = -12271.669923990576
Iteration 5600: Loss = -12271.668521904016
Iteration 5700: Loss = -12271.667215823636
Iteration 5800: Loss = -12271.666801257788
Iteration 5900: Loss = -12271.66463744221
Iteration 6000: Loss = -12271.663415522578
Iteration 6100: Loss = -12271.662315088723
Iteration 6200: Loss = -12271.661195813824
Iteration 6300: Loss = -12271.660156540724
Iteration 6400: Loss = -12271.659222294562
Iteration 6500: Loss = -12271.658477363506
Iteration 6600: Loss = -12271.657568295373
Iteration 6700: Loss = -12271.656826693026
Iteration 6800: Loss = -12271.670134403326
1
Iteration 6900: Loss = -12271.65551177676
Iteration 7000: Loss = -12271.654918644952
Iteration 7100: Loss = -12271.672750196132
1
Iteration 7200: Loss = -12271.653919181183
Iteration 7300: Loss = -12271.653451985658
Iteration 7400: Loss = -12271.653082735325
Iteration 7500: Loss = -12271.654639113116
1
Iteration 7600: Loss = -12271.652371011192
Iteration 7700: Loss = -12271.652039359593
Iteration 7800: Loss = -12271.6517463159
Iteration 7900: Loss = -12271.651529292867
Iteration 8000: Loss = -12271.65129040594
Iteration 8100: Loss = -12271.651102195432
Iteration 8200: Loss = -12271.651538001004
1
Iteration 8300: Loss = -12271.650787906343
Iteration 8400: Loss = -12271.65061567092
Iteration 8500: Loss = -12271.650465780349
Iteration 8600: Loss = -12271.652069684907
1
Iteration 8700: Loss = -12271.650205115397
Iteration 8800: Loss = -12271.65012306623
Iteration 8900: Loss = -12271.694660368967
1
Iteration 9000: Loss = -12271.649960485192
Iteration 9100: Loss = -12271.649866168125
Iteration 9200: Loss = -12271.649835462966
Iteration 9300: Loss = -12271.649776486283
Iteration 9400: Loss = -12271.649714343412
Iteration 9500: Loss = -12271.649677174431
Iteration 9600: Loss = -12271.651831064337
1
Iteration 9700: Loss = -12271.649580293653
Iteration 9800: Loss = -12271.649559924426
Iteration 9900: Loss = -12271.649533831345
Iteration 10000: Loss = -12271.654650448521
1
Iteration 10100: Loss = -12271.649513236502
Iteration 10200: Loss = -12271.6494870603
Iteration 10300: Loss = -12271.649475078959
Iteration 10400: Loss = -12271.650950870477
1
Iteration 10500: Loss = -12271.649418975527
Iteration 10600: Loss = -12271.649421759215
1
Iteration 10700: Loss = -12272.335274152229
2
Iteration 10800: Loss = -12271.649405037506
Iteration 10900: Loss = -12271.649398128473
Iteration 11000: Loss = -12271.649382999178
Iteration 11100: Loss = -12271.660774468084
1
Iteration 11200: Loss = -12271.649350143789
Iteration 11300: Loss = -12271.649368074935
1
Iteration 11400: Loss = -12271.649340224765
Iteration 11500: Loss = -12271.649388496846
1
Iteration 11600: Loss = -12271.649336432665
Iteration 11700: Loss = -12271.64933754397
1
Iteration 11800: Loss = -12271.712365766389
2
Iteration 11900: Loss = -12271.649307345528
Iteration 12000: Loss = -12271.64948501992
1
Iteration 12100: Loss = -12271.649309495226
2
Iteration 12200: Loss = -12271.652144169908
3
Iteration 12300: Loss = -12271.649324825341
4
Iteration 12400: Loss = -12271.649355942893
5
Iteration 12500: Loss = -12271.65091123678
6
Iteration 12600: Loss = -12271.649355179874
7
Iteration 12700: Loss = -12271.64937318452
8
Iteration 12800: Loss = -12271.682321445047
9
Iteration 12900: Loss = -12271.649387577869
10
Stopping early at iteration 12900 due to no improvement.
tensor([[ 4.4726, -5.8762],
        [ 6.7759, -8.6504],
        [ 6.5130, -7.9125],
        [ 3.4361, -4.8918],
        [ 4.2998, -6.6139],
        [ 2.6580, -4.7414],
        [ 6.3517, -7.7818],
        [ 4.2968, -5.7750],
        [ 3.7968, -5.5016],
        [ 3.9306, -5.6689],
        [ 3.8513, -5.8616],
        [ 4.2834, -7.2865],
        [ 4.1851, -7.0122],
        [ 5.7925, -7.8801],
        [ 6.2816, -8.3529],
        [ 6.5420, -7.9337],
        [ 4.2910, -5.6806],
        [ 4.3891, -8.2386],
        [ 3.6948, -5.8981],
        [ 5.3610, -7.8800],
        [ 2.7534, -4.3189],
        [ 5.2805, -6.7160],
        [ 5.7781, -7.7922],
        [ 1.5909, -3.4976],
        [ 6.2993, -7.6856],
        [ 4.3326, -5.7387],
        [ 5.0783, -7.5917],
        [ 5.8903, -7.5193],
        [ 2.6055, -6.2370],
        [ 6.5860, -8.5300],
        [ 5.4495, -6.8959],
        [ 4.2317, -6.2094],
        [ 6.0829, -7.5668],
        [ 2.1266, -3.6122],
        [ 3.5326, -4.9992],
        [ 4.3500, -6.0018],
        [ 5.5615, -8.5693],
        [ 3.8557, -5.5483],
        [ 5.5454, -7.1933],
        [ 3.2784, -6.2376],
        [ 4.7574, -6.2669],
        [ 2.8847, -4.2822],
        [ 2.2230, -4.8178],
        [ 4.2146, -6.8959],
        [ 6.1515, -7.6045],
        [ 1.3597, -3.1932],
        [ 5.4963, -6.9095],
        [ 5.9943, -7.9276],
        [ 6.2896, -7.6981],
        [ 4.7586, -6.2516],
        [ 6.4482, -7.8925],
        [ 3.2900, -5.6453],
        [ 4.1451, -5.8205],
        [ 6.2649, -8.0581],
        [ 5.6492, -7.0407],
        [-4.2316,  2.6739],
        [ 2.8432, -4.2351],
        [ 6.0826, -7.4689],
        [ 4.9345, -7.0438],
        [ 0.9217, -3.7013],
        [ 3.5341, -6.8385],
        [ 3.4560, -5.0674],
        [ 5.4969, -6.9147],
        [-1.0467, -0.4370],
        [ 3.7147, -5.6584],
        [ 5.2288, -8.0891],
        [ 5.5274, -6.9656],
        [ 5.7311, -8.8451],
        [ 1.1588, -2.6725],
        [ 6.7766, -8.3003],
        [ 6.0492, -7.4590],
        [ 4.8698, -6.2696],
        [ 3.0949, -4.5125],
        [ 3.3521, -5.5550],
        [ 2.9527, -4.6742],
        [ 5.6199, -8.0409],
        [ 3.9016, -5.8929],
        [ 5.5172, -7.3055],
        [ 3.0311, -4.4718],
        [ 2.8826, -6.0795],
        [ 5.5270, -7.2179],
        [ 6.1973, -9.0920],
        [ 5.3102, -6.6996],
        [ 3.7837, -5.2823],
        [ 2.0552, -3.4429],
        [ 6.4180, -7.8195],
        [ 4.6892, -6.2037],
        [ 1.5764, -3.6412],
        [ 2.6177, -4.6969],
        [ 4.1705, -7.7829],
        [ 6.1670, -7.5812],
        [ 5.7442, -7.1343],
        [ 3.9178, -6.6221],
        [ 3.0826, -5.1756],
        [ 4.0782, -5.5265],
        [ 3.3164, -5.5546],
        [ 6.0849, -7.6776],
        [ 5.8117, -7.6249],
        [ 3.4702, -5.5043],
        [ 4.0093, -6.4796]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.9977e-01, 2.3416e-04],
        [1.6257e-01, 8.3743e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9828, 0.0172], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1964, 0.3323],
         [0.6503, 0.0072]],

        [[0.3568, 0.2351],
         [0.8607, 0.7141]],

        [[0.5829, 0.1151],
         [0.6138, 0.0642]],

        [[0.4815, 0.1129],
         [0.4108, 0.0719]],

        [[0.2565, 0.2129],
         [0.4649, 0.0870]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.007760604115428168
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.005453571602473584
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0006983710272640102
Average Adjusted Rand Index: -0.0014964751227121986
Iteration 0: Loss = -24601.44778240957
Iteration 10: Loss = -12277.768691475925
Iteration 20: Loss = -12277.768691476129
1
Iteration 30: Loss = -12277.76869148968
2
Iteration 40: Loss = -12277.768692371732
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[2.0619e-22, 1.0000e+00],
        [2.0585e-11, 1.0000e+00]], dtype=torch.float64)
alpha: tensor([3.1050e-11, 1.0000e+00])
beta: tensor([[[1.4360e-01, 3.2388e-01],
         [8.8477e-01, 1.9492e-01]],

        [[2.3001e-01, 2.1760e-01],
         [5.9095e-01, 4.2412e-01]],

        [[8.3649e-01, 8.9127e-02],
         [5.1857e-01, 6.1986e-01]],

        [[1.4297e-01, 3.2047e-01],
         [3.6882e-01, 9.0956e-04]],

        [[3.8885e-01, 2.5401e-01],
         [9.1487e-01, 1.4845e-01]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24602.30527177176
Iteration 100: Loss = -12298.235690438269
Iteration 200: Loss = -12280.275099703102
Iteration 300: Loss = -12279.040540291115
Iteration 400: Loss = -12278.368413130123
Iteration 500: Loss = -12277.876636305658
Iteration 600: Loss = -12276.968134422343
Iteration 700: Loss = -12276.522143306951
Iteration 800: Loss = -12275.695497957813
Iteration 900: Loss = -12275.423536232505
Iteration 1000: Loss = -12275.26367726568
Iteration 1100: Loss = -12274.88492508245
Iteration 1200: Loss = -12249.434794688736
Iteration 1300: Loss = -12225.276213217743
Iteration 1400: Loss = -12186.600308758332
Iteration 1500: Loss = -12182.474842238202
Iteration 1600: Loss = -12182.282752293242
Iteration 1700: Loss = -12182.177211548622
Iteration 1800: Loss = -12182.102096092387
Iteration 1900: Loss = -12182.042604885664
Iteration 2000: Loss = -12181.978545987606
Iteration 2100: Loss = -12181.910283043486
Iteration 2200: Loss = -12181.82339001374
Iteration 2300: Loss = -12181.720039470727
Iteration 2400: Loss = -12181.628478713468
Iteration 2500: Loss = -12181.562489145514
Iteration 2600: Loss = -12181.516051506696
Iteration 2700: Loss = -12181.482083777502
Iteration 2800: Loss = -12181.456152913703
Iteration 2900: Loss = -12181.435685835444
Iteration 3000: Loss = -12181.419092809107
Iteration 3100: Loss = -12181.406858681195
Iteration 3200: Loss = -12181.3937801552
Iteration 3300: Loss = -12181.383883389184
Iteration 3400: Loss = -12181.381021164218
Iteration 3500: Loss = -12181.368004503713
Iteration 3600: Loss = -12181.36152115068
Iteration 3700: Loss = -12181.359333731672
Iteration 3800: Loss = -12181.350792894085
Iteration 3900: Loss = -12181.346122504885
Iteration 4000: Loss = -12181.34202153845
Iteration 4100: Loss = -12181.364995507553
1
Iteration 4200: Loss = -12181.334928223234
Iteration 4300: Loss = -12181.33188509015
Iteration 4400: Loss = -12181.401291384573
1
Iteration 4500: Loss = -12181.326564216244
Iteration 4600: Loss = -12181.324178695872
Iteration 4700: Loss = -12181.322065959697
Iteration 4800: Loss = -12181.321239112189
Iteration 4900: Loss = -12181.318271723567
Iteration 5000: Loss = -12181.3165728314
Iteration 5100: Loss = -12181.332453348985
1
Iteration 5200: Loss = -12181.313593594721
Iteration 5300: Loss = -12181.312284353915
Iteration 5400: Loss = -12181.311030437837
Iteration 5500: Loss = -12181.316840991696
1
Iteration 5600: Loss = -12181.30883847541
Iteration 5700: Loss = -12181.307855148954
Iteration 5800: Loss = -12181.30995066562
1
Iteration 5900: Loss = -12181.306065018081
Iteration 6000: Loss = -12181.305240461656
Iteration 6100: Loss = -12181.307468994972
1
Iteration 6200: Loss = -12181.304250972447
Iteration 6300: Loss = -12181.303170890711
Iteration 6400: Loss = -12181.302503429855
Iteration 6500: Loss = -12181.30313181251
1
Iteration 6600: Loss = -12181.30143579562
Iteration 6700: Loss = -12181.300895332108
Iteration 6800: Loss = -12181.434513967779
1
Iteration 6900: Loss = -12181.299957456984
Iteration 7000: Loss = -12181.299534748505
Iteration 7100: Loss = -12181.299172972545
Iteration 7200: Loss = -12181.298955030177
Iteration 7300: Loss = -12181.298399350664
Iteration 7400: Loss = -12181.298055027473
Iteration 7500: Loss = -12181.29789075675
Iteration 7600: Loss = -12181.297449460733
Iteration 7700: Loss = -12181.297179636982
Iteration 7800: Loss = -12181.296917339949
Iteration 7900: Loss = -12181.298910686526
1
Iteration 8000: Loss = -12181.296451345303
Iteration 8100: Loss = -12181.299980787382
1
Iteration 8200: Loss = -12181.296441192866
Iteration 8300: Loss = -12181.295788176283
Iteration 8400: Loss = -12181.300232096506
1
Iteration 8500: Loss = -12181.29544204007
Iteration 8600: Loss = -12181.296186366053
1
Iteration 8700: Loss = -12181.29507421798
Iteration 8800: Loss = -12181.294944845704
Iteration 8900: Loss = -12181.352091984196
1
Iteration 9000: Loss = -12181.294673081855
Iteration 9100: Loss = -12181.294548931699
Iteration 9200: Loss = -12181.58909472884
1
Iteration 9300: Loss = -12181.294301623115
Iteration 9400: Loss = -12181.294332571493
1
Iteration 9500: Loss = -12181.294124092043
Iteration 9600: Loss = -12181.294033752421
Iteration 9700: Loss = -12181.294242362563
1
Iteration 9800: Loss = -12181.293776009803
Iteration 9900: Loss = -12181.295601573232
1
Iteration 10000: Loss = -12181.293636721457
Iteration 10100: Loss = -12181.308355921088
1
Iteration 10200: Loss = -12181.293484592246
Iteration 10300: Loss = -12181.32314541646
1
Iteration 10400: Loss = -12181.302541587069
2
Iteration 10500: Loss = -12181.293248935423
Iteration 10600: Loss = -12181.701850746455
1
Iteration 10700: Loss = -12181.293172487309
Iteration 10800: Loss = -12181.293093722348
Iteration 10900: Loss = -12181.300446326395
1
Iteration 11000: Loss = -12181.293098950986
2
Iteration 11100: Loss = -12181.292954743247
Iteration 11200: Loss = -12181.31315745113
1
Iteration 11300: Loss = -12181.292874248002
Iteration 11400: Loss = -12181.29290905691
1
Iteration 11500: Loss = -12181.294032376234
2
Iteration 11600: Loss = -12181.29288666666
3
Iteration 11700: Loss = -12181.296030849177
4
Iteration 11800: Loss = -12181.294084281857
5
Iteration 11900: Loss = -12181.29265545411
Iteration 12000: Loss = -12181.293030980702
1
Iteration 12100: Loss = -12181.292580268559
Iteration 12200: Loss = -12181.317243735308
1
Iteration 12300: Loss = -12181.292542065292
Iteration 12400: Loss = -12181.293219987921
1
Iteration 12500: Loss = -12181.299915798609
2
Iteration 12600: Loss = -12181.292579868674
3
Iteration 12700: Loss = -12181.440964916683
4
Iteration 12800: Loss = -12181.294211732973
5
Iteration 12900: Loss = -12181.451067841394
6
Iteration 13000: Loss = -12181.29259762199
7
Iteration 13100: Loss = -12181.293492442834
8
Iteration 13200: Loss = -12181.296816175538
9
Iteration 13300: Loss = -12181.292422052897
Iteration 13400: Loss = -12181.292340894337
Iteration 13500: Loss = -12181.293844296728
1
Iteration 13600: Loss = -12181.292509832523
2
Iteration 13700: Loss = -12181.292567649461
3
Iteration 13800: Loss = -12181.299614552432
4
Iteration 13900: Loss = -12181.292375176225
5
Iteration 14000: Loss = -12181.293388569151
6
Iteration 14100: Loss = -12181.2924267787
7
Iteration 14200: Loss = -12181.293875436166
8
Iteration 14300: Loss = -12181.292265982967
Iteration 14400: Loss = -12181.29227934586
1
Iteration 14500: Loss = -12181.315536393942
2
Iteration 14600: Loss = -12181.29224253248
Iteration 14700: Loss = -12181.302813176275
1
Iteration 14800: Loss = -12181.292220039313
Iteration 14900: Loss = -12181.316737429219
1
Iteration 15000: Loss = -12181.309627954492
2
Iteration 15100: Loss = -12181.29888114631
3
Iteration 15200: Loss = -12181.303838169159
4
Iteration 15300: Loss = -12181.292679991591
5
Iteration 15400: Loss = -12181.292931727516
6
Iteration 15500: Loss = -12181.505018422476
7
Iteration 15600: Loss = -12181.287899964846
Iteration 15700: Loss = -12181.28925573109
1
Iteration 15800: Loss = -12181.29814052735
2
Iteration 15900: Loss = -12181.287787002744
Iteration 16000: Loss = -12181.286467182155
Iteration 16100: Loss = -12181.287257379907
1
Iteration 16200: Loss = -12181.28683237463
2
Iteration 16300: Loss = -12181.407955604327
3
Iteration 16400: Loss = -12181.286604349549
4
Iteration 16500: Loss = -12181.292395019544
5
Iteration 16600: Loss = -12181.287398111544
6
Iteration 16700: Loss = -12181.306608783538
7
Iteration 16800: Loss = -12181.286155701158
Iteration 16900: Loss = -12181.286893599114
1
Iteration 17000: Loss = -12181.291537951287
2
Iteration 17100: Loss = -12181.289592049427
3
Iteration 17200: Loss = -12181.428760573745
4
Iteration 17300: Loss = -12181.288008508569
5
Iteration 17400: Loss = -12181.291778588522
6
Iteration 17500: Loss = -12181.286747078457
7
Iteration 17600: Loss = -12181.472880271162
8
Iteration 17700: Loss = -12181.299216557914
9
Iteration 17800: Loss = -12181.291692505942
10
Stopping early at iteration 17800 due to no improvement.
tensor([[  2.5139,  -4.6728],
        [  5.9117,  -7.3014],
        [ -6.7659,   5.1152],
        [  4.9603,  -6.7541],
        [ -9.0749,   7.6885],
        [ -7.8384,   6.3681],
        [  3.6550,  -5.8817],
        [  8.1871,  -9.5960],
        [ -4.9164,   3.2577],
        [ -8.5230,   3.9078],
        [  3.3133,  -5.3288],
        [ -8.1959,   6.0135],
        [  2.8823,  -7.4975],
        [  6.1679,  -7.6752],
        [ -7.1199,   5.2471],
        [ -5.3542,   3.9674],
        [  6.1663,  -7.6585],
        [ -5.9655,   4.5489],
        [ -8.8825,   7.1466],
        [  2.5513,  -4.4138],
        [ -9.4420,   6.7618],
        [ -9.8247,   5.2095],
        [ -6.5184,   4.8466],
        [ -6.0565,   3.1466],
        [ -8.5043,   5.4757],
        [ -3.9883,   2.5158],
        [ -5.1382,   3.5288],
        [  3.1076,  -4.8835],
        [  7.4996,  -8.8868],
        [ -8.6198,   7.1992],
        [  8.3333,  -9.9917],
        [ -5.8820,   4.4742],
        [ -7.9709,   6.3664],
        [ -8.6547,   7.2404],
        [  4.7367,  -6.7946],
        [  2.2549,  -3.7292],
        [  4.4430,  -7.0857],
        [ -6.8141,   5.2286],
        [  5.9888,  -8.5431],
        [  2.4608,  -3.8804],
        [  6.9428,  -8.6536],
        [  7.7481,  -9.4323],
        [  5.3789,  -6.9501],
        [  7.5982, -10.1346],
        [ -7.0825,   2.4673],
        [ -4.1344,   2.1139],
        [  6.3032,  -7.6974],
        [  4.1277,  -5.5293],
        [ -7.0588,   2.4436],
        [  7.0763,  -8.4819],
        [ -8.1584,   6.6005],
        [ -8.6327,   4.2830],
        [ -9.1529,   7.7031],
        [  3.9129,  -6.4675],
        [ -8.7568,   7.0466],
        [  7.9369,  -9.5691],
        [ -8.7375,   7.0674],
        [  3.3733,  -6.1243],
        [  7.2023,  -8.7597],
        [  4.8526,  -6.6380],
        [ -4.2857,   2.8776],
        [  4.3066,  -5.8825],
        [ -7.9620,   6.5437],
        [  5.7960,  -7.5448],
        [  6.6445,  -9.0426],
        [ -4.0458,   2.6430],
        [  1.8106,  -3.2176],
        [  6.4212,  -8.2221],
        [ -8.9915,   6.9560],
        [  5.0518,  -8.3528],
        [  2.6314,  -5.2301],
        [  4.9159,  -6.4388],
        [  3.7247,  -7.1435],
        [  5.1071,  -7.4506],
        [  1.8455,  -4.2793],
        [ -7.8290,   6.3639],
        [  3.6174,  -5.0865],
        [  5.4392,  -7.1086],
        [  7.4424, -10.3232],
        [  7.1180,  -8.6815],
        [ -7.3553,   5.5503],
        [ -6.9250,   5.4361],
        [  5.7162,  -7.4597],
        [  1.2989,  -2.6879],
        [  6.7506,  -8.3736],
        [  2.4652,  -6.0585],
        [  1.6142,  -5.4018],
        [ -4.5852,   3.1063],
        [  7.4225,  -8.8089],
        [ -7.4012,   5.9685],
        [ -9.0915,   7.6460],
        [ -4.4940,   3.0164],
        [ -9.0779,   7.6718],
        [  5.1374,  -6.5724],
        [  5.9587,  -9.5046],
        [ -5.4894,   3.5293],
        [  2.9877,  -4.8579],
        [  4.2202,  -6.4632],
        [  8.0010, -10.1564],
        [ -9.3901,   6.5456]], dtype=torch.float64, requires_grad=True)
pi: tensor([[4.0543e-06, 1.0000e+00],
        [1.0578e-02, 9.8942e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5595, 0.4405], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[3.2578e-01, 9.6866e-02],
         [8.8477e-01, 1.9902e-01]],

        [[2.3001e-01, 2.1083e-01],
         [5.9095e-01, 4.2412e-01]],

        [[8.3649e-01, 1.0234e-01],
         [5.1857e-01, 6.1986e-01]],

        [[1.4297e-01, 3.0142e-01],
         [3.6882e-01, 9.0956e-04]],

        [[3.8885e-01, 2.0632e-01],
         [9.1487e-01, 1.4845e-01]]], dtype=torch.float64,
       grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.005453571602473584
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.03717995708285488
Average Adjusted Rand Index: 0.20142415987108064
11733.446935995691
new:  [-0.0014962626821396297, 0.349161053296719, -0.0006983710272640102, 0.03717995708285488] [-0.0012186752724996992, 0.9841616161616162, -0.0014964751227121986, 0.20142415987108064] [12274.95745606304, 11763.954001024993, 12271.649387577869, 12181.291692505942]
prior:  [-0.00042302607439050034, -0.00042302607439050034, 0.03649415378018524, 0.0] [-0.00012796095200498238, -0.00012796095200498238, 0.9761612713656115, 0.0] [12275.570674180226, 12275.566359444867, 11764.315020029728, 12277.768692371732]
-----------------------------------------------------------------------------------------
This iteration is 3
True Objective function: Loss = -11862.235998495691
Iteration 0: Loss = -25673.808102246257
Iteration 10: Loss = -12087.326795259993
Iteration 20: Loss = -11891.506150159592
Iteration 30: Loss = -11891.505544915648
Iteration 40: Loss = -11891.505540899476
Iteration 50: Loss = -11891.505538764708
Iteration 60: Loss = -11891.505538764708
1
Iteration 70: Loss = -11891.505538764708
2
Iteration 80: Loss = -11891.505538764708
3
Stopping early at iteration 79 due to no improvement.
pi: tensor([[0.6068, 0.3932],
        [0.3556, 0.6444]], dtype=torch.float64)
alpha: tensor([0.4619, 0.5381])
beta: tensor([[[0.2927, 0.1011],
         [0.3541, 0.2966]],

        [[0.0599, 0.0964],
         [0.9163, 0.0556]],

        [[0.5697, 0.1043],
         [0.8326, 0.2934]],

        [[0.3253, 0.1017],
         [0.2333, 0.1929]],

        [[0.9462, 0.0909],
         [0.0521, 0.9265]]], dtype=torch.float64)
time is 0
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 1
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.03806238180181878
Average Adjusted Rand Index: 0.9839995611635629
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25653.53570932125
Iteration 100: Loss = -12394.38004544633
Iteration 200: Loss = -12389.764683326717
Iteration 300: Loss = -12388.210361322757
Iteration 400: Loss = -12387.416628441459
Iteration 500: Loss = -12386.999849420878
Iteration 600: Loss = -12386.811188963573
Iteration 700: Loss = -12386.711642936783
Iteration 800: Loss = -12386.64955074521
Iteration 900: Loss = -12386.605267161563
Iteration 1000: Loss = -12386.571343571266
Iteration 1100: Loss = -12386.544363225881
Iteration 1200: Loss = -12386.522115134527
Iteration 1300: Loss = -12386.503416204885
Iteration 1400: Loss = -12386.487429569588
Iteration 1500: Loss = -12386.473641530261
Iteration 1600: Loss = -12386.46148597941
Iteration 1700: Loss = -12386.450777547945
Iteration 1800: Loss = -12386.441145362418
Iteration 1900: Loss = -12386.432342360025
Iteration 2000: Loss = -12386.424255418837
Iteration 2100: Loss = -12386.416471286833
Iteration 2200: Loss = -12386.408863516224
Iteration 2300: Loss = -12386.400789968977
Iteration 2400: Loss = -12386.391526004627
Iteration 2500: Loss = -12386.380590502355
Iteration 2600: Loss = -12386.369356896936
Iteration 2700: Loss = -12386.358993599088
Iteration 2800: Loss = -12386.348117501571
Iteration 2900: Loss = -12386.33350022375
Iteration 3000: Loss = -12386.298254420588
Iteration 3100: Loss = -12386.238079600678
Iteration 3200: Loss = -12386.202069189108
Iteration 3300: Loss = -12386.189357384794
Iteration 3400: Loss = -12386.182020182976
Iteration 3500: Loss = -12386.1752909438
Iteration 3600: Loss = -12386.168900638835
Iteration 3700: Loss = -12386.164521512812
Iteration 3800: Loss = -12386.161294983951
Iteration 3900: Loss = -12386.158460027857
Iteration 4000: Loss = -12386.155926126841
Iteration 4100: Loss = -12386.153625638206
Iteration 4200: Loss = -12386.151523634697
Iteration 4300: Loss = -12386.149636244678
Iteration 4400: Loss = -12386.147901060793
Iteration 4500: Loss = -12386.146210652749
Iteration 4600: Loss = -12386.144604071864
Iteration 4700: Loss = -12386.142956979265
Iteration 4800: Loss = -12386.14110194514
Iteration 4900: Loss = -12386.138685030932
Iteration 5000: Loss = -12386.134269705277
Iteration 5100: Loss = -12386.118450613078
Iteration 5200: Loss = -12385.907820738003
Iteration 5300: Loss = -12385.808839946785
Iteration 5400: Loss = -12385.735472149192
Iteration 5500: Loss = -12384.182919322191
Iteration 5600: Loss = -12383.902538322738
Iteration 5700: Loss = -12383.86952177112
Iteration 5800: Loss = -12383.854857695293
Iteration 5900: Loss = -12383.846301646905
Iteration 6000: Loss = -12383.840400201756
Iteration 6100: Loss = -12383.836203871742
Iteration 6200: Loss = -12383.832941602346
Iteration 6300: Loss = -12383.830328842043
Iteration 6400: Loss = -12383.828146474601
Iteration 6500: Loss = -12383.826397864927
Iteration 6600: Loss = -12383.824859628141
Iteration 6700: Loss = -12383.82351938517
Iteration 6800: Loss = -12383.822358237681
Iteration 6900: Loss = -12383.821320232917
Iteration 7000: Loss = -12383.820415388396
Iteration 7100: Loss = -12383.819589820188
Iteration 7200: Loss = -12383.818825983273
Iteration 7300: Loss = -12383.818202612943
Iteration 7400: Loss = -12383.817616367629
Iteration 7500: Loss = -12383.817118512272
Iteration 7600: Loss = -12383.816661451834
Iteration 7700: Loss = -12383.816176405577
Iteration 7800: Loss = -12383.815828316505
Iteration 7900: Loss = -12383.815478051782
Iteration 8000: Loss = -12383.815139982456
Iteration 8100: Loss = -12383.814785188923
Iteration 8200: Loss = -12383.814568106343
Iteration 8300: Loss = -12383.814257108277
Iteration 8400: Loss = -12383.814032898068
Iteration 8500: Loss = -12383.813920848683
Iteration 8600: Loss = -12383.813594644886
Iteration 8700: Loss = -12383.813960475658
1
Iteration 8800: Loss = -12383.813164606396
Iteration 8900: Loss = -12383.83366675738
1
Iteration 9000: Loss = -12383.812889122739
Iteration 9100: Loss = -12383.81274343505
Iteration 9200: Loss = -12383.840290372442
1
Iteration 9300: Loss = -12383.812444765575
Iteration 9400: Loss = -12383.812355159236
Iteration 9500: Loss = -12383.978120957227
1
Iteration 9600: Loss = -12383.81213031244
Iteration 9700: Loss = -12383.812031773385
Iteration 9800: Loss = -12383.81195992241
Iteration 9900: Loss = -12383.811923309135
Iteration 10000: Loss = -12383.811755920307
Iteration 10100: Loss = -12383.81190533449
1
Iteration 10200: Loss = -12383.811623731075
Iteration 10300: Loss = -12383.81164974954
1
Iteration 10400: Loss = -12383.811490417009
Iteration 10500: Loss = -12383.813854719494
1
Iteration 10600: Loss = -12383.811377020718
Iteration 10700: Loss = -12383.811344658849
Iteration 10800: Loss = -12383.811526919051
1
Iteration 10900: Loss = -12383.812032083904
2
Iteration 11000: Loss = -12383.812542404212
3
Iteration 11100: Loss = -12383.811276256363
Iteration 11200: Loss = -12383.81115996551
Iteration 11300: Loss = -12383.81104526552
Iteration 11400: Loss = -12383.811015001456
Iteration 11500: Loss = -12383.810998612651
Iteration 11600: Loss = -12384.170225901225
1
Iteration 11700: Loss = -12383.810941317875
Iteration 11800: Loss = -12383.813872229062
1
Iteration 11900: Loss = -12383.810883996026
Iteration 12000: Loss = -12383.811378693403
1
Iteration 12100: Loss = -12383.987184902655
2
Iteration 12200: Loss = -12383.810775990949
Iteration 12300: Loss = -12383.837084552362
1
Iteration 12400: Loss = -12383.810731619718
Iteration 12500: Loss = -12383.821124512368
1
Iteration 12600: Loss = -12383.81859584341
2
Iteration 12700: Loss = -12383.82178905257
3
Iteration 12800: Loss = -12383.81069856289
Iteration 12900: Loss = -12383.815658573592
1
Iteration 13000: Loss = -12383.863317142805
2
Iteration 13100: Loss = -12383.810645507874
Iteration 13200: Loss = -12383.83126819525
1
Iteration 13300: Loss = -12383.810655433224
2
Iteration 13400: Loss = -12383.829631500694
3
Iteration 13500: Loss = -12383.810904311598
4
Iteration 13600: Loss = -12383.876037477497
5
Iteration 13700: Loss = -12383.810558494595
Iteration 13800: Loss = -12383.811689826647
1
Iteration 13900: Loss = -12383.811875096239
2
Iteration 14000: Loss = -12383.812741220343
3
Iteration 14100: Loss = -12383.811144083273
4
Iteration 14200: Loss = -12383.810542222958
Iteration 14300: Loss = -12383.812904123726
1
Iteration 14400: Loss = -12383.853871840667
2
Iteration 14500: Loss = -12383.810474348014
Iteration 14600: Loss = -12383.818026017792
1
Iteration 14700: Loss = -12383.810439348077
Iteration 14800: Loss = -12383.840123452108
1
Iteration 14900: Loss = -12383.810422121733
Iteration 15000: Loss = -12383.820549265696
1
Iteration 15100: Loss = -12383.859518438752
2
Iteration 15200: Loss = -12383.811275987819
3
Iteration 15300: Loss = -12383.810575022731
4
Iteration 15400: Loss = -12383.812540869374
5
Iteration 15500: Loss = -12383.81052822869
6
Iteration 15600: Loss = -12383.85565861771
7
Iteration 15700: Loss = -12383.817117214712
8
Iteration 15800: Loss = -12383.810357512371
Iteration 15900: Loss = -12383.81107137502
1
Iteration 16000: Loss = -12383.810469671726
2
Iteration 16100: Loss = -12383.813880839809
3
Iteration 16200: Loss = -12383.810309959972
Iteration 16300: Loss = -12383.815432295018
1
Iteration 16400: Loss = -12383.81078766039
2
Iteration 16500: Loss = -12383.81098358617
3
Iteration 16600: Loss = -12383.83674944741
4
Iteration 16700: Loss = -12383.810297185822
Iteration 16800: Loss = -12383.81040592481
1
Iteration 16900: Loss = -12383.810836744447
2
Iteration 17000: Loss = -12383.810645615962
3
Iteration 17100: Loss = -12383.82846573771
4
Iteration 17200: Loss = -12383.810382029242
5
Iteration 17300: Loss = -12383.81120324905
6
Iteration 17400: Loss = -12383.810293107459
Iteration 17500: Loss = -12383.862960559962
1
Iteration 17600: Loss = -12383.810970566785
2
Iteration 17700: Loss = -12383.876628436574
3
Iteration 17800: Loss = -12383.81024232023
Iteration 17900: Loss = -12383.842133495598
1
Iteration 18000: Loss = -12383.810373217335
2
Iteration 18100: Loss = -12383.816939404895
3
Iteration 18200: Loss = -12383.813060810984
4
Iteration 18300: Loss = -12383.824155867267
5
Iteration 18400: Loss = -12383.815809282223
6
Iteration 18500: Loss = -12383.810267867366
7
Iteration 18600: Loss = -12383.811045301294
8
Iteration 18700: Loss = -12383.820719305922
9
Iteration 18800: Loss = -12383.811757582105
10
Stopping early at iteration 18800 due to no improvement.
tensor([[-13.0206,   8.4054],
        [-13.4321,   8.8169],
        [-12.3310,   7.7158],
        [-13.1309,   8.5157],
        [-13.3601,   8.7448],
        [-11.8911,   7.2759],
        [-12.2430,   7.6278],
        [-12.2432,   7.6279],
        [-10.9973,   6.3821],
        [-12.3844,   7.7692],
        [ -0.9004,  -3.7148],
        [-11.6905,   7.0753],
        [-12.8507,   8.2355],
        [-11.8640,   7.2488],
        [ -0.4742,  -4.1411],
        [-11.6760,   7.0608],
        [-11.7892,   7.1739],
        [-11.7811,   7.1659],
        [-12.5794,   7.9642],
        [-12.4156,   7.8004],
        [-13.2399,   8.6247],
        [-11.9009,   7.2857],
        [-11.0699,   6.4547],
        [-12.1587,   7.5435],
        [-11.9037,   7.2885],
        [-12.3068,   7.6916],
        [-13.2591,   8.6438],
        [-11.0223,   6.4071],
        [-11.8733,   7.2580],
        [-13.7105,   9.0953],
        [-12.0254,   7.4101],
        [-11.8271,   7.2119],
        [-13.3243,   8.7091],
        [-10.8063,   6.1911],
        [-11.6360,   7.0207],
        [-11.5097,   6.8945],
        [-13.3433,   8.7280],
        [-11.3423,   6.7271],
        [-12.1898,   7.5746],
        [-13.1653,   8.5501],
        [-13.2683,   8.6530],
        [-12.0131,   7.3979],
        [-13.2029,   8.5876],
        [-10.3168,   5.7016],
        [-13.2101,   8.5949],
        [-13.2153,   8.6001],
        [-12.8938,   8.2786],
        [-10.6713,   6.0561],
        [-10.6274,   6.0122],
        [-13.0457,   8.4304],
        [-13.1885,   8.5733],
        [-12.0074,   7.3922],
        [-11.1075,   6.4923],
        [-12.4669,   7.8517],
        [-11.7758,   7.1606],
        [-12.0503,   7.4351],
        [-12.3255,   7.7103],
        [-10.8730,   6.2578],
        [-13.2761,   8.6609],
        [-11.7823,   7.1670],
        [-11.5926,   6.9774],
        [-11.5275,   6.9123],
        [ -0.9291,  -3.6861],
        [-11.1939,   6.5787],
        [-10.7090,   6.0938],
        [-11.0736,   6.4584],
        [-10.9234,   6.3082],
        [-11.4907,   6.8755],
        [-12.4303,   7.8151],
        [-11.5828,   6.9676],
        [-13.3619,   8.7466],
        [-13.7141,   9.0989],
        [-13.3801,   8.7649],
        [-12.7271,   8.1119],
        [-12.3319,   7.7167],
        [-13.2299,   8.6147],
        [-11.4655,   6.8502],
        [-13.3896,   8.7744],
        [-12.3445,   7.7293],
        [-13.2357,   8.6204],
        [-12.2055,   7.5903],
        [-13.3533,   8.7380],
        [-11.3314,   6.7162],
        [-12.9835,   8.3683],
        [-12.1202,   7.5050],
        [-13.3930,   8.7778],
        [-11.4587,   6.8434],
        [-12.2348,   7.6196],
        [-10.9079,   6.2927],
        [-13.6498,   9.0346],
        [-11.0500,   6.4348],
        [-12.2381,   7.6228],
        [-11.8122,   7.1970],
        [-11.9422,   7.3270],
        [-12.0109,   7.3957],
        [-12.2417,   7.6265],
        [-11.1569,   6.5417],
        [-11.7163,   7.1010],
        [-13.3658,   8.7506],
        [-13.6182,   9.0029]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 1.2071e-07],
        [3.9437e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0283, 0.9717], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[2.8923e-07, 1.4433e-01],
         [3.5413e-01, 1.9907e-01]],

        [[5.9919e-02, 2.8849e-01],
         [9.1628e-01, 5.5639e-02]],

        [[5.6974e-01, 1.9864e-01],
         [8.3263e-01, 2.9343e-01]],

        [[3.2526e-01, 2.1956e-01],
         [2.3330e-01, 1.9287e-01]],

        [[9.4617e-01, 2.5421e-01],
         [5.2147e-02, 9.2647e-01]]], dtype=torch.float64,
       grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.01575757575757576
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: -0.0041478895259491316
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: -0.0041478895259491316
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.002764517368444376
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.002764517368444376
Global Adjusted Rand Index: 0.0010701169225605896
Average Adjusted Rand Index: 0.0003865523937577487
Iteration 0: Loss = -22200.46728332717
Iteration 10: Loss = -12386.734728479434
Iteration 20: Loss = -12386.519268515696
Iteration 30: Loss = -12386.48901988844
Iteration 40: Loss = -12386.48168653349
Iteration 50: Loss = -12386.479457668493
Iteration 60: Loss = -12386.478400305587
Iteration 70: Loss = -12386.47775183374
Iteration 80: Loss = -12386.477140747389
Iteration 90: Loss = -12386.47662182369
Iteration 100: Loss = -12386.475998903543
Iteration 110: Loss = -12386.47528115583
Iteration 120: Loss = -12386.474584919499
Iteration 130: Loss = -12386.473755871399
Iteration 140: Loss = -12386.4728885459
Iteration 150: Loss = -12386.472005006313
Iteration 160: Loss = -12386.47101850422
Iteration 170: Loss = -12386.469899621392
Iteration 180: Loss = -12386.468749203526
Iteration 190: Loss = -12386.467430322078
Iteration 200: Loss = -12386.466052867712
Iteration 210: Loss = -12386.464458470662
Iteration 220: Loss = -12386.46275675243
Iteration 230: Loss = -12386.460980918724
Iteration 240: Loss = -12386.458972435883
Iteration 250: Loss = -12386.456759430426
Iteration 260: Loss = -12386.454394243785
Iteration 270: Loss = -12386.451868340211
Iteration 280: Loss = -12386.449182656932
Iteration 290: Loss = -12386.446325098605
Iteration 300: Loss = -12386.443367332246
Iteration 310: Loss = -12386.440318481998
Iteration 320: Loss = -12386.437248907614
Iteration 330: Loss = -12386.434156795656
Iteration 340: Loss = -12386.431230667498
Iteration 350: Loss = -12386.428341825069
Iteration 360: Loss = -12386.425610214928
Iteration 370: Loss = -12386.423129494913
Iteration 380: Loss = -12386.420841169256
Iteration 390: Loss = -12386.41882517761
Iteration 400: Loss = -12386.417029555123
Iteration 410: Loss = -12386.415450779607
Iteration 420: Loss = -12386.414121513022
Iteration 430: Loss = -12386.412916439776
Iteration 440: Loss = -12386.411930836823
Iteration 450: Loss = -12386.411092166047
Iteration 460: Loss = -12386.410377984557
Iteration 470: Loss = -12386.409776779674
Iteration 480: Loss = -12386.409310319856
Iteration 490: Loss = -12386.408865868043
Iteration 500: Loss = -12386.408547651277
Iteration 510: Loss = -12386.408231078616
Iteration 520: Loss = -12386.40796671563
Iteration 530: Loss = -12386.407795147636
Iteration 540: Loss = -12386.407564314015
Iteration 550: Loss = -12386.407482377266
Iteration 560: Loss = -12386.407261157021
Iteration 570: Loss = -12386.407210263851
Iteration 580: Loss = -12386.40710357987
Iteration 590: Loss = -12386.407002226139
Iteration 600: Loss = -12386.406924084542
Iteration 610: Loss = -12386.406891566256
Iteration 620: Loss = -12386.40683097132
Iteration 630: Loss = -12386.406781441856
Iteration 640: Loss = -12386.40675751864
Iteration 650: Loss = -12386.406712715405
Iteration 660: Loss = -12386.406677479517
Iteration 670: Loss = -12386.406688696405
1
Iteration 680: Loss = -12386.406643133045
Iteration 690: Loss = -12386.406632014567
Iteration 700: Loss = -12386.406598861984
Iteration 710: Loss = -12386.40655335233
Iteration 720: Loss = -12386.406607777933
1
Iteration 730: Loss = -12386.406548563687
Iteration 740: Loss = -12386.406542644048
Iteration 750: Loss = -12386.406551290087
1
Iteration 760: Loss = -12386.406544931968
2
Iteration 770: Loss = -12386.406541049824
Iteration 780: Loss = -12386.40655179947
1
Iteration 790: Loss = -12386.406560283907
2
Iteration 800: Loss = -12386.406504422363
Iteration 810: Loss = -12386.406544346324
1
Iteration 820: Loss = -12386.406557978762
2
Iteration 830: Loss = -12386.406541160331
3
Stopping early at iteration 829 due to no improvement.
pi: tensor([[0.0344, 0.9656],
        [0.0435, 0.9565]], dtype=torch.float64)
alpha: tensor([0.0430, 0.9570])
beta: tensor([[[0.2202, 0.2002],
         [0.4847, 0.1969]],

        [[0.3986, 0.2416],
         [0.6167, 0.6757]],

        [[0.9344, 0.2085],
         [0.1627, 0.2558]],

        [[0.8767, 0.2058],
         [0.7570, 0.2008]],

        [[0.5795, 0.1934],
         [0.4126, 0.8228]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22200.126183237455
Iteration 100: Loss = -12399.628990825506
Iteration 200: Loss = -12387.994688413919
Iteration 300: Loss = -12386.751768917931
Iteration 400: Loss = -12386.430085625909
Iteration 500: Loss = -12386.30008035236
Iteration 600: Loss = -12386.241049892838
Iteration 700: Loss = -12386.20189220192
Iteration 800: Loss = -12386.163698504562
Iteration 900: Loss = -12386.138794251776
Iteration 1000: Loss = -12386.126989885852
Iteration 1100: Loss = -12386.12053748762
Iteration 1200: Loss = -12386.116025341978
Iteration 1300: Loss = -12386.112310522947
Iteration 1400: Loss = -12386.109181191148
Iteration 1500: Loss = -12386.106302112556
Iteration 1600: Loss = -12386.103667372276
Iteration 1700: Loss = -12386.10119886228
Iteration 1800: Loss = -12386.098484396038
Iteration 1900: Loss = -12386.094836517876
Iteration 2000: Loss = -12386.091234081621
Iteration 2100: Loss = -12386.089946354135
Iteration 2200: Loss = -12386.089036909021
Iteration 2300: Loss = -12386.088165697887
Iteration 2400: Loss = -12386.087225819112
Iteration 2500: Loss = -12386.086070618267
Iteration 2600: Loss = -12386.084855780666
Iteration 2700: Loss = -12386.08389948336
Iteration 2800: Loss = -12386.08327467913
Iteration 2900: Loss = -12386.082980625819
Iteration 3000: Loss = -12386.082747847864
Iteration 3100: Loss = -12386.082561288453
Iteration 3200: Loss = -12386.082440321812
Iteration 3300: Loss = -12386.082269763354
Iteration 3400: Loss = -12386.08213755676
Iteration 3500: Loss = -12386.082015816994
Iteration 3600: Loss = -12386.081912846912
Iteration 3700: Loss = -12386.081813075898
Iteration 3800: Loss = -12386.081689153663
Iteration 3900: Loss = -12386.08155943393
Iteration 4000: Loss = -12386.081474251798
Iteration 4100: Loss = -12386.081412018943
Iteration 4200: Loss = -12386.08134211006
Iteration 4300: Loss = -12386.081273884038
Iteration 4400: Loss = -12386.081163163983
Iteration 4500: Loss = -12386.08113050291
Iteration 4600: Loss = -12386.081055367418
Iteration 4700: Loss = -12386.080986158766
Iteration 4800: Loss = -12386.080930108923
Iteration 4900: Loss = -12386.080838837717
Iteration 5000: Loss = -12386.080785561138
Iteration 5100: Loss = -12386.080750497316
Iteration 5200: Loss = -12386.080663926314
Iteration 5300: Loss = -12386.08058954584
Iteration 5400: Loss = -12386.08054313793
Iteration 5500: Loss = -12386.08048662412
Iteration 5600: Loss = -12386.08044345474
Iteration 5700: Loss = -12386.099475957953
1
Iteration 5800: Loss = -12386.080289311543
Iteration 5900: Loss = -12386.08022141178
Iteration 6000: Loss = -12386.085007136242
1
Iteration 6100: Loss = -12386.080054554424
Iteration 6200: Loss = -12386.07998025599
Iteration 6300: Loss = -12386.085204572608
1
Iteration 6400: Loss = -12386.079803957005
Iteration 6500: Loss = -12386.079603841144
Iteration 6600: Loss = -12386.079412523048
Iteration 6700: Loss = -12386.07925431159
Iteration 6800: Loss = -12386.078757517485
Iteration 6900: Loss = -12386.078191611337
Iteration 7000: Loss = -12386.095758138872
1
Iteration 7100: Loss = -12386.076255838248
Iteration 7200: Loss = -12386.07448194003
Iteration 7300: Loss = -12386.072167861812
Iteration 7400: Loss = -12386.069041471193
Iteration 7500: Loss = -12386.062626642062
Iteration 7600: Loss = -12386.039547317214
Iteration 7700: Loss = -12385.402413097674
Iteration 7800: Loss = -12383.335688446032
Iteration 7900: Loss = -12383.27190503091
Iteration 8000: Loss = -12383.257691752719
Iteration 8100: Loss = -12383.251116374158
Iteration 8200: Loss = -12383.247358937646
Iteration 8300: Loss = -12383.24511250568
Iteration 8400: Loss = -12383.24324570345
Iteration 8500: Loss = -12383.241962435455
Iteration 8600: Loss = -12383.24260907155
1
Iteration 8700: Loss = -12383.240216979168
Iteration 8800: Loss = -12383.239603224023
Iteration 8900: Loss = -12383.240001194123
1
Iteration 9000: Loss = -12383.238640241176
Iteration 9100: Loss = -12383.238234074855
Iteration 9200: Loss = -12383.237939425906
Iteration 9300: Loss = -12383.238195373622
1
Iteration 9400: Loss = -12383.237419714178
Iteration 9500: Loss = -12383.237240111364
Iteration 9600: Loss = -12383.239005240428
1
Iteration 9700: Loss = -12383.236824572083
Iteration 9800: Loss = -12383.23666082521
Iteration 9900: Loss = -12383.24004485857
1
Iteration 10000: Loss = -12383.236400575746
Iteration 10100: Loss = -12383.236278246057
Iteration 10200: Loss = -12383.28791377419
1
Iteration 10300: Loss = -12383.248207721383
2
Iteration 10400: Loss = -12383.235990378582
Iteration 10500: Loss = -12383.236312666304
1
Iteration 10600: Loss = -12383.235866113737
Iteration 10700: Loss = -12383.235815762408
Iteration 10800: Loss = -12383.235647928675
Iteration 10900: Loss = -12383.235911267768
1
Iteration 11000: Loss = -12383.235251758206
Iteration 11100: Loss = -12383.239736235002
1
Iteration 11200: Loss = -12383.235142672833
Iteration 11300: Loss = -12383.23501964024
Iteration 11400: Loss = -12383.235695896457
1
Iteration 11500: Loss = -12383.234981742326
Iteration 11600: Loss = -12383.234897267108
Iteration 11700: Loss = -12383.23568936513
1
Iteration 11800: Loss = -12383.234874512638
Iteration 11900: Loss = -12383.234844036962
Iteration 12000: Loss = -12383.289184630434
1
Iteration 12100: Loss = -12383.234742468108
Iteration 12200: Loss = -12383.23470969974
Iteration 12300: Loss = -12383.243255168896
1
Iteration 12400: Loss = -12383.234704662156
Iteration 12500: Loss = -12383.236495609355
1
Iteration 12600: Loss = -12383.234680512398
Iteration 12700: Loss = -12383.234626422865
Iteration 12800: Loss = -12383.25696656214
1
Iteration 12900: Loss = -12383.234592900562
Iteration 13000: Loss = -12383.244726684414
1
Iteration 13100: Loss = -12383.234561884188
Iteration 13200: Loss = -12383.294263951993
1
Iteration 13300: Loss = -12383.234552729871
Iteration 13400: Loss = -12383.23567924776
1
Iteration 13500: Loss = -12383.234565813225
2
Iteration 13600: Loss = -12383.23448485234
Iteration 13700: Loss = -12383.234651665774
1
Iteration 13800: Loss = -12383.23444156306
Iteration 13900: Loss = -12383.234950020962
1
Iteration 14000: Loss = -12383.242487766054
2
Iteration 14100: Loss = -12383.235116998345
3
Iteration 14200: Loss = -12383.234554493227
4
Iteration 14300: Loss = -12383.268753930024
5
Iteration 14400: Loss = -12383.234413894808
Iteration 14500: Loss = -12383.23513397966
1
Iteration 14600: Loss = -12383.23455387105
2
Iteration 14700: Loss = -12383.234651828947
3
Iteration 14800: Loss = -12383.246300827372
4
Iteration 14900: Loss = -12383.234455353222
5
Iteration 15000: Loss = -12383.23454825176
6
Iteration 15100: Loss = -12383.23563609187
7
Iteration 15200: Loss = -12383.235112517355
8
Iteration 15300: Loss = -12383.234336389649
Iteration 15400: Loss = -12383.237493334846
1
Iteration 15500: Loss = -12383.23528266901
2
Iteration 15600: Loss = -12383.28723928524
3
Iteration 15700: Loss = -12383.234340782808
4
Iteration 15800: Loss = -12383.234335458797
Iteration 15900: Loss = -12383.234981884749
1
Iteration 16000: Loss = -12383.238886689414
2
Iteration 16100: Loss = -12383.245467436944
3
Iteration 16200: Loss = -12383.234524005702
4
Iteration 16300: Loss = -12383.234936340708
5
Iteration 16400: Loss = -12383.259966576905
6
Iteration 16500: Loss = -12383.234301484155
Iteration 16600: Loss = -12383.23545308782
1
Iteration 16700: Loss = -12383.234274487515
Iteration 16800: Loss = -12383.234440559685
1
Iteration 16900: Loss = -12383.236223046644
2
Iteration 17000: Loss = -12383.234447814806
3
Iteration 17100: Loss = -12383.237326756598
4
Iteration 17200: Loss = -12383.2510626291
5
Iteration 17300: Loss = -12383.234276986892
6
Iteration 17400: Loss = -12383.234380363521
7
Iteration 17500: Loss = -12383.235479160723
8
Iteration 17600: Loss = -12383.238585604335
9
Iteration 17700: Loss = -12383.234287766667
10
Stopping early at iteration 17700 due to no improvement.
tensor([[ -6.5329,   4.9506],
        [-10.0253,   8.5856],
        [ -7.0715,   3.9848],
        [ -2.6862,   1.1152],
        [ -6.1236,   4.4409],
        [ -4.1797,   1.7460],
        [ -7.2204,   5.1575],
        [ -5.8818,   4.3319],
        [ -8.6393,   6.8089],
        [ -9.7603,   7.4394],
        [ -2.6042,   1.0561],
        [ -7.0437,   2.5874],
        [ -3.8175,   2.0014],
        [ -5.3227,   3.9219],
        [  0.1467,  -2.2568],
        [ -7.8200,   6.2709],
        [ -3.3073,   1.9202],
        [ -9.7768,   8.2736],
        [ -4.8411,   2.9538],
        [ -6.5172,   5.1308],
        [ -5.7927,   4.3564],
        [ -5.1978,   3.7987],
        [ -5.1844,   3.7867],
        [ -8.3759,   6.7487],
        [ -7.0992,   5.1534],
        [ -5.7873,   4.2672],
        [ -5.9365,   4.3605],
        [ -8.1127,   5.7196],
        [ -9.3748,   6.2302],
        [ -5.7967,   4.3008],
        [ -8.0162,   6.2635],
        [ -6.2476,   4.8596],
        [ -5.3546,   3.9266],
        [ -5.8443,   4.0820],
        [ -6.2524,   4.2776],
        [  0.4695,  -1.8588],
        [ -6.5864,   4.6243],
        [ -4.7693,   3.3019],
        [ -7.2212,   5.8114],
        [ -4.9566,   3.5691],
        [ -3.3975,   1.7099],
        [ -9.4769,   7.0648],
        [ -7.6901,   6.1284],
        [  0.3604,  -1.8059],
        [ -4.2947,   2.3794],
        [ -4.0980,   2.3696],
        [ -6.1765,   3.3916],
        [ -4.4530,   3.0467],
        [ -4.8581,   3.3878],
        [-10.1147,   7.0788],
        [ -6.2669,   4.6997],
        [ -9.6981,   8.2760],
        [ -0.4478,  -1.1093],
        [ -5.1693,   2.9075],
        [ -6.3810,   4.9947],
        [ -5.5641,   3.4280],
        [ -7.4166,   4.4403],
        [ -4.1162,   2.5268],
        [ -9.2369,   7.8050],
        [ -6.7476,   5.3612],
        [ -3.9024,   2.1587],
        [ -1.8849,   0.3525],
        [  1.3319,  -3.4685],
        [ -4.4227,   2.9784],
        [ -6.3262,   4.8063],
        [ -4.0321,   2.5991],
        [ -7.6925,   5.9320],
        [ -2.9120,   1.0812],
        [ -9.7125,   7.6942],
        [ -6.0618,   3.8851],
        [ -9.8015,   7.8290],
        [ -6.6582,   5.2401],
        [-10.0522,   8.6411],
        [ -0.5383,  -0.8867],
        [ -6.5502,   5.1637],
        [ -7.5818,   5.6742],
        [ -4.1355,   2.0568],
        [ -5.5679,   3.8006],
        [ -6.6125,   4.8305],
        [ -7.1077,   5.6083],
        [ -6.1245,   4.3985],
        [ -8.6657,   7.2789],
        [ -3.0255,   1.5941],
        [ -5.7926,   1.9113],
        [ -9.9677,   8.0267],
        [-10.8171,   8.1900],
        [ -2.9826,   1.4220],
        [ -8.8283,   6.6306],
        [ -2.2916,   0.7282],
        [ -2.2412,   0.8217],
        [ -5.1507,   3.3947],
        [ -6.6391,   4.6107],
        [ -4.6916,   3.2214],
        [-10.8053,   6.1901],
        [ -9.3226,   7.9335],
        [ -7.0289,   5.6184],
        [ -6.5481,   4.9710],
        [ -3.9874,   1.4590],
        [ -6.3483,   4.9130],
        [ -6.0668,   4.3917]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 4.5684e-07],
        [7.1354e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0528, 0.9472], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.0533, 0.1389],
         [0.4847, 0.1998]],

        [[0.3986, 0.2638],
         [0.6167, 0.6757]],

        [[0.9344, 0.1766],
         [0.1627, 0.2558]],

        [[0.8767, 0.2228],
         [0.7570, 0.2008]],

        [[0.5795, 0.2249],
         [0.4126, 0.8228]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.009696969696969697
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0022264033076300205
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: -0.007543717590214484
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.0014658390783536795
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.0014658390783536795
Global Adjusted Rand Index: 0.0008577951798631267
Average Adjusted Rand Index: 0.00028959545153557485
Iteration 0: Loss = -26463.322992153175
Iteration 10: Loss = -12386.657163613621
Iteration 20: Loss = -12386.657163615377
1
Iteration 30: Loss = -12386.657163618136
2
Iteration 40: Loss = -12386.657163622316
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[1.0000e+00, 4.9537e-13],
        [1.0000e+00, 4.0711e-18]], dtype=torch.float64)
alpha: tensor([1.0000e+00, 4.9263e-13])
beta: tensor([[[0.1980, 0.1756],
         [0.2250, 0.2148]],

        [[0.5323, 0.2574],
         [0.8842, 0.0834]],

        [[0.1918, 0.2084],
         [0.5553, 0.4138]],

        [[0.3870, 0.2001],
         [0.5793, 0.5149]],

        [[0.1202, 0.1853],
         [0.5875, 0.8097]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26463.443314566128
Iteration 100: Loss = -12389.633185894374
Iteration 200: Loss = -12387.78128775218
Iteration 300: Loss = -12387.241880808404
Iteration 400: Loss = -12386.984243529068
Iteration 500: Loss = -12386.826746670262
Iteration 600: Loss = -12386.719633681767
Iteration 700: Loss = -12386.642317440881
Iteration 800: Loss = -12386.58403960688
Iteration 900: Loss = -12386.53910003977
Iteration 1000: Loss = -12386.503269227363
Iteration 1100: Loss = -12386.474054332099
Iteration 1200: Loss = -12386.449773773265
Iteration 1300: Loss = -12386.429041693606
Iteration 1400: Loss = -12386.41116313772
Iteration 1500: Loss = -12386.395483345505
Iteration 1600: Loss = -12386.381680049903
Iteration 1700: Loss = -12386.36950897122
Iteration 1800: Loss = -12386.35886369726
Iteration 1900: Loss = -12386.349475274703
Iteration 2000: Loss = -12386.341396096885
Iteration 2100: Loss = -12386.334368513855
Iteration 2200: Loss = -12386.32817838376
Iteration 2300: Loss = -12386.3227650456
Iteration 2400: Loss = -12386.31789965054
Iteration 2500: Loss = -12386.31343102521
Iteration 2600: Loss = -12386.309268210653
Iteration 2700: Loss = -12386.305448968465
Iteration 2800: Loss = -12386.301685457502
Iteration 2900: Loss = -12386.298100236918
Iteration 3000: Loss = -12386.294542836427
Iteration 3100: Loss = -12386.29089228566
Iteration 3200: Loss = -12386.287156564547
Iteration 3300: Loss = -12386.283285211039
Iteration 3400: Loss = -12386.279168246001
Iteration 3500: Loss = -12386.27469341085
Iteration 3600: Loss = -12386.269792636425
Iteration 3700: Loss = -12386.264341545886
Iteration 3800: Loss = -12386.258226730635
Iteration 3900: Loss = -12386.251091224613
Iteration 4000: Loss = -12386.242917842284
Iteration 4100: Loss = -12386.233188746715
Iteration 4200: Loss = -12386.221758280419
Iteration 4300: Loss = -12386.208023467982
Iteration 4400: Loss = -12386.19167313673
Iteration 4500: Loss = -12386.172808523916
Iteration 4600: Loss = -12386.152081179845
Iteration 4700: Loss = -12386.131530298515
Iteration 4800: Loss = -12386.113844084466
Iteration 4900: Loss = -12386.100619216664
Iteration 5000: Loss = -12386.091483064994
Iteration 5100: Loss = -12386.085483538784
Iteration 5200: Loss = -12386.081936412562
Iteration 5300: Loss = -12386.080107020778
Iteration 5400: Loss = -12386.079206049879
Iteration 5500: Loss = -12386.078682073328
Iteration 5600: Loss = -12386.078447568692
Iteration 5700: Loss = -12386.078150661693
Iteration 5800: Loss = -12386.077927279423
Iteration 5900: Loss = -12386.077665798746
Iteration 6000: Loss = -12386.077356000618
Iteration 6100: Loss = -12386.077059101253
Iteration 6200: Loss = -12386.0767632639
Iteration 6300: Loss = -12386.076454349166
Iteration 6400: Loss = -12386.076108853229
Iteration 6500: Loss = -12386.075808818652
Iteration 6600: Loss = -12386.07549197761
Iteration 6700: Loss = -12386.075202420652
Iteration 6800: Loss = -12386.074899747258
Iteration 6900: Loss = -12386.074633655402
Iteration 7000: Loss = -12386.074373848172
Iteration 7100: Loss = -12386.074124503342
Iteration 7200: Loss = -12386.073735189204
Iteration 7300: Loss = -12386.073400036543
Iteration 7400: Loss = -12386.07306045867
Iteration 7500: Loss = -12386.072648233101
Iteration 7600: Loss = -12386.072187867885
Iteration 7700: Loss = -12386.072047675976
Iteration 7800: Loss = -12386.22345280064
1
Iteration 7900: Loss = -12386.069549882051
Iteration 8000: Loss = -12386.067859984552
Iteration 8100: Loss = -12386.066561074424
Iteration 8200: Loss = -12386.059298292279
Iteration 8300: Loss = -12386.04364319987
Iteration 8400: Loss = -12385.945299296762
Iteration 8500: Loss = -12384.069001259817
Iteration 8600: Loss = -12383.329736444792
Iteration 8700: Loss = -12383.282193740992
Iteration 8800: Loss = -12383.2667933371
Iteration 8900: Loss = -12383.258178988359
Iteration 9000: Loss = -12383.253286805997
Iteration 9100: Loss = -12383.250251025387
Iteration 9200: Loss = -12383.24755696881
Iteration 9300: Loss = -12383.245704267694
Iteration 9400: Loss = -12383.244485302044
Iteration 9500: Loss = -12383.243139625763
Iteration 9600: Loss = -12383.45863071942
1
Iteration 9700: Loss = -12383.241383470311
Iteration 9800: Loss = -12383.240681297002
Iteration 9900: Loss = -12383.270271214767
1
Iteration 10000: Loss = -12383.239667021791
Iteration 10100: Loss = -12383.239235810717
Iteration 10200: Loss = -12383.31571005466
1
Iteration 10300: Loss = -12383.238556059443
Iteration 10400: Loss = -12383.238170648809
Iteration 10500: Loss = -12383.23789675898
Iteration 10600: Loss = -12383.237700858796
Iteration 10700: Loss = -12383.237452921618
Iteration 10800: Loss = -12383.237438872364
Iteration 10900: Loss = -12383.237243849686
Iteration 11000: Loss = -12383.25136524556
1
Iteration 11100: Loss = -12383.237352275626
2
Iteration 11200: Loss = -12383.248303501088
3
Iteration 11300: Loss = -12383.236625185462
Iteration 11400: Loss = -12383.343895189977
1
Iteration 11500: Loss = -12383.236417236843
Iteration 11600: Loss = -12383.251284309403
1
Iteration 11700: Loss = -12383.320752618027
2
Iteration 11800: Loss = -12383.238590934247
3
Iteration 11900: Loss = -12383.23596578772
Iteration 12000: Loss = -12383.23790845517
1
Iteration 12100: Loss = -12383.240113184009
2
Iteration 12200: Loss = -12383.235740630087
Iteration 12300: Loss = -12383.236282695287
1
Iteration 12400: Loss = -12383.239020468252
2
Iteration 12500: Loss = -12383.237127085824
3
Iteration 12600: Loss = -12383.235695862808
Iteration 12700: Loss = -12383.265467997493
1
Iteration 12800: Loss = -12383.24885450711
2
Iteration 12900: Loss = -12383.235357626823
Iteration 13000: Loss = -12383.235429951936
1
Iteration 13100: Loss = -12383.471493263922
2
Iteration 13200: Loss = -12383.235660918908
3
Iteration 13300: Loss = -12383.245539009324
4
Iteration 13400: Loss = -12383.235252024431
Iteration 13500: Loss = -12383.237538354597
1
Iteration 13600: Loss = -12383.235238867755
Iteration 13700: Loss = -12383.235181574437
Iteration 13800: Loss = -12383.25062976672
1
Iteration 13900: Loss = -12383.235145232939
Iteration 14000: Loss = -12383.235656222925
1
Iteration 14100: Loss = -12383.248481859022
2
Iteration 14200: Loss = -12383.235204325501
3
Iteration 14300: Loss = -12383.235103087924
Iteration 14400: Loss = -12383.23502748984
Iteration 14500: Loss = -12383.235376376582
1
Iteration 14600: Loss = -12383.234953245681
Iteration 14700: Loss = -12383.23570189014
1
Iteration 14800: Loss = -12383.234917685722
Iteration 14900: Loss = -12383.234969588533
1
Iteration 15000: Loss = -12383.263864767288
2
Iteration 15100: Loss = -12383.234875121345
Iteration 15200: Loss = -12383.235078545336
1
Iteration 15300: Loss = -12383.260270072411
2
Iteration 15400: Loss = -12383.250173476212
3
Iteration 15500: Loss = -12383.23497506488
4
Iteration 15600: Loss = -12383.235354297234
5
Iteration 15700: Loss = -12383.24586311824
6
Iteration 15800: Loss = -12383.234807655697
Iteration 15900: Loss = -12383.240569923477
1
Iteration 16000: Loss = -12383.254633847979
2
Iteration 16100: Loss = -12383.234839393172
3
Iteration 16200: Loss = -12383.239046672363
4
Iteration 16300: Loss = -12383.234782243337
Iteration 16400: Loss = -12383.240657164886
1
Iteration 16500: Loss = -12383.234753874223
Iteration 16600: Loss = -12383.235845267674
1
Iteration 16700: Loss = -12383.23473439135
Iteration 16800: Loss = -12383.236495905749
1
Iteration 16900: Loss = -12383.246105624294
2
Iteration 17000: Loss = -12383.236281888792
3
Iteration 17100: Loss = -12383.234735717024
4
Iteration 17200: Loss = -12383.240296246204
5
Iteration 17300: Loss = -12383.234730073162
Iteration 17400: Loss = -12383.23502116061
1
Iteration 17500: Loss = -12383.234835233303
2
Iteration 17600: Loss = -12383.234748680357
3
Iteration 17700: Loss = -12383.235674387151
4
Iteration 17800: Loss = -12383.246149615115
5
Iteration 17900: Loss = -12383.40764703756
6
Iteration 18000: Loss = -12383.247776507429
7
Iteration 18100: Loss = -12383.23470411385
Iteration 18200: Loss = -12383.236468346919
1
Iteration 18300: Loss = -12383.234675010342
Iteration 18400: Loss = -12383.25551440093
1
Iteration 18500: Loss = -12383.234698692682
2
Iteration 18600: Loss = -12383.23484623586
3
Iteration 18700: Loss = -12383.234871343038
4
Iteration 18800: Loss = -12383.234710869985
5
Iteration 18900: Loss = -12383.23941780154
6
Iteration 19000: Loss = -12383.2348802694
7
Iteration 19100: Loss = -12383.236133360591
8
Iteration 19200: Loss = -12383.237989935093
9
Iteration 19300: Loss = -12383.234430427483
Iteration 19400: Loss = -12383.238292391967
1
Iteration 19500: Loss = -12383.234475512829
2
Iteration 19600: Loss = -12383.23439166567
Iteration 19700: Loss = -12383.355353583896
1
Iteration 19800: Loss = -12383.234368625364
Iteration 19900: Loss = -12383.235316227661
1
tensor([[  4.6010,  -6.8912],
        [  8.1593,  -9.7101],
        [  8.3610,  -9.7843],
        [  0.8657,  -2.9400],
        [  4.0344,  -6.5325],
        [  0.8529,  -5.0775],
        [  5.3897,  -6.9840],
        [  4.3871,  -5.8243],
        [  7.0737,  -8.5355],
        [  7.9518,  -9.7694],
        [  1.0499,  -2.6113],
        [  3.5445,  -6.0930],
        [  2.2039,  -3.6187],
        [  3.9127,  -5.3335],
        [ -2.0388,   0.3636],
        [  6.3376,  -7.7249],
        [  1.7569,  -3.4757],
        [  6.5337,  -7.9372],
        [  2.7322,  -5.0623],
        [  3.5193,  -8.1346],
        [  3.8595,  -6.2896],
        [  3.2897,  -5.7067],
        [  3.5078,  -5.4672],
        [  7.5808,  -9.0687],
        [  4.8580,  -7.3982],
        [  4.2903,  -5.7643],
        [  4.4377,  -5.8568],
        [  6.1160,  -7.7271],
        [  7.0244,  -8.8681],
        [  7.8035,  -9.4642],
        [  7.7244,  -9.1290],
        [  4.5608,  -6.5472],
        [  3.9482,  -5.3347],
        [  3.8319,  -6.0975],
        [  4.5684,  -5.9638],
        [ -2.3102,   0.0196],
        [  4.9060,  -6.3065],
        [  2.9265,  -5.1453],
        [  5.3842,  -7.6503],
        [  3.4827,  -5.0409],
        [  1.8432,  -3.2624],
        [  5.3517,  -8.2821],
        [  6.0310,  -7.7494],
        [ -1.7790,   0.3905],
        [  2.3433,  -4.3339],
        [  2.4114,  -4.0575],
        [  3.9066,  -5.6691],
        [  2.9819,  -4.5144],
        [  3.4263,  -4.8222],
        [  7.2670, -10.2468],
        [  4.5994,  -6.3687],
        [  8.3741,  -9.7646],
        [ -1.0826,  -0.4193],
        [  3.0638,  -5.0177],
        [  8.6803, -10.2351],
        [  3.8042,  -5.1905],
        [  8.8384, -10.2803],
        [  2.1685,  -4.4732],
        [  8.1059,  -9.5268],
        [  5.3067,  -6.8010],
        [  2.3151,  -3.7473],
        [  0.3024,  -1.9342],
        [ -4.0359,   0.7671],
        [  1.4330,  -5.9745],
        [  3.8471,  -7.2887],
        [  2.6018,  -4.0350],
        [  5.9708,  -7.6821],
        [  1.3013,  -2.6926],
        [  8.4074, -10.1257],
        [  4.0638,  -5.8824],
        [  5.7444,  -7.9406],
        [  5.2487,  -6.6422],
        [  8.1894, -10.3258],
        [ -0.8677,  -0.5205],
        [  4.9035,  -6.8215],
        [  5.6048,  -7.6179],
        [  0.7871,  -5.4023],
        [  3.4794,  -5.8906],
        [  7.8811, -10.6161],
        [  5.0580,  -7.6512],
        [  2.9580,  -7.5732],
        [  5.5911,  -9.9728],
        [  1.0682,  -3.5492],
        [  2.9527,  -4.7494],
        [  5.6326,  -7.0807],
        [  8.3739,  -9.7888],
        [  1.4238,  -2.9823],
        [  6.9386,  -8.3630],
        [  0.8105,  -2.2052],
        [  0.7286,  -2.3332],
        [  3.5650,  -4.9872],
        [  4.9177,  -6.3389],
        [  2.9551,  -4.9603],
        [  7.5097, -10.3277],
        [  7.9705,  -9.6815],
        [  7.0966, -11.1529],
        [  4.4118,  -7.1111],
        [  1.9922,  -3.4589],
        [  4.2018,  -7.0596],
        [  4.4419,  -6.0162]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 4.8067e-08],
        [2.8669e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9473, 0.0527], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1996, 0.1391],
         [0.2250, 0.0533]],

        [[0.5323, 0.2638],
         [0.8842, 0.0834]],

        [[0.1918, 0.1763],
         [0.5553, 0.4138]],

        [[0.3870, 0.2228],
         [0.5793, 0.5149]],

        [[0.1202, 0.2250],
         [0.5875, 0.8097]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.009696969696969697
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0022264033076300205
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: -0.007543717590214484
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.0014658390783536795
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.0014658390783536795
Global Adjusted Rand Index: 0.0008577951798631267
Average Adjusted Rand Index: 0.00028959545153557485
Iteration 0: Loss = -14632.233822874492
Iteration 10: Loss = -12386.491317158103
Iteration 20: Loss = -12386.490638835643
Iteration 30: Loss = -12386.490656319114
1
Iteration 40: Loss = -12386.490625709863
Iteration 50: Loss = -12386.490642422936
1
Iteration 60: Loss = -12386.490682709662
2
Iteration 70: Loss = -12386.49066571308
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[0.3758, 0.6242],
        [0.9723, 0.0277]], dtype=torch.float64)
alpha: tensor([0.6090, 0.3910])
beta: tensor([[[0.1981, 0.1965],
         [0.2704, 0.1980]],

        [[0.1596, 0.2005],
         [0.0451, 0.3365]],

        [[0.6615, 0.2002],
         [0.1363, 0.2203]],

        [[0.8305, 0.1980],
         [0.0207, 0.1654]],

        [[0.8676, 0.1949],
         [0.4511, 0.8796]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -14632.044764630504
Iteration 100: Loss = -12390.084094971266
Iteration 200: Loss = -12386.236992806093
Iteration 300: Loss = -12386.177693378093
Iteration 400: Loss = -12386.16834539753
Iteration 500: Loss = -12386.164249652307
Iteration 600: Loss = -12386.16229348172
Iteration 700: Loss = -12386.16133985331
Iteration 800: Loss = -12386.160868999308
Iteration 900: Loss = -12386.160652377534
Iteration 1000: Loss = -12386.160507616705
Iteration 1100: Loss = -12386.160347379504
Iteration 1200: Loss = -12386.160284852422
Iteration 1300: Loss = -12386.160166703956
Iteration 1400: Loss = -12386.160114376162
Iteration 1500: Loss = -12386.159989176671
Iteration 1600: Loss = -12386.15987754027
Iteration 1700: Loss = -12386.159822169173
Iteration 1800: Loss = -12386.159737338998
Iteration 1900: Loss = -12386.15969236085
Iteration 2000: Loss = -12386.159823647142
1
Iteration 2100: Loss = -12386.159618432464
Iteration 2200: Loss = -12386.159243049946
Iteration 2300: Loss = -12386.159079563899
Iteration 2400: Loss = -12386.159028861959
Iteration 2500: Loss = -12386.158992235833
Iteration 2600: Loss = -12386.158803938893
Iteration 2700: Loss = -12386.158699749998
Iteration 2800: Loss = -12386.159364294283
1
Iteration 2900: Loss = -12386.158521250092
Iteration 3000: Loss = -12386.158483740872
Iteration 3100: Loss = -12386.158863069915
1
Iteration 3200: Loss = -12386.158354458998
Iteration 3300: Loss = -12386.158180545253
Iteration 3400: Loss = -12386.158978971196
1
Iteration 3500: Loss = -12386.163713580829
2
Iteration 3600: Loss = -12386.157816435387
Iteration 3700: Loss = -12386.197254258052
1
Iteration 3800: Loss = -12386.15754685289
Iteration 3900: Loss = -12386.16046155594
1
Iteration 4000: Loss = -12386.161605477244
2
Iteration 4100: Loss = -12386.165307379275
3
Iteration 4200: Loss = -12386.157193129871
Iteration 4300: Loss = -12386.166173730237
1
Iteration 4400: Loss = -12386.157240734045
2
Iteration 4500: Loss = -12386.157000313546
Iteration 4600: Loss = -12386.161698832024
1
Iteration 4700: Loss = -12386.156653516726
Iteration 4800: Loss = -12386.156555141553
Iteration 4900: Loss = -12386.156538043724
Iteration 5000: Loss = -12386.15637730909
Iteration 5100: Loss = -12386.159835295552
1
Iteration 5200: Loss = -12386.156779354002
2
Iteration 5300: Loss = -12386.38520904768
3
Iteration 5400: Loss = -12386.156015222301
Iteration 5500: Loss = -12386.190993079224
1
Iteration 5600: Loss = -12386.156346109663
2
Iteration 5700: Loss = -12386.155694484043
Iteration 5800: Loss = -12386.155613156829
Iteration 5900: Loss = -12386.155483998089
Iteration 6000: Loss = -12386.160511473236
1
Iteration 6100: Loss = -12386.155157497406
Iteration 6200: Loss = -12386.154927987804
Iteration 6300: Loss = -12386.180645929824
1
Iteration 6400: Loss = -12386.154537432747
Iteration 6500: Loss = -12386.154398952607
Iteration 6600: Loss = -12386.153986488041
Iteration 6700: Loss = -12386.158197290442
1
Iteration 6800: Loss = -12386.160117242414
2
Iteration 6900: Loss = -12386.519849384873
3
Iteration 7000: Loss = -12386.15159029139
Iteration 7100: Loss = -12386.16884505838
1
Iteration 7200: Loss = -12386.243903358509
2
Iteration 7300: Loss = -12385.317521208404
Iteration 7400: Loss = -12384.328484355441
Iteration 7500: Loss = -12384.39972958535
1
Iteration 7600: Loss = -12384.20723042143
Iteration 7700: Loss = -12384.203176534329
Iteration 7800: Loss = -12384.193412937098
Iteration 7900: Loss = -12384.191040075915
Iteration 8000: Loss = -12384.188884216892
Iteration 8100: Loss = -12384.191101296256
1
Iteration 8200: Loss = -12384.185278440253
Iteration 8300: Loss = -12384.18453251134
Iteration 8400: Loss = -12384.183624246834
Iteration 8500: Loss = -12384.183547313283
Iteration 8600: Loss = -12384.182462854596
Iteration 8700: Loss = -12384.317407519036
1
Iteration 8800: Loss = -12384.181600414755
Iteration 8900: Loss = -12384.186589317036
1
Iteration 9000: Loss = -12384.181924850238
2
Iteration 9100: Loss = -12384.181372567356
Iteration 9200: Loss = -12384.193533745689
1
Iteration 9300: Loss = -12384.18030719899
Iteration 9400: Loss = -12384.180163379764
Iteration 9500: Loss = -12384.180651800445
1
Iteration 9600: Loss = -12384.179980768635
Iteration 9700: Loss = -12384.180621009136
1
Iteration 9800: Loss = -12384.179934575204
Iteration 9900: Loss = -12384.179442007462
Iteration 10000: Loss = -12384.179288870211
Iteration 10100: Loss = -12384.18550861023
1
Iteration 10200: Loss = -12384.19596206852
2
Iteration 10300: Loss = -12384.183490702553
3
Iteration 10400: Loss = -12384.180636029632
4
Iteration 10500: Loss = -12384.188385166182
5
Iteration 10600: Loss = -12384.178856210092
Iteration 10700: Loss = -12384.179345597684
1
Iteration 10800: Loss = -12384.201839883166
2
Iteration 10900: Loss = -12384.187631385332
3
Iteration 11000: Loss = -12384.181343510601
4
Iteration 11100: Loss = -12384.18285654226
5
Iteration 11200: Loss = -12384.23087972838
6
Iteration 11300: Loss = -12384.19056374729
7
Iteration 11400: Loss = -12384.180372116549
8
Iteration 11500: Loss = -12384.183131097683
9
Iteration 11600: Loss = -12384.281865215766
10
Stopping early at iteration 11600 due to no improvement.
tensor([[-8.9411e-01, -2.4935e+00],
        [ 3.2004e+00, -4.6328e+00],
        [ 3.4868e+00, -6.5955e+00],
        [-1.4855e+00,  9.9129e-02],
        [ 2.8558e+00, -4.4310e+00],
        [ 4.8357e-01, -1.9391e+00],
        [ 3.8106e+00, -5.2223e+00],
        [ 3.6486e+00, -5.4371e+00],
        [ 2.8521e+00, -4.5279e+00],
        [ 3.9348e+00, -5.7041e+00],
        [-9.1365e-01, -4.9243e-01],
        [ 1.7352e+00, -3.3039e+00],
        [ 3.5355e-01, -1.9150e+00],
        [ 1.9222e+00, -3.7539e+00],
        [-2.4241e+00, -2.8935e-01],
        [ 2.6038e+00, -4.0174e+00],
        [ 1.3640e+00, -4.6218e+00],
        [ 2.9021e+00, -4.8361e+00],
        [ 2.4299e+00, -4.6478e+00],
        [ 2.3487e+00, -3.8394e+00],
        [ 1.0214e+00, -4.9258e+00],
        [ 3.2055e+00, -4.5920e+00],
        [ 2.4980e+00, -3.9037e+00],
        [ 2.9211e+00, -4.4260e+00],
        [ 1.8364e+00, -4.2362e+00],
        [ 2.6546e+00, -4.0511e+00],
        [ 2.6876e+00, -4.0813e+00],
        [ 1.6765e+00, -3.5504e+00],
        [-2.6971e-01, -4.3455e+00],
        [ 2.0325e+00, -3.6533e+00],
        [ 3.4979e+00, -5.4182e+00],
        [ 2.2450e+00, -4.1791e+00],
        [ 3.5953e+00, -5.0727e+00],
        [ 1.5888e+00, -3.4006e+00],
        [ 1.3675e+00, -3.6564e+00],
        [-3.5876e-01, -1.0498e+00],
        [ 1.5807e+00, -3.1257e+00],
        [ 1.5409e+00, -3.4311e+00],
        [ 1.4803e+00, -3.6679e+00],
        [ 2.6405e+00, -4.1456e+00],
        [ 1.1535e-01, -1.5445e+00],
        [ 1.7778e+00, -5.0550e+00],
        [ 2.2293e+00, -3.9277e+00],
        [-7.0426e-01, -6.8244e-01],
        [ 1.2482e+00, -2.6833e+00],
        [ 1.3710e+00, -2.7609e+00],
        [-6.7758e-01, -2.9483e+00],
        [ 1.2420e+00, -2.8549e+00],
        [-6.9215e-03, -3.0791e+00],
        [ 1.8669e+00, -5.7565e+00],
        [ 4.0353e-01, -3.3352e+00],
        [ 2.6184e+00, -5.9104e+00],
        [ 3.5426e-01, -1.8382e+00],
        [ 2.0831e+00, -3.6324e+00],
        [ 1.1274e+00, -4.2340e+00],
        [ 1.9591e+00, -3.7451e+00],
        [ 3.0112e+00, -4.6333e+00],
        [ 8.8903e-01, -2.8386e+00],
        [ 3.5013e+00, -5.0440e+00],
        [ 1.7142e+00, -3.1116e+00],
        [ 1.6662e+00, -4.8936e+00],
        [ 6.2863e-01, -2.4984e+00],
        [-2.0414e+00,  3.3363e-01],
        [ 9.8627e-01, -2.8096e+00],
        [ 2.5311e+00, -3.9319e+00],
        [ 1.4573e+00, -2.8781e+00],
        [ 2.3876e+00, -4.1619e+00],
        [ 7.6920e-01, -2.2311e+00],
        [ 4.1242e+00, -5.5116e+00],
        [ 1.3949e+00, -3.4947e+00],
        [ 3.6899e+00, -5.2041e+00],
        [-9.3562e-02, -3.6112e+00],
        [ 4.5134e+00, -5.9410e+00],
        [-1.8556e+00, -7.5763e-01],
        [ 2.8693e+00, -4.2727e+00],
        [ 2.2962e+00, -3.7554e+00],
        [ 1.4474e+00, -3.6919e+00],
        [ 2.1119e+00, -4.1922e+00],
        [ 3.6030e+00, -5.1023e+00],
        [ 2.3866e+00, -4.0427e+00],
        [-5.2822e-02, -1.7118e+00],
        [ 2.3231e+00, -3.7619e+00],
        [ 1.0871e+00, -2.6478e+00],
        [ 2.7701e+00, -4.4273e+00],
        [ 3.1589e+00, -4.5533e+00],
        [ 3.2018e+00, -7.0452e+00],
        [ 8.6142e-02, -4.1431e+00],
        [ 2.6222e+00, -4.2763e+00],
        [ 1.0042e+00, -2.7609e+00],
        [ 6.9546e-02, -3.1089e+00],
        [ 6.6080e-01, -2.1551e+00],
        [ 1.5533e+00, -3.2695e+00],
        [ 1.5576e+00, -3.4372e+00],
        [ 3.4518e+00, -5.0436e+00],
        [ 3.3115e+00, -4.7302e+00],
        [ 2.6631e+00, -4.2990e+00],
        [ 3.4858e+00, -4.9305e+00],
        [ 1.2598e+00, -2.7105e+00],
        [ 1.8695e+00, -3.8445e+00],
        [ 1.8239e+00, -4.0185e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.4208e-05, 9.9999e-01],
        [9.9999e-01, 1.0945e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9375, 0.0625], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1994, 0.1525],
         [0.2704, 0.1950]],

        [[0.1596, 0.2708],
         [0.0451, 0.3365]],

        [[0.6615, 0.1903],
         [0.1363, 0.2203]],

        [[0.8305, 0.2294],
         [0.0207, 0.1654]],

        [[0.8676, 0.2245],
         [0.4511, 0.8796]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.009696969696969697
time is 1
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: -0.007543717590214484
time is 2
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.01405713152211082
time is 3
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.0014658390783536795
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.004682108332720131
Global Adjusted Rand Index: -0.0008153527360286502
Average Adjusted Rand Index: -0.0036103653652858835
Iteration 0: Loss = -43656.763848228264
Iteration 10: Loss = -12386.621857371332
Iteration 20: Loss = -12386.600652016943
Iteration 30: Loss = -12386.577062841461
Iteration 40: Loss = -12386.55062151323
Iteration 50: Loss = -12386.524499896617
Iteration 60: Loss = -12386.502550864825
Iteration 70: Loss = -12386.486098583306
Iteration 80: Loss = -12386.473684532559
Iteration 90: Loss = -12386.462888173635
Iteration 100: Loss = -12386.452534760885
Iteration 110: Loss = -12386.44280122758
Iteration 120: Loss = -12386.43433484471
Iteration 130: Loss = -12386.427498386978
Iteration 140: Loss = -12386.422249600948
Iteration 150: Loss = -12386.418362213202
Iteration 160: Loss = -12386.415540280965
Iteration 170: Loss = -12386.413521444289
Iteration 180: Loss = -12386.41213424989
Iteration 190: Loss = -12386.41117292704
Iteration 200: Loss = -12386.410461155747
Iteration 210: Loss = -12386.410009921243
Iteration 220: Loss = -12386.409742509093
Iteration 230: Loss = -12386.409553079535
Iteration 240: Loss = -12386.409469267128
Iteration 250: Loss = -12386.409422127963
Iteration 260: Loss = -12386.409393427271
Iteration 270: Loss = -12386.409386760655
Iteration 280: Loss = -12386.409431038028
1
Iteration 290: Loss = -12386.409505090849
2
Iteration 300: Loss = -12386.409521086649
3
Stopping early at iteration 299 due to no improvement.
pi: tensor([[3.6168e-07, 1.0000e+00],
        [4.1862e-02, 9.5814e-01]], dtype=torch.float64)
alpha: tensor([0.0401, 0.9599])
beta: tensor([[[0.2196, 0.1994],
         [0.3910, 0.1970]],

        [[0.6496, 0.2424],
         [0.3945, 0.4148]],

        [[0.3583, 0.2083],
         [0.3176, 0.2304]],

        [[0.5745, 0.2057],
         [0.8999, 0.8478]],

        [[0.5090, 0.1928],
         [0.4637, 0.9538]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -43656.494573399475
Iteration 100: Loss = -12439.342047514263
Iteration 200: Loss = -12353.550453649224
Iteration 300: Loss = -12234.32907101527
Iteration 400: Loss = -12105.091512586778
Iteration 500: Loss = -12054.145840393778
Iteration 600: Loss = -12027.073406858559
Iteration 700: Loss = -12013.410861436074
Iteration 800: Loss = -11982.292032902544
Iteration 900: Loss = -11946.593752883133
Iteration 1000: Loss = -11931.869256985929
Iteration 1100: Loss = -11931.683032874114
Iteration 1200: Loss = -11931.565106140064
Iteration 1300: Loss = -11926.721831525201
Iteration 1400: Loss = -11926.412287926867
Iteration 1500: Loss = -11914.808057160179
Iteration 1600: Loss = -11914.760238375366
Iteration 1700: Loss = -11914.727825522306
Iteration 1800: Loss = -11914.7027688452
Iteration 1900: Loss = -11914.682493737133
Iteration 2000: Loss = -11914.665540690175
Iteration 2100: Loss = -11914.650914346485
Iteration 2200: Loss = -11914.636222659035
Iteration 2300: Loss = -11912.542322826213
Iteration 2400: Loss = -11912.33564250634
Iteration 2500: Loss = -11912.322832718532
Iteration 2600: Loss = -11912.312141331699
Iteration 2700: Loss = -11912.299990683907
Iteration 2800: Loss = -11912.291495998865
Iteration 2900: Loss = -11912.286432274603
Iteration 3000: Loss = -11912.281924689376
Iteration 3100: Loss = -11912.27790560819
Iteration 3200: Loss = -11912.274274074092
Iteration 3300: Loss = -11912.271038194951
Iteration 3400: Loss = -11912.268075671616
Iteration 3500: Loss = -11912.265412949473
Iteration 3600: Loss = -11912.262969130998
Iteration 3700: Loss = -11912.26070589022
Iteration 3800: Loss = -11912.258642834144
Iteration 3900: Loss = -11912.271763646573
1
Iteration 4000: Loss = -11912.255038202702
Iteration 4100: Loss = -11912.253464021167
Iteration 4200: Loss = -11912.251971652737
Iteration 4300: Loss = -11912.250645814143
Iteration 4400: Loss = -11912.249350646636
Iteration 4500: Loss = -11912.26050736266
1
Iteration 4600: Loss = -11912.247145173613
Iteration 4700: Loss = -11912.247716156562
1
Iteration 4800: Loss = -11912.247953375847
2
Iteration 4900: Loss = -11912.244360818666
Iteration 5000: Loss = -11912.251264479726
1
Iteration 5100: Loss = -11912.244408292821
2
Iteration 5200: Loss = -11912.250494589216
3
Iteration 5300: Loss = -11912.243997456359
Iteration 5400: Loss = -11912.241700564227
Iteration 5500: Loss = -11912.245023297042
1
Iteration 5600: Loss = -11912.23989445236
Iteration 5700: Loss = -11912.224656993083
Iteration 5800: Loss = -11912.239064155541
1
Iteration 5900: Loss = -11912.223355267657
Iteration 6000: Loss = -11912.223249822307
Iteration 6100: Loss = -11912.228211130447
1
Iteration 6200: Loss = -11912.222512078588
Iteration 6300: Loss = -11912.221912183253
Iteration 6400: Loss = -11912.221682486579
Iteration 6500: Loss = -11912.222661765694
1
Iteration 6600: Loss = -11912.22190460719
2
Iteration 6700: Loss = -11912.22548639843
3
Iteration 6800: Loss = -11912.22260437162
4
Iteration 6900: Loss = -11912.238918248782
5
Iteration 7000: Loss = -11912.219490430047
Iteration 7100: Loss = -11912.221526263143
1
Iteration 7200: Loss = -11912.224709858021
2
Iteration 7300: Loss = -11912.219896406294
3
Iteration 7400: Loss = -11912.223747137947
4
Iteration 7500: Loss = -11912.219433028322
Iteration 7600: Loss = -11912.222670606247
1
Iteration 7700: Loss = -11912.21896105028
Iteration 7800: Loss = -11912.235179150746
1
Iteration 7900: Loss = -11912.22225868782
2
Iteration 8000: Loss = -11912.219358376053
3
Iteration 8100: Loss = -11912.218129945872
Iteration 8200: Loss = -11912.188391132819
Iteration 8300: Loss = -11912.181016491948
Iteration 8400: Loss = -11912.184330636466
1
Iteration 8500: Loss = -11912.183996918118
2
Iteration 8600: Loss = -11912.18063067348
Iteration 8700: Loss = -11912.181737784738
1
Iteration 8800: Loss = -11912.18353351639
2
Iteration 8900: Loss = -11912.183613520507
3
Iteration 9000: Loss = -11912.207980096897
4
Iteration 9100: Loss = -11912.186337701763
5
Iteration 9200: Loss = -11912.17861205277
Iteration 9300: Loss = -11912.189733983474
1
Iteration 9400: Loss = -11912.185089628787
2
Iteration 9500: Loss = -11912.181504024573
3
Iteration 9600: Loss = -11912.178606131885
Iteration 9700: Loss = -11912.185419313035
1
Iteration 9800: Loss = -11912.179011050062
2
Iteration 9900: Loss = -11912.17916296516
3
Iteration 10000: Loss = -11912.1830943606
4
Iteration 10100: Loss = -11912.191512062254
5
Iteration 10200: Loss = -11912.178350625247
Iteration 10300: Loss = -11912.184153809978
1
Iteration 10400: Loss = -11912.177802720826
Iteration 10500: Loss = -11912.181989940422
1
Iteration 10600: Loss = -11912.184182705356
2
Iteration 10700: Loss = -11912.270332174618
3
Iteration 10800: Loss = -11912.17870488386
4
Iteration 10900: Loss = -11912.179141675928
5
Iteration 11000: Loss = -11912.186779932057
6
Iteration 11100: Loss = -11912.177288230807
Iteration 11200: Loss = -11912.17710210407
Iteration 11300: Loss = -11912.178174118797
1
Iteration 11400: Loss = -11912.243556551357
2
Iteration 11500: Loss = -11912.177956106447
3
Iteration 11600: Loss = -11912.178028160522
4
Iteration 11700: Loss = -11912.177227690012
5
Iteration 11800: Loss = -11912.189925902429
6
Iteration 11900: Loss = -11912.188923015696
7
Iteration 12000: Loss = -11912.17978673398
8
Iteration 12100: Loss = -11912.179622386237
9
Iteration 12200: Loss = -11912.182140480243
10
Stopping early at iteration 12200 due to no improvement.
tensor([[-6.1752,  4.7792],
        [ 6.8323, -8.6458],
        [-7.7965,  6.1257],
        [-4.3235,  2.7184],
        [ 4.8403, -6.7421],
        [-1.4375, -0.0241],
        [ 4.7618, -6.2153],
        [-5.3350,  3.6046],
        [-9.2381,  5.0414],
        [-7.9900,  6.5530],
        [ 4.7820, -6.2270],
        [ 6.2939, -8.0987],
        [ 1.1829, -3.1959],
        [-7.7732,  3.1579],
        [ 0.5064, -1.8930],
        [-8.5740,  6.2989],
        [-6.9752,  5.1265],
        [-7.9934,  6.4620],
        [-9.4370,  6.0338],
        [ 5.5833, -7.0826],
        [ 4.0392, -6.3702],
        [ 6.5059, -8.1840],
        [-9.1111,  7.4012],
        [-6.5688,  4.2673],
        [ 5.9187, -7.5423],
        [-7.5009,  6.1128],
        [-6.9466,  5.3463],
        [-7.5691,  6.1774],
        [ 1.0309, -3.2674],
        [ 6.2085, -7.6075],
        [-7.6268,  6.2111],
        [ 5.3424, -7.1467],
        [ 2.1240, -4.0586],
        [-7.9757,  6.3818],
        [-5.5216,  3.9248],
        [-4.9592,  3.2288],
        [ 6.2296, -8.0242],
        [ 2.5580, -3.9446],
        [-8.1663,  6.5226],
        [-4.2598,  2.5595],
        [ 5.5074, -7.5487],
        [-6.6096,  3.6749],
        [ 5.7086, -8.0063],
        [-4.9000,  2.6834],
        [ 3.4554, -4.8731],
        [ 6.6067, -7.9969],
        [-0.4202, -3.9955],
        [-3.2692,  0.8822],
        [-4.7481,  3.3618],
        [-9.9342,  5.7259],
        [-4.4541,  1.0151],
        [ 4.4805, -6.0055],
        [ 3.8854, -5.2768],
        [-8.8076,  6.9890],
        [-6.1460,  4.7256],
        [ 4.4467, -6.0118],
        [-6.3631,  4.9753],
        [ 6.2534, -8.1660],
        [-6.7167,  4.7853],
        [ 5.1192, -6.6194],
        [-7.9699,  5.6253],
        [-8.2467,  6.8456],
        [ 4.1241, -5.5140],
        [-8.4616,  6.7331],
        [-4.3375,  2.4503],
        [ 4.8081, -7.4721],
        [-6.7514,  5.3091],
        [ 5.3237, -7.8328],
        [-5.2978,  3.6315],
        [-9.9374,  5.3222],
        [-8.3967,  6.6285],
        [ 6.6710, -9.3913],
        [-9.7028,  7.5874],
        [ 6.1901, -7.5903],
        [-8.0664,  6.1920],
        [ 6.0111, -9.5395],
        [-7.7707,  6.3217],
        [-6.3474,  4.9564],
        [-8.1457,  6.2067],
        [-5.9867,  4.1814],
        [-7.6216,  5.7141],
        [ 4.7745, -6.2137],
        [ 5.6689, -7.1022],
        [ 5.9137, -9.8957],
        [-8.8037,  6.6891],
        [-7.3141,  5.8343],
        [-8.0759,  6.4173],
        [ 5.6830, -7.0698],
        [ 1.6336, -4.0496],
        [ 5.2542, -6.9735],
        [-5.1952,  2.9128],
        [-8.7555,  7.1475],
        [-4.8095,  3.3023],
        [ 6.6376, -8.4649],
        [-7.7649,  6.2307],
        [-7.2319,  5.3853],
        [-8.3036,  5.0964],
        [ 4.8107, -6.1985],
        [ 3.7237, -5.9331],
        [ 4.4747, -5.9363]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6029, 0.3971],
        [0.4004, 0.5996]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4317, 0.5683], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2993, 0.1019],
         [0.3910, 0.3003]],

        [[0.6496, 0.0964],
         [0.3945, 0.4148]],

        [[0.3583, 0.1062],
         [0.3176, 0.2304]],

        [[0.5745, 0.1036],
         [0.8999, 0.8478]],

        [[0.5090, 0.0910],
         [0.4637, 0.9538]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 1
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 3
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208047711084835
time is 4
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.036492543089442935
Average Adjusted Rand Index: 0.9603222255873082
11862.235998495691
new:  [0.0008577951798631267, 0.0008577951798631267, -0.0008153527360286502, 0.036492543089442935] [0.00028959545153557485, 0.00028959545153557485, -0.0036103653652858835, 0.9603222255873082] [12383.234287766667, 12383.236230418075, 12384.281865215766, 11912.182140480243]
prior:  [0.0, 0.0, 0.0, 0.0] [0.0, 0.0, 0.0, 0.0] [12386.406541160331, 12386.657163622316, 12386.49066571308, 12386.409521086649]
-----------------------------------------------------------------------------------------
This iteration is 4
True Objective function: Loss = -11755.519160573027
Iteration 0: Loss = -69057.89458400682
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.4134,    nan]],

        [[0.4000,    nan],
         [0.9545, 0.8365]],

        [[0.9544,    nan],
         [0.9879, 0.4866]],

        [[0.6055,    nan],
         [0.5332, 0.7530]],

        [[0.0702,    nan],
         [0.7517, 0.5933]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -67705.92053017339
Iteration 100: Loss = -12293.700080925191
Iteration 200: Loss = -12287.85566625064
Iteration 300: Loss = -12285.525483228312
Iteration 400: Loss = -12284.401399424747
Iteration 500: Loss = -12283.751011620358
Iteration 600: Loss = -12283.323953484423
Iteration 700: Loss = -12283.023254884398
Iteration 800: Loss = -12282.801192179824
Iteration 900: Loss = -12282.631358078146
Iteration 1000: Loss = -12282.497802431866
Iteration 1100: Loss = -12282.390432869077
Iteration 1200: Loss = -12282.302377237185
Iteration 1300: Loss = -12282.229106768475
Iteration 1400: Loss = -12282.167325683717
Iteration 1500: Loss = -12282.114481365505
Iteration 1600: Loss = -12282.068849760095
Iteration 1700: Loss = -12282.029031390319
Iteration 1800: Loss = -12281.993939038573
Iteration 1900: Loss = -12281.962757286772
Iteration 2000: Loss = -12281.934777631792
Iteration 2100: Loss = -12281.90953903721
Iteration 2200: Loss = -12281.886678494955
Iteration 2300: Loss = -12281.866030837715
Iteration 2400: Loss = -12281.847511633732
Iteration 2500: Loss = -12281.831257196003
Iteration 2600: Loss = -12281.81722435607
Iteration 2700: Loss = -12281.805098670367
Iteration 2800: Loss = -12281.794581202319
Iteration 2900: Loss = -12281.785311469557
Iteration 3000: Loss = -12281.777023771012
Iteration 3100: Loss = -12281.769478743428
Iteration 3200: Loss = -12281.762618136101
Iteration 3300: Loss = -12281.756359402865
Iteration 3400: Loss = -12281.750531475152
Iteration 3500: Loss = -12281.74519282761
Iteration 3600: Loss = -12281.74024067474
Iteration 3700: Loss = -12281.735633054544
Iteration 3800: Loss = -12281.731336375822
Iteration 3900: Loss = -12281.727392884512
Iteration 4000: Loss = -12281.723739640542
Iteration 4100: Loss = -12281.720256261617
Iteration 4200: Loss = -12281.717034200226
Iteration 4300: Loss = -12281.714042731492
Iteration 4400: Loss = -12281.901963341139
1
Iteration 4500: Loss = -12281.708506215187
Iteration 4600: Loss = -12281.706064033378
Iteration 4700: Loss = -12281.703693213147
Iteration 4800: Loss = -12281.701531257802
Iteration 4900: Loss = -12281.699411423919
Iteration 5000: Loss = -12281.697442623697
Iteration 5100: Loss = -12281.696345546125
Iteration 5200: Loss = -12281.693803386019
Iteration 5300: Loss = -12281.69212207899
Iteration 5400: Loss = -12281.690474682273
Iteration 5500: Loss = -12281.694376214798
1
Iteration 5600: Loss = -12281.687524549454
Iteration 5700: Loss = -12281.686136282584
Iteration 5800: Loss = -12281.684804108974
Iteration 5900: Loss = -12281.710387136498
1
Iteration 6000: Loss = -12281.682286686657
Iteration 6100: Loss = -12281.681083266662
Iteration 6200: Loss = -12281.679912538591
Iteration 6300: Loss = -12281.680081785613
1
Iteration 6400: Loss = -12281.677706064196
Iteration 6500: Loss = -12281.676592063957
Iteration 6600: Loss = -12281.67556002993
Iteration 6700: Loss = -12281.674623709358
Iteration 6800: Loss = -12281.673503895032
Iteration 6900: Loss = -12281.672504495189
Iteration 7000: Loss = -12281.679767644047
1
Iteration 7100: Loss = -12281.670542516313
Iteration 7200: Loss = -12281.669598954413
Iteration 7300: Loss = -12281.668641944994
Iteration 7400: Loss = -12281.77629942572
1
Iteration 7500: Loss = -12281.666837529305
Iteration 7600: Loss = -12281.666008311373
Iteration 7700: Loss = -12281.66514179977
Iteration 7800: Loss = -12281.67296421159
1
Iteration 7900: Loss = -12281.663583725129
Iteration 8000: Loss = -12281.66286302377
Iteration 8100: Loss = -12281.662188849483
Iteration 8200: Loss = -12281.662879210831
1
Iteration 8300: Loss = -12281.660875891972
Iteration 8400: Loss = -12281.660272869232
Iteration 8500: Loss = -12281.972007353192
1
Iteration 8600: Loss = -12281.65914619323
Iteration 8700: Loss = -12281.658598052612
Iteration 8800: Loss = -12281.658107857498
Iteration 8900: Loss = -12281.742469600256
1
Iteration 9000: Loss = -12281.657108171634
Iteration 9100: Loss = -12281.65664030161
Iteration 9200: Loss = -12281.65615273879
Iteration 9300: Loss = -12281.660164135663
1
Iteration 9400: Loss = -12281.655298291505
Iteration 9500: Loss = -12281.654802442526
Iteration 9600: Loss = -12281.664980947688
1
Iteration 9700: Loss = -12281.65384849574
Iteration 9800: Loss = -12281.653400022611
Iteration 9900: Loss = -12281.652897879534
Iteration 10000: Loss = -12281.66076809569
1
Iteration 10100: Loss = -12281.651774898359
Iteration 10200: Loss = -12281.651232686447
Iteration 10300: Loss = -12281.650553668163
Iteration 10400: Loss = -12281.652584382255
1
Iteration 10500: Loss = -12281.64917908375
Iteration 10600: Loss = -12281.648342938446
Iteration 10700: Loss = -12281.64752555538
Iteration 10800: Loss = -12281.660219144265
1
Iteration 10900: Loss = -12281.64538949432
Iteration 11000: Loss = -12281.644146788058
Iteration 11100: Loss = -12281.642666606629
Iteration 11200: Loss = -12281.641028669983
Iteration 11300: Loss = -12281.638961418123
Iteration 11400: Loss = -12281.636400392517
Iteration 11500: Loss = -12281.633368155824
Iteration 11600: Loss = -12281.629378536976
Iteration 11700: Loss = -12281.624094625397
Iteration 11800: Loss = -12281.616747117041
Iteration 11900: Loss = -12281.606155352412
Iteration 12000: Loss = -12281.586077582991
Iteration 12100: Loss = -12281.544000604588
Iteration 12200: Loss = -12281.398670295976
Iteration 12300: Loss = -12280.915883405927
Iteration 12400: Loss = -12280.730922257437
Iteration 12500: Loss = -12279.90164650818
Iteration 12600: Loss = -12279.78524711621
Iteration 12700: Loss = -12279.663461786591
Iteration 12800: Loss = -12279.654614081797
Iteration 12900: Loss = -12279.653033738025
Iteration 13000: Loss = -12279.651333902986
Iteration 13100: Loss = -12279.650922159053
Iteration 13200: Loss = -12279.656264981577
1
Iteration 13300: Loss = -12279.816544079049
2
Iteration 13400: Loss = -12279.662260844514
3
Iteration 13500: Loss = -12279.659134072972
4
Iteration 13600: Loss = -12279.655245460766
5
Iteration 13700: Loss = -12279.650198296891
Iteration 13800: Loss = -12279.653710518123
1
Iteration 13900: Loss = -12279.650353860912
2
Iteration 14000: Loss = -12279.65013829945
Iteration 14100: Loss = -12279.7090197532
1
Iteration 14200: Loss = -12279.653956475431
2
Iteration 14300: Loss = -12279.663002860136
3
Iteration 14400: Loss = -12279.650626521852
4
Iteration 14500: Loss = -12279.6528311689
5
Iteration 14600: Loss = -12279.650449510147
6
Iteration 14700: Loss = -12279.660509806168
7
Iteration 14800: Loss = -12279.650144585647
8
Iteration 14900: Loss = -12279.65003558738
Iteration 15000: Loss = -12279.65013960058
1
Iteration 15100: Loss = -12279.649911878098
Iteration 15200: Loss = -12279.66073386642
1
Iteration 15300: Loss = -12279.64984568286
Iteration 15400: Loss = -12279.688933518004
1
Iteration 15500: Loss = -12279.649874766597
2
Iteration 15600: Loss = -12279.701111773178
3
Iteration 15700: Loss = -12279.650560936612
4
Iteration 15800: Loss = -12279.650138070496
5
Iteration 15900: Loss = -12279.67343810747
6
Iteration 16000: Loss = -12279.651705684451
7
Iteration 16100: Loss = -12279.700569584978
8
Iteration 16200: Loss = -12279.649887953958
9
Iteration 16300: Loss = -12279.649897326011
10
Stopping early at iteration 16300 due to no improvement.
tensor([[-2.5058, -2.1094],
        [-2.4254, -2.1898],
        [-2.4535, -2.1617],
        [-2.5842, -2.0311],
        [-2.4936, -2.1216],
        [-2.6303, -1.9849],
        [-2.5636, -2.0516],
        [-2.4471, -2.1681],
        [-2.5265, -2.0887],
        [-2.5251, -2.0901],
        [-2.4672, -2.1481],
        [-2.5862, -2.0291],
        [-2.5436, -2.0717],
        [-2.4260, -2.1892],
        [-2.4622, -2.1530],
        [-2.4872, -2.1280],
        [-2.3090, -2.3062],
        [-2.5645, -2.0507],
        [-2.6611, -1.9542],
        [-2.4640, -2.1512],
        [-2.4302, -2.1850],
        [-2.5854, -2.0298],
        [-2.6093, -2.0059],
        [-2.5656, -2.0497],
        [-2.5480, -2.0672],
        [-2.4464, -2.1689],
        [-2.5062, -2.1090],
        [-2.6466, -1.9686],
        [-2.5449, -2.0703],
        [-2.4819, -2.1333],
        [-2.5057, -2.1095],
        [-2.3861, -2.2291],
        [-2.5237, -2.0915],
        [-2.3430, -2.2722],
        [-2.4432, -2.1720],
        [-2.4678, -2.1474],
        [-2.5469, -2.0683],
        [-2.6660, -1.9492],
        [-2.5637, -2.0515],
        [-2.4486, -2.1666],
        [-2.5032, -2.1121],
        [-3.1859, -1.4294],
        [-2.5358, -2.0794],
        [-2.6048, -2.0105],
        [-2.4233, -2.1919],
        [-2.5295, -2.0857],
        [-2.4718, -2.1434],
        [-3.1386, -1.4767],
        [-2.6438, -1.9715],
        [-2.5641, -2.0511],
        [-2.5243, -2.0910],
        [-2.5374, -2.0779],
        [-2.5067, -2.1085],
        [-2.4424, -2.1728],
        [-2.2838, -2.3315],
        [-2.5635, -2.0517],
        [-2.4215, -2.1937],
        [-2.6270, -1.9882],
        [-2.5361, -2.0791],
        [-2.5227, -2.0926],
        [-2.4315, -2.1837],
        [-2.4439, -2.1714],
        [-2.5117, -2.1035],
        [-2.5249, -2.0903],
        [-2.3830, -2.2322],
        [-2.4401, -2.1751],
        [-2.4678, -2.1475],
        [-2.6251, -1.9901],
        [-2.6068, -2.0084],
        [-2.5041, -2.1111],
        [-2.5411, -2.0742],
        [-2.5259, -2.0894],
        [-2.4635, -2.1517],
        [-2.4861, -2.1291],
        [-2.3609, -2.2543],
        [-2.5312, -2.0840],
        [-2.4678, -2.1475],
        [-2.4048, -2.2105],
        [-2.4654, -2.1498],
        [-2.4866, -2.1286],
        [-2.4410, -2.1742],
        [-2.5057, -2.1096],
        [-2.3056, -2.3096],
        [-2.3629, -2.2523],
        [-2.4852, -2.1300],
        [-2.4449, -2.1703],
        [-2.5694, -2.0458],
        [-2.6046, -2.0106],
        [-2.4609, -2.1543],
        [-2.4939, -2.1213],
        [-2.4659, -2.1493],
        [-2.3841, -2.2312],
        [-2.5616, -2.0536],
        [-2.5869, -2.0283],
        [-2.4635, -2.1517],
        [-2.3947, -2.2205],
        [-2.4458, -2.1695],
        [-2.4327, -2.1826],
        [-2.5033, -2.1120],
        [-2.4645, -2.1507]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 9.5111e-08],
        [9.6197e-01, 3.8030e-02]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4020, 0.5980], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1978, 0.1906],
         [0.4134, 0.1851]],

        [[0.4000, 0.2976],
         [0.9545, 0.8365]],

        [[0.9544, 0.1272],
         [0.9879, 0.4866]],

        [[0.6055, 0.6716],
         [0.5332, 0.7530]],

        [[0.0702, 0.2075],
         [0.7517, 0.5933]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.0037221143673841777
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: -0.009009494263563287
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -3.82715035040419e-05
Average Adjusted Rand Index: -0.002546321726189493
Iteration 0: Loss = -19366.958208632353
Iteration 10: Loss = -12280.533247206607
Iteration 20: Loss = -12280.473094259116
Iteration 30: Loss = -12280.460443569913
Iteration 40: Loss = -12280.453632441293
Iteration 50: Loss = -12280.447749351419
Iteration 60: Loss = -12280.441986373282
Iteration 70: Loss = -12280.436122685256
Iteration 80: Loss = -12280.430324109086
Iteration 90: Loss = -12280.424488502775
Iteration 100: Loss = -12280.418755548659
Iteration 110: Loss = -12280.413049043611
Iteration 120: Loss = -12280.407484145959
Iteration 130: Loss = -12280.401961377842
Iteration 140: Loss = -12280.396536212178
Iteration 150: Loss = -12280.39123264037
Iteration 160: Loss = -12280.386040594753
Iteration 170: Loss = -12280.381031683517
Iteration 180: Loss = -12280.376345840927
Iteration 190: Loss = -12280.37194122263
Iteration 200: Loss = -12280.367869516493
Iteration 210: Loss = -12280.364389151051
Iteration 220: Loss = -12280.361430404804
Iteration 230: Loss = -12280.35922999796
Iteration 240: Loss = -12280.35793500414
Iteration 250: Loss = -12280.357718186258
Iteration 260: Loss = -12280.358645464823
1
Iteration 270: Loss = -12280.36101916506
2
Iteration 280: Loss = -12280.364819002036
3
Stopping early at iteration 279 due to no improvement.
pi: tensor([[0.2074, 0.7926],
        [0.1193, 0.8807]], dtype=torch.float64)
alpha: tensor([0.1326, 0.8674])
beta: tensor([[[0.1659, 0.1677],
         [0.9447, 0.1998]],

        [[0.2166, 0.2020],
         [0.3496, 0.5628]],

        [[0.7079, 0.1697],
         [0.4222, 0.1770]],

        [[0.1916, 0.1838],
         [0.9759, 0.9900]],

        [[0.7587, 0.1867],
         [0.9149, 0.6562]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19366.612311497065
Iteration 100: Loss = -12335.633850668963
Iteration 200: Loss = -12300.897810993294
Iteration 300: Loss = -12283.230363985713
Iteration 400: Loss = -12281.636037672803
Iteration 500: Loss = -12281.051633173069
Iteration 600: Loss = -12280.720612504825
Iteration 700: Loss = -12280.498465940203
Iteration 800: Loss = -12280.334040018219
Iteration 900: Loss = -12280.202806430047
Iteration 1000: Loss = -12280.096303330498
Iteration 1100: Loss = -12280.014138345752
Iteration 1200: Loss = -12279.95319639804
Iteration 1300: Loss = -12279.909071635002
Iteration 1400: Loss = -12279.877548316204
Iteration 1500: Loss = -12279.855211648724
Iteration 1600: Loss = -12279.838762228266
Iteration 1700: Loss = -12279.826253765545
Iteration 1800: Loss = -12279.816363125063
Iteration 1900: Loss = -12279.808310811677
Iteration 2000: Loss = -12279.801546454208
Iteration 2100: Loss = -12279.795760134733
Iteration 2200: Loss = -12279.790793026634
Iteration 2300: Loss = -12279.786354145988
Iteration 2400: Loss = -12279.782443548693
Iteration 2500: Loss = -12279.778819692567
Iteration 2600: Loss = -12279.775598310704
Iteration 2700: Loss = -12279.772476511003
Iteration 2800: Loss = -12279.76958877521
Iteration 2900: Loss = -12279.766751702735
Iteration 3000: Loss = -12279.764032254529
Iteration 3100: Loss = -12279.761441587596
Iteration 3200: Loss = -12279.75898540987
Iteration 3300: Loss = -12279.756570600191
Iteration 3400: Loss = -12279.754160832928
Iteration 3500: Loss = -12279.751624166942
Iteration 3600: Loss = -12279.748983909532
Iteration 3700: Loss = -12279.746062950435
Iteration 3800: Loss = -12279.742834944382
Iteration 3900: Loss = -12279.73900008892
Iteration 4000: Loss = -12279.73432660973
Iteration 4100: Loss = -12279.728253150332
Iteration 4200: Loss = -12279.72031903544
Iteration 4300: Loss = -12279.710378893593
Iteration 4400: Loss = -12279.69879696218
Iteration 4500: Loss = -12279.687664996834
Iteration 4600: Loss = -12279.676783969171
Iteration 4700: Loss = -12279.66904269836
Iteration 4800: Loss = -12279.907119024894
1
Iteration 4900: Loss = -12279.656228567756
Iteration 5000: Loss = -12279.651528497421
Iteration 5100: Loss = -12279.644298535659
Iteration 5200: Loss = -12279.634977004358
Iteration 5300: Loss = -12279.61913018932
Iteration 5400: Loss = -12279.61038179684
Iteration 5500: Loss = -12279.612503333276
1
Iteration 5600: Loss = -12279.45954937656
Iteration 5700: Loss = -12279.38949904613
Iteration 5800: Loss = -12279.360110548474
Iteration 5900: Loss = -12279.320014173585
Iteration 6000: Loss = -12279.275290052441
Iteration 6100: Loss = -12279.259096972588
Iteration 6200: Loss = -12279.264563950162
1
Iteration 6300: Loss = -12279.241611300995
Iteration 6400: Loss = -12279.231967693822
Iteration 6500: Loss = -12279.229511164289
Iteration 6600: Loss = -12279.226728329724
Iteration 6700: Loss = -12279.2343408595
1
Iteration 6800: Loss = -12279.225488203449
Iteration 6900: Loss = -12279.223329575714
Iteration 7000: Loss = -12279.230053405721
1
Iteration 7100: Loss = -12279.222887195794
Iteration 7200: Loss = -12279.224171965048
1
Iteration 7300: Loss = -12279.222469234082
Iteration 7400: Loss = -12279.222718821593
1
Iteration 7500: Loss = -12279.221843308473
Iteration 7600: Loss = -12279.221365366982
Iteration 7700: Loss = -12279.218914384346
Iteration 7800: Loss = -12279.191177081926
Iteration 7900: Loss = -12279.057429805476
Iteration 8000: Loss = -12278.986981576647
Iteration 8100: Loss = -12278.983701646233
Iteration 8200: Loss = -12278.984380721504
1
Iteration 8300: Loss = -12278.983624557135
Iteration 8400: Loss = -12278.983599173236
Iteration 8500: Loss = -12279.059984169506
1
Iteration 8600: Loss = -12278.983500311027
Iteration 8700: Loss = -12278.983610917261
1
Iteration 8800: Loss = -12278.984446025706
2
Iteration 8900: Loss = -12278.983890529282
3
Iteration 9000: Loss = -12278.98532613632
4
Iteration 9100: Loss = -12278.99053782243
5
Iteration 9200: Loss = -12279.054413968615
6
Iteration 9300: Loss = -12279.011282988542
7
Iteration 9400: Loss = -12278.985259685736
8
Iteration 9500: Loss = -12279.001114245966
9
Iteration 9600: Loss = -12278.990886616552
10
Stopping early at iteration 9600 due to no improvement.
tensor([[  4.8924,  -7.0049],
        [  6.2537,  -7.7165],
        [  4.9288,  -8.6807],
        [  5.1970,  -6.5853],
        [  5.3322,  -6.7513],
        [  5.1093,  -6.5928],
        [  5.0336,  -6.6463],
        [  5.8035,  -8.2009],
        [  5.3446,  -6.7309],
        [  6.2147,  -7.6503],
        [  4.5697,  -8.6016],
        [  5.3952,  -7.8382],
        [  4.8465,  -6.7426],
        [  5.1743,  -7.9152],
        [  4.0474,  -8.6626],
        [  5.2571,  -6.7980],
        [  5.7670,  -7.2003],
        [  5.1453,  -6.9525],
        [  3.1343,  -7.3625],
        [  5.6062,  -8.1553],
        [  5.3339,  -7.0655],
        [  3.6735,  -8.2887],
        [  4.7963,  -6.8545],
        [  5.1290,  -6.7047],
        [  5.1801,  -7.0049],
        [  5.8820,  -8.4069],
        [  5.1967,  -6.5836],
        [  6.1295,  -7.6713],
        [  6.4675,  -7.9569],
        [  5.2853,  -7.1974],
        [  5.8930,  -8.4167],
        [  6.4266,  -8.0473],
        [  5.2959,  -6.7024],
        [  5.9168,  -7.3814],
        [  5.4648,  -7.0556],
        [  5.7598,  -7.2020],
        [  4.5359,  -6.5611],
        [  5.0498,  -6.6443],
        [  5.2177,  -6.6270],
        [  5.5506,  -7.0907],
        [  5.4346,  -7.0328],
        [  2.5490,  -6.4432],
        [  5.1698,  -7.0905],
        [  4.9984,  -6.5911],
        [  5.5242,  -6.9250],
        [  5.3103,  -6.7700],
        [  5.3317,  -7.0251],
        [  3.2491,  -5.0349],
        [  5.0075,  -6.4963],
        [  4.7917,  -6.1789],
        [  5.2351,  -6.6282],
        [  4.5184,  -7.3867],
        [  5.7361,  -7.5857],
        [  4.5817,  -8.2419],
        [  5.8265,  -7.5645],
        [  4.2354,  -7.9032],
        [  5.5388,  -7.0617],
        [  4.7847,  -7.1068],
        [  5.6419,  -7.0742],
        [  5.4604,  -6.8491],
        [  5.1660,  -7.9982],
        [  5.5444,  -6.9634],
        [  5.1688,  -6.7798],
        [  4.5139,  -7.4473],
        [  6.5940,  -8.0239],
        [  5.4953,  -6.9546],
        [  5.4649,  -6.9260],
        [  5.1489,  -6.5352],
        [  5.0500,  -6.8079],
        [  5.3518,  -6.7648],
        [  5.0917,  -6.9312],
        [  4.9118,  -7.4944],
        [  5.4000,  -6.7886],
        [  5.4535,  -6.9114],
        [  5.5664,  -7.0004],
        [  5.4379,  -6.8639],
        [  5.4289,  -6.9610],
        [  5.0033,  -7.5953],
        [  6.0958,  -7.5935],
        [  5.2110,  -6.7662],
        [  5.1797,  -7.3966],
        [  5.1512,  -6.9128],
        [  5.5451,  -7.0403],
        [  5.8654,  -7.2724],
        [  5.0143,  -7.2937],
        [  5.0077,  -7.3692],
        [  5.0627,  -7.0344],
        [  5.0818,  -6.4990],
        [  4.7580,  -7.1519],
        [  5.3569,  -6.8255],
        [  5.3291,  -6.8573],
        [  5.4507,  -6.8753],
        [  4.8385,  -6.7041],
        [  4.8316,  -7.1052],
        [  5.6897, -10.1534],
        [  5.4089,  -6.9528],
        [  6.1451,  -7.7571],
        [  5.4077,  -8.1774],
        [  4.9383,  -7.5355],
        [  4.6968,  -7.6947]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.0101, 0.9899],
        [0.6091, 0.3909]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9999e-01, 8.6423e-06], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1897, 0.1745],
         [0.9447, 0.2029]],

        [[0.2166, 0.3083],
         [0.3496, 0.5628]],

        [[0.7079, 0.1928],
         [0.4222, 0.1770]],

        [[0.1916, 0.1962],
         [0.9759, 0.9900]],

        [[0.7587, 0.1977],
         [0.9149, 0.6562]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.002193311857154732
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.02673385152382511
Global Adjusted Rand Index: -0.0006746506725940951
Average Adjusted Rand Index: 0.004908107933334076
Iteration 0: Loss = -23253.39487401529
Iteration 10: Loss = -12281.985335875179
Iteration 20: Loss = -12281.985348683476
1
Iteration 30: Loss = -12281.985214860724
Iteration 40: Loss = -12281.917118889409
Iteration 50: Loss = -12280.535430163
Iteration 60: Loss = -12280.388992841128
Iteration 70: Loss = -12280.372725431605
Iteration 80: Loss = -12280.366281517414
Iteration 90: Loss = -12280.345753101252
Iteration 100: Loss = -12280.332033378381
Iteration 110: Loss = -12280.329305714731
Iteration 120: Loss = -12280.32862126118
Iteration 130: Loss = -12280.328366055828
Iteration 140: Loss = -12280.328232116262
Iteration 150: Loss = -12280.328202828523
Iteration 160: Loss = -12280.328215979356
1
Iteration 170: Loss = -12280.32816342925
Iteration 180: Loss = -12280.328151562398
Iteration 190: Loss = -12280.3281825822
1
Iteration 200: Loss = -12280.328187576684
2
Iteration 210: Loss = -12280.328171629904
3
Stopping early at iteration 209 due to no improvement.
pi: tensor([[0.9827, 0.0173],
        [0.9649, 0.0351]], dtype=torch.float64)
alpha: tensor([0.9821, 0.0179])
beta: tensor([[[0.1941, 0.1457],
         [0.3666, 0.2091]],

        [[0.9436, 0.3031],
         [0.2604, 0.1410]],

        [[0.3145, 0.1458],
         [0.1676, 0.8642]],

        [[0.2115, 0.2277],
         [0.8887, 0.9184]],

        [[0.8230, 0.2860],
         [0.0034, 0.6350]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: -0.009009494263563287
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.006509078451524495
Global Adjusted Rand Index: 0.00038770756378241793
Average Adjusted Rand Index: -0.0005000831624077586
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23253.13835393459
Iteration 100: Loss = -12290.271031046892
Iteration 200: Loss = -12282.779748091993
Iteration 300: Loss = -12281.833668031784
Iteration 400: Loss = -12281.309969068732
Iteration 500: Loss = -12281.00044890625
Iteration 600: Loss = -12280.799440115894
Iteration 700: Loss = -12280.657624197
Iteration 800: Loss = -12280.550761374294
Iteration 900: Loss = -12280.467109803018
Iteration 1000: Loss = -12280.400052800413
Iteration 1100: Loss = -12280.345830342741
Iteration 1200: Loss = -12280.301850558624
Iteration 1300: Loss = -12280.2657377137
Iteration 1400: Loss = -12280.235687724002
Iteration 1500: Loss = -12280.209840360993
Iteration 1600: Loss = -12280.186999563075
Iteration 1700: Loss = -12280.16646742592
Iteration 1800: Loss = -12280.14782931817
Iteration 1900: Loss = -12280.130978535599
Iteration 2000: Loss = -12280.115751098454
Iteration 2100: Loss = -12280.101889618643
Iteration 2200: Loss = -12280.08894735192
Iteration 2300: Loss = -12280.076363296548
Iteration 2400: Loss = -12280.061679907261
Iteration 2500: Loss = -12280.023258199395
Iteration 2600: Loss = -12279.99941184938
Iteration 2700: Loss = -12279.9906973886
Iteration 2800: Loss = -12279.98314710961
Iteration 2900: Loss = -12279.97605633115
Iteration 3000: Loss = -12279.96864491596
Iteration 3100: Loss = -12279.960821237066
Iteration 3200: Loss = -12279.9536321993
Iteration 3300: Loss = -12279.947459011506
Iteration 3400: Loss = -12279.941244079007
Iteration 3500: Loss = -12279.934985259597
Iteration 3600: Loss = -12279.928440207215
Iteration 3700: Loss = -12279.921565457513
Iteration 3800: Loss = -12279.914104444957
Iteration 3900: Loss = -12279.90597205082
Iteration 4000: Loss = -12279.927289627169
1
Iteration 4100: Loss = -12279.88573994029
Iteration 4200: Loss = -12279.871699412342
Iteration 4300: Loss = -12279.953481484596
1
Iteration 4400: Loss = -12279.816710322648
Iteration 4500: Loss = -12279.73589662338
Iteration 4600: Loss = -12279.545336992896
Iteration 4700: Loss = -12279.345077844113
Iteration 4800: Loss = -12279.1819667242
Iteration 4900: Loss = -12279.080382607974
Iteration 5000: Loss = -12279.049106012622
Iteration 5100: Loss = -12278.954836221981
Iteration 5200: Loss = -12278.914891278415
Iteration 5300: Loss = -12278.883670364508
Iteration 5400: Loss = -12278.87969684296
Iteration 5500: Loss = -12278.838645456543
Iteration 5600: Loss = -12278.821899154666
Iteration 5700: Loss = -12278.807707033557
Iteration 5800: Loss = -12278.797639612494
Iteration 5900: Loss = -12278.785809576813
Iteration 6000: Loss = -12278.777316456592
Iteration 6100: Loss = -12278.868447286954
1
Iteration 6200: Loss = -12278.762849491999
Iteration 6300: Loss = -12278.75845929937
Iteration 6400: Loss = -12278.750573007404
Iteration 6500: Loss = -12278.747933388137
Iteration 6600: Loss = -12278.741240884034
Iteration 6700: Loss = -12278.737641082967
Iteration 6800: Loss = -12278.734197249361
Iteration 6900: Loss = -12278.881146102953
1
Iteration 7000: Loss = -12278.727018485635
Iteration 7100: Loss = -12278.724541962496
Iteration 7200: Loss = -12278.72190027344
Iteration 7300: Loss = -12278.719385597706
Iteration 7400: Loss = -12278.725874826203
1
Iteration 7500: Loss = -12278.715816413473
Iteration 7600: Loss = -12278.714450722977
Iteration 7700: Loss = -12278.71445172567
1
Iteration 7800: Loss = -12278.7120550299
Iteration 7900: Loss = -12278.711016503376
Iteration 8000: Loss = -12278.711832190249
1
Iteration 8100: Loss = -12278.709191853237
Iteration 8200: Loss = -12278.708358150852
Iteration 8300: Loss = -12278.70883213475
1
Iteration 8400: Loss = -12278.706863396703
Iteration 8500: Loss = -12278.706933928068
1
Iteration 8600: Loss = -12278.705352967112
Iteration 8700: Loss = -12278.704436964597
Iteration 8800: Loss = -12278.703265180391
Iteration 8900: Loss = -12278.702586365253
Iteration 9000: Loss = -12278.70157516126
Iteration 9100: Loss = -12278.713601414054
1
Iteration 9200: Loss = -12278.699654429449
Iteration 9300: Loss = -12278.699175391564
Iteration 9400: Loss = -12278.699274234748
1
Iteration 9500: Loss = -12278.698404547877
Iteration 9600: Loss = -12278.7920000445
1
Iteration 9700: Loss = -12278.69777487744
Iteration 9800: Loss = -12278.711993562465
1
Iteration 9900: Loss = -12278.697181758376
Iteration 10000: Loss = -12278.708448186862
1
Iteration 10100: Loss = -12278.696662258471
Iteration 10200: Loss = -12278.739358644925
1
Iteration 10300: Loss = -12278.696199143491
Iteration 10400: Loss = -12278.696002287617
Iteration 10500: Loss = -12278.747254096086
1
Iteration 10600: Loss = -12278.695771792658
Iteration 10700: Loss = -12278.695695865648
Iteration 10800: Loss = -12278.697681107698
1
Iteration 10900: Loss = -12278.69607377265
2
Iteration 11000: Loss = -12278.883707848858
3
Iteration 11100: Loss = -12278.694850671283
Iteration 11200: Loss = -12278.69466238954
Iteration 11300: Loss = -12278.699990108202
1
Iteration 11400: Loss = -12278.694212163906
Iteration 11500: Loss = -12278.696135262657
1
Iteration 11600: Loss = -12278.693801465626
Iteration 11700: Loss = -12278.69364728443
Iteration 11800: Loss = -12278.693743759291
1
Iteration 11900: Loss = -12278.693463810627
Iteration 12000: Loss = -12278.703325427157
1
Iteration 12100: Loss = -12278.692935666717
Iteration 12200: Loss = -12278.692673523201
Iteration 12300: Loss = -12278.695046463648
1
Iteration 12400: Loss = -12278.691755117026
Iteration 12500: Loss = -12278.69100482293
Iteration 12600: Loss = -12278.691003377757
Iteration 12700: Loss = -12278.69088522516
Iteration 12800: Loss = -12278.690784963741
Iteration 12900: Loss = -12278.691105093752
1
Iteration 13000: Loss = -12278.690463143894
Iteration 13100: Loss = -12278.69894575001
1
Iteration 13200: Loss = -12278.690044640032
Iteration 13300: Loss = -12278.690152687552
1
Iteration 13400: Loss = -12278.772321590332
2
Iteration 13500: Loss = -12278.698416192547
3
Iteration 13600: Loss = -12278.866659800635
4
Iteration 13700: Loss = -12278.690015966478
Iteration 13800: Loss = -12278.694241117613
1
Iteration 13900: Loss = -12278.689437806313
Iteration 14000: Loss = -12278.68848677913
Iteration 14100: Loss = -12278.69155410631
1
Iteration 14200: Loss = -12278.688462616117
Iteration 14300: Loss = -12278.688373296005
Iteration 14400: Loss = -12278.688747818915
1
Iteration 14500: Loss = -12278.687643391975
Iteration 14600: Loss = -12278.686238862232
Iteration 14700: Loss = -12278.714719768592
1
Iteration 14800: Loss = -12278.686085466823
Iteration 14900: Loss = -12278.930513273277
1
Iteration 15000: Loss = -12278.686058481313
Iteration 15100: Loss = -12278.686073197796
1
Iteration 15200: Loss = -12278.688853667785
2
Iteration 15300: Loss = -12278.687120730276
3
Iteration 15400: Loss = -12278.685657512713
Iteration 15500: Loss = -12278.863741316365
1
Iteration 15600: Loss = -12278.685618246842
Iteration 15700: Loss = -12278.68561134175
Iteration 15800: Loss = -12278.75298603059
1
Iteration 15900: Loss = -12278.685594144437
Iteration 16000: Loss = -12278.78038949066
1
Iteration 16100: Loss = -12278.687335328095
2
Iteration 16200: Loss = -12278.907546087676
3
Iteration 16300: Loss = -12278.686551714944
4
Iteration 16400: Loss = -12278.685822275482
5
Iteration 16500: Loss = -12278.684628029368
Iteration 16600: Loss = -12278.878766174787
1
Iteration 16700: Loss = -12278.683720993875
Iteration 16800: Loss = -12278.699366524566
1
Iteration 16900: Loss = -12278.683696965463
Iteration 17000: Loss = -12278.735497475696
1
Iteration 17100: Loss = -12278.683706424597
2
Iteration 17200: Loss = -12278.683833069457
3
Iteration 17300: Loss = -12278.6836408729
Iteration 17400: Loss = -12278.684068602708
1
Iteration 17500: Loss = -12278.68718240301
2
Iteration 17600: Loss = -12278.68360044542
Iteration 17700: Loss = -12278.683747453742
1
Iteration 17800: Loss = -12278.683498321621
Iteration 17900: Loss = -12278.685156245154
1
Iteration 18000: Loss = -12278.689019118587
2
Iteration 18100: Loss = -12278.683871640327
3
Iteration 18200: Loss = -12278.685589410417
4
Iteration 18300: Loss = -12278.685629263919
5
Iteration 18400: Loss = -12278.684406328002
6
Iteration 18500: Loss = -12278.683606466519
7
Iteration 18600: Loss = -12278.684253939253
8
Iteration 18700: Loss = -12278.683508441072
9
Iteration 18800: Loss = -12278.68409411961
10
Stopping early at iteration 18800 due to no improvement.
tensor([[ 2.0481, -4.2694],
        [ 1.3630, -3.2363],
        [ 1.0497, -2.5133],
        [ 6.0644, -8.2163],
        [ 1.2191, -2.6575],
        [ 1.4425, -2.8309],
        [ 1.5536, -5.7914],
        [ 2.8998, -4.4220],
        [ 2.4330, -4.0968],
        [ 6.1942, -8.3299],
        [ 6.0101, -8.2582],
        [ 3.2721, -4.8267],
        [ 2.0518, -3.5704],
        [ 1.6951, -3.0893],
        [ 6.5571, -7.9435],
        [ 1.6815, -3.6567],
        [ 2.2157, -5.3697],
        [ 3.1203, -4.5573],
        [-0.7014, -1.5296],
        [ 2.8482, -4.3682],
        [ 0.9927, -3.0374],
        [ 2.2863, -4.1791],
        [ 2.4969, -3.9366],
        [ 2.6815, -4.1713],
        [ 1.7809, -3.8032],
        [ 2.2441, -3.9179],
        [ 1.9603, -4.6410],
        [ 6.2984, -7.7399],
        [ 6.0175, -8.1065],
        [ 0.7773, -2.8887],
        [ 2.1845, -3.5750],
        [ 1.9520, -3.7819],
        [ 6.5336, -8.0532],
        [ 6.7811, -8.1685],
        [ 3.4463, -5.4303],
        [ 1.3555, -3.9727],
        [ 0.6028, -3.6151],
        [ 3.2585, -4.6634],
        [ 0.4815, -2.3679],
        [ 3.4066, -4.8279],
        [ 4.1959, -5.9817],
        [-2.1275,  0.1416],
        [-0.6888, -3.5199],
        [ 6.3042, -8.0484],
        [ 6.3118, -8.0084],
        [ 2.7891, -4.4154],
        [ 1.7518, -3.2405],
        [-1.7840,  0.3492],
        [ 2.3264, -3.8455],
        [-0.3752, -1.0845],
        [ 2.1382, -5.1823],
        [ 1.4867, -3.6153],
        [ 2.4498, -3.9043],
        [ 1.0924, -2.4936],
        [ 3.2629, -4.7834],
        [ 6.1006, -7.6945],
        [ 6.6853, -8.0747],
        [ 1.8824, -3.4015],
        [ 0.6086, -3.1972],
        [ 3.0721, -4.9099],
        [ 6.2219, -8.2225],
        [ 3.4764, -5.0873],
        [ 1.6850, -3.9304],
        [ 6.5490, -8.0354],
        [ 6.6863, -8.1127],
        [ 2.2525, -4.0609],
        [ 1.2224, -4.4772],
        [ 2.8078, -5.1049],
        [ 6.1710, -7.9037],
        [ 2.1694, -3.6258],
        [ 2.6403, -4.0311],
        [ 2.0221, -3.8038],
        [ 3.5733, -4.9643],
        [ 1.1543, -4.3796],
        [ 1.7160, -3.8997],
        [-0.2277, -1.5131],
        [ 1.7871, -3.3063],
        [ 3.4481, -4.8434],
        [ 2.2284, -4.5700],
        [ 1.8499, -3.3513],
        [ 0.8389, -2.2273],
        [ 6.4259, -7.8164],
        [ 3.4127, -4.8413],
        [ 0.5615, -5.1020],
        [ 3.1744, -5.0493],
        [ 2.4552, -3.8431],
        [ 0.9866, -3.2623],
        [ 2.6813, -4.1715],
        [ 1.6570, -4.5877],
        [ 1.2759, -2.9009],
        [ 2.5693, -4.3180],
        [ 6.0232, -8.8042],
        [ 1.8389, -3.5764],
        [ 3.4752, -5.1975],
        [ 2.9891, -5.8950],
        [ 1.9806, -3.4408],
        [ 2.3121, -3.7178],
        [ 0.4537, -1.9911],
        [ 6.0932, -8.4277],
        [ 2.5188, -3.9376]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 1.0604e-06],
        [1.1356e-05, 9.9999e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9678, 0.0322], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1958, 0.1856],
         [0.3666, 0.2354]],

        [[0.9436, 0.2978],
         [0.2604, 0.1410]],

        [[0.3145, 0.1563],
         [0.1676, 0.8642]],

        [[0.2115, 0.2294],
         [0.8887, 0.9184]],

        [[0.8230, 0.2034],
         [0.0034, 0.6350]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.0005089183229907909
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: -0.009009494263563287
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.0006667223888488251
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.00028700959432072443
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0007748402262652058
Global Adjusted Rand Index: 0.0001102299408497439
Average Adjusted Rand Index: -0.0019394608686916844
Iteration 0: Loss = -25210.600044420844
Iteration 10: Loss = -12281.979434936131
Iteration 20: Loss = -12281.115594419363
Iteration 30: Loss = -12280.409813703754
Iteration 40: Loss = -12280.376911783995
Iteration 50: Loss = -12280.369247079212
Iteration 60: Loss = -12280.363478280758
Iteration 70: Loss = -12280.341212777073
Iteration 80: Loss = -12280.330984046368
Iteration 90: Loss = -12280.329014929543
Iteration 100: Loss = -12280.328477403606
Iteration 110: Loss = -12280.328297623979
Iteration 120: Loss = -12280.328241587433
Iteration 130: Loss = -12280.328210687368
Iteration 140: Loss = -12280.328188470286
Iteration 150: Loss = -12280.328125685764
Iteration 160: Loss = -12280.328177619253
1
Iteration 170: Loss = -12280.32820032619
2
Iteration 180: Loss = -12280.328139382102
3
Stopping early at iteration 179 due to no improvement.
pi: tensor([[0.0351, 0.9649],
        [0.0173, 0.9827]], dtype=torch.float64)
alpha: tensor([0.0179, 0.9821])
beta: tensor([[[0.2091, 0.1457],
         [0.3183, 0.1941]],

        [[0.1539, 0.3031],
         [0.0367, 0.7167]],

        [[0.8232, 0.1458],
         [0.8760, 0.7179]],

        [[0.8720, 0.2277],
         [0.9636, 0.6453]],

        [[0.4041, 0.2860],
         [0.1584, 0.3144]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: -0.009009494263563287
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.006509078451524495
Global Adjusted Rand Index: 0.00038770756378241793
Average Adjusted Rand Index: -0.0005000831624077586
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25210.26787713218
Iteration 100: Loss = -12291.770543838447
Iteration 200: Loss = -12282.917411011678
Iteration 300: Loss = -12282.049162634727
Iteration 400: Loss = -12281.284118700833
Iteration 500: Loss = -12280.909682933665
Iteration 600: Loss = -12280.696669154317
Iteration 700: Loss = -12280.555446299746
Iteration 800: Loss = -12280.455402422234
Iteration 900: Loss = -12280.37980971178
Iteration 1000: Loss = -12280.32006340209
Iteration 1100: Loss = -12280.270406650836
Iteration 1200: Loss = -12280.230304351415
Iteration 1300: Loss = -12280.196707333784
Iteration 1400: Loss = -12280.167240076931
Iteration 1500: Loss = -12280.14113225008
Iteration 1600: Loss = -12280.118084507894
Iteration 1700: Loss = -12280.098166932054
Iteration 1800: Loss = -12280.080997156745
Iteration 1900: Loss = -12280.065920976864
Iteration 2000: Loss = -12280.052595026524
Iteration 2100: Loss = -12280.040578291824
Iteration 2200: Loss = -12280.029387045777
Iteration 2300: Loss = -12280.018256174948
Iteration 2400: Loss = -12280.007392126437
Iteration 2500: Loss = -12279.998661358695
Iteration 2600: Loss = -12279.991350193824
Iteration 2700: Loss = -12279.984638605856
Iteration 2800: Loss = -12279.978392575107
Iteration 2900: Loss = -12279.972265630602
Iteration 3000: Loss = -12279.966197367905
Iteration 3100: Loss = -12279.959880238492
Iteration 3200: Loss = -12279.953212194992
Iteration 3300: Loss = -12279.946153826708
Iteration 3400: Loss = -12279.938849873075
Iteration 3500: Loss = -12279.931042831928
Iteration 3600: Loss = -12279.92260853167
Iteration 3700: Loss = -12279.91318835443
Iteration 3800: Loss = -12279.902470827787
Iteration 3900: Loss = -12279.88962204343
Iteration 4000: Loss = -12279.872851741271
Iteration 4100: Loss = -12279.847133217205
Iteration 4200: Loss = -12279.793545705475
Iteration 4300: Loss = -12279.644215377684
Iteration 4400: Loss = -12279.39090843535
Iteration 4500: Loss = -12279.20559520349
Iteration 4600: Loss = -12279.090367485805
Iteration 4700: Loss = -12279.015577957347
Iteration 4800: Loss = -12278.961930847778
Iteration 4900: Loss = -12278.919948213737
Iteration 5000: Loss = -12278.884882000451
Iteration 5100: Loss = -12278.860638649541
Iteration 5200: Loss = -12278.842828464987
Iteration 5300: Loss = -12278.828649523803
Iteration 5400: Loss = -12278.817025805112
Iteration 5500: Loss = -12278.80738459135
Iteration 5600: Loss = -12278.799045021806
Iteration 5700: Loss = -12278.791595206701
Iteration 5800: Loss = -12278.784636775013
Iteration 5900: Loss = -12278.777414939199
Iteration 6000: Loss = -12278.77078115774
Iteration 6100: Loss = -12278.765731031794
Iteration 6200: Loss = -12278.761784111728
Iteration 6300: Loss = -12278.758103208085
Iteration 6400: Loss = -12278.754210044386
Iteration 6500: Loss = -12278.749285367774
Iteration 6600: Loss = -12278.745666212648
Iteration 6700: Loss = -12278.742912226107
Iteration 6800: Loss = -12278.740725976637
Iteration 6900: Loss = -12278.738908619747
Iteration 7000: Loss = -12278.737288896767
Iteration 7100: Loss = -12278.736019659898
Iteration 7200: Loss = -12278.734142977633
Iteration 7300: Loss = -12278.727078988564
Iteration 7400: Loss = -12278.721500739255
Iteration 7500: Loss = -12278.720630736994
Iteration 7600: Loss = -12278.717522487967
Iteration 7700: Loss = -12278.715560474186
Iteration 7800: Loss = -12278.713384634899
Iteration 7900: Loss = -12278.712398238324
Iteration 8000: Loss = -12278.716179652787
1
Iteration 8100: Loss = -12278.710844107147
Iteration 8200: Loss = -12278.710166971023
Iteration 8300: Loss = -12278.860021787566
1
Iteration 8400: Loss = -12278.70900290626
Iteration 8500: Loss = -12278.708611506609
Iteration 8600: Loss = -12278.708154810536
Iteration 8700: Loss = -12278.707941675391
Iteration 8800: Loss = -12278.70741799866
Iteration 8900: Loss = -12278.70708062771
Iteration 9000: Loss = -12278.707753948069
1
Iteration 9100: Loss = -12278.706432170111
Iteration 9200: Loss = -12278.70623260942
Iteration 9300: Loss = -12278.706226564716
Iteration 9400: Loss = -12278.705646188168
Iteration 9500: Loss = -12278.705317044607
Iteration 9600: Loss = -12278.705067417699
Iteration 9700: Loss = -12278.705378827735
1
Iteration 9800: Loss = -12278.704656173828
Iteration 9900: Loss = -12278.704487490737
Iteration 10000: Loss = -12278.724055336676
1
Iteration 10100: Loss = -12278.70410627557
Iteration 10200: Loss = -12278.703911102091
Iteration 10300: Loss = -12278.704216518097
1
Iteration 10400: Loss = -12278.703355629976
Iteration 10500: Loss = -12278.703170084435
Iteration 10600: Loss = -12278.974437977855
1
Iteration 10700: Loss = -12278.702748827624
Iteration 10800: Loss = -12278.702513900733
Iteration 10900: Loss = -12278.713500281652
1
Iteration 11000: Loss = -12278.70160499474
Iteration 11100: Loss = -12278.93123646133
1
Iteration 11200: Loss = -12278.701374056573
Iteration 11300: Loss = -12278.70129256603
Iteration 11400: Loss = -12278.70098235291
Iteration 11500: Loss = -12278.699774382152
Iteration 11600: Loss = -12278.699776041329
1
Iteration 11700: Loss = -12278.699240284353
Iteration 11800: Loss = -12278.699218305144
Iteration 11900: Loss = -12278.70114655061
1
Iteration 12000: Loss = -12278.698921151086
Iteration 12100: Loss = -12278.715878259303
1
Iteration 12200: Loss = -12278.698290003142
Iteration 12300: Loss = -12278.69818438017
Iteration 12400: Loss = -12278.711756825838
1
Iteration 12500: Loss = -12278.698043643131
Iteration 12600: Loss = -12278.698034449277
Iteration 12700: Loss = -12278.698005322682
Iteration 12800: Loss = -12278.697964065203
Iteration 12900: Loss = -12278.698250051475
1
Iteration 13000: Loss = -12278.697832509813
Iteration 13100: Loss = -12278.703799312088
1
Iteration 13200: Loss = -12278.697613052253
Iteration 13300: Loss = -12278.697165193085
Iteration 13400: Loss = -12278.697443882222
1
Iteration 13500: Loss = -12278.69821011985
2
Iteration 13600: Loss = -12278.697981430565
3
Iteration 13700: Loss = -12278.697811305094
4
Iteration 13800: Loss = -12278.697736260598
5
Iteration 13900: Loss = -12278.701409487094
6
Iteration 14000: Loss = -12278.696933935149
Iteration 14100: Loss = -12278.714509843225
1
Iteration 14200: Loss = -12278.696738604554
Iteration 14300: Loss = -12278.695455192454
Iteration 14400: Loss = -12278.695480285323
1
Iteration 14500: Loss = -12278.694995880693
Iteration 14600: Loss = -12278.697802546265
1
Iteration 14700: Loss = -12278.694857214401
Iteration 14800: Loss = -12278.718481697102
1
Iteration 14900: Loss = -12278.694870553061
2
Iteration 15000: Loss = -12278.694830580134
Iteration 15100: Loss = -12278.695389185785
1
Iteration 15200: Loss = -12278.69479996851
Iteration 15300: Loss = -12278.694888288366
1
Iteration 15400: Loss = -12278.694879229934
2
Iteration 15500: Loss = -12278.694466534762
Iteration 15600: Loss = -12278.702230924919
1
Iteration 15700: Loss = -12278.694333582898
Iteration 15800: Loss = -12278.694300431736
Iteration 15900: Loss = -12278.699960977547
1
Iteration 16000: Loss = -12278.694257119598
Iteration 16100: Loss = -12278.694623121715
1
Iteration 16200: Loss = -12278.694302923504
2
Iteration 16300: Loss = -12278.86857399895
3
Iteration 16400: Loss = -12278.694256351046
Iteration 16500: Loss = -12278.704136090611
1
Iteration 16600: Loss = -12278.694619701177
2
Iteration 16700: Loss = -12278.694242772252
Iteration 16800: Loss = -12278.696850577076
1
Iteration 16900: Loss = -12278.694224325172
Iteration 17000: Loss = -12278.694531976764
1
Iteration 17100: Loss = -12278.694249474433
2
Iteration 17200: Loss = -12278.690177811952
Iteration 17300: Loss = -12278.690625295741
1
Iteration 17400: Loss = -12278.690185316436
2
Iteration 17500: Loss = -12278.690566183932
3
Iteration 17600: Loss = -12278.690150555429
Iteration 17700: Loss = -12278.891087624012
1
Iteration 17800: Loss = -12278.694742474305
2
Iteration 17900: Loss = -12278.698792255647
3
Iteration 18000: Loss = -12278.691561700467
4
Iteration 18100: Loss = -12278.690274177237
5
Iteration 18200: Loss = -12278.690165038115
6
Iteration 18300: Loss = -12278.693103389052
7
Iteration 18400: Loss = -12278.698765350955
8
Iteration 18500: Loss = -12278.68978003071
Iteration 18600: Loss = -12278.718434121296
1
Iteration 18700: Loss = -12278.688643223495
Iteration 18800: Loss = -12278.688566502185
Iteration 18900: Loss = -12278.688916607958
1
Iteration 19000: Loss = -12278.693040416658
2
Iteration 19100: Loss = -12278.689088863874
3
Iteration 19200: Loss = -12278.691841319089
4
Iteration 19300: Loss = -12278.688609760698
5
Iteration 19400: Loss = -12278.937185959678
6
Iteration 19500: Loss = -12278.688584899244
7
Iteration 19600: Loss = -12278.903694426923
8
Iteration 19700: Loss = -12278.688555528495
Iteration 19800: Loss = -12278.976155271324
1
Iteration 19900: Loss = -12278.688587984198
2
tensor([[ -3.9132,   2.4426],
        [ -3.1151,   1.5275],
        [ -2.9631,   0.6227],
        [ -8.3936,   7.0073],
        [ -2.8286,   1.0850],
        [ -2.9444,   1.3624],
        [ -4.7644,   2.6332],
        [ -4.7072,   2.6646],
        [ -4.0014,   2.5999],
        [ -8.4693,   7.0124],
        [ -5.5253,   3.3669],
        [ -4.7667,   3.3777],
        [ -3.7138,   1.9351],
        [ -3.3959,   1.4329],
        [ -8.7517,   6.9988],
        [ -3.3813,   1.9835],
        [ -4.6069,   3.0474],
        [-10.0959,   5.5537],
        [ -1.9277,  -1.0883],
        [ -4.5050,   2.7688],
        [ -2.8053,   1.2576],
        [ -3.9952,   2.5033],
        [ -4.1914,   2.2914],
        [ -4.1932,   2.6818],
        [ -3.6293,   1.9984],
        [ -4.0600,   2.1772],
        [ -4.0792,   2.5624],
        [ -8.8942,   6.2084],
        [ -9.3924,   5.9203],
        [ -2.5542,   1.1515],
        [ -8.1234,   6.6972],
        [ -4.1784,   1.6126],
        [ -8.5651,   7.1225],
        [ -8.7047,   7.3047],
        [ -8.3726,   6.9738],
        [ -3.4820,   1.8703],
        [ -2.8690,   1.3782],
        [ -4.7115,   3.2598],
        [ -2.7477,   0.1296],
        [ -4.8455,   3.4450],
        [-10.2477,   5.6325],
        [  0.3793,  -1.8755],
        [ -2.7395,   0.1163],
        [ -8.3947,   6.9626],
        [ -9.2547,   6.5549],
        [ -4.3617,   2.8762],
        [ -3.2850,   1.7333],
        [  0.0380,  -2.1039],
        [ -3.8447,   2.3913],
        [ -2.1920,  -1.4408],
        [ -4.3875,   2.9773],
        [ -3.4939,   1.6519],
        [ -4.1459,   2.2457],
        [ -3.6999,  -0.0846],
        [ -4.7803,   3.3437],
        [ -8.5291,   7.1058],
        [ -9.1656,   6.8032],
        [ -3.7779,   1.5420],
        [ -2.8439,   0.9731],
        [ -5.0788,   2.9747],
        [ -8.3694,   6.9679],
        [ -8.7179,   6.8686],
        [ -3.5557,   2.0935],
        [ -9.7673,   5.8634],
        [ -8.9867,   7.1183],
        [ -3.8774,   2.4693],
        [ -5.0606,   0.6704],
        [ -8.7411,   6.4131],
        [ -4.3222,   2.9357],
        [ -3.6817,   2.1538],
        [ -4.0475,   2.6601],
        [ -3.7681,   2.0913],
        [ -5.2565,   3.3497],
        [ -3.5224,   2.0476],
        [ -3.5236,   2.1324],
        [ -1.3451,  -0.0453],
        [ -4.8794,   0.2642],
        [ -5.0478,   3.3194],
        [ -4.2611,   2.5947],
        [ -3.6768,   1.5645],
        [ -2.8326,   0.2772],
        [ -8.5759,   6.9239],
        [ -8.5538,   6.8398],
        [ -3.5578,   2.1712],
        [ -4.8470,   3.4467],
        [ -4.7198,   1.6197],
        [ -3.3001,   0.9706],
        [ -4.2119,   2.6778],
        [ -4.4038,   1.8846],
        [ -3.0406,   1.1704],
        [ -4.6814,   2.2477],
        [ -8.9516,   6.6608],
        [ -3.4236,   2.0247],
        [ -5.1907,   3.5217],
        [ -8.8009,   6.7409],
        [ -3.4776,   1.9963],
        [ -4.0775,   1.9912],
        [ -1.9489,   0.5498],
        [ -8.5184,   7.0144],
        [ -3.9825,   2.5290]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 4.2692e-06],
        [3.7708e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0319, 0.9681], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2362, 0.1859],
         [0.3183, 0.1957]],

        [[0.1539, 0.2982],
         [0.0367, 0.7167]],

        [[0.8232, 0.1563],
         [0.8760, 0.7179]],

        [[0.8720, 0.2295],
         [0.9636, 0.6453]],

        [[0.4041, 0.2032],
         [0.1584, 0.3144]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.0005089183229907909
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: -0.009009494263563287
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: -0.0006667223888488251
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.00028700959432072443
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0007748402262652058
Global Adjusted Rand Index: 0.0001102299408497439
Average Adjusted Rand Index: -0.0019394608686916844
Iteration 0: Loss = -23971.987522041723
Iteration 10: Loss = -12280.660318780181
Iteration 20: Loss = -12280.593032107561
Iteration 30: Loss = -12280.574104568504
Iteration 40: Loss = -12280.56583015239
Iteration 50: Loss = -12280.56078470108
Iteration 60: Loss = -12280.556714368076
Iteration 70: Loss = -12280.552792250079
Iteration 80: Loss = -12280.548677015038
Iteration 90: Loss = -12280.544240467325
Iteration 100: Loss = -12280.539487701604
Iteration 110: Loss = -12280.534380668485
Iteration 120: Loss = -12280.52900675685
Iteration 130: Loss = -12280.523371905858
Iteration 140: Loss = -12280.517615330671
Iteration 150: Loss = -12280.51168214541
Iteration 160: Loss = -12280.505755254957
Iteration 170: Loss = -12280.499677508049
Iteration 180: Loss = -12280.493698494427
Iteration 190: Loss = -12280.487648468526
Iteration 200: Loss = -12280.481618471826
Iteration 210: Loss = -12280.475675296215
Iteration 220: Loss = -12280.46965889197
Iteration 230: Loss = -12280.46374911632
Iteration 240: Loss = -12280.457842414997
Iteration 250: Loss = -12280.451976809129
Iteration 260: Loss = -12280.446160674797
Iteration 270: Loss = -12280.440348394759
Iteration 280: Loss = -12280.43456713171
Iteration 290: Loss = -12280.42887289089
Iteration 300: Loss = -12280.423131961512
Iteration 310: Loss = -12280.417494248022
Iteration 320: Loss = -12280.41190151734
Iteration 330: Loss = -12280.406401182241
Iteration 340: Loss = -12280.400934857811
Iteration 350: Loss = -12280.39550925036
Iteration 360: Loss = -12280.390283420085
Iteration 370: Loss = -12280.38519592699
Iteration 380: Loss = -12280.380255319636
Iteration 390: Loss = -12280.375565938735
Iteration 400: Loss = -12280.371213748145
Iteration 410: Loss = -12280.36729322179
Iteration 420: Loss = -12280.363778718003
Iteration 430: Loss = -12280.36099474079
Iteration 440: Loss = -12280.358928753494
Iteration 450: Loss = -12280.357760660076
Iteration 460: Loss = -12280.357763536818
1
Iteration 470: Loss = -12280.358971582342
2
Iteration 480: Loss = -12280.361576142484
3
Stopping early at iteration 479 due to no improvement.
pi: tensor([[0.8725, 0.1275],
        [0.7853, 0.2147]], dtype=torch.float64)
alpha: tensor([0.8585, 0.1415])
beta: tensor([[[0.2001, 0.1687],
         [0.7176, 0.1665]],

        [[0.1540, 0.2015],
         [0.6989, 0.9752]],

        [[0.5062, 0.1708],
         [0.5211, 0.5194]],

        [[0.9903, 0.1843],
         [0.0980, 0.8721]],

        [[0.0971, 0.1869],
         [0.5792, 0.5514]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23971.716656622815
Iteration 100: Loss = -12376.874927006436
Iteration 200: Loss = -12321.020966491067
Iteration 300: Loss = -12298.971262553194
Iteration 400: Loss = -12287.071738207562
Iteration 500: Loss = -12283.623693161866
Iteration 600: Loss = -12282.764008645147
Iteration 700: Loss = -12282.225420458624
Iteration 800: Loss = -12281.737020448329
Iteration 900: Loss = -12281.144801560878
Iteration 1000: Loss = -12280.766344642594
Iteration 1100: Loss = -12280.675927188431
Iteration 1200: Loss = -12280.637592782976
Iteration 1300: Loss = -12280.605263901829
Iteration 1400: Loss = -12280.577112525543
Iteration 1500: Loss = -12280.54250533667
Iteration 1600: Loss = -12280.501498760726
Iteration 1700: Loss = -12280.461586414005
Iteration 1800: Loss = -12280.426825563887
Iteration 1900: Loss = -12280.39186294602
Iteration 2000: Loss = -12280.351604171707
Iteration 2100: Loss = -12280.290944445676
Iteration 2200: Loss = -12279.9330419468
Iteration 2300: Loss = -12279.86093169005
Iteration 2400: Loss = -12279.790973523595
Iteration 2500: Loss = -12279.728970699389
Iteration 2600: Loss = -12279.685815583482
Iteration 2700: Loss = -12279.737732271515
1
Iteration 2800: Loss = -12279.647991137526
Iteration 2900: Loss = -12279.641862700642
Iteration 3000: Loss = -12279.637330641464
Iteration 3100: Loss = -12279.635379604835
Iteration 3200: Loss = -12279.629983933557
Iteration 3300: Loss = -12279.626183797522
Iteration 3400: Loss = -12279.631845671489
1
Iteration 3500: Loss = -12279.620501708641
Iteration 3600: Loss = -12279.618542069935
Iteration 3700: Loss = -12279.616788918494
Iteration 3800: Loss = -12279.620981211712
1
Iteration 3900: Loss = -12279.613125158749
Iteration 4000: Loss = -12279.608444417063
Iteration 4100: Loss = -12279.614742189078
1
Iteration 4200: Loss = -12279.604520704688
Iteration 4300: Loss = -12279.602406368713
Iteration 4400: Loss = -12279.600397438437
Iteration 4500: Loss = -12279.59847612165
Iteration 4600: Loss = -12279.595218858944
Iteration 4700: Loss = -12279.591141865641
Iteration 4800: Loss = -12279.68286485163
1
Iteration 4900: Loss = -12279.565659515365
Iteration 5000: Loss = -12279.518035771913
Iteration 5100: Loss = -12279.463440539987
Iteration 5200: Loss = -12279.435917629775
Iteration 5300: Loss = -12279.425082286656
Iteration 5400: Loss = -12279.41340144393
Iteration 5500: Loss = -12279.400002964538
Iteration 5600: Loss = -12279.39539965131
Iteration 5700: Loss = -12279.392803285293
Iteration 5800: Loss = -12279.49673057843
1
Iteration 5900: Loss = -12279.389399786589
Iteration 6000: Loss = -12279.38837748469
Iteration 6100: Loss = -12279.404332771364
1
Iteration 6200: Loss = -12279.386843877863
Iteration 6300: Loss = -12279.386332496086
Iteration 6400: Loss = -12279.419657757182
1
Iteration 6500: Loss = -12279.385533930523
Iteration 6600: Loss = -12279.385245201298
Iteration 6700: Loss = -12279.398098292913
1
Iteration 6800: Loss = -12279.384776830879
Iteration 6900: Loss = -12279.393649767933
1
Iteration 7000: Loss = -12279.388280463138
2
Iteration 7100: Loss = -12279.384281182953
Iteration 7200: Loss = -12279.385236506589
1
Iteration 7300: Loss = -12279.383942714754
Iteration 7400: Loss = -12279.457471715454
1
Iteration 7500: Loss = -12279.383540975334
Iteration 7600: Loss = -12279.387076380055
1
Iteration 7700: Loss = -12279.382167428124
Iteration 7800: Loss = -12279.391550717557
1
Iteration 7900: Loss = -12279.380391391016
Iteration 8000: Loss = -12279.390800699714
1
Iteration 8100: Loss = -12279.37955964513
Iteration 8200: Loss = -12279.37709952935
Iteration 8300: Loss = -12279.377638784517
1
Iteration 8400: Loss = -12279.37862819988
2
Iteration 8500: Loss = -12279.50759076597
3
Iteration 8600: Loss = -12279.376764236611
Iteration 8700: Loss = -12279.418261494679
1
Iteration 8800: Loss = -12279.376626686702
Iteration 8900: Loss = -12279.379798684968
1
Iteration 9000: Loss = -12279.377587120296
2
Iteration 9100: Loss = -12279.37624961475
Iteration 9200: Loss = -12279.37549191026
Iteration 9300: Loss = -12279.37540454116
Iteration 9400: Loss = -12279.37604082106
1
Iteration 9500: Loss = -12279.37536297587
Iteration 9600: Loss = -12279.41568408721
1
Iteration 9700: Loss = -12279.375023044207
Iteration 9800: Loss = -12279.374208112675
Iteration 9900: Loss = -12279.37445239943
1
Iteration 10000: Loss = -12279.37498182201
2
Iteration 10100: Loss = -12279.374217388482
3
Iteration 10200: Loss = -12279.410147006256
4
Iteration 10300: Loss = -12279.37352635334
Iteration 10400: Loss = -12279.373991374625
1
Iteration 10500: Loss = -12279.373534972121
2
Iteration 10600: Loss = -12279.3734621128
Iteration 10700: Loss = -12279.37371859074
1
Iteration 10800: Loss = -12279.373484844196
2
Iteration 10900: Loss = -12279.373702183213
3
Iteration 11000: Loss = -12279.390861461465
4
Iteration 11100: Loss = -12279.373690638484
5
Iteration 11200: Loss = -12279.373647453338
6
Iteration 11300: Loss = -12279.373492473971
7
Iteration 11400: Loss = -12279.398243867874
8
Iteration 11500: Loss = -12279.373609774519
9
Iteration 11600: Loss = -12279.373462552185
10
Stopping early at iteration 11600 due to no improvement.
tensor([[-0.5537, -0.8496],
        [-0.7347, -0.8543],
        [-0.6256, -0.8005],
        [-1.3500, -1.8191],
        [-0.6474, -0.9110],
        [-0.5315, -1.0976],
        [-1.0500, -1.4743],
        [-0.7799, -0.9458],
        [-0.5572, -0.8981],
        [-1.1011, -1.4399],
        [-0.9974, -1.2065],
        [-0.4593, -0.9314],
        [-0.6068, -0.9870],
        [-2.2478, -2.3674],
        [-0.6436, -0.8462],
        [-0.7891, -1.0437],
        [-1.9307, -1.7917],
        [-1.1529, -1.5777],
        [-0.4458, -1.0061],
        [-0.6087, -0.8144],
        [-0.6319, -0.7581],
        [-2.0727, -2.5426],
        [-2.0478, -2.5674],
        [-0.5391, -0.9652],
        [-0.6696, -1.0565],
        [-1.1939, -1.3583],
        [-0.6940, -0.9902],
        [-0.7979, -1.4017],
        [-1.0685, -1.4506],
        [-0.6212, -0.8368],
        [-0.5709, -0.8659],
        [-0.6767, -0.7099],
        [-0.9920, -1.3288],
        [-1.2468, -1.1875],
        [-0.6278, -0.7870],
        [-0.7135, -0.9237],
        [-0.5392, -0.8927],
        [-0.9263, -1.5739],
        [-0.5035, -0.9126],
        [-0.6365, -0.8051],
        [-0.9084, -1.2003],
        [-0.5329, -1.5310],
        [-1.3883, -1.7427],
        [-0.8921, -1.4060],
        [-1.3490, -1.4652],
        [-0.5318, -0.8769],
        [-0.6074, -0.8237],
        [-0.5774, -1.7414],
        [-0.4147, -1.0145],
        [-0.5520, -0.8436],
        [-0.5452, -0.8829],
        [-0.7880, -1.1433],
        [-0.8736, -1.1703],
        [-0.8335, -0.9771],
        [-0.8273, -0.6363],
        [-0.8295, -1.2537],
        [-1.1585, -1.2721],
        [-0.8100, -1.3710],
        [-0.5625, -0.9164],
        [-0.9082, -1.2431],
        [-1.0705, -1.1993],
        [-0.7962, -0.9572],
        [-0.5413, -0.8453],
        [-0.5523, -0.8909],
        [-0.9048, -0.9332],
        [-0.7141, -0.8553],
        [-0.7103, -0.9208],
        [-0.4496, -1.0074],
        [-1.6704, -2.1870],
        [-0.6186, -0.9117],
        [-0.6132, -0.9758],
        [-0.8499, -1.1890],
        [-1.2590, -1.4639],
        [-0.5922, -0.8441],
        [-0.8125, -0.7793],
        [-0.8252, -1.1549],
        [-2.2021, -2.4131],
        [-0.6662, -0.7416],
        [-0.6305, -0.8381],
        [-0.6929, -0.9459],
        [-0.9829, -1.1241],
        [-2.1597, -2.4555],
        [-0.8348, -0.6902],
        [-0.7081, -0.6918],
        [-1.2073, -1.4579],
        [-0.7186, -0.8803],
        [-1.8307, -2.2631],
        [-1.6289, -2.1424],
        [-0.7903, -0.9768],
        [-1.7076, -1.9390],
        [-0.6755, -0.8830],
        [-0.6807, -0.7099],
        [-0.5701, -0.9767],
        [-0.5039, -0.9770],
        [-0.7755, -0.9802],
        [-0.7483, -0.7786],
        [-0.6174, -0.7813],
        [-0.6600, -0.7884],
        [-0.5610, -0.8534],
        [-1.9137, -2.1199]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.0342, 0.9658],
        [0.0074, 0.9926]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5703, 0.4297], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1842, 0.1903],
         [0.7176, 0.1980]],

        [[0.1540, 0.2971],
         [0.6989, 0.9752]],

        [[0.5062, 0.1348],
         [0.5211, 0.5194]],

        [[0.9903, 0.1305],
         [0.0980, 0.8721]],

        [[0.0971, 0.3121],
         [0.5792, 0.5514]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.004682108332720131
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: -0.009009494263563287
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.006509078451524495
Global Adjusted Rand Index: -0.0002835433214465032
Average Adjusted Rand Index: -0.001436504828951785
11755.519160573027
new:  [-0.0006746506725940951, 0.0001102299408497439, 0.0001102299408497439, -0.0002835433214465032] [0.004908107933334076, -0.0019394608686916844, -0.0019394608686916844, -0.001436504828951785] [12278.990886616552, 12278.68409411961, 12278.687755007131, 12279.373462552185]
prior:  [0.0, 0.00038770756378241793, 0.00038770756378241793, 0.0] [0.0, -0.0005000831624077586, -0.0005000831624077586, 0.0] [12280.364819002036, 12280.328171629904, 12280.328139382102, 12280.361576142484]
-----------------------------------------------------------------------------------------
This iteration is 5
True Objective function: Loss = -11969.978062852699
Iteration 0: Loss = -19725.413371672526
Iteration 10: Loss = -12494.544532536482
Iteration 20: Loss = -12494.507114498958
Iteration 30: Loss = -12494.499521525731
Iteration 40: Loss = -12494.49446807687
Iteration 50: Loss = -12494.490806299353
Iteration 60: Loss = -12494.488321058487
Iteration 70: Loss = -12494.486927227334
Iteration 80: Loss = -12494.486391659197
Iteration 90: Loss = -12494.486508922997
1
Iteration 100: Loss = -12494.48708717138
2
Iteration 110: Loss = -12494.48800126017
3
Stopping early at iteration 109 due to no improvement.
pi: tensor([[0.0607, 0.9393],
        [0.0668, 0.9332]], dtype=torch.float64)
alpha: tensor([0.0667, 0.9333])
beta: tensor([[[0.2360, 0.2288],
         [0.7172, 0.1988]],

        [[0.7158, 0.2123],
         [0.9139, 0.2344]],

        [[0.4693, 0.1988],
         [0.8864, 0.4047]],

        [[0.9507, 0.2432],
         [0.2529, 0.7961]],

        [[0.6472, 0.1992],
         [0.7392, 0.2170]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19693.47644069617
Iteration 100: Loss = -12497.57495061785
Iteration 200: Loss = -12495.467739618267
Iteration 300: Loss = -12494.314843009694
Iteration 400: Loss = -12493.776360068568
Iteration 500: Loss = -12493.439729188847
Iteration 600: Loss = -12493.251023156783
Iteration 700: Loss = -12493.123087867301
Iteration 800: Loss = -12493.017320270434
Iteration 900: Loss = -12492.928558246747
Iteration 1000: Loss = -12492.85762530004
Iteration 1100: Loss = -12492.800955797005
Iteration 1200: Loss = -12492.755798306209
Iteration 1300: Loss = -12492.720427642374
Iteration 1400: Loss = -12492.69308280666
Iteration 1500: Loss = -12492.671394546676
Iteration 1600: Loss = -12492.653749413223
Iteration 1700: Loss = -12492.638931201316
Iteration 1800: Loss = -12492.626262180389
Iteration 1900: Loss = -12492.615314333412
Iteration 2000: Loss = -12492.605746690739
Iteration 2100: Loss = -12492.59727251307
Iteration 2200: Loss = -12492.589767905369
Iteration 2300: Loss = -12492.58299668443
Iteration 2400: Loss = -12492.576928935241
Iteration 2500: Loss = -12492.571397538448
Iteration 2600: Loss = -12492.566395913815
Iteration 2700: Loss = -12492.561872037146
Iteration 2800: Loss = -12492.557730295679
Iteration 2900: Loss = -12492.55391320774
Iteration 3000: Loss = -12492.550496588818
Iteration 3100: Loss = -12492.547367921361
Iteration 3200: Loss = -12492.544501467084
Iteration 3300: Loss = -12492.541930200807
Iteration 3400: Loss = -12492.539512638079
Iteration 3500: Loss = -12492.537341385805
Iteration 3600: Loss = -12492.535304046369
Iteration 3700: Loss = -12492.53344423607
Iteration 3800: Loss = -12492.531674616901
Iteration 3900: Loss = -12492.53008037808
Iteration 4000: Loss = -12492.528561224704
Iteration 4100: Loss = -12492.52711625765
Iteration 4200: Loss = -12492.525771016251
Iteration 4300: Loss = -12492.524494396977
Iteration 4400: Loss = -12492.523229870245
Iteration 4500: Loss = -12492.522050111504
Iteration 4600: Loss = -12492.520843141881
Iteration 4700: Loss = -12492.519674929885
Iteration 4800: Loss = -12492.518445893653
Iteration 4900: Loss = -12492.517222967586
Iteration 5000: Loss = -12492.515848361052
Iteration 5100: Loss = -12492.51466676947
Iteration 5200: Loss = -12492.513886223425
Iteration 5300: Loss = -12492.512880529575
Iteration 5400: Loss = -12492.512181770515
Iteration 5500: Loss = -12492.512316541033
1
Iteration 5600: Loss = -12492.510979038068
Iteration 5700: Loss = -12492.510431312543
Iteration 5800: Loss = -12492.510839309916
1
Iteration 5900: Loss = -12492.509452546094
Iteration 6000: Loss = -12492.50897986811
Iteration 6100: Loss = -12492.509328164877
1
Iteration 6200: Loss = -12492.508176970943
Iteration 6300: Loss = -12492.507737720473
Iteration 6400: Loss = -12492.553747656595
1
Iteration 6500: Loss = -12492.507058166759
Iteration 6600: Loss = -12492.50674182874
Iteration 6700: Loss = -12492.755851582324
1
Iteration 6800: Loss = -12492.50609867663
Iteration 6900: Loss = -12492.505824260157
Iteration 7000: Loss = -12492.610628162922
1
Iteration 7100: Loss = -12492.505324319363
Iteration 7200: Loss = -12492.505093639924
Iteration 7300: Loss = -12492.504879676499
Iteration 7400: Loss = -12492.5050047003
1
Iteration 7500: Loss = -12492.504410544463
Iteration 7600: Loss = -12492.504174184429
Iteration 7700: Loss = -12492.507405636294
1
Iteration 7800: Loss = -12492.503672456445
Iteration 7900: Loss = -12492.503377334435
Iteration 8000: Loss = -12492.506178213545
1
Iteration 8100: Loss = -12492.502955256052
Iteration 8200: Loss = -12492.502844475272
Iteration 8300: Loss = -12492.650661844884
1
Iteration 8400: Loss = -12492.502526361966
Iteration 8500: Loss = -12492.502418736243
Iteration 8600: Loss = -12492.652050123987
1
Iteration 8700: Loss = -12492.5021515718
Iteration 8800: Loss = -12492.501965122576
Iteration 8900: Loss = -12492.501835251845
Iteration 9000: Loss = -12492.501847442727
1
Iteration 9100: Loss = -12492.501651927914
Iteration 9200: Loss = -12492.501521470762
Iteration 9300: Loss = -12492.560049959427
1
Iteration 9400: Loss = -12492.501406724143
Iteration 9500: Loss = -12492.501360928309
Iteration 9600: Loss = -12492.530130434883
1
Iteration 9700: Loss = -12492.501277373753
Iteration 9800: Loss = -12492.501154281585
Iteration 9900: Loss = -12492.794659438838
1
Iteration 10000: Loss = -12492.501054746966
Iteration 10100: Loss = -12492.501038842733
Iteration 10200: Loss = -12492.50500059789
1
Iteration 10300: Loss = -12492.500984464774
Iteration 10400: Loss = -12492.500896601088
Iteration 10500: Loss = -12492.500878540946
Iteration 10600: Loss = -12492.500941516255
1
Iteration 10700: Loss = -12492.500814904328
Iteration 10800: Loss = -12492.500765930534
Iteration 10900: Loss = -12492.504335587628
1
Iteration 11000: Loss = -12492.500730622725
Iteration 11100: Loss = -12492.500675903828
Iteration 11200: Loss = -12492.504598935797
1
Iteration 11300: Loss = -12492.50062771844
Iteration 11400: Loss = -12492.500630044126
1
Iteration 11500: Loss = -12492.500809720428
2
Iteration 11600: Loss = -12492.50059177187
Iteration 11700: Loss = -12492.500572704592
Iteration 11800: Loss = -12492.507189145323
1
Iteration 11900: Loss = -12492.500504588885
Iteration 12000: Loss = -12492.500503825435
Iteration 12100: Loss = -12492.522909567675
1
Iteration 12200: Loss = -12492.500513625058
2
Iteration 12300: Loss = -12492.500425602906
Iteration 12400: Loss = -12492.500443219295
1
Iteration 12500: Loss = -12492.500762990954
2
Iteration 12600: Loss = -12492.50040003495
Iteration 12700: Loss = -12492.50040318867
1
Iteration 12800: Loss = -12492.531680160197
2
Iteration 12900: Loss = -12492.500396503046
Iteration 13000: Loss = -12492.500351615085
Iteration 13100: Loss = -12492.607643721207
1
Iteration 13200: Loss = -12492.500357171617
2
Iteration 13300: Loss = -12492.50036327671
3
Iteration 13400: Loss = -12492.581333593807
4
Iteration 13500: Loss = -12492.50034893996
Iteration 13600: Loss = -12492.500299073079
Iteration 13700: Loss = -12492.51086126558
1
Iteration 13800: Loss = -12492.500340911352
2
Iteration 13900: Loss = -12492.500280206374
Iteration 14000: Loss = -12492.511823934972
1
Iteration 14100: Loss = -12492.500304984405
2
Iteration 14200: Loss = -12492.500308684328
3
Iteration 14300: Loss = -12492.922902436623
4
Iteration 14400: Loss = -12492.500275432101
Iteration 14500: Loss = -12492.500274464657
Iteration 14600: Loss = -12492.500275811915
1
Iteration 14700: Loss = -12492.500690651319
2
Iteration 14800: Loss = -12492.500265804605
Iteration 14900: Loss = -12492.500249893465
Iteration 15000: Loss = -12492.50589667687
1
Iteration 15100: Loss = -12492.500272602814
2
Iteration 15200: Loss = -12492.500224017174
Iteration 15300: Loss = -12492.810169151873
1
Iteration 15400: Loss = -12492.500267626468
2
Iteration 15500: Loss = -12492.500223246812
Iteration 15600: Loss = -12492.50022684485
1
Iteration 15700: Loss = -12492.586272364679
2
Iteration 15800: Loss = -12492.500231855147
3
Iteration 15900: Loss = -12492.50023005519
4
Iteration 16000: Loss = -12492.500238334316
5
Iteration 16100: Loss = -12492.512126285232
6
Iteration 16200: Loss = -12492.50022465182
7
Iteration 16300: Loss = -12492.500220761098
Iteration 16400: Loss = -12492.500179205497
Iteration 16500: Loss = -12492.500546216976
1
Iteration 16600: Loss = -12492.50017367384
Iteration 16700: Loss = -12492.500192547906
1
Iteration 16800: Loss = -12492.504900006592
2
Iteration 16900: Loss = -12492.50020596809
3
Iteration 17000: Loss = -12492.500207696929
4
Iteration 17100: Loss = -12492.527374426712
5
Iteration 17200: Loss = -12492.500206241008
6
Iteration 17300: Loss = -12492.500171884902
Iteration 17400: Loss = -12492.506352287139
1
Iteration 17500: Loss = -12492.5001945022
2
Iteration 17600: Loss = -12492.500145793727
Iteration 17700: Loss = -12492.532853466371
1
Iteration 17800: Loss = -12492.500167773569
2
Iteration 17900: Loss = -12492.500169838693
3
Iteration 18000: Loss = -12492.699291910634
4
Iteration 18100: Loss = -12492.500198223455
5
Iteration 18200: Loss = -12492.50016766626
6
Iteration 18300: Loss = -12492.504956772456
7
Iteration 18400: Loss = -12492.500215102804
8
Iteration 18500: Loss = -12492.50016399045
9
Iteration 18600: Loss = -12492.500175079993
10
Stopping early at iteration 18600 due to no improvement.
tensor([[-1.8998, -2.7154],
        [-7.4208,  2.8056],
        [-4.1572, -0.4580],
        [-2.3085, -2.3067],
        [-4.6232,  0.0079],
        [-5.5200,  0.9048],
        [-5.3893,  0.7741],
        [-5.5904,  0.9751],
        [-5.4648,  0.8496],
        [-4.6327,  0.0174],
        [-5.6339,  1.0187],
        [-6.0406,  1.4254],
        [-3.6537, -0.9616],
        [-4.1056, -0.5096],
        [-2.7674, -1.8478],
        [-4.4022, -0.2131],
        [-3.7570, -0.8582],
        [-4.9123,  0.2971],
        [-5.3877,  0.7725],
        [-6.7183,  2.1031],
        [-5.3734,  0.7582],
        [-6.9420,  2.3268],
        [-5.6041,  0.9889],
        [-4.2295, -0.3857],
        [-4.3834, -0.2319],
        [-5.9656,  1.3503],
        [-6.5759,  1.9607],
        [-3.5242, -1.0910],
        [-4.9778,  0.3626],
        [-7.0461,  2.4309],
        [-5.4263,  0.8110],
        [-4.3802, -0.2350],
        [-6.1321,  1.5168],
        [-6.0148,  1.3996],
        [-5.7316,  1.1164],
        [-3.1267, -1.4885],
        [-6.4554,  1.8402],
        [-5.3369,  0.7217],
        [-6.4081,  1.7928],
        [-5.2570,  0.6418],
        [-6.4380,  1.8228],
        [-7.0527,  2.4375],
        [-4.7560,  0.1408],
        [-6.3300,  1.7148],
        [-3.8993, -0.7159],
        [-5.3682,  0.7530],
        [-4.8217,  0.2064],
        [-6.6171,  2.0019],
        [-7.0557,  2.4405],
        [-4.3607, -0.2545],
        [-6.1956,  1.5804],
        [-6.0748,  1.4595],
        [-1.3349, -3.2803],
        [-4.8805,  0.2653],
        [-2.0561, -2.5592],
        [-6.5420,  1.9268],
        [-5.7600,  1.1447],
        [-6.7614,  2.1462],
        [-7.3909,  2.7756],
        [-6.6244,  2.0092],
        [-5.2798,  0.6646],
        [-5.5044,  0.8892],
        [-4.9460,  0.3308],
        [-6.4493,  1.8340],
        [-4.0812, -0.5340],
        [-5.2907,  0.6755],
        [-6.0161,  1.4009],
        [-5.6058,  0.9906],
        [-5.5503,  0.9351],
        [-5.0179,  0.4027],
        [-6.1051,  1.4899],
        [-2.9717, -1.6435],
        [-3.5684, -1.0468],
        [-6.0608,  1.4456],
        [-6.1278,  1.5126],
        [-3.8724, -0.7428],
        [-4.8442,  0.2289],
        [-4.8473,  0.2321],
        [-6.2175,  1.6023],
        [-7.1103,  2.4951],
        [-5.8955,  1.2803],
        [-5.8743,  1.2590],
        [-4.8685,  0.2533],
        [-6.5650,  1.9498],
        [-6.3383,  1.7231],
        [-5.2485,  0.6333],
        [-5.5602,  0.9450],
        [-4.1008, -0.5144],
        [-5.1160,  0.5008],
        [-5.3680,  0.7528],
        [-6.0884,  1.4732],
        [-6.1831,  1.5679],
        [-4.6570,  0.0418],
        [-6.6973,  2.0821],
        [-5.6352,  1.0200],
        [-5.2617,  0.6465],
        [-4.2196, -0.3957],
        [-5.0749,  0.4597],
        [-5.1464,  0.5312],
        [-4.9042,  0.2890]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 3.4645e-06],
        [1.6927e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0407, 0.9593], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2503, 0.2114],
         [0.7172, 0.2018]],

        [[0.7158, 0.2494],
         [0.9139, 0.2344]],

        [[0.4693, 0.1819],
         [0.8864, 0.4047]],

        [[0.9507, 0.2852],
         [0.2529, 0.7961]],

        [[0.6472, 0.1696],
         [0.7392, 0.2170]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0031985518644306096
time is 2
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: -0.0041478895259491316
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: -0.0041478895259491316
time is 4
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
Global Adjusted Rand Index: -0.00011146982936504288
Average Adjusted Rand Index: -0.0013113826389462243
Iteration 0: Loss = -21157.224693592456
Iteration 10: Loss = -12495.374977479883
Iteration 20: Loss = -12495.37285180147
Iteration 30: Loss = -12495.36671118468
Iteration 40: Loss = -12495.348743988436
Iteration 50: Loss = -12495.300523368074
Iteration 60: Loss = -12495.195868759163
Iteration 70: Loss = -12495.03950089452
Iteration 80: Loss = -12494.88520960751
Iteration 90: Loss = -12494.76895688329
Iteration 100: Loss = -12494.689474976502
Iteration 110: Loss = -12494.635492467634
Iteration 120: Loss = -12494.59840134594
Iteration 130: Loss = -12494.572731510034
Iteration 140: Loss = -12494.554846002995
Iteration 150: Loss = -12494.5422566892
Iteration 160: Loss = -12494.533240950543
Iteration 170: Loss = -12494.526651416818
Iteration 180: Loss = -12494.521773671951
Iteration 190: Loss = -12494.518063560832
Iteration 200: Loss = -12494.51520948137
Iteration 210: Loss = -12494.512965056943
Iteration 220: Loss = -12494.511162771008
Iteration 230: Loss = -12494.509749871553
Iteration 240: Loss = -12494.508614037817
Iteration 250: Loss = -12494.50769234646
Iteration 260: Loss = -12494.506935500684
Iteration 270: Loss = -12494.506286214746
Iteration 280: Loss = -12494.505767967246
Iteration 290: Loss = -12494.505318209549
Iteration 300: Loss = -12494.505026085779
Iteration 310: Loss = -12494.504711654285
Iteration 320: Loss = -12494.504454846116
Iteration 330: Loss = -12494.504246998331
Iteration 340: Loss = -12494.504092249506
Iteration 350: Loss = -12494.503957442817
Iteration 360: Loss = -12494.50380332506
Iteration 370: Loss = -12494.503757536348
Iteration 380: Loss = -12494.503644982082
Iteration 390: Loss = -12494.503561561616
Iteration 400: Loss = -12494.503473587896
Iteration 410: Loss = -12494.503465716043
Iteration 420: Loss = -12494.503423250515
Iteration 430: Loss = -12494.503375626078
Iteration 440: Loss = -12494.503342980537
Iteration 450: Loss = -12494.503334672047
Iteration 460: Loss = -12494.503311311093
Iteration 470: Loss = -12494.5032637521
Iteration 480: Loss = -12494.503294969814
1
Iteration 490: Loss = -12494.503243695293
Iteration 500: Loss = -12494.503249532532
1
Iteration 510: Loss = -12494.5032085566
Iteration 520: Loss = -12494.503198848108
Iteration 530: Loss = -12494.503185130756
Iteration 540: Loss = -12494.50322589438
1
Iteration 550: Loss = -12494.50319547796
2
Iteration 560: Loss = -12494.503210508192
3
Stopping early at iteration 559 due to no improvement.
pi: tensor([[0.9468, 0.0532],
        [0.9349, 0.0651]], dtype=torch.float64)
alpha: tensor([0.9459, 0.0541])
beta: tensor([[[0.1992, 0.2324],
         [0.8742, 0.2377]],

        [[0.3465, 0.2124],
         [0.2348, 0.7611]],

        [[0.2305, 0.1968],
         [0.7322, 0.1775]],

        [[0.1560, 0.2477],
         [0.4284, 0.4120]],

        [[0.8034, 0.1973],
         [0.2547, 0.2713]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21156.491176016978
Iteration 100: Loss = -12499.818628643565
Iteration 200: Loss = -12496.190192112592
Iteration 300: Loss = -12495.415128223489
Iteration 400: Loss = -12495.041134212055
Iteration 500: Loss = -12494.837012603502
Iteration 600: Loss = -12494.696698438114
Iteration 700: Loss = -12494.608443024961
Iteration 800: Loss = -12494.560343829986
Iteration 900: Loss = -12494.509170410283
Iteration 1000: Loss = -12494.475994014794
Iteration 1100: Loss = -12494.447533928704
Iteration 1200: Loss = -12494.425066207146
Iteration 1300: Loss = -12494.398027083605
Iteration 1400: Loss = -12494.375398056858
Iteration 1500: Loss = -12494.638320697899
1
Iteration 1600: Loss = -12494.331140486467
Iteration 1700: Loss = -12494.309189077967
Iteration 1800: Loss = -12494.288127120388
Iteration 1900: Loss = -12494.267450054638
Iteration 2000: Loss = -12494.247169502554
Iteration 2100: Loss = -12494.227283478976
Iteration 2200: Loss = -12494.21432898329
Iteration 2300: Loss = -12494.189204637094
Iteration 2400: Loss = -12494.171572640089
Iteration 2500: Loss = -12494.155075511717
Iteration 2600: Loss = -12494.139249441823
Iteration 2700: Loss = -12494.124578086985
Iteration 2800: Loss = -12494.111985956224
Iteration 2900: Loss = -12494.100883613397
Iteration 3000: Loss = -12494.09184699092
Iteration 3100: Loss = -12494.084546784854
Iteration 3200: Loss = -12494.613671608344
1
Iteration 3300: Loss = -12494.074213003529
Iteration 3400: Loss = -12494.07096070105
Iteration 3500: Loss = -12494.068677022116
Iteration 3600: Loss = -12494.070519021427
1
Iteration 3700: Loss = -12494.06585572439
Iteration 3800: Loss = -12494.065148830407
Iteration 3900: Loss = -12494.07936229655
1
Iteration 4000: Loss = -12494.064361065566
Iteration 4100: Loss = -12494.064123304865
Iteration 4200: Loss = -12494.17866446996
1
Iteration 4300: Loss = -12494.063915482446
Iteration 4400: Loss = -12494.06390204684
Iteration 4500: Loss = -12494.063921913703
1
Iteration 4600: Loss = -12494.06384181475
Iteration 4700: Loss = -12494.063801236362
Iteration 4800: Loss = -12494.063802531102
1
Iteration 4900: Loss = -12494.066737890073
2
Iteration 5000: Loss = -12494.063754035411
Iteration 5100: Loss = -12494.063712725494
Iteration 5200: Loss = -12494.063992180332
1
Iteration 5300: Loss = -12494.06367591187
Iteration 5400: Loss = -12494.063605284375
Iteration 5500: Loss = -12494.102094163884
1
Iteration 5600: Loss = -12494.063493939395
Iteration 5700: Loss = -12494.063466657697
Iteration 5800: Loss = -12494.063444396192
Iteration 5900: Loss = -12494.063501484163
1
Iteration 6000: Loss = -12494.063285404221
Iteration 6100: Loss = -12494.06315076489
Iteration 6200: Loss = -12494.063290739996
1
Iteration 6300: Loss = -12494.062948864019
Iteration 6400: Loss = -12494.06286848391
Iteration 6500: Loss = -12494.062871140728
1
Iteration 6600: Loss = -12494.062571471986
Iteration 6700: Loss = -12494.062357920706
Iteration 6800: Loss = -12494.062146734741
Iteration 6900: Loss = -12494.061888324122
Iteration 7000: Loss = -12494.061574865646
Iteration 7100: Loss = -12494.061596191628
1
Iteration 7200: Loss = -12494.060591325699
Iteration 7300: Loss = -12494.059812139774
Iteration 7400: Loss = -12494.058912423245
Iteration 7500: Loss = -12494.056915817046
Iteration 7600: Loss = -12494.053571954211
Iteration 7700: Loss = -12494.045577264245
Iteration 7800: Loss = -12494.00576311029
Iteration 7900: Loss = -12493.615059490592
Iteration 8000: Loss = -12492.77901646879
Iteration 8100: Loss = -12492.596174723712
Iteration 8200: Loss = -12492.610972861063
1
Iteration 8300: Loss = -12492.540751470222
Iteration 8400: Loss = -12492.531294646864
Iteration 8500: Loss = -12492.531460596532
1
Iteration 8600: Loss = -12492.520896957945
Iteration 8700: Loss = -12492.517776430157
Iteration 8800: Loss = -12492.85440360392
1
Iteration 8900: Loss = -12492.51345643349
Iteration 9000: Loss = -12492.51189255671
Iteration 9100: Loss = -12492.510583852772
Iteration 9200: Loss = -12492.509541886551
Iteration 9300: Loss = -12492.508553089465
Iteration 9400: Loss = -12492.507779149382
Iteration 9500: Loss = -12492.64624431436
1
Iteration 9600: Loss = -12492.506501307333
Iteration 9700: Loss = -12492.505956936535
Iteration 9800: Loss = -12492.507279432688
1
Iteration 9900: Loss = -12492.505221291547
Iteration 10000: Loss = -12492.504739637514
Iteration 10100: Loss = -12492.504418979526
Iteration 10200: Loss = -12492.505212691107
1
Iteration 10300: Loss = -12492.503841905414
Iteration 10400: Loss = -12492.503554080517
Iteration 10500: Loss = -12492.50967179772
1
Iteration 10600: Loss = -12492.503103060404
Iteration 10700: Loss = -12492.502941076076
Iteration 10800: Loss = -12492.505978971809
1
Iteration 10900: Loss = -12492.502597031797
Iteration 11000: Loss = -12492.502445961838
Iteration 11100: Loss = -12492.573779279053
1
Iteration 11200: Loss = -12492.502187868095
Iteration 11300: Loss = -12492.50208390117
Iteration 11400: Loss = -12493.154920233912
1
Iteration 11500: Loss = -12492.501879846699
Iteration 11600: Loss = -12492.501772116648
Iteration 11700: Loss = -12492.501692942298
Iteration 11800: Loss = -12492.501641259334
Iteration 11900: Loss = -12492.501511418237
Iteration 12000: Loss = -12492.501424648372
Iteration 12100: Loss = -12492.818188394176
1
Iteration 12200: Loss = -12492.501317593516
Iteration 12300: Loss = -12492.501265635012
Iteration 12400: Loss = -12492.501211441178
Iteration 12500: Loss = -12492.501682625061
1
Iteration 12600: Loss = -12492.501115713882
Iteration 12700: Loss = -12492.501018968642
Iteration 12800: Loss = -12492.504249897358
1
Iteration 12900: Loss = -12492.500948715193
Iteration 13000: Loss = -12492.500952361434
1
Iteration 13100: Loss = -12492.54353651347
2
Iteration 13200: Loss = -12492.500880608906
Iteration 13300: Loss = -12492.500817461947
Iteration 13400: Loss = -12492.500790831975
Iteration 13500: Loss = -12492.501038771747
1
Iteration 13600: Loss = -12492.500711441124
Iteration 13700: Loss = -12492.500645090728
Iteration 13800: Loss = -12492.51546473117
1
Iteration 13900: Loss = -12492.50057002404
Iteration 14000: Loss = -12492.500576388851
1
Iteration 14100: Loss = -12492.509491492032
2
Iteration 14200: Loss = -12492.500472959271
Iteration 14300: Loss = -12492.500539376137
1
Iteration 14400: Loss = -12492.50849915677
2
Iteration 14500: Loss = -12492.500468287826
Iteration 14600: Loss = -12492.500473151214
1
Iteration 14700: Loss = -12492.535037645956
2
Iteration 14800: Loss = -12492.500457773878
Iteration 14900: Loss = -12492.500363788793
Iteration 15000: Loss = -12492.501180797843
1
Iteration 15100: Loss = -12492.500385157782
2
Iteration 15200: Loss = -12492.500348355503
Iteration 15300: Loss = -12492.500386720163
1
Iteration 15400: Loss = -12492.50043451583
2
Iteration 15500: Loss = -12492.500338381084
Iteration 15600: Loss = -12492.50034410218
1
Iteration 15700: Loss = -12492.501824505762
2
Iteration 15800: Loss = -12492.50030267654
Iteration 15900: Loss = -12492.500298765777
Iteration 16000: Loss = -12492.509474769344
1
Iteration 16100: Loss = -12492.500303592291
2
Iteration 16200: Loss = -12492.500268916956
Iteration 16300: Loss = -12492.548018829582
1
Iteration 16400: Loss = -12492.500260990268
Iteration 16500: Loss = -12492.500242900624
Iteration 16600: Loss = -12492.60080965865
1
Iteration 16700: Loss = -12492.50025179047
2
Iteration 16800: Loss = -12492.500240299418
Iteration 16900: Loss = -12492.52686564462
1
Iteration 17000: Loss = -12492.500226087779
Iteration 17100: Loss = -12492.500197972115
Iteration 17200: Loss = -12492.513304314269
1
Iteration 17300: Loss = -12492.500171009233
Iteration 17400: Loss = -12492.500198133417
1
Iteration 17500: Loss = -12492.523346112184
2
Iteration 17600: Loss = -12492.500235411739
3
Iteration 17700: Loss = -12492.500191320956
4
Iteration 17800: Loss = -12492.500214487533
5
Iteration 17900: Loss = -12492.500370052854
6
Iteration 18000: Loss = -12492.50016501542
Iteration 18100: Loss = -12492.500177419439
1
Iteration 18200: Loss = -12492.500401435858
2
Iteration 18300: Loss = -12492.500181130215
3
Iteration 18400: Loss = -12492.500179112198
4
Iteration 18500: Loss = -12492.501324216595
5
Iteration 18600: Loss = -12492.500176014972
6
Iteration 18700: Loss = -12492.500208161393
7
Iteration 18800: Loss = -12492.508065786906
8
Iteration 18900: Loss = -12492.500185729952
9
Iteration 19000: Loss = -12492.500178584867
10
Stopping early at iteration 19000 due to no improvement.
tensor([[-1.2942, -0.4787],
        [ 4.3877, -5.8377],
        [ 1.1468, -2.5525],
        [-1.4782, -1.4801],
        [ 1.3966, -3.2346],
        [ 2.4556, -3.9691],
        [ 1.9682, -4.1952],
        [ 1.4921, -5.0734],
        [ 2.1595, -4.1547],
        [ 1.5753, -3.0749],
        [ 2.6051, -4.0476],
        [ 3.0085, -4.4576],
        [ 0.6523, -2.0399],
        [ 0.6897, -2.9063],
        [-0.8273, -1.7470],
        [ 0.6010, -3.5881],
        [ 0.6153, -2.2834],
        [ 1.3668, -3.8427],
        [ 2.1154, -4.0450],
        [ 3.6631, -5.1582],
        [ 1.7512, -4.3804],
        [ 3.8292, -5.4392],
        [ 2.4120, -4.1810],
        [ 0.8905, -2.9534],
        [ 1.3368, -2.8147],
        [ 2.2132, -5.1027],
        [ 2.7964, -5.7398],
        [ 0.4734, -1.9599],
        [ 1.9199, -3.4204],
        [ 4.0438, -5.4328],
        [ 2.3954, -3.8419],
        [ 1.2945, -2.8508],
        [ 3.1296, -4.5192],
        [ 2.5288, -4.8855],
        [ 2.5450, -4.3030],
        [ 0.0121, -1.6261],
        [ 3.4431, -4.8527],
        [ 1.9965, -4.0623],
        [ 2.7122, -5.4884],
        [ 2.0514, -3.8474],
        [ 3.1994, -5.0615],
        [ 4.0378, -5.4522],
        [ 0.4535, -4.4434],
        [ 3.1814, -4.8632],
        [ 0.8714, -2.3120],
        [ 2.2839, -3.8372],
        [ 1.6271, -3.4012],
        [ 2.3882, -6.2307],
        [ 3.1662, -6.3297],
        [ 1.0141, -3.0921],
        [ 3.1800, -4.5960],
        [ 2.4255, -5.1087],
        [-2.1418, -0.1967],
        [ 1.1109, -4.0349],
        [-1.1025, -0.5997],
        [ 3.5277, -4.9408],
        [ 2.6162, -4.2884],
        [ 2.1464, -6.7616],
        [ 4.0183, -6.1466],
        [ 3.5075, -5.1260],
        [ 2.1874, -3.7571],
        [ 2.0815, -4.3123],
        [ 1.3587, -3.9181],
        [ 3.1413, -5.1420],
        [ 1.0750, -2.4722],
        [ 2.1617, -3.8045],
        [ 2.2654, -5.1516],
        [ 1.6981, -4.8982],
        [ 2.5275, -3.9579],
        [ 1.9936, -3.4270],
        [ 2.9992, -4.5957],
        [-0.0692, -1.3975],
        [-0.3283, -2.8499],
        [ 2.0153, -5.4910],
        [ 3.1271, -4.5134],
        [ 0.8304, -2.2993],
        [ 1.5635, -3.5096],
        [ 1.8136, -3.2658],
        [ 2.6763, -5.1433],
        [ 2.4949, -7.1101],
        [ 2.8079, -4.3680],
        [ 2.8724, -4.2610],
        [ 1.7245, -3.3973],
        [ 3.4747, -5.0399],
        [ 3.2393, -4.8220],
        [ 2.2447, -3.6371],
        [ 1.8683, -4.6369],
        [ 1.0969, -2.4895],
        [ 1.2804, -4.3365],
        [ 2.3566, -3.7643],
        [ 2.9357, -4.6259],
        [ 3.1823, -4.5686],
        [ 1.6556, -3.0433],
        [ 3.4776, -5.3018],
        [ 2.6341, -4.0211],
        [ 2.2513, -3.6569],
        [ 1.2058, -2.6181],
        [ 1.7738, -3.7608],
        [ 2.1383, -3.5394],
        [ 1.8413, -3.3519]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 1.0092e-06],
        [4.1927e-06, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9593, 0.0407], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2018, 0.2114],
         [0.8742, 0.2503]],

        [[0.3465, 0.2494],
         [0.2348, 0.7611]],

        [[0.2305, 0.1819],
         [0.7322, 0.1775]],

        [[0.1560, 0.2852],
         [0.4284, 0.4120]],

        [[0.8034, 0.1696],
         [0.2547, 0.2713]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 7.13818506546007e-05
time is 1
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0031985518644306096
time is 2
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: -0.0041478895259491316
time is 3
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: -0.0041478895259491316
time is 4
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
Global Adjusted Rand Index: -0.00011146982936504288
Average Adjusted Rand Index: -0.0013113826389462243
Iteration 0: Loss = -24254.094783026772
Iteration 10: Loss = -12495.466742674244
Iteration 20: Loss = -12495.249535141513
Iteration 30: Loss = -12495.101402398845
Iteration 40: Loss = -12494.961948014468
Iteration 50: Loss = -12494.829621238898
Iteration 60: Loss = -12494.717855545387
Iteration 70: Loss = -12494.645208291782
Iteration 80: Loss = -12494.607426979663
Iteration 90: Loss = -12494.586308800725
Iteration 100: Loss = -12494.570748365404
Iteration 110: Loss = -12494.55715447071
Iteration 120: Loss = -12494.544691084833
Iteration 130: Loss = -12494.533345272575
Iteration 140: Loss = -12494.523104272317
Iteration 150: Loss = -12494.51393820992
Iteration 160: Loss = -12494.506155826823
Iteration 170: Loss = -12494.499656245343
Iteration 180: Loss = -12494.49450192697
Iteration 190: Loss = -12494.490604032466
Iteration 200: Loss = -12494.487982760722
Iteration 210: Loss = -12494.486303702695
Iteration 220: Loss = -12494.485546221198
Iteration 230: Loss = -12494.485553766946
1
Iteration 240: Loss = -12494.486078439619
2
Iteration 250: Loss = -12494.486941183726
3
Stopping early at iteration 249 due to no improvement.
pi: tensor([[0.0693, 0.9307],
        [0.0675, 0.9325]], dtype=torch.float64)
alpha: tensor([0.0679, 0.9321])
beta: tensor([[[0.2358, 0.2285],
         [0.6017, 0.1988]],

        [[0.2536, 0.2123],
         [0.3885, 0.0613]],

        [[0.0353, 0.1989],
         [0.5200, 0.5946]],

        [[0.2579, 0.2428],
         [0.6047, 0.1240]],

        [[0.6963, 0.1992],
         [0.9211, 0.0338]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24254.014374360184
Iteration 100: Loss = -12567.429790665567
Iteration 200: Loss = -12531.866458422237
Iteration 300: Loss = -12511.926373011082
Iteration 400: Loss = -12502.66036431193
Iteration 500: Loss = -12497.803483715172
Iteration 600: Loss = -12496.867282295014
Iteration 700: Loss = -12496.391084137233
Iteration 800: Loss = -12496.090849475046
Iteration 900: Loss = -12495.883835024359
Iteration 1000: Loss = -12495.732570392636
Iteration 1100: Loss = -12495.617152776253
Iteration 1200: Loss = -12495.526311356396
Iteration 1300: Loss = -12495.452679071606
Iteration 1400: Loss = -12495.391853533607
Iteration 1500: Loss = -12495.340632934116
Iteration 1600: Loss = -12495.297032946006
Iteration 1700: Loss = -12495.25958454177
Iteration 1800: Loss = -12495.226962268032
Iteration 1900: Loss = -12495.198245958074
Iteration 2000: Loss = -12495.172352146525
Iteration 2100: Loss = -12495.147868098586
Iteration 2200: Loss = -12495.123611763762
Iteration 2300: Loss = -12495.101522828383
Iteration 2400: Loss = -12495.083789505568
Iteration 2500: Loss = -12495.068100622238
Iteration 2600: Loss = -12495.053657430268
Iteration 2700: Loss = -12495.040306107256
Iteration 2800: Loss = -12495.028005363467
Iteration 2900: Loss = -12495.016751669767
Iteration 3000: Loss = -12495.006639855968
Iteration 3100: Loss = -12494.997570654787
Iteration 3200: Loss = -12494.9893510222
Iteration 3300: Loss = -12494.981862744991
Iteration 3400: Loss = -12494.974597954602
Iteration 3500: Loss = -12494.9673145005
Iteration 3600: Loss = -12494.960556460841
Iteration 3700: Loss = -12494.953973241349
Iteration 3800: Loss = -12494.947310377082
Iteration 3900: Loss = -12494.940757567254
Iteration 4000: Loss = -12494.934298126851
Iteration 4100: Loss = -12494.929281024863
Iteration 4200: Loss = -12494.926022324207
Iteration 4300: Loss = -12494.923484027328
Iteration 4400: Loss = -12494.92151336739
Iteration 4500: Loss = -12494.919835569746
Iteration 4600: Loss = -12494.918331292096
Iteration 4700: Loss = -12494.916989700607
Iteration 4800: Loss = -12494.915574999544
Iteration 4900: Loss = -12494.914037046978
Iteration 5000: Loss = -12494.911787046423
Iteration 5100: Loss = -12495.167128775927
1
Iteration 5200: Loss = -12494.871879033964
Iteration 5300: Loss = -12494.86527988428
Iteration 5400: Loss = -12494.859676661623
Iteration 5500: Loss = -12494.873951300233
1
Iteration 5600: Loss = -12494.850651618019
Iteration 5700: Loss = -12494.846941866692
Iteration 5800: Loss = -12494.843953813597
Iteration 5900: Loss = -12494.839950501995
Iteration 6000: Loss = -12494.837244241406
Iteration 6100: Loss = -12494.834640458483
Iteration 6200: Loss = -12494.851094748992
1
Iteration 6300: Loss = -12494.828934554116
Iteration 6400: Loss = -12494.825735113876
Iteration 6500: Loss = -12494.821956566637
Iteration 6600: Loss = -12494.855759331875
1
Iteration 6700: Loss = -12494.810430353078
Iteration 6800: Loss = -12494.800474312098
Iteration 6900: Loss = -12494.788630131734
Iteration 7000: Loss = -12494.760686871696
Iteration 7100: Loss = -12494.723776375307
Iteration 7200: Loss = -12494.678156330378
Iteration 7300: Loss = -12494.632361525199
Iteration 7400: Loss = -12494.570918196572
Iteration 7500: Loss = -12494.533400559458
Iteration 7600: Loss = -12494.48160927932
Iteration 7700: Loss = -12494.473953677714
Iteration 7800: Loss = -12494.461580112029
Iteration 7900: Loss = -12494.427045989776
Iteration 8000: Loss = -12494.398902178542
Iteration 8100: Loss = -12494.518162799943
1
Iteration 8200: Loss = -12494.334513162374
Iteration 8300: Loss = -12494.316776031284
Iteration 8400: Loss = -12494.29028231601
Iteration 8500: Loss = -12494.270388339066
Iteration 8600: Loss = -12494.2500088736
Iteration 8700: Loss = -12494.23257146533
Iteration 8800: Loss = -12494.222348890502
Iteration 8900: Loss = -12494.22041481751
Iteration 9000: Loss = -12494.43689278147
1
Iteration 9100: Loss = -12494.207669927915
Iteration 9200: Loss = -12494.197162224524
Iteration 9300: Loss = -12494.188487460895
Iteration 9400: Loss = -12494.18624957171
Iteration 9500: Loss = -12494.192483592793
1
Iteration 9600: Loss = -12494.188872273342
2
Iteration 9700: Loss = -12494.382182882653
3
Iteration 9800: Loss = -12494.178273388852
Iteration 9900: Loss = -12494.177894514061
Iteration 10000: Loss = -12494.185095924695
1
Iteration 10100: Loss = -12494.177032952364
Iteration 10200: Loss = -12494.176919223228
Iteration 10300: Loss = -12494.224321964082
1
Iteration 10400: Loss = -12494.176743106093
Iteration 10500: Loss = -12494.176556588716
Iteration 10600: Loss = -12494.179261556681
1
Iteration 10700: Loss = -12494.179862071018
2
Iteration 10800: Loss = -12494.176311050722
Iteration 10900: Loss = -12494.176321349969
1
Iteration 11000: Loss = -12494.188896641572
2
Iteration 11100: Loss = -12494.175946415538
Iteration 11200: Loss = -12494.175872639637
Iteration 11300: Loss = -12494.176146156544
1
Iteration 11400: Loss = -12494.175743441549
Iteration 11500: Loss = -12494.175798824153
1
Iteration 11600: Loss = -12494.17830917479
2
Iteration 11700: Loss = -12494.175652988784
Iteration 11800: Loss = -12494.198809158293
1
Iteration 11900: Loss = -12494.210981659746
2
Iteration 12000: Loss = -12494.238445894942
3
Iteration 12100: Loss = -12494.18462852769
4
Iteration 12200: Loss = -12494.397194487117
5
Iteration 12300: Loss = -12494.175442875918
Iteration 12400: Loss = -12494.363353830631
1
Iteration 12500: Loss = -12494.175385749386
Iteration 12600: Loss = -12494.175379757873
Iteration 12700: Loss = -12494.190652148278
1
Iteration 12800: Loss = -12494.175323153984
Iteration 12900: Loss = -12494.175289734349
Iteration 13000: Loss = -12494.502378349387
1
Iteration 13100: Loss = -12494.176370226807
2
Iteration 13200: Loss = -12494.278875407364
3
Iteration 13300: Loss = -12494.175221680707
Iteration 13400: Loss = -12494.233438399604
1
Iteration 13500: Loss = -12494.175189694684
Iteration 13600: Loss = -12494.265319351285
1
Iteration 13700: Loss = -12494.17513769162
Iteration 13800: Loss = -12494.20795476083
1
Iteration 13900: Loss = -12494.175403806206
2
Iteration 14000: Loss = -12494.175371544372
3
Iteration 14100: Loss = -12494.189715789546
4
Iteration 14200: Loss = -12494.175062520855
Iteration 14300: Loss = -12494.175656019841
1
Iteration 14400: Loss = -12494.175137007736
2
Iteration 14500: Loss = -12494.175164889844
3
Iteration 14600: Loss = -12494.175025219163
Iteration 14700: Loss = -12494.175480614877
1
Iteration 14800: Loss = -12494.175025344663
2
Iteration 14900: Loss = -12494.227393069585
3
Iteration 15000: Loss = -12494.17504295404
4
Iteration 15100: Loss = -12494.18497579063
5
Iteration 15200: Loss = -12494.203652504206
6
Iteration 15300: Loss = -12494.182219348095
7
Iteration 15400: Loss = -12494.228573509039
8
Iteration 15500: Loss = -12494.200458144285
9
Iteration 15600: Loss = -12494.220912577854
10
Stopping early at iteration 15600 due to no improvement.
tensor([[ 3.5021, -5.3732],
        [ 3.5384, -5.6127],
        [ 3.5494, -4.9574],
        [ 3.3504, -5.3417],
        [ 3.6824, -5.2933],
        [ 3.6582, -5.2957],
        [ 3.7537, -5.1673],
        [ 3.3161, -5.7311],
        [ 3.5005, -5.5159],
        [ 3.6204, -5.3980],
        [ 3.1286, -5.8979],
        [ 3.4130, -5.4085],
        [ 2.2296, -6.2468],
        [ 3.7402, -5.5660],
        [ 3.4898, -5.4311],
        [ 3.8415, -5.6141],
        [ 3.0371, -6.1075],
        [ 3.4546, -5.0154],
        [ 3.1916, -5.3780],
        [ 3.6588, -5.3133],
        [ 3.3402, -5.2048],
        [ 3.5898, -5.3149],
        [ 3.6833, -5.2777],
        [ 3.4986, -5.3024],
        [ 2.4969, -6.1566],
        [ 3.8188, -5.2062],
        [ 2.6674, -5.6948],
        [ 3.4823, -5.4329],
        [ 3.6722, -5.1784],
        [ 3.7760, -5.1626],
        [ 3.6186, -5.0506],
        [ 3.2327, -5.5679],
        [ 2.4280, -6.4985],
        [ 2.8246, -6.2368],
        [ 2.4966, -6.2934],
        [ 3.7621, -5.3943],
        [ 3.1031, -5.6258],
        [ 2.9207, -5.9557],
        [ 3.6228, -5.1478],
        [ 3.5937, -5.7344],
        [ 3.5290, -5.0896],
        [ 3.7266, -5.3195],
        [ 3.8027, -5.2283],
        [ 3.6155, -5.3458],
        [ 3.8180, -5.2159],
        [ 2.7658, -6.7890],
        [ 3.6861, -5.1011],
        [ 3.8741, -5.4722],
        [ 3.5234, -5.9055],
        [ 3.6422, -5.0732],
        [ 3.8163, -5.2110],
        [ 3.5706, -5.2005],
        [ 3.7596, -5.3047],
        [ 2.9368, -6.0344],
        [ 3.2909, -5.3563],
        [ 3.9049, -5.2921],
        [ 3.4724, -5.2438],
        [ 3.3437, -5.3361],
        [ 2.9043, -5.6449],
        [ 3.7701, -5.2733],
        [ 3.2934, -5.5594],
        [ 2.9021, -5.5445],
        [ 3.5726, -5.3016],
        [ 3.8309, -5.2172],
        [ 2.2882, -6.9035],
        [ 3.5156, -5.3333],
        [ 3.8680, -5.2831],
        [ 4.0374, -5.4517],
        [ 3.3944, -5.6329],
        [ 3.6069, -5.5591],
        [ 3.8517, -5.2691],
        [ 3.1971, -5.4845],
        [ 3.6194, -5.3880],
        [ 3.6868, -5.2885],
        [ 3.2042, -5.2286],
        [ 3.4813, -5.1116],
        [ 3.5856, -5.1298],
        [ 3.8739, -5.2632],
        [ 3.5267, -5.4893],
        [ 3.1917, -5.4190],
        [ 3.5028, -5.4023],
        [ 3.3627, -5.7472],
        [ 2.3494, -6.8499],
        [ 3.1097, -5.6948],
        [ 3.3571, -5.3961],
        [ 3.7213, -5.1088],
        [ 3.5426, -5.2916],
        [ 3.4248, -5.6346],
        [ 3.8072, -5.4337],
        [ 3.6260, -5.0123],
        [ 3.4396, -5.4075],
        [ 2.1453, -6.7605],
        [ 3.8151, -5.5272],
        [ 3.1645, -5.6911],
        [ 3.6412, -5.0276],
        [ 3.5212, -5.0380],
        [ 3.7368, -5.1877],
        [ 3.9999, -5.3865],
        [ 2.9726, -5.8430],
        [ 3.6319, -5.1203]], dtype=torch.float64, requires_grad=True)
pi: tensor([[8.4636e-06, 9.9999e-01],
        [5.4813e-02, 9.4519e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9986e-01, 1.3876e-04], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2076, 0.1969],
         [0.6017, 0.2007]],

        [[0.2536, 0.2290],
         [0.3885, 0.0613]],

        [[0.0353, 0.1951],
         [0.5200, 0.5946]],

        [[0.2579, 0.2478],
         [0.6047, 0.1240]],

        [[0.6963, 0.1962],
         [0.9211, 0.0338]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0016114094845201586
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -18392.9691913913
Iteration 10: Loss = -12495.361375599972
Iteration 20: Loss = -12495.33173415606
Iteration 30: Loss = -12495.256740058432
Iteration 40: Loss = -12495.112062563348
Iteration 50: Loss = -12494.935022968666
Iteration 60: Loss = -12494.791588122194
Iteration 70: Loss = -12494.697254003544
Iteration 80: Loss = -12494.63767869775
Iteration 90: Loss = -12494.59928871253
Iteration 100: Loss = -12494.573737708712
Iteration 110: Loss = -12494.55609569792
Iteration 120: Loss = -12494.543683608328
Iteration 130: Loss = -12494.534663535704
Iteration 140: Loss = -12494.527952280263
Iteration 150: Loss = -12494.522860896011
Iteration 160: Loss = -12494.51901217475
Iteration 170: Loss = -12494.515961867328
Iteration 180: Loss = -12494.513617956954
Iteration 190: Loss = -12494.51172327609
Iteration 200: Loss = -12494.510179314848
Iteration 210: Loss = -12494.508956500718
Iteration 220: Loss = -12494.508002469862
Iteration 230: Loss = -12494.507203957264
Iteration 240: Loss = -12494.506506343569
Iteration 250: Loss = -12494.505986853816
Iteration 260: Loss = -12494.505505151667
Iteration 270: Loss = -12494.505136697499
Iteration 280: Loss = -12494.504791440746
Iteration 290: Loss = -12494.50454141223
Iteration 300: Loss = -12494.504349603361
Iteration 310: Loss = -12494.504156699111
Iteration 320: Loss = -12494.504026554663
Iteration 330: Loss = -12494.503867668634
Iteration 340: Loss = -12494.503774592375
Iteration 350: Loss = -12494.503684491037
Iteration 360: Loss = -12494.503581050005
Iteration 370: Loss = -12494.503550621615
Iteration 380: Loss = -12494.503481875132
Iteration 390: Loss = -12494.503421057127
Iteration 400: Loss = -12494.503390185422
Iteration 410: Loss = -12494.503376263896
Iteration 420: Loss = -12494.503308981613
Iteration 430: Loss = -12494.503325359059
1
Iteration 440: Loss = -12494.503279468168
Iteration 450: Loss = -12494.50326725472
Iteration 460: Loss = -12494.503267122042
Iteration 470: Loss = -12494.503246552093
Iteration 480: Loss = -12494.503241978648
Iteration 490: Loss = -12494.503210026673
Iteration 500: Loss = -12494.503207570016
Iteration 510: Loss = -12494.503239484433
1
Iteration 520: Loss = -12494.503176915145
Iteration 530: Loss = -12494.503203995673
1
Iteration 540: Loss = -12494.503165114815
Iteration 550: Loss = -12494.503200845935
1
Iteration 560: Loss = -12494.503214150674
2
Iteration 570: Loss = -12494.503182086404
3
Stopping early at iteration 569 due to no improvement.
pi: tensor([[0.0651, 0.9349],
        [0.0532, 0.9468]], dtype=torch.float64)
alpha: tensor([0.0541, 0.9459])
beta: tensor([[[0.2377, 0.2324],
         [0.8150, 0.1992]],

        [[0.3938, 0.2124],
         [0.1118, 0.7268]],

        [[0.0621, 0.1968],
         [0.5591, 0.0142]],

        [[0.3561, 0.2477],
         [0.5139, 0.8868]],

        [[0.4808, 0.1973],
         [0.0983, 0.1801]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18393.021257095024
Iteration 100: Loss = -12525.080985582434
Iteration 200: Loss = -12502.382035345507
Iteration 300: Loss = -12496.792984433623
Iteration 400: Loss = -12495.311042375391
Iteration 500: Loss = -12494.818761065231
Iteration 600: Loss = -12494.632265720831
Iteration 700: Loss = -12494.532092822974
Iteration 800: Loss = -12494.467680297328
Iteration 900: Loss = -12494.42174603006
Iteration 1000: Loss = -12494.387033228622
Iteration 1100: Loss = -12494.358212344998
Iteration 1200: Loss = -12494.333722220614
Iteration 1300: Loss = -12494.505710416033
1
Iteration 1400: Loss = -12494.298488892404
Iteration 1500: Loss = -12494.284534817045
Iteration 1600: Loss = -12494.271781126285
Iteration 1700: Loss = -12494.260087386183
Iteration 1800: Loss = -12494.249644425054
Iteration 1900: Loss = -12494.241710453918
Iteration 2000: Loss = -12494.238229479763
Iteration 2100: Loss = -12494.22846256675
Iteration 2200: Loss = -12494.222886875996
Iteration 2300: Loss = -12494.44428327975
1
Iteration 2400: Loss = -12494.21326882835
Iteration 2500: Loss = -12494.209092331079
Iteration 2600: Loss = -12494.20558632939
Iteration 2700: Loss = -12494.20419341252
Iteration 2800: Loss = -12494.199604363976
Iteration 2900: Loss = -12494.197047647536
Iteration 3000: Loss = -12494.194732823866
Iteration 3100: Loss = -12494.192591751686
Iteration 3200: Loss = -12494.190705273611
Iteration 3300: Loss = -12494.189013167417
Iteration 3400: Loss = -12494.191734292375
1
Iteration 3500: Loss = -12494.186050863798
Iteration 3600: Loss = -12494.184829837253
Iteration 3700: Loss = -12494.219927636377
1
Iteration 3800: Loss = -12494.18275547934
Iteration 3900: Loss = -12494.181869811226
Iteration 4000: Loss = -12494.181055118357
Iteration 4100: Loss = -12494.18323914036
1
Iteration 4200: Loss = -12494.17961509209
Iteration 4300: Loss = -12494.179040167752
Iteration 4400: Loss = -12494.17862214254
Iteration 4500: Loss = -12494.177982744739
Iteration 4600: Loss = -12494.177407549892
Iteration 4700: Loss = -12494.176929666713
Iteration 4800: Loss = -12494.178379564024
1
Iteration 4900: Loss = -12494.176118607691
Iteration 5000: Loss = -12494.17574928437
Iteration 5100: Loss = -12494.227081595209
1
Iteration 5200: Loss = -12494.175055878182
Iteration 5300: Loss = -12494.174744410122
Iteration 5400: Loss = -12494.174464552492
Iteration 5500: Loss = -12494.174207813307
Iteration 5600: Loss = -12494.173901382826
Iteration 5700: Loss = -12494.173584973061
Iteration 5800: Loss = -12494.189488849615
1
Iteration 5900: Loss = -12494.17305671191
Iteration 6000: Loss = -12494.172750046788
Iteration 6100: Loss = -12494.229790791045
1
Iteration 6200: Loss = -12494.172288330283
Iteration 6300: Loss = -12494.171935477825
Iteration 6400: Loss = -12494.171651626057
Iteration 6500: Loss = -12494.17208952562
1
Iteration 6600: Loss = -12494.171004444297
Iteration 6700: Loss = -12494.170668578397
Iteration 6800: Loss = -12494.172738340701
1
Iteration 6900: Loss = -12494.16986128446
Iteration 7000: Loss = -12494.169470413428
Iteration 7100: Loss = -12494.212201844362
1
Iteration 7200: Loss = -12494.168354177542
Iteration 7300: Loss = -12494.167663302582
Iteration 7400: Loss = -12494.168258229436
1
Iteration 7500: Loss = -12494.165994759103
Iteration 7600: Loss = -12494.164890151302
Iteration 7700: Loss = -12494.163671344411
Iteration 7800: Loss = -12494.16198423891
Iteration 7900: Loss = -12494.160246133759
Iteration 8000: Loss = -12494.157294082257
Iteration 8100: Loss = -12494.154192466915
Iteration 8200: Loss = -12494.148991707743
Iteration 8300: Loss = -12494.141341757379
Iteration 8400: Loss = -12494.148387083684
1
Iteration 8500: Loss = -12494.096974812734
Iteration 8600: Loss = -12493.987659866274
Iteration 8700: Loss = -12493.962663026052
Iteration 8800: Loss = -12493.960382733922
Iteration 8900: Loss = -12493.948371162107
Iteration 9000: Loss = -12493.946199706841
Iteration 9100: Loss = -12493.947641279987
1
Iteration 9200: Loss = -12493.943583349857
Iteration 9300: Loss = -12494.266587806302
1
Iteration 9400: Loss = -12493.942218173626
Iteration 9500: Loss = -12493.943550341579
1
Iteration 9600: Loss = -12493.941365106251
Iteration 9700: Loss = -12493.940965848373
Iteration 9800: Loss = -12493.942570007124
1
Iteration 9900: Loss = -12493.942571409814
2
Iteration 10000: Loss = -12493.940325235477
Iteration 10100: Loss = -12493.953000499212
1
Iteration 10200: Loss = -12493.940044176608
Iteration 10300: Loss = -12494.024724958515
1
Iteration 10400: Loss = -12493.939739344976
Iteration 10500: Loss = -12493.93964053793
Iteration 10600: Loss = -12493.939981119804
1
Iteration 10700: Loss = -12493.939502146202
Iteration 10800: Loss = -12494.367256466918
1
Iteration 10900: Loss = -12493.939392185433
Iteration 11000: Loss = -12493.939401164385
1
Iteration 11100: Loss = -12493.939292630199
Iteration 11200: Loss = -12493.939299564068
1
Iteration 11300: Loss = -12494.212845753314
2
Iteration 11400: Loss = -12493.939115416735
Iteration 11500: Loss = -12493.93917824371
1
Iteration 11600: Loss = -12493.994732051546
2
Iteration 11700: Loss = -12493.938971054304
Iteration 11800: Loss = -12493.938956377711
Iteration 11900: Loss = -12494.032671821476
1
Iteration 12000: Loss = -12493.958204553053
2
Iteration 12100: Loss = -12493.938864036285
Iteration 12200: Loss = -12494.017758171818
1
Iteration 12300: Loss = -12493.939050974135
2
Iteration 12400: Loss = -12493.93879949631
Iteration 12500: Loss = -12493.94096921932
1
Iteration 12600: Loss = -12493.972021517455
2
Iteration 12700: Loss = -12493.938791918106
Iteration 12800: Loss = -12493.99263809412
1
Iteration 12900: Loss = -12493.938781502213
Iteration 13000: Loss = -12493.945868883166
1
Iteration 13100: Loss = -12493.938732091252
Iteration 13200: Loss = -12493.939236766799
1
Iteration 13300: Loss = -12493.95239488952
2
Iteration 13400: Loss = -12493.961888033966
3
Iteration 13500: Loss = -12493.93862955799
Iteration 13600: Loss = -12493.93909459133
1
Iteration 13700: Loss = -12493.938617680411
Iteration 13800: Loss = -12493.95488810499
1
Iteration 13900: Loss = -12493.93862450969
2
Iteration 14000: Loss = -12493.969064041037
3
Iteration 14100: Loss = -12493.938673886809
4
Iteration 14200: Loss = -12493.938606979103
Iteration 14300: Loss = -12493.93868724254
1
Iteration 14400: Loss = -12493.939370119051
2
Iteration 14500: Loss = -12493.938689698794
3
Iteration 14600: Loss = -12493.968751472741
4
Iteration 14700: Loss = -12493.965129090364
5
Iteration 14800: Loss = -12493.941493873033
6
Iteration 14900: Loss = -12493.952021300145
7
Iteration 15000: Loss = -12493.939207540285
8
Iteration 15100: Loss = -12493.939972873148
9
Iteration 15200: Loss = -12493.941582727572
10
Stopping early at iteration 15200 due to no improvement.
tensor([[ 3.1066, -6.0228],
        [ 3.7661, -5.1708],
        [ 3.2853, -4.8110],
        [ 3.6130, -5.1008],
        [ 2.8049, -5.9524],
        [ 3.6494, -5.0454],
        [ 1.9940, -6.6021],
        [ 3.2902, -5.5894],
        [ 3.6742, -5.1188],
        [ 3.7376, -5.1782],
        [ 3.6413, -5.1276],
        [ 3.4397, -5.0429],
        [ 3.3787, -4.7729],
        [ 3.8814, -5.3404],
        [ 3.6523, -5.3048],
        [ 3.8133, -5.6618],
        [ 3.4072, -5.7330],
        [ 3.2501, -4.7738],
        [ 2.8815, -5.1061],
        [ 3.5636, -5.0974],
        [ 3.3080, -4.8122],
        [ 2.7968, -5.7612],
        [ 2.0242, -6.6394],
        [ 3.1162, -5.5205],
        [ 3.2884, -5.0572],
        [ 3.5572, -5.2171],
        [ 2.2678, -5.5470],
        [ 3.2495, -5.5176],
        [ 2.8078, -5.8489],
        [ 3.2739, -5.2893],
        [ 3.4620, -4.8546],
        [ 3.1297, -5.5215],
        [ 3.0495, -5.6474],
        [ 2.7146, -6.0693],
        [ 3.5483, -4.9409],
        [ 3.8113, -5.3563],
        [ 3.1117, -5.1559],
        [ 3.2614, -5.2361],
        [ 2.6849, -5.7055],
        [ 3.7536, -5.5182],
        [ 3.0942, -5.0973],
        [ 3.6283, -5.0865],
        [ 2.6920, -6.1168],
        [ 3.3703, -5.2897],
        [ 3.7945, -5.1912],
        [ 3.7239, -6.0461],
        [ 3.4778, -4.9393],
        [ 3.0981, -6.1314],
        [ 3.9522, -5.3954],
        [ 3.2440, -5.1597],
        [ 2.5491, -6.2016],
        [ 2.8440, -5.5495],
        [ 3.9175, -5.8039],
        [ 3.5562, -5.1319],
        [ 3.4816, -5.2395],
        [ 3.8324, -5.2216],
        [ 3.2949, -5.1218],
        [ 3.3501, -4.8023],
        [ 2.5492, -5.4069],
        [ 3.5720, -5.1722],
        [ 2.2496, -6.2367],
        [ 3.2557, -4.6557],
        [ 3.4669, -5.1651],
        [ 3.6591, -5.0725],
        [ 3.8804, -5.2668],
        [ 3.5327, -4.9290],
        [ 3.7131, -5.2497],
        [ 4.0270, -5.4264],
        [ 3.6485, -5.1265],
        [ 3.9144, -5.3066],
        [ 2.1765, -6.7917],
        [ 3.4436, -5.0323],
        [ 3.7497, -5.1379],
        [ 3.4730, -5.1701],
        [ 3.1896, -4.5842],
        [ 2.2452, -5.9575],
        [ 3.4589, -4.8566],
        [ 3.5543, -5.4320],
        [ 2.0776, -6.6928],
        [ 3.1159, -4.9348],
        [ 3.4451, -5.1163],
        [ 2.7969, -6.0466],
        [ 3.8387, -5.2844],
        [ 3.5293, -4.9680],
        [ 3.4441, -4.9490],
        [ 3.5487, -4.9388],
        [ 3.6063, -5.0038],
        [ 3.6285, -5.3321],
        [ 3.8212, -5.2544],
        [ 2.8761, -5.3388],
        [ 3.4133, -5.0422],
        [ 3.5639, -5.0130],
        [ 3.7261, -5.5637],
        [ 3.4010, -5.0483],
        [ 3.3282, -4.8726],
        [ 3.0895, -5.0282],
        [ 3.5334, -5.2248],
        [ 3.8275, -5.5565],
        [ 2.7232, -5.6854],
        [ 3.5100, -4.9113]], dtype=torch.float64, requires_grad=True)
pi: tensor([[3.5637e-01, 6.4363e-01],
        [9.6316e-06, 9.9999e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9981e-01, 1.8711e-04], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2071, 0.1915],
         [0.8150, 0.2007]],

        [[0.3938, 0.2046],
         [0.1118, 0.7268]],

        [[0.0621, 0.1954],
         [0.5591, 0.0142]],

        [[0.3561, 0.2614],
         [0.5139, 0.8868]],

        [[0.4808, 0.1752],
         [0.0983, 0.1801]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0031985518644306096
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.006509078451524495
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.006509078451524495
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.003594178668686027
Average Adjusted Rand Index: 0.0032433417534959196
Iteration 0: Loss = -18361.428048620517
Iteration 10: Loss = -12495.073807481698
Iteration 20: Loss = -12494.829645441614
Iteration 30: Loss = -12494.785959529527
Iteration 40: Loss = -12494.770334849945
Iteration 50: Loss = -12494.761500133382
Iteration 60: Loss = -12494.754633122999
Iteration 70: Loss = -12494.748308431634
Iteration 80: Loss = -12494.741831266125
Iteration 90: Loss = -12494.735147798869
Iteration 100: Loss = -12494.728098139742
Iteration 110: Loss = -12494.72064890889
Iteration 120: Loss = -12494.712862594723
Iteration 130: Loss = -12494.704626323271
Iteration 140: Loss = -12494.695994700543
Iteration 150: Loss = -12494.686938927241
Iteration 160: Loss = -12494.677575056612
Iteration 170: Loss = -12494.667749457034
Iteration 180: Loss = -12494.657526788611
Iteration 190: Loss = -12494.646855152952
Iteration 200: Loss = -12494.63592059441
Iteration 210: Loss = -12494.624545360559
Iteration 220: Loss = -12494.612984884972
Iteration 230: Loss = -12494.601084673755
Iteration 240: Loss = -12494.589090721885
Iteration 250: Loss = -12494.577117679066
Iteration 260: Loss = -12494.565191990396
Iteration 270: Loss = -12494.55354662679
Iteration 280: Loss = -12494.542349335665
Iteration 290: Loss = -12494.531839578398
Iteration 300: Loss = -12494.52214448376
Iteration 310: Loss = -12494.513382119281
Iteration 320: Loss = -12494.50587436756
Iteration 330: Loss = -12494.499539343771
Iteration 340: Loss = -12494.494489539771
Iteration 350: Loss = -12494.49069266812
Iteration 360: Loss = -12494.488010472098
Iteration 370: Loss = -12494.48637360624
Iteration 380: Loss = -12494.485614206367
Iteration 390: Loss = -12494.485563457849
Iteration 400: Loss = -12494.486027405399
1
Iteration 410: Loss = -12494.486905595346
2
Iteration 420: Loss = -12494.488045831138
3
Stopping early at iteration 419 due to no improvement.
pi: tensor([[0.9344, 0.0656],
        [0.9330, 0.0670]], dtype=torch.float64)
alpha: tensor([0.9340, 0.0660])
beta: tensor([[[0.1988, 0.2290],
         [0.2081, 0.2361]],

        [[0.0769, 0.2123],
         [0.7629, 0.5932]],

        [[0.2528, 0.1987],
         [0.6512, 0.2199]],

        [[0.3885, 0.2434],
         [0.3529, 0.6782]],

        [[0.7635, 0.1991],
         [0.3098, 0.7183]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18339.61042901507
Iteration 100: Loss = -12500.546786568933
Iteration 200: Loss = -12495.32653609918
Iteration 300: Loss = -12494.779577977595
Iteration 400: Loss = -12494.63306438115
Iteration 500: Loss = -12494.571054028911
Iteration 600: Loss = -12494.535862257073
Iteration 700: Loss = -12494.509969998588
Iteration 800: Loss = -12494.491526160227
Iteration 900: Loss = -12494.476528481418
Iteration 1000: Loss = -12494.46361481315
Iteration 1100: Loss = -12494.452361271125
Iteration 1200: Loss = -12494.442461220968
Iteration 1300: Loss = -12494.43370237521
Iteration 1400: Loss = -12494.426004345087
Iteration 1500: Loss = -12494.418891311521
Iteration 1600: Loss = -12494.412161806657
Iteration 1700: Loss = -12494.405563399217
Iteration 1800: Loss = -12494.398930197647
Iteration 1900: Loss = -12494.392226564862
Iteration 2000: Loss = -12494.385330832454
Iteration 2100: Loss = -12494.378152001707
Iteration 2200: Loss = -12494.370564330715
Iteration 2300: Loss = -12494.362356795322
Iteration 2400: Loss = -12494.3531949549
Iteration 2500: Loss = -12494.34238599627
Iteration 2600: Loss = -12494.328697104174
Iteration 2700: Loss = -12494.309050414844
Iteration 2800: Loss = -12494.275794933028
Iteration 2900: Loss = -12494.213369939233
Iteration 3000: Loss = -12494.135357529156
Iteration 3100: Loss = -12494.092608078277
Iteration 3200: Loss = -12494.067444243065
Iteration 3300: Loss = -12494.045095874386
Iteration 3400: Loss = -12494.025357183311
Iteration 3500: Loss = -12494.008709898178
Iteration 3600: Loss = -12493.995281894155
Iteration 3700: Loss = -12493.984714202228
Iteration 3800: Loss = -12493.976599636537
Iteration 3900: Loss = -12493.970394098482
Iteration 4000: Loss = -12493.96546771441
Iteration 4100: Loss = -12493.961567091748
Iteration 4200: Loss = -12493.958542348782
Iteration 4300: Loss = -12493.956049480088
Iteration 4400: Loss = -12493.96016468987
1
Iteration 4500: Loss = -12493.952229923996
Iteration 4600: Loss = -12493.950788198745
Iteration 4700: Loss = -12493.949502653897
Iteration 4800: Loss = -12493.94847265933
Iteration 4900: Loss = -12493.947544086586
Iteration 5000: Loss = -12493.946773697759
Iteration 5100: Loss = -12493.949565585419
1
Iteration 5200: Loss = -12493.945364787482
Iteration 5300: Loss = -12493.960752114343
1
Iteration 5400: Loss = -12493.944240612667
Iteration 5500: Loss = -12493.94377297179
Iteration 5600: Loss = -12493.944352357044
1
Iteration 5700: Loss = -12493.942930214605
Iteration 5800: Loss = -12493.942576118265
Iteration 5900: Loss = -12493.947083163433
1
Iteration 6000: Loss = -12493.942277416312
Iteration 6100: Loss = -12493.941721754572
Iteration 6200: Loss = -12493.992204383729
1
Iteration 6300: Loss = -12493.941224496035
Iteration 6400: Loss = -12494.0925637905
1
Iteration 6500: Loss = -12493.944686113031
2
Iteration 6600: Loss = -12493.940604662796
Iteration 6700: Loss = -12493.940659950078
1
Iteration 6800: Loss = -12493.943181417548
2
Iteration 6900: Loss = -12494.397799339273
3
Iteration 7000: Loss = -12493.939991324754
Iteration 7100: Loss = -12493.94014039728
1
Iteration 7200: Loss = -12493.942694233952
2
Iteration 7300: Loss = -12493.93951246742
Iteration 7400: Loss = -12493.940009109978
1
Iteration 7500: Loss = -12493.939305314116
Iteration 7600: Loss = -12493.962564158895
1
Iteration 7700: Loss = -12493.939097734274
Iteration 7800: Loss = -12493.939177688566
1
Iteration 7900: Loss = -12493.938921483536
Iteration 8000: Loss = -12493.939429679906
1
Iteration 8100: Loss = -12493.941641110605
2
Iteration 8200: Loss = -12493.96557486091
3
Iteration 8300: Loss = -12493.943212457192
4
Iteration 8400: Loss = -12493.938543497881
Iteration 8500: Loss = -12493.949045060297
1
Iteration 8600: Loss = -12493.938391549369
Iteration 8700: Loss = -12493.93853229305
1
Iteration 8800: Loss = -12493.938276638282
Iteration 8900: Loss = -12493.938274438877
Iteration 9000: Loss = -12493.938872317565
1
Iteration 9100: Loss = -12493.938132182126
Iteration 9200: Loss = -12493.940811516466
1
Iteration 9300: Loss = -12493.937945741907
Iteration 9400: Loss = -12493.938265421872
1
Iteration 9500: Loss = -12493.937895497887
Iteration 9600: Loss = -12493.96504042494
1
Iteration 9700: Loss = -12493.937680491863
Iteration 9800: Loss = -12493.939201434849
1
Iteration 9900: Loss = -12493.937484531396
Iteration 10000: Loss = -12493.937597597993
1
Iteration 10100: Loss = -12493.968325095668
2
Iteration 10200: Loss = -12493.937223812658
Iteration 10300: Loss = -12493.936861514243
Iteration 10400: Loss = -12493.937717372224
1
Iteration 10500: Loss = -12493.939490059001
2
Iteration 10600: Loss = -12493.937806225926
3
Iteration 10700: Loss = -12493.93478060132
Iteration 10800: Loss = -12493.933121921542
Iteration 10900: Loss = -12493.930410292274
Iteration 11000: Loss = -12493.955219154646
1
Iteration 11100: Loss = -12493.899609762768
Iteration 11200: Loss = -12493.744283682114
Iteration 11300: Loss = -12492.577761320952
Iteration 11400: Loss = -12492.526336279496
Iteration 11500: Loss = -12492.517623522868
Iteration 11600: Loss = -12492.511612508495
Iteration 11700: Loss = -12492.508987548641
Iteration 11800: Loss = -12492.507400621
Iteration 11900: Loss = -12492.506086362398
Iteration 12000: Loss = -12492.588242267357
1
Iteration 12100: Loss = -12492.504564602004
Iteration 12200: Loss = -12492.503947054063
Iteration 12300: Loss = -12492.670157841187
1
Iteration 12400: Loss = -12492.503187407237
Iteration 12500: Loss = -12492.50286104022
Iteration 12600: Loss = -12492.50260766579
Iteration 12700: Loss = -12492.502505104367
Iteration 12800: Loss = -12492.502155662403
Iteration 12900: Loss = -12492.5020333947
Iteration 13000: Loss = -12492.50188056147
Iteration 13100: Loss = -12492.501726661498
Iteration 13200: Loss = -12492.501603492316
Iteration 13300: Loss = -12492.513560242638
1
Iteration 13400: Loss = -12492.501408602824
Iteration 13500: Loss = -12492.5013054057
Iteration 13600: Loss = -12492.519241462165
1
Iteration 13700: Loss = -12492.501164162437
Iteration 13800: Loss = -12492.501076657109
Iteration 13900: Loss = -12492.50674335153
1
Iteration 14000: Loss = -12492.501007851819
Iteration 14100: Loss = -12492.500887769895
Iteration 14200: Loss = -12492.500871367563
Iteration 14300: Loss = -12492.5065248275
1
Iteration 14400: Loss = -12492.500787658402
Iteration 14500: Loss = -12492.500739695724
Iteration 14600: Loss = -12492.501029155663
1
Iteration 14700: Loss = -12492.500684821342
Iteration 14800: Loss = -12492.500654360389
Iteration 14900: Loss = -12492.514961580695
1
Iteration 15000: Loss = -12492.50056096407
Iteration 15100: Loss = -12492.50056886233
1
Iteration 15200: Loss = -12492.502060719129
2
Iteration 15300: Loss = -12492.500509275293
Iteration 15400: Loss = -12492.5004795799
Iteration 15500: Loss = -12492.50046630135
Iteration 15600: Loss = -12492.502488654967
1
Iteration 15700: Loss = -12492.500445571352
Iteration 15800: Loss = -12492.500412702846
Iteration 15900: Loss = -12492.602906884847
1
Iteration 16000: Loss = -12492.500431372695
2
Iteration 16100: Loss = -12492.500357050043
Iteration 16200: Loss = -12492.500525622574
1
Iteration 16300: Loss = -12492.500381398442
2
Iteration 16400: Loss = -12492.500348072914
Iteration 16500: Loss = -12492.500323042234
Iteration 16600: Loss = -12492.500508583424
1
Iteration 16700: Loss = -12492.500309484498
Iteration 16800: Loss = -12492.50029652662
Iteration 16900: Loss = -12492.501419563083
1
Iteration 17000: Loss = -12492.500297860825
2
Iteration 17100: Loss = -12492.500293187028
Iteration 17200: Loss = -12492.501148716365
1
Iteration 17300: Loss = -12492.500282723622
Iteration 17400: Loss = -12492.500256252146
Iteration 17500: Loss = -12492.504867128373
1
Iteration 17600: Loss = -12492.500249531236
Iteration 17700: Loss = -12492.500209357517
Iteration 17800: Loss = -12492.514912198603
1
Iteration 17900: Loss = -12492.50022010477
2
Iteration 18000: Loss = -12492.500203901907
Iteration 18100: Loss = -12492.62984328665
1
Iteration 18200: Loss = -12492.50025317269
2
Iteration 18300: Loss = -12492.500211169863
3
Iteration 18400: Loss = -12492.516680151784
4
Iteration 18500: Loss = -12492.500205143535
5
Iteration 18600: Loss = -12492.50020373729
Iteration 18700: Loss = -12492.507211609
1
Iteration 18800: Loss = -12492.500217872948
2
Iteration 18900: Loss = -12492.500219528354
3
Iteration 19000: Loss = -12492.500218512563
4
Iteration 19100: Loss = -12492.500712629828
5
Iteration 19200: Loss = -12492.500204220847
6
Iteration 19300: Loss = -12492.5001632591
Iteration 19400: Loss = -12492.501470523706
1
Iteration 19500: Loss = -12492.500183798398
2
Iteration 19600: Loss = -12492.500179335242
3
Iteration 19700: Loss = -12492.50320888862
4
Iteration 19800: Loss = -12492.500193272073
5
Iteration 19900: Loss = -12492.500192472087
6
tensor([[-1.1793, -0.3623],
        [ 4.2827, -5.9279],
        [ 0.2965, -3.4049],
        [-0.7084, -0.7085],
        [ 1.5504, -3.0809],
        [ 2.5061, -3.9206],
        [ 2.2443, -3.9236],
        [ 2.5338, -4.0334],
        [ 1.9709, -4.3464],
        [ 1.6180, -3.0331],
        [ 2.0956, -4.5576],
        [ 2.7425, -4.7259],
        [-0.5468, -3.2411],
        [ 1.0999, -2.4952],
        [-0.2915, -1.2107],
        [ 1.1571, -3.0306],
        [ 0.7563, -2.1440],
        [ 1.7848, -3.4314],
        [ 2.3482, -3.8118],
        [ 3.2386, -5.5785],
        [ 2.2895, -3.8475],
        [ 3.8539, -5.4102],
        [ 2.4700, -4.1255],
        [ 0.9951, -2.8488],
        [ 1.3378, -2.8156],
        [ 2.9165, -4.4001],
        [ 3.5438, -4.9892],
        [ 0.0542, -2.3782],
        [ 1.8425, -3.4989],
        [ 3.8186, -5.6516],
        [ 2.4131, -3.8271],
        [ 0.9424, -3.2025],
        [ 2.6583, -4.9874],
        [ 2.8716, -4.5430],
        [ 2.5871, -4.2635],
        [-0.5545, -2.1909],
        [ 3.4313, -4.8644],
        [ 2.3292, -3.7318],
        [ 3.2594, -4.9427],
        [ 2.1116, -3.7882],
        [ 2.7952, -5.4699],
        [ 3.7859, -5.6982],
        [ 1.0826, -3.8173],
        [ 2.7874, -5.2580],
        [ 0.8916, -2.2916],
        [ 2.0803, -4.0403],
        [ 1.7891, -3.2432],
        [ 3.1082, -5.5079],
        [ 3.4002, -6.0802],
        [ 1.1875, -2.9211],
        [ 1.9589, -5.8172],
        [ 2.8923, -4.6383],
        [-1.9983, -0.0492],
        [ 1.7289, -3.4177],
        [-1.3488, -0.8449],
        [ 3.3968, -5.0671],
        [ 2.5081, -4.3999],
        [ 3.5432, -5.3642],
        [ 3.5131, -6.6459],
        [ 3.4387, -5.1904],
        [ 2.2168, -3.7300],
        [ 2.2125, -4.1840],
        [ 1.8194, -3.4580],
        [ 3.3643, -4.9194],
        [ 0.1710, -3.3761],
        [ 0.6776, -5.2928],
        [ 2.9723, -4.4416],
        [ 1.1762, -5.4220],
        [ 2.4000, -4.0875],
        [ 1.8443, -3.5757],
        [ 3.1060, -4.4989],
        [-0.1462, -1.4745],
        [ 0.2254, -2.2967],
        [ 2.9806, -4.5279],
        [ 2.6274, -5.0161],
        [ 0.8346, -2.2969],
        [ 1.8455, -3.2318],
        [ 1.7129, -3.3686],
        [ 2.0425, -5.7752],
        [ 4.0047, -5.5946],
        [ 2.8947, -4.2863],
        [ 2.6313, -4.5035],
        [ 1.1507, -3.9728],
        [ 3.4188, -5.0939],
        [ 3.3286, -4.7369],
        [ 2.2488, -3.6356],
        [ 2.1784, -4.3281],
        [-0.0615, -3.6488],
        [ 2.1065, -3.5091],
        [ 2.1131, -4.0114],
        [ 3.0496, -4.5106],
        [ 2.1976, -5.5511],
        [ 1.4135, -3.2849],
        [ 3.5325, -5.2430],
        [ 2.3418, -4.3160],
        [ 2.1519, -3.7581],
        [ 1.2133, -2.6116],
        [ 2.0528, -3.4809],
        [ 1.6406, -4.0411],
        [ 1.9004, -3.2953]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 1.1818e-07],
        [1.1095e-05, 9.9999e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9594, 0.0406], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2020, 0.2113],
         [0.2081, 0.2503]],

        [[0.0769, 0.2494],
         [0.7629, 0.5932]],

        [[0.2528, 0.1822],
         [0.6512, 0.2199]],

        [[0.3885, 0.2851],
         [0.3529, 0.6782]],

        [[0.7635, 0.1698],
         [0.3098, 0.7183]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 7.13818506546007e-05
time is 1
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0031985518644306096
time is 2
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: -0.0041478895259491316
time is 3
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: -0.0041478895259491316
time is 4
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
Global Adjusted Rand Index: -0.00011146982936504288
Average Adjusted Rand Index: -0.0013113826389462243
11969.978062852699
new:  [-0.00011146982936504288, 0.0016114094845201586, 0.003594178668686027, -0.00011146982936504288] [-0.0013113826389462243, 0.0, 0.0032433417534959196, -0.0013113826389462243] [12492.500178584867, 12494.220912577854, 12493.941582727572, 12492.501111984953]
prior:  [0.0, 0.0, 0.0, 0.0] [0.0, 0.0, 0.0, 0.0] [12494.503210508192, 12494.486941183726, 12494.503182086404, 12494.488045831138]
-----------------------------------------------------------------------------------------
This iteration is 6
True Objective function: Loss = -11685.494931225014
Iteration 0: Loss = -20863.605843874666
Iteration 10: Loss = -12190.108342845524
Iteration 20: Loss = -12190.108514294207
1
Iteration 30: Loss = -12190.1083177594
Iteration 40: Loss = -12190.108089763104
Iteration 50: Loss = -12190.107706936316
Iteration 60: Loss = -12190.104760765174
Iteration 70: Loss = -12190.086414347921
Iteration 80: Loss = -12189.990882537111
Iteration 90: Loss = -12189.72997366574
Iteration 100: Loss = -12189.521725786546
Iteration 110: Loss = -12189.508128197145
Iteration 120: Loss = -12189.506981320343
Iteration 130: Loss = -12189.507028947068
1
Iteration 140: Loss = -12189.507228265871
2
Iteration 150: Loss = -12189.507469601747
3
Stopping early at iteration 149 due to no improvement.
pi: tensor([[1.4633e-25, 1.0000e+00],
        [1.3642e-02, 9.8636e-01]], dtype=torch.float64)
alpha: tensor([0.0135, 0.9865])
beta: tensor([[[0.1931, 0.1872],
         [0.6227, 0.1921]],

        [[0.1738, 0.1678],
         [0.1304, 0.3094]],

        [[0.9420, 0.2555],
         [0.1793, 0.3093]],

        [[0.9348, 0.2788],
         [0.0319, 0.9985]],

        [[0.9063, 0.1115],
         [0.4830, 0.9089]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: -0.00491035742834304
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00036641132057026226
Average Adjusted Rand Index: -0.000982071485668608
tensor([[-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20961.339947901226
Iteration 100: Loss = -12193.161619232142
Iteration 200: Loss = -12191.115698347503
Iteration 300: Loss = -12190.544059762558
Iteration 400: Loss = -12190.299883481179
Iteration 500: Loss = -12190.159304409337
Iteration 600: Loss = -12190.06369960537
Iteration 700: Loss = -12189.991699176036
Iteration 800: Loss = -12189.934384813423
Iteration 900: Loss = -12189.888503995206
Iteration 1000: Loss = -12189.853148003882
Iteration 1100: Loss = -12189.827630669703
Iteration 1200: Loss = -12189.810751255418
Iteration 1300: Loss = -12189.800216552792
Iteration 1400: Loss = -12189.793373896608
Iteration 1500: Loss = -12189.787753848448
Iteration 1600: Loss = -12189.781457880325
Iteration 1700: Loss = -12189.772748994126
Iteration 1800: Loss = -12189.759149788926
Iteration 1900: Loss = -12189.736837128434
Iteration 2000: Loss = -12189.701440925495
Iteration 2100: Loss = -12189.64829165955
Iteration 2200: Loss = -12189.57444638356
Iteration 2300: Loss = -12189.496013545013
Iteration 2400: Loss = -12189.44329655776
Iteration 2500: Loss = -12189.399536341443
Iteration 2600: Loss = -12189.355633948899
Iteration 2700: Loss = -12189.308286002239
Iteration 2800: Loss = -12189.25590733298
Iteration 2900: Loss = -12189.197976076663
Iteration 3000: Loss = -12189.136795822877
Iteration 3100: Loss = -12189.07688031379
Iteration 3200: Loss = -12189.023417894188
Iteration 3300: Loss = -12188.977921432144
Iteration 3400: Loss = -12188.944482526687
Iteration 3500: Loss = -12188.914749885102
Iteration 3600: Loss = -12188.892241480928
Iteration 3700: Loss = -12188.873075950734
Iteration 3800: Loss = -12188.895326911905
1
Iteration 3900: Loss = -12188.828900300876
Iteration 4000: Loss = -12188.802590758605
Iteration 4100: Loss = -12188.78317801946
Iteration 4200: Loss = -12188.76347912172
Iteration 4300: Loss = -12188.74773454456
Iteration 4400: Loss = -12188.73750223504
Iteration 4500: Loss = -12188.729170502824
Iteration 4600: Loss = -12188.72459827399
Iteration 4700: Loss = -12188.741016092355
1
Iteration 4800: Loss = -12188.739789964586
2
Iteration 4900: Loss = -12188.710469305754
Iteration 5000: Loss = -12188.70782724703
Iteration 5100: Loss = -12188.780518880596
1
Iteration 5200: Loss = -12188.703986331457
Iteration 5300: Loss = -12188.706241703898
1
Iteration 5400: Loss = -12188.701497113956
Iteration 5500: Loss = -12188.701264926021
Iteration 5600: Loss = -12188.707923578617
1
Iteration 5700: Loss = -12188.702729414243
2
Iteration 5800: Loss = -12188.700703061315
Iteration 5900: Loss = -12188.710581430008
1
Iteration 6000: Loss = -12188.698710575387
Iteration 6100: Loss = -12188.699269373303
1
Iteration 6200: Loss = -12188.698053602493
Iteration 6300: Loss = -12188.704095164621
1
Iteration 6400: Loss = -12188.69804202837
Iteration 6500: Loss = -12188.706072649591
1
Iteration 6600: Loss = -12188.697216803794
Iteration 6700: Loss = -12188.697258017313
1
Iteration 6800: Loss = -12188.703955042218
2
Iteration 6900: Loss = -12188.698478951355
3
Iteration 7000: Loss = -12188.76060761109
4
Iteration 7100: Loss = -12188.697037816266
Iteration 7200: Loss = -12188.699738888725
1
Iteration 7300: Loss = -12188.697122535701
2
Iteration 7400: Loss = -12188.718301228573
3
Iteration 7500: Loss = -12188.697007517663
Iteration 7600: Loss = -12188.697763724791
1
Iteration 7700: Loss = -12188.73994912153
2
Iteration 7800: Loss = -12188.698059163453
3
Iteration 7900: Loss = -12188.69802270479
4
Iteration 8000: Loss = -12188.70148071386
5
Iteration 8100: Loss = -12188.69703178637
6
Iteration 8200: Loss = -12188.699895131556
7
Iteration 8300: Loss = -12188.69926366426
8
Iteration 8400: Loss = -12188.697857280784
9
Iteration 8500: Loss = -12188.697164349029
10
Stopping early at iteration 8500 due to no improvement.
tensor([[-5.3179,  0.7027],
        [-5.3439,  0.7287],
        [-5.2935,  0.6783],
        [-5.3334,  0.7182],
        [-5.3458,  0.7306],
        [-5.3063,  0.6911],
        [-5.3172,  0.7020],
        [-5.3017,  0.6865],
        [-5.3321,  0.7168],
        [-5.3343,  0.7191],
        [-5.3745,  0.7593],
        [-5.3132,  0.6980],
        [-5.3024,  0.6871],
        [-5.3121,  0.6969],
        [-5.3324,  0.7171],
        [-5.3196,  0.7044],
        [-5.3136,  0.6984],
        [-5.3402,  0.7250],
        [-5.3734,  0.7582],
        [-5.3485,  0.7333],
        [-5.3431,  0.7279],
        [-5.4153,  0.8000],
        [-5.3084,  0.6932],
        [-5.3166,  0.7014],
        [-5.3592,  0.7440],
        [-5.3362,  0.7210],
        [-5.2741,  0.6589],
        [-5.3567,  0.7414],
        [-5.3323,  0.7170],
        [-5.3377,  0.7225],
        [-5.3201,  0.7049],
        [-5.3494,  0.7341],
        [-5.3191,  0.7039],
        [-5.3211,  0.7059],
        [-5.3438,  0.7285],
        [-5.2972,  0.6820],
        [-5.3200,  0.7048],
        [-5.3244,  0.7092],
        [-5.3491,  0.7339],
        [-5.2975,  0.6823],
        [-5.3231,  0.7079],
        [-5.2693,  0.6541],
        [-5.3057,  0.6904],
        [-5.3227,  0.7075],
        [-5.3322,  0.7170],
        [-5.2811,  0.6659],
        [-5.3122,  0.6970],
        [-5.3085,  0.6933],
        [-5.3272,  0.7120],
        [-5.3292,  0.7139],
        [-5.2867,  0.6715],
        [-5.2991,  0.6839],
        [-5.3123,  0.6971],
        [-5.3400,  0.7248],
        [-5.3068,  0.6916],
        [-5.3203,  0.7051],
        [-5.2990,  0.6837],
        [-5.3337,  0.7184],
        [-5.3372,  0.7220],
        [-5.3614,  0.7462],
        [-5.3460,  0.7308],
        [-5.3158,  0.7006],
        [-5.3521,  0.7369],
        [-5.3516,  0.7364],
        [-5.3641,  0.7488],
        [-5.3431,  0.7279],
        [-5.3350,  0.7198],
        [-5.3543,  0.7391],
        [-5.2791,  0.6638],
        [-5.2966,  0.6814],
        [-5.3054,  0.6902],
        [-5.3552,  0.7399],
        [-5.3589,  0.7437],
        [-5.3181,  0.7029],
        [-5.3481,  0.7329],
        [-5.3266,  0.7113],
        [-5.3093,  0.6941],
        [-5.3102,  0.6950],
        [-5.2811,  0.6659],
        [-5.3154,  0.7002],
        [-5.2992,  0.6840],
        [-5.3689,  0.7537],
        [-5.3433,  0.7280],
        [-5.3422,  0.7270],
        [-5.3084,  0.6932],
        [-5.3141,  0.6989],
        [-5.3308,  0.7155],
        [-5.2999,  0.6846],
        [-5.3565,  0.7413],
        [-5.2690,  0.6538],
        [-5.3594,  0.7442],
        [-5.3697,  0.7545],
        [-5.3182,  0.7029],
        [-5.3134,  0.6982],
        [-5.3424,  0.7272],
        [-5.3347,  0.7195],
        [-5.3035,  0.6883],
        [-5.2990,  0.6837],
        [-5.3626,  0.7474],
        [-5.3162,  0.7010]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.8644, 0.1356],
        [0.3635, 0.6365]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0024, 0.9976], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1980, 0.1940],
         [0.6227, 0.1917]],

        [[0.1738, 0.1878],
         [0.1304, 0.3094]],

        [[0.9420, 0.2023],
         [0.1793, 0.3093]],

        [[0.9348, 0.1960],
         [0.0319, 0.9985]],

        [[0.9063, 0.1910],
         [0.4830, 0.9089]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0018708551122627716
Average Adjusted Rand Index: 0.0001633280491905654
Iteration 0: Loss = -22062.151383861423
Iteration 10: Loss = -12190.108269292532
Iteration 20: Loss = -12190.108269292536
1
Iteration 30: Loss = -12190.108269292554
2
Iteration 40: Loss = -12190.108269292667
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[1.0000e+00, 5.2376e-15],
        [1.0000e+00, 9.4746e-21]], dtype=torch.float64)
alpha: tensor([1.0000e+00, 5.0290e-15])
beta: tensor([[[0.1924, 0.1851],
         [0.2194, 0.2048]],

        [[0.5637, 0.1641],
         [0.9046, 0.9784]],

        [[0.1738, 0.2701],
         [0.7369, 0.8300]],

        [[0.5886, 0.2939],
         [0.7510, 0.1847]],

        [[0.1310, 0.1725],
         [0.9317, 0.5500]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22062.749146958828
Iteration 100: Loss = -12202.113020108982
Iteration 200: Loss = -12191.748096166368
Iteration 300: Loss = -12190.628761406206
Iteration 400: Loss = -12190.174239349062
Iteration 500: Loss = -12189.949527655694
Iteration 600: Loss = -12189.815883231202
Iteration 700: Loss = -12189.719424052117
Iteration 800: Loss = -12189.643646353945
Iteration 900: Loss = -12189.579583316481
Iteration 1000: Loss = -12189.52044195533
Iteration 1100: Loss = -12189.466249230167
Iteration 1200: Loss = -12189.414863147416
Iteration 1300: Loss = -12189.36237615181
Iteration 1400: Loss = -12189.317322598898
Iteration 1500: Loss = -12189.28165640207
Iteration 1600: Loss = -12189.248961260944
Iteration 1700: Loss = -12189.218582074765
Iteration 1800: Loss = -12189.189765673582
Iteration 1900: Loss = -12189.157921170123
Iteration 2000: Loss = -12189.118347005338
Iteration 2100: Loss = -12189.095383695183
Iteration 2200: Loss = -12189.074512020625
Iteration 2300: Loss = -12189.057138697286
Iteration 2400: Loss = -12189.042834950673
Iteration 2500: Loss = -12189.031002127876
Iteration 2600: Loss = -12189.021027337449
Iteration 2700: Loss = -12189.018254142864
Iteration 2800: Loss = -12189.004719357778
Iteration 2900: Loss = -12188.997963908674
Iteration 3000: Loss = -12189.150717285262
1
Iteration 3100: Loss = -12188.986334983292
Iteration 3200: Loss = -12188.98107449619
Iteration 3300: Loss = -12188.982246718982
1
Iteration 3400: Loss = -12189.011170652031
2
Iteration 3500: Loss = -12188.966661885994
Iteration 3600: Loss = -12188.962227571845
Iteration 3700: Loss = -12188.959487467695
Iteration 3800: Loss = -12188.954356108577
Iteration 3900: Loss = -12188.96077548413
1
Iteration 4000: Loss = -12188.947993168014
Iteration 4100: Loss = -12188.948109060317
1
Iteration 4200: Loss = -12188.94260878099
Iteration 4300: Loss = -12188.940174076499
Iteration 4400: Loss = -12188.943513223894
1
Iteration 4500: Loss = -12188.935643094845
Iteration 4600: Loss = -12188.986201985328
1
Iteration 4700: Loss = -12188.931858066431
Iteration 4800: Loss = -12188.942383051199
1
Iteration 4900: Loss = -12188.928768644004
Iteration 5000: Loss = -12188.930003579977
1
Iteration 5100: Loss = -12188.926150892463
Iteration 5200: Loss = -12188.925174297714
Iteration 5300: Loss = -12188.924090621707
Iteration 5400: Loss = -12188.991962096881
1
Iteration 5500: Loss = -12188.922304924046
Iteration 5600: Loss = -12188.94177745998
1
Iteration 5700: Loss = -12188.920878493669
Iteration 5800: Loss = -12188.920613215261
Iteration 5900: Loss = -12188.919510347088
Iteration 6000: Loss = -12188.918903488366
Iteration 6100: Loss = -12188.925123744397
1
Iteration 6200: Loss = -12188.917661358575
Iteration 6300: Loss = -12188.917364316656
Iteration 6400: Loss = -12188.916577781294
Iteration 6500: Loss = -12188.915597937541
Iteration 6600: Loss = -12188.91518857763
Iteration 6700: Loss = -12188.913837708038
Iteration 6800: Loss = -12188.912735900307
Iteration 6900: Loss = -12188.91324876238
1
Iteration 7000: Loss = -12188.910810725029
Iteration 7100: Loss = -12188.90860911346
Iteration 7200: Loss = -12188.908351884584
Iteration 7300: Loss = -12188.903024148887
Iteration 7400: Loss = -12188.924913982019
1
Iteration 7500: Loss = -12188.892687639165
Iteration 7600: Loss = -12188.897747690795
1
Iteration 7700: Loss = -12189.01672848418
2
Iteration 7800: Loss = -12188.873754866165
Iteration 7900: Loss = -12188.869611195758
Iteration 8000: Loss = -12188.866547994023
Iteration 8100: Loss = -12188.863150457179
Iteration 8200: Loss = -12188.86080602652
Iteration 8300: Loss = -12188.85868714282
Iteration 8400: Loss = -12188.858408576032
Iteration 8500: Loss = -12188.856944871755
Iteration 8600: Loss = -12188.857076558977
1
Iteration 8700: Loss = -12188.852787692835
Iteration 8800: Loss = -12188.85298889246
1
Iteration 8900: Loss = -12188.872320673161
2
Iteration 9000: Loss = -12188.807792950032
Iteration 9100: Loss = -12188.7803297417
Iteration 9200: Loss = -12188.76380519559
Iteration 9300: Loss = -12188.756056756642
Iteration 9400: Loss = -12188.737320539647
Iteration 9500: Loss = -12188.75886454537
1
Iteration 9600: Loss = -12188.728875109904
Iteration 9700: Loss = -12188.726473274042
Iteration 9800: Loss = -12188.759500556856
1
Iteration 9900: Loss = -12188.739992220311
2
Iteration 10000: Loss = -12188.854167189304
3
Iteration 10100: Loss = -12188.722730928892
Iteration 10200: Loss = -12188.726025555221
1
Iteration 10300: Loss = -12188.7228591872
2
Iteration 10400: Loss = -12188.722631553708
Iteration 10500: Loss = -12188.728042587107
1
Iteration 10600: Loss = -12188.722380122294
Iteration 10700: Loss = -12189.066196688173
1
Iteration 10800: Loss = -12188.72242109673
2
Iteration 10900: Loss = -12188.825079545382
3
Iteration 11000: Loss = -12188.722415902224
4
Iteration 11100: Loss = -12188.817196541942
5
Iteration 11200: Loss = -12188.722136256343
Iteration 11300: Loss = -12188.850576006518
1
Iteration 11400: Loss = -12188.723959980904
2
Iteration 11500: Loss = -12188.722153104121
3
Iteration 11600: Loss = -12188.724602573511
4
Iteration 11700: Loss = -12188.72209265057
Iteration 11800: Loss = -12188.808446453402
1
Iteration 11900: Loss = -12188.722001462465
Iteration 12000: Loss = -12188.726534549716
1
Iteration 12100: Loss = -12188.722031403682
2
Iteration 12200: Loss = -12188.72244018413
3
Iteration 12300: Loss = -12188.721957333768
Iteration 12400: Loss = -12188.722465931063
1
Iteration 12500: Loss = -12188.721892164886
Iteration 12600: Loss = -12188.724902576338
1
Iteration 12700: Loss = -12188.721903032447
2
Iteration 12800: Loss = -12188.792717284763
3
Iteration 12900: Loss = -12188.722094925375
4
Iteration 13000: Loss = -12188.757680704985
5
Iteration 13100: Loss = -12188.723643962516
6
Iteration 13200: Loss = -12188.721887927702
Iteration 13300: Loss = -12188.725255150406
1
Iteration 13400: Loss = -12188.825805868295
2
Iteration 13500: Loss = -12188.72180288706
Iteration 13600: Loss = -12188.722087140606
1
Iteration 13700: Loss = -12188.722906169387
2
Iteration 13800: Loss = -12188.72194233396
3
Iteration 13900: Loss = -12188.855414123815
4
Iteration 14000: Loss = -12188.728581386418
5
Iteration 14100: Loss = -12188.72310967839
6
Iteration 14200: Loss = -12188.721872881164
7
Iteration 14300: Loss = -12188.724207211648
8
Iteration 14400: Loss = -12188.924279291605
9
Iteration 14500: Loss = -12188.7215959237
Iteration 14600: Loss = -12188.83927189777
1
Iteration 14700: Loss = -12188.721561373244
Iteration 14800: Loss = -12188.723014735438
1
Iteration 14900: Loss = -12188.72188651875
2
Iteration 15000: Loss = -12188.721978989824
3
Iteration 15100: Loss = -12189.125091896396
4
Iteration 15200: Loss = -12188.721559465806
Iteration 15300: Loss = -12188.725719120328
1
Iteration 15400: Loss = -12188.730806655529
2
Iteration 15500: Loss = -12188.731959360552
3
Iteration 15600: Loss = -12188.721826885749
4
Iteration 15700: Loss = -12188.747279242187
5
Iteration 15800: Loss = -12188.721539637918
Iteration 15900: Loss = -12188.723127172749
1
Iteration 16000: Loss = -12188.723289108419
2
Iteration 16100: Loss = -12188.721789464946
3
Iteration 16200: Loss = -12188.742567784511
4
Iteration 16300: Loss = -12188.723832478216
5
Iteration 16400: Loss = -12188.722876480493
6
Iteration 16500: Loss = -12188.721462273126
Iteration 16600: Loss = -12188.721663248689
1
Iteration 16700: Loss = -12188.748770120521
2
Iteration 16800: Loss = -12188.72544061625
3
Iteration 16900: Loss = -12188.721513364986
4
Iteration 17000: Loss = -12188.892276562701
5
Iteration 17100: Loss = -12188.72145257657
Iteration 17200: Loss = -12188.722173269021
1
Iteration 17300: Loss = -12188.722162000033
2
Iteration 17400: Loss = -12188.801899604741
3
Iteration 17500: Loss = -12188.72162216209
4
Iteration 17600: Loss = -12188.733717642459
5
Iteration 17700: Loss = -12188.721414046222
Iteration 17800: Loss = -12188.726885130942
1
Iteration 17900: Loss = -12188.745534763244
2
Iteration 18000: Loss = -12188.723102283459
3
Iteration 18100: Loss = -12188.721572854072
4
Iteration 18200: Loss = -12188.722286114866
5
Iteration 18300: Loss = -12188.721432954717
6
Iteration 18400: Loss = -12188.722240487776
7
Iteration 18500: Loss = -12188.725206076342
8
Iteration 18600: Loss = -12188.80468323686
9
Iteration 18700: Loss = -12188.722962036652
10
Stopping early at iteration 18700 due to no improvement.
tensor([[-6.2551e-02, -1.3786e+00],
        [-1.2474e-01, -1.5263e+00],
        [-2.5140e-01, -1.9078e+00],
        [-1.3598e-01, -1.5825e+00],
        [-5.7068e-01, -1.9294e+00],
        [-2.1118e-01, -1.6880e+00],
        [-3.7957e-02, -1.3659e+00],
        [ 3.9096e-02, -1.4938e+00],
        [-1.1521e-01, -1.3049e+00],
        [-1.1366e-01, -1.6384e+00],
        [-6.4872e-01, -1.7376e+00],
        [-7.5610e-01, -2.4340e+00],
        [ 5.0979e-02, -1.4586e+00],
        [ 3.9850e-03, -1.5382e+00],
        [ 3.0290e-03, -1.3956e+00],
        [-5.0522e-01, -1.7510e+00],
        [-1.3019e+00, -2.5726e+00],
        [-1.2284e-01, -1.5702e+00],
        [-1.2662e-01, -1.3597e+00],
        [-7.0276e-01, -2.0065e+00],
        [-3.9106e-01, -1.5648e+00],
        [-5.2399e-01, -1.3496e+00],
        [ 1.5321e-02, -1.4405e+00],
        [-2.0571e-01, -1.6455e+00],
        [-2.7734e-01, -1.6456e+00],
        [-1.8518e-01, -1.5418e+00],
        [-9.6793e-01, -2.5224e+00],
        [-1.0498e+00, -2.4197e+00],
        [-3.7385e-01, -1.7322e+00],
        [-6.8017e-02, -1.3283e+00],
        [-6.4270e-01, -2.1514e+00],
        [-4.6799e-01, -1.9441e+00],
        [ 1.0279e-01, -1.5123e+00],
        [-1.2909e-01, -1.3059e+00],
        [-4.6022e-01, -1.7867e+00],
        [ 3.6606e-02, -1.4618e+00],
        [-9.6600e-02, -1.2958e+00],
        [-6.7209e-02, -1.4572e+00],
        [ 3.9809e-02, -1.5421e+00],
        [-4.4760e-02, -1.6054e+00],
        [-7.7432e-01, -2.2108e+00],
        [ 5.5509e-02, -1.4650e+00],
        [-8.2679e-01, -2.4922e+00],
        [-1.0374e+00, -2.2916e+00],
        [ 1.8603e-02, -1.4599e+00],
        [ 2.0155e-02, -1.4251e+00],
        [-1.4174e+00, -3.0530e+00],
        [-9.5585e-02, -1.5517e+00],
        [ 1.0695e-01, -1.7294e+00],
        [-1.7406e-02, -1.5772e+00],
        [-2.4055e-01, -1.6985e+00],
        [-6.1500e-02, -1.9080e+00],
        [-3.8025e-01, -1.6122e+00],
        [-5.8158e-01, -1.8249e+00],
        [-3.6089e-01, -1.9742e+00],
        [ 1.2519e-01, -1.5496e+00],
        [-1.5516e+00, -3.0636e+00],
        [-9.3599e-01, -2.2602e+00],
        [-5.6159e-02, -1.3366e+00],
        [-2.8966e-01, -1.5977e+00],
        [-9.9814e-02, -1.2868e+00],
        [-3.8629e-01, -1.8770e+00],
        [-4.4436e-01, -1.6928e+00],
        [-6.2059e-02, -1.6536e+00],
        [-9.9343e-02, -1.3104e+00],
        [-5.8018e-02, -1.3343e+00],
        [-3.5028e-01, -1.7825e+00],
        [-2.0590e-02, -1.3991e+00],
        [-1.8737e-01, -1.5507e+00],
        [-9.3747e-02, -1.3225e+00],
        [-9.5188e-01, -2.3186e+00],
        [ 1.6918e-02, -1.4090e+00],
        [-1.8995e-01, -1.2167e+00],
        [-6.9397e-01, -2.2765e+00],
        [-1.9766e-01, -1.5317e+00],
        [ 8.7332e-02, -1.4964e+00],
        [-2.8534e-01, -1.7168e+00],
        [-1.4083e-01, -1.5073e+00],
        [ 5.0414e-02, -1.4390e+00],
        [ 3.1100e-02, -1.5094e+00],
        [ 8.1218e-02, -1.6529e+00],
        [-1.3088e-02, -1.4659e+00],
        [-1.3527e-01, -1.5481e+00],
        [-1.0017e-01, -1.5391e+00],
        [-2.4688e-02, -1.6294e+00],
        [-6.3011e-01, -1.7358e+00],
        [-6.2861e-01, -1.9275e+00],
        [ 4.6829e-02, -1.4770e+00],
        [-7.4980e-02, -1.3619e+00],
        [-3.4625e-01, -2.0167e+00],
        [-1.3985e+00, -2.5805e+00],
        [-8.9818e-02, -1.3117e+00],
        [-1.9113e-01, -1.3978e+00],
        [-1.7023e-02, -1.3771e+00],
        [-1.6424e-01, -1.5142e+00],
        [-3.0599e-01, -1.8369e+00],
        [-8.8210e-01, -2.3572e+00],
        [-2.1881e-01, -1.5842e+00],
        [-1.1737e-01, -1.7028e+00],
        [ 4.7166e-02, -1.4346e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.0386, 0.9614],
        [0.9336, 0.0664]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8018, 0.1982], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1968, 0.1901],
         [0.2194, 0.1909]],

        [[0.5637, 0.1872],
         [0.9046, 0.9784]],

        [[0.1738, 0.2034],
         [0.7369, 0.8300]],

        [[0.5886, 0.1997],
         [0.7510, 0.1847]],

        [[0.1310, 0.1912],
         [0.9317, 0.5500]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0002930142699234603
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -20845.52904933556
Iteration 10: Loss = -12189.7021534291
Iteration 20: Loss = -12189.483625315004
Iteration 30: Loss = -12189.390623131278
Iteration 40: Loss = -12189.347136177834
Iteration 50: Loss = -12189.324064177732
Iteration 60: Loss = -12189.310571811418
Iteration 70: Loss = -12189.30213214377
Iteration 80: Loss = -12189.296457661498
Iteration 90: Loss = -12189.292438736995
Iteration 100: Loss = -12189.289491087218
Iteration 110: Loss = -12189.28724702561
Iteration 120: Loss = -12189.285433668521
Iteration 130: Loss = -12189.28400704809
Iteration 140: Loss = -12189.282819371565
Iteration 150: Loss = -12189.281754325682
Iteration 160: Loss = -12189.280850875466
Iteration 170: Loss = -12189.280015837589
Iteration 180: Loss = -12189.279184512321
Iteration 190: Loss = -12189.278324886604
Iteration 200: Loss = -12189.27748385044
Iteration 210: Loss = -12189.276592533612
Iteration 220: Loss = -12189.275602608077
Iteration 230: Loss = -12189.274531655037
Iteration 240: Loss = -12189.273390679433
Iteration 250: Loss = -12189.272120592366
Iteration 260: Loss = -12189.270879764656
Iteration 270: Loss = -12189.269664012398
Iteration 280: Loss = -12189.268296104396
Iteration 290: Loss = -12189.266988382993
Iteration 300: Loss = -12189.265691566163
Iteration 310: Loss = -12189.264481699112
Iteration 320: Loss = -12189.263179844758
Iteration 330: Loss = -12189.261946191085
Iteration 340: Loss = -12189.260667269695
Iteration 350: Loss = -12189.25944147029
Iteration 360: Loss = -12189.258059599755
Iteration 370: Loss = -12189.256697680185
Iteration 380: Loss = -12189.255256931492
Iteration 390: Loss = -12189.253772071272
Iteration 400: Loss = -12189.252191714593
Iteration 410: Loss = -12189.250510387747
Iteration 420: Loss = -12189.24878946423
Iteration 430: Loss = -12189.246965056594
Iteration 440: Loss = -12189.24509680467
Iteration 450: Loss = -12189.243123045577
Iteration 460: Loss = -12189.241035533474
Iteration 470: Loss = -12189.238906912205
Iteration 480: Loss = -12189.236744522042
Iteration 490: Loss = -12189.234499801401
Iteration 500: Loss = -12189.232216139082
Iteration 510: Loss = -12189.229944283446
Iteration 520: Loss = -12189.227590673208
Iteration 530: Loss = -12189.225268997194
Iteration 540: Loss = -12189.22291697778
Iteration 550: Loss = -12189.220566659942
Iteration 560: Loss = -12189.21825796344
Iteration 570: Loss = -12189.215925672546
Iteration 580: Loss = -12189.213627818466
Iteration 590: Loss = -12189.211357633962
Iteration 600: Loss = -12189.209151858164
Iteration 610: Loss = -12189.206941210057
Iteration 620: Loss = -12189.20478310321
Iteration 630: Loss = -12189.202683816806
Iteration 640: Loss = -12189.200628421526
Iteration 650: Loss = -12189.198604737243
Iteration 660: Loss = -12189.196643628697
Iteration 670: Loss = -12189.194700931583
Iteration 680: Loss = -12189.192828326284
Iteration 690: Loss = -12189.190980968486
Iteration 700: Loss = -12189.189195808542
Iteration 710: Loss = -12189.187498712918
Iteration 720: Loss = -12189.185793701112
Iteration 730: Loss = -12189.184127615792
Iteration 740: Loss = -12189.182533853647
Iteration 750: Loss = -12189.18095459275
Iteration 760: Loss = -12189.179440177282
Iteration 770: Loss = -12189.177973091955
Iteration 780: Loss = -12189.176572423568
Iteration 790: Loss = -12189.175201249516
Iteration 800: Loss = -12189.173834993797
Iteration 810: Loss = -12189.172547207263
Iteration 820: Loss = -12189.171242389639
Iteration 830: Loss = -12189.170032782504
Iteration 840: Loss = -12189.168800326866
Iteration 850: Loss = -12189.167617577037
Iteration 860: Loss = -12189.166481213546
Iteration 870: Loss = -12189.165440068346
Iteration 880: Loss = -12189.16426696367
Iteration 890: Loss = -12189.163240861923
Iteration 900: Loss = -12189.162225820248
Iteration 910: Loss = -12189.161267453004
Iteration 920: Loss = -12189.160265126924
Iteration 930: Loss = -12189.159318557757
Iteration 940: Loss = -12189.158416953393
Iteration 950: Loss = -12189.157514105456
Iteration 960: Loss = -12189.15663667552
Iteration 970: Loss = -12189.155788440628
Iteration 980: Loss = -12189.154947180063
Iteration 990: Loss = -12189.154157142442
Iteration 1000: Loss = -12189.15339437855
Iteration 1010: Loss = -12189.152623683824
Iteration 1020: Loss = -12189.151883982122
Iteration 1030: Loss = -12189.151149247735
Iteration 1040: Loss = -12189.150463433054
Iteration 1050: Loss = -12189.14978864599
Iteration 1060: Loss = -12189.149085858442
Iteration 1070: Loss = -12189.1483937133
Iteration 1080: Loss = -12189.14778515387
Iteration 1090: Loss = -12189.147184345702
Iteration 1100: Loss = -12189.146507359363
Iteration 1110: Loss = -12189.145931827474
Iteration 1120: Loss = -12189.145349355673
Iteration 1130: Loss = -12189.144752204571
Iteration 1140: Loss = -12189.144192270427
Iteration 1150: Loss = -12189.14362859309
Iteration 1160: Loss = -12189.143149407098
Iteration 1170: Loss = -12189.142605038513
Iteration 1180: Loss = -12189.142133493784
Iteration 1190: Loss = -12189.141572355678
Iteration 1200: Loss = -12189.141148557332
Iteration 1210: Loss = -12189.140682157149
Iteration 1220: Loss = -12189.140206411332
Iteration 1230: Loss = -12189.139687488238
Iteration 1240: Loss = -12189.139266738952
Iteration 1250: Loss = -12189.138831826476
Iteration 1260: Loss = -12189.138397942841
Iteration 1270: Loss = -12189.13796862875
Iteration 1280: Loss = -12189.137609187148
Iteration 1290: Loss = -12189.13718424451
Iteration 1300: Loss = -12189.136818363559
Iteration 1310: Loss = -12189.136394564413
Iteration 1320: Loss = -12189.136038857416
Iteration 1330: Loss = -12189.135728538526
Iteration 1340: Loss = -12189.135262052589
Iteration 1350: Loss = -12189.134925013044
Iteration 1360: Loss = -12189.134586507436
Iteration 1370: Loss = -12189.134235631507
Iteration 1380: Loss = -12189.133988160866
Iteration 1390: Loss = -12189.133576741006
Iteration 1400: Loss = -12189.13327677434
Iteration 1410: Loss = -12189.132983642628
Iteration 1420: Loss = -12189.132631966646
Iteration 1430: Loss = -12189.132354960904
Iteration 1440: Loss = -12189.132091087675
Iteration 1450: Loss = -12189.131785933596
Iteration 1460: Loss = -12189.131489686626
Iteration 1470: Loss = -12189.131178812911
Iteration 1480: Loss = -12189.130942105858
Iteration 1490: Loss = -12189.130642009433
Iteration 1500: Loss = -12189.130437972659
Iteration 1510: Loss = -12189.130158678649
Iteration 1520: Loss = -12189.129886912231
Iteration 1530: Loss = -12189.129691610186
Iteration 1540: Loss = -12189.129379389651
Iteration 1550: Loss = -12189.129184209527
Iteration 1560: Loss = -12189.128900245718
Iteration 1570: Loss = -12189.128749033816
Iteration 1580: Loss = -12189.128452071129
Iteration 1590: Loss = -12189.128265032305
Iteration 1600: Loss = -12189.128044162448
Iteration 1610: Loss = -12189.127781949152
Iteration 1620: Loss = -12189.127592212066
Iteration 1630: Loss = -12189.127391772481
Iteration 1640: Loss = -12189.1272243615
Iteration 1650: Loss = -12189.126980305127
Iteration 1660: Loss = -12189.126800488706
Iteration 1670: Loss = -12189.126600451087
Iteration 1680: Loss = -12189.126427925283
Iteration 1690: Loss = -12189.126232705627
Iteration 1700: Loss = -12189.126007295445
Iteration 1710: Loss = -12189.12587896809
Iteration 1720: Loss = -12189.125714358463
Iteration 1730: Loss = -12189.125523270905
Iteration 1740: Loss = -12189.12535748741
Iteration 1750: Loss = -12189.125157563838
Iteration 1760: Loss = -12189.125046644203
Iteration 1770: Loss = -12189.124862510938
Iteration 1780: Loss = -12189.124650445663
Iteration 1790: Loss = -12189.12455466457
Iteration 1800: Loss = -12189.124375599638
Iteration 1810: Loss = -12189.124199004345
Iteration 1820: Loss = -12189.124125689625
Iteration 1830: Loss = -12189.123959669389
Iteration 1840: Loss = -12189.123821235053
Iteration 1850: Loss = -12189.123668437082
Iteration 1860: Loss = -12189.123544123322
Iteration 1870: Loss = -12189.12337413778
Iteration 1880: Loss = -12189.123213458326
Iteration 1890: Loss = -12189.123105898256
Iteration 1900: Loss = -12189.122985231108
Iteration 1910: Loss = -12189.122852041137
Iteration 1920: Loss = -12189.12275519349
Iteration 1930: Loss = -12189.122592339896
Iteration 1940: Loss = -12189.122480752774
Iteration 1950: Loss = -12189.12239194333
Iteration 1960: Loss = -12189.122222434604
Iteration 1970: Loss = -12189.122130333135
Iteration 1980: Loss = -12189.122017083073
Iteration 1990: Loss = -12189.121884033602
Iteration 2000: Loss = -12189.121777739354
Iteration 2010: Loss = -12189.121695929636
Iteration 2020: Loss = -12189.121612843768
Iteration 2030: Loss = -12189.121493652885
Iteration 2040: Loss = -12189.121381759602
Iteration 2050: Loss = -12189.121238218184
Iteration 2060: Loss = -12189.121133066346
Iteration 2070: Loss = -12189.121072935866
Iteration 2080: Loss = -12189.12096799321
Iteration 2090: Loss = -12189.12088321035
Iteration 2100: Loss = -12189.120789170764
Iteration 2110: Loss = -12189.12070230024
Iteration 2120: Loss = -12189.120590819048
Iteration 2130: Loss = -12189.120532932504
Iteration 2140: Loss = -12189.120414868961
Iteration 2150: Loss = -12189.120334022746
Iteration 2160: Loss = -12189.120273388773
Iteration 2170: Loss = -12189.120188809384
Iteration 2180: Loss = -12189.120106843466
Iteration 2190: Loss = -12189.120021028577
Iteration 2200: Loss = -12189.119945290777
Iteration 2210: Loss = -12189.119895349171
Iteration 2220: Loss = -12189.119778937638
Iteration 2230: Loss = -12189.11969949301
Iteration 2240: Loss = -12189.11961643604
Iteration 2250: Loss = -12189.119548064235
Iteration 2260: Loss = -12189.119512949495
Iteration 2270: Loss = -12189.119441750818
Iteration 2280: Loss = -12189.11933413393
Iteration 2290: Loss = -12189.119266565976
Iteration 2300: Loss = -12189.119215207878
Iteration 2310: Loss = -12189.11911808119
Iteration 2320: Loss = -12189.119063301243
Iteration 2330: Loss = -12189.119007414738
Iteration 2340: Loss = -12189.118920880659
Iteration 2350: Loss = -12189.11888468916
Iteration 2360: Loss = -12189.118840781133
Iteration 2370: Loss = -12189.118753760897
Iteration 2380: Loss = -12189.118713444264
Iteration 2390: Loss = -12189.118629336604
Iteration 2400: Loss = -12189.118561048343
Iteration 2410: Loss = -12189.118534225248
Iteration 2420: Loss = -12189.118508632302
Iteration 2430: Loss = -12189.118400476567
Iteration 2440: Loss = -12189.118331703849
Iteration 2450: Loss = -12189.118314517134
Iteration 2460: Loss = -12189.118255662228
Iteration 2470: Loss = -12189.118203220205
Iteration 2480: Loss = -12189.118107651178
Iteration 2490: Loss = -12189.118083460782
Iteration 2500: Loss = -12189.11800429281
Iteration 2510: Loss = -12189.117964213228
Iteration 2520: Loss = -12189.11794277077
Iteration 2530: Loss = -12189.117874356285
Iteration 2540: Loss = -12189.117870661312
Iteration 2550: Loss = -12189.117800739954
Iteration 2560: Loss = -12189.117745331903
Iteration 2570: Loss = -12189.117704081686
Iteration 2580: Loss = -12189.117644773272
Iteration 2590: Loss = -12189.117622005959
Iteration 2600: Loss = -12189.117522810888
Iteration 2610: Loss = -12189.117528170067
1
Iteration 2620: Loss = -12189.11750601616
Iteration 2630: Loss = -12189.117420558117
Iteration 2640: Loss = -12189.117379904283
Iteration 2650: Loss = -12189.117357563184
Iteration 2660: Loss = -12189.117337116526
Iteration 2670: Loss = -12189.117284969527
Iteration 2680: Loss = -12189.117277799709
Iteration 2690: Loss = -12189.117204999244
Iteration 2700: Loss = -12189.11719683414
Iteration 2710: Loss = -12189.117133234367
Iteration 2720: Loss = -12189.11708176141
Iteration 2730: Loss = -12189.117087547957
1
Iteration 2740: Loss = -12189.117037642212
Iteration 2750: Loss = -12189.1169533668
Iteration 2760: Loss = -12189.11694706747
Iteration 2770: Loss = -12189.116952452605
1
Iteration 2780: Loss = -12189.116905799116
Iteration 2790: Loss = -12189.116842981546
Iteration 2800: Loss = -12189.1168197839
Iteration 2810: Loss = -12189.116789487245
Iteration 2820: Loss = -12189.116782371462
Iteration 2830: Loss = -12189.116734264448
Iteration 2840: Loss = -12189.116700650353
Iteration 2850: Loss = -12189.116661084418
Iteration 2860: Loss = -12189.116639165853
Iteration 2870: Loss = -12189.116625627317
Iteration 2880: Loss = -12189.116563144666
Iteration 2890: Loss = -12189.11654065586
Iteration 2900: Loss = -12189.116571489156
1
Iteration 2910: Loss = -12189.116486111136
Iteration 2920: Loss = -12189.11647489121
Iteration 2930: Loss = -12189.116450055872
Iteration 2940: Loss = -12189.116441436394
Iteration 2950: Loss = -12189.116358620266
Iteration 2960: Loss = -12189.116378545863
1
Iteration 2970: Loss = -12189.116368284178
2
Iteration 2980: Loss = -12189.116264811335
Iteration 2990: Loss = -12189.116303597457
1
Iteration 3000: Loss = -12189.116247657264
Iteration 3010: Loss = -12189.116219524589
Iteration 3020: Loss = -12189.116231800841
1
Iteration 3030: Loss = -12189.116171059015
Iteration 3040: Loss = -12189.116173509192
1
Iteration 3050: Loss = -12189.116145651367
Iteration 3060: Loss = -12189.116139825263
Iteration 3070: Loss = -12189.116082627696
Iteration 3080: Loss = -12189.116102695558
1
Iteration 3090: Loss = -12189.116057235575
Iteration 3100: Loss = -12189.115997398334
Iteration 3110: Loss = -12189.116031395008
1
Iteration 3120: Loss = -12189.116009407464
2
Iteration 3130: Loss = -12189.115995341153
Iteration 3140: Loss = -12189.115998721669
1
Iteration 3150: Loss = -12189.11595850923
Iteration 3160: Loss = -12189.115929863348
Iteration 3170: Loss = -12189.115887974845
Iteration 3180: Loss = -12189.115925871201
1
Iteration 3190: Loss = -12189.1158471433
Iteration 3200: Loss = -12189.11584771642
1
Iteration 3210: Loss = -12189.115826541036
Iteration 3220: Loss = -12189.115806581669
Iteration 3230: Loss = -12189.115789257268
Iteration 3240: Loss = -12189.115793900242
1
Iteration 3250: Loss = -12189.115793347357
2
Iteration 3260: Loss = -12189.115760353074
Iteration 3270: Loss = -12189.11571855906
Iteration 3280: Loss = -12189.115678172884
Iteration 3290: Loss = -12189.115669841549
Iteration 3300: Loss = -12189.11568430474
1
Iteration 3310: Loss = -12189.115697440766
2
Iteration 3320: Loss = -12189.115677361991
3
Stopping early at iteration 3319 due to no improvement.
pi: tensor([[0.5327, 0.4673],
        [0.5701, 0.4299]], dtype=torch.float64)
alpha: tensor([0.5495, 0.4505])
beta: tensor([[[0.1922, 0.1906],
         [0.1004, 0.1927]],

        [[0.6084, 0.1856],
         [0.5323, 0.2950]],

        [[0.3432, 0.2005],
         [0.8621, 0.1946]],

        [[0.5664, 0.1951],
         [0.2583, 0.0469]],

        [[0.2488, 0.1906],
         [0.1125, 0.4855]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20845.22543844214
Iteration 100: Loss = -12206.660832327087
Iteration 200: Loss = -12195.767653761228
Iteration 300: Loss = -12191.215387684975
Iteration 400: Loss = -12190.526060451572
Iteration 500: Loss = -12190.220171095394
Iteration 600: Loss = -12190.05705095343
Iteration 700: Loss = -12189.960820274478
Iteration 800: Loss = -12189.90233651864
Iteration 900: Loss = -12189.857078296864
Iteration 1000: Loss = -12189.823564402664
Iteration 1100: Loss = -12189.791146905285
Iteration 1200: Loss = -12189.753455169459
Iteration 1300: Loss = -12189.701244458622
Iteration 1400: Loss = -12189.62611289098
Iteration 1500: Loss = -12189.545268126947
Iteration 1600: Loss = -12189.479560026248
Iteration 1700: Loss = -12189.425054022488
Iteration 1800: Loss = -12189.374569827316
Iteration 1900: Loss = -12189.323311623724
Iteration 2000: Loss = -12189.268591131053
Iteration 2100: Loss = -12189.210436153906
Iteration 2200: Loss = -12189.152312771588
Iteration 2300: Loss = -12189.103081738165
Iteration 2400: Loss = -12189.068577200685
Iteration 2500: Loss = -12189.044123287926
Iteration 2600: Loss = -12189.024503669305
Iteration 2700: Loss = -12189.007816192667
Iteration 2800: Loss = -12188.993458035158
Iteration 2900: Loss = -12188.981094600065
Iteration 3000: Loss = -12188.970657915186
Iteration 3100: Loss = -12188.961828241914
Iteration 3200: Loss = -12188.954418101424
Iteration 3300: Loss = -12188.948523280767
Iteration 3400: Loss = -12188.943697552875
Iteration 3500: Loss = -12188.940180047153
Iteration 3600: Loss = -12188.936278216073
Iteration 3700: Loss = -12188.933231173987
Iteration 3800: Loss = -12188.930711141995
Iteration 3900: Loss = -12188.928611673009
Iteration 4000: Loss = -12188.93154250124
1
Iteration 4100: Loss = -12188.925464154123
Iteration 4200: Loss = -12188.924219751916
Iteration 4300: Loss = -12188.923306450404
Iteration 4400: Loss = -12188.922216906842
Iteration 4500: Loss = -12188.9213601989
Iteration 4600: Loss = -12188.920589550522
Iteration 4700: Loss = -12188.919813693887
Iteration 4800: Loss = -12188.920083571402
1
Iteration 4900: Loss = -12188.918426546028
Iteration 5000: Loss = -12188.91769495514
Iteration 5100: Loss = -12188.917198964651
Iteration 5200: Loss = -12188.91605067812
Iteration 5300: Loss = -12188.931028648673
1
Iteration 5400: Loss = -12188.913969405852
Iteration 5500: Loss = -12188.91255003174
Iteration 5600: Loss = -12188.936406689229
1
Iteration 5700: Loss = -12188.907809814838
Iteration 5800: Loss = -12188.904532485261
Iteration 5900: Loss = -12188.924170963339
1
Iteration 6000: Loss = -12188.933519799742
2
Iteration 6100: Loss = -12188.888232459314
Iteration 6200: Loss = -12188.928644549862
1
Iteration 6300: Loss = -12188.882435491647
Iteration 6400: Loss = -12188.870860329362
Iteration 6500: Loss = -12188.866655445887
Iteration 6600: Loss = -12188.862834567497
Iteration 6700: Loss = -12188.85876933071
Iteration 6800: Loss = -12188.85387572818
Iteration 6900: Loss = -12188.847428182698
Iteration 7000: Loss = -12188.839171034593
Iteration 7100: Loss = -12188.812120627848
Iteration 7200: Loss = -12188.784948334014
Iteration 7300: Loss = -12188.766598665436
Iteration 7400: Loss = -12188.76179490838
Iteration 7500: Loss = -12188.747209258428
Iteration 7600: Loss = -12188.741862140136
Iteration 7700: Loss = -12188.727527490655
Iteration 7800: Loss = -12188.725485110173
Iteration 7900: Loss = -12188.72586301405
1
Iteration 8000: Loss = -12188.725729785358
2
Iteration 8100: Loss = -12188.723470801893
Iteration 8200: Loss = -12188.723361675431
Iteration 8300: Loss = -12188.816542989212
1
Iteration 8400: Loss = -12188.723152863644
Iteration 8500: Loss = -12188.722921447123
Iteration 8600: Loss = -12188.72738395229
1
Iteration 8700: Loss = -12188.72283792227
Iteration 8800: Loss = -12188.722841469997
1
Iteration 8900: Loss = -12188.72364289519
2
Iteration 9000: Loss = -12188.72275722065
Iteration 9100: Loss = -12188.722710910104
Iteration 9200: Loss = -12188.722850683205
1
Iteration 9300: Loss = -12188.722716297161
2
Iteration 9400: Loss = -12188.728468270516
3
Iteration 9500: Loss = -12188.722661828704
Iteration 9600: Loss = -12188.736755531101
1
Iteration 9700: Loss = -12188.723325441386
2
Iteration 9800: Loss = -12188.723370480662
3
Iteration 9900: Loss = -12188.772789933146
4
Iteration 10000: Loss = -12188.722814249028
5
Iteration 10100: Loss = -12189.11664017786
6
Iteration 10200: Loss = -12188.72261097371
Iteration 10300: Loss = -12188.7628928984
1
Iteration 10400: Loss = -12188.736147226085
2
Iteration 10500: Loss = -12188.722628730859
3
Iteration 10600: Loss = -12188.722621892404
4
Iteration 10700: Loss = -12188.756375478533
5
Iteration 10800: Loss = -12188.722393614418
Iteration 10900: Loss = -12188.723878266006
1
Iteration 11000: Loss = -12188.887440343158
2
Iteration 11100: Loss = -12188.729760941853
3
Iteration 11200: Loss = -12188.722562481324
4
Iteration 11300: Loss = -12188.722372398945
Iteration 11400: Loss = -12188.727814763284
1
Iteration 11500: Loss = -12188.722149742829
Iteration 11600: Loss = -12188.722304907353
1
Iteration 11700: Loss = -12188.742370834252
2
Iteration 11800: Loss = -12188.812288299876
3
Iteration 11900: Loss = -12188.722639061178
4
Iteration 12000: Loss = -12188.726455638789
5
Iteration 12100: Loss = -12188.722412767635
6
Iteration 12200: Loss = -12188.729419567762
7
Iteration 12300: Loss = -12188.722533864853
8
Iteration 12400: Loss = -12189.025480910583
9
Iteration 12500: Loss = -12188.722255176112
10
Stopping early at iteration 12500 due to no improvement.
tensor([[-2.1256e-01, -1.5599e+00],
        [-4.6210e-01, -1.9306e+00],
        [-6.0961e-04, -1.7181e+00],
        [-5.4960e-01, -2.0674e+00],
        [-2.0039e-01, -1.6047e+00],
        [ 3.8195e-02, -1.4925e+00],
        [-3.7559e-01, -1.7684e+00],
        [ 9.0774e-02, -1.4896e+00],
        [-8.6622e-02, -1.3279e+00],
        [ 7.1247e-02, -1.4896e+00],
        [-8.1723e-01, -1.9640e+00],
        [ 1.7449e-01, -1.5758e+00],
        [ 6.7045e-02, -1.4947e+00],
        [-4.3626e-02, -1.6441e+00],
        [-4.7670e-02, -1.5056e+00],
        [-5.1468e-02, -1.3434e+00],
        [-1.3007e-01, -1.4483e+00],
        [-4.7649e-01, -2.0012e+00],
        [-5.4533e-01, -1.8325e+00],
        [-4.0540e-02, -1.4118e+00],
        [-9.6485e-02, -1.3432e+00],
        [-5.8543e-01, -1.4534e+00],
        [-5.2266e-01, -2.0357e+00],
        [-1.0369e+00, -2.5512e+00],
        [-9.7583e-02, -1.5369e+00],
        [-1.3987e-02, -1.4385e+00],
        [-6.1001e-01, -2.2300e+00],
        [-2.2329e-01, -1.6597e+00],
        [-8.8445e-01, -2.3152e+00],
        [-5.3256e-02, -1.3805e+00],
        [-4.7319e-01, -2.0457e+00],
        [-4.5612e-02, -1.5703e+00],
        [ 4.3216e-02, -1.6498e+00],
        [-1.0473e-01, -1.3358e+00],
        [-1.1647e-02, -1.4035e+00],
        [-1.8626e-02, -1.5653e+00],
        [-5.4282e-02, -1.3321e+00],
        [ 2.8656e-02, -1.4308e+00],
        [-6.8872e-04, -1.6386e+00],
        [-1.3145e+00, -2.9226e+00],
        [ 4.1923e-02, -1.4554e+00],
        [-7.7028e-01, -2.3863e+00],
        [ 1.1533e-01, -1.6146e+00],
        [-2.8583e-01, -1.6165e+00],
        [-6.8134e-01, -2.2126e+00],
        [-6.3103e-01, -2.1425e+00],
        [-8.1546e-02, -1.8034e+00],
        [ 7.3626e-02, -1.4628e+00],
        [-3.4017e-01, -2.2469e+00],
        [ 8.2732e-02, -1.5208e+00],
        [-1.5418e+00, -3.0734e+00],
        [-9.2904e-02, -2.0069e+00],
        [-1.7349e-01, -1.4716e+00],
        [-8.0568e-02, -1.3863e+00],
        [-2.7631e-01, -1.9548e+00],
        [ 1.5707e-01, -1.5662e+00],
        [-1.5027e-02, -1.5570e+00],
        [-1.3473e+00, -2.7320e+00],
        [-7.6369e-01, -2.0670e+00],
        [-2.4872e-01, -1.6156e+00],
        [-7.1023e-02, -1.3160e+00],
        [-1.2277e+00, -2.7686e+00],
        [-5.4440e-01, -1.8388e+00],
        [ 9.9560e-02, -1.5544e+00],
        [-6.2859e-02, -1.3456e+00],
        [-5.6694e-02, -1.4040e+00],
        [ 4.1450e-02, -1.4304e+00],
        [-8.9339e-01, -2.3560e+00],
        [-6.9088e-01, -2.1250e+00],
        [-8.5694e-02, -1.3577e+00],
        [ 9.4750e-03, -1.4099e+00],
        [-1.0456e-01, -1.5802e+00],
        [-1.8221e-01, -1.2761e+00],
        [-4.3537e-01, -2.1040e+00],
        [-6.9975e-03, -1.4052e+00],
        [ 1.3256e-01, -1.5225e+00],
        [-5.0817e-02, -1.5312e+00],
        [-5.4949e-02, -1.4805e+00],
        [-1.8381e-01, -1.7265e+00],
        [-1.3815e+00, -2.9693e+00],
        [ 2.0654e-01, -1.6008e+00],
        [-1.5522e+00, -3.0630e+00],
        [-3.1747e-02, -1.4791e+00],
        [-2.3310e-01, -1.7348e+00],
        [ 4.3622e-02, -1.6077e+00],
        [-1.8117e-01, -1.3577e+00],
        [-1.3629e-02, -1.3912e+00],
        [-8.8238e-02, -1.6687e+00],
        [-3.3258e-01, -1.6656e+00],
        [-9.0339e-02, -1.8018e+00],
        [-4.5315e-01, -1.7120e+00],
        [-3.6482e-01, -1.6310e+00],
        [-1.1539e-01, -1.3703e+00],
        [-2.4527e-01, -1.6572e+00],
        [-6.0645e-03, -1.4051e+00],
        [ 2.8399e-02, -1.5601e+00],
        [-4.8145e-03, -1.5242e+00],
        [-1.9042e-01, -1.6195e+00],
        [ 1.2634e-01, -1.5164e+00],
        [-4.8472e-01, -2.0209e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.0647, 0.9353],
        [0.9628, 0.0372]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8116, 0.1884], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1968, 0.1899],
         [0.1004, 0.1907]],

        [[0.6084, 0.1876],
         [0.5323, 0.2950]],

        [[0.3432, 0.2033],
         [0.8621, 0.1946]],

        [[0.5664, 0.1995],
         [0.2583, 0.0469]],

        [[0.2488, 0.1910],
         [0.1125, 0.4855]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0002930142699234603
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -18664.213927791014
Iteration 10: Loss = -12190.562511330982
Iteration 20: Loss = -12189.28187379482
Iteration 30: Loss = -12189.166048482295
Iteration 40: Loss = -12189.150020245597
Iteration 50: Loss = -12189.147454689557
Iteration 60: Loss = -12189.146520749251
Iteration 70: Loss = -12189.145909407642
Iteration 80: Loss = -12189.145275081773
Iteration 90: Loss = -12189.144624162636
Iteration 100: Loss = -12189.144074132691
Iteration 110: Loss = -12189.143437706669
Iteration 120: Loss = -12189.14288296682
Iteration 130: Loss = -12189.142360739139
Iteration 140: Loss = -12189.141809747487
Iteration 150: Loss = -12189.141294239176
Iteration 160: Loss = -12189.140774580923
Iteration 170: Loss = -12189.140267653584
Iteration 180: Loss = -12189.13982884371
Iteration 190: Loss = -12189.13931244466
Iteration 200: Loss = -12189.138813776379
Iteration 210: Loss = -12189.138436855697
Iteration 220: Loss = -12189.137941379802
Iteration 230: Loss = -12189.137538215793
Iteration 240: Loss = -12189.137074053364
Iteration 250: Loss = -12189.136691746795
Iteration 260: Loss = -12189.1362838628
Iteration 270: Loss = -12189.135904870027
Iteration 280: Loss = -12189.135491949548
Iteration 290: Loss = -12189.135125453826
Iteration 300: Loss = -12189.13470054798
Iteration 310: Loss = -12189.134339427657
Iteration 320: Loss = -12189.134025497513
Iteration 330: Loss = -12189.1337356488
Iteration 340: Loss = -12189.133357812541
Iteration 350: Loss = -12189.133034855913
Iteration 360: Loss = -12189.132713354156
Iteration 370: Loss = -12189.13236066643
Iteration 380: Loss = -12189.132076372025
Iteration 390: Loss = -12189.131808620501
Iteration 400: Loss = -12189.131514109895
Iteration 410: Loss = -12189.131170567904
Iteration 420: Loss = -12189.130889808239
Iteration 430: Loss = -12189.130602642508
Iteration 440: Loss = -12189.130315785891
Iteration 450: Loss = -12189.130135100157
Iteration 460: Loss = -12189.129799751081
Iteration 470: Loss = -12189.129556797896
Iteration 480: Loss = -12189.129280740417
Iteration 490: Loss = -12189.129026354049
Iteration 500: Loss = -12189.128861228257
Iteration 510: Loss = -12189.12856617553
Iteration 520: Loss = -12189.128350778712
Iteration 530: Loss = -12189.128111771606
Iteration 540: Loss = -12189.127917918244
Iteration 550: Loss = -12189.127654657621
Iteration 560: Loss = -12189.127486849065
Iteration 570: Loss = -12189.12725188588
Iteration 580: Loss = -12189.127032540659
Iteration 590: Loss = -12189.12684629327
Iteration 600: Loss = -12189.126651306988
Iteration 610: Loss = -12189.126425518398
Iteration 620: Loss = -12189.126262661866
Iteration 630: Loss = -12189.126009791076
Iteration 640: Loss = -12189.125830242778
Iteration 650: Loss = -12189.12567983324
Iteration 660: Loss = -12189.125485880695
Iteration 670: Loss = -12189.125317379885
Iteration 680: Loss = -12189.12515000227
Iteration 690: Loss = -12189.124994174994
Iteration 700: Loss = -12189.124797300201
Iteration 710: Loss = -12189.124636720995
Iteration 720: Loss = -12189.124521395073
Iteration 730: Loss = -12189.124338984624
Iteration 740: Loss = -12189.12417809355
Iteration 750: Loss = -12189.124000978767
Iteration 760: Loss = -12189.123881198191
Iteration 770: Loss = -12189.123715535825
Iteration 780: Loss = -12189.123588329074
Iteration 790: Loss = -12189.12341890951
Iteration 800: Loss = -12189.123285333326
Iteration 810: Loss = -12189.123164730578
Iteration 820: Loss = -12189.123037850868
Iteration 830: Loss = -12189.122909249068
Iteration 840: Loss = -12189.122757569043
Iteration 850: Loss = -12189.12265774827
Iteration 860: Loss = -12189.122509284733
Iteration 870: Loss = -12189.12236555752
Iteration 880: Loss = -12189.122268681314
Iteration 890: Loss = -12189.122117398892
Iteration 900: Loss = -12189.122024485056
Iteration 910: Loss = -12189.121887590965
Iteration 920: Loss = -12189.121781342126
Iteration 930: Loss = -12189.121687367244
Iteration 940: Loss = -12189.121592217702
Iteration 950: Loss = -12189.12147722748
Iteration 960: Loss = -12189.12138046121
Iteration 970: Loss = -12189.12125558476
Iteration 980: Loss = -12189.121137139002
Iteration 990: Loss = -12189.121052573231
Iteration 1000: Loss = -12189.120960984279
Iteration 1010: Loss = -12189.120835326352
Iteration 1020: Loss = -12189.120781297363
Iteration 1030: Loss = -12189.120676755767
Iteration 1040: Loss = -12189.120618445326
Iteration 1050: Loss = -12189.120496936992
Iteration 1060: Loss = -12189.120397737817
Iteration 1070: Loss = -12189.120304890594
Iteration 1080: Loss = -12189.120241645027
Iteration 1090: Loss = -12189.120154867482
Iteration 1100: Loss = -12189.12005921809
Iteration 1110: Loss = -12189.11999582641
Iteration 1120: Loss = -12189.119864054921
Iteration 1130: Loss = -12189.119829253812
Iteration 1140: Loss = -12189.119702743958
Iteration 1150: Loss = -12189.119675380967
Iteration 1160: Loss = -12189.119561432974
Iteration 1170: Loss = -12189.119513939415
Iteration 1180: Loss = -12189.119445493967
Iteration 1190: Loss = -12189.119341839314
Iteration 1200: Loss = -12189.119278999593
Iteration 1210: Loss = -12189.119193638997
Iteration 1220: Loss = -12189.119097562645
Iteration 1230: Loss = -12189.11908677944
Iteration 1240: Loss = -12189.11898290203
Iteration 1250: Loss = -12189.118950128543
Iteration 1260: Loss = -12189.118870668204
Iteration 1270: Loss = -12189.118840851706
Iteration 1280: Loss = -12189.118737954754
Iteration 1290: Loss = -12189.118668140456
Iteration 1300: Loss = -12189.11862016497
Iteration 1310: Loss = -12189.118562041102
Iteration 1320: Loss = -12189.118507294255
Iteration 1330: Loss = -12189.118405904455
Iteration 1340: Loss = -12189.118378463796
Iteration 1350: Loss = -12189.11832933316
Iteration 1360: Loss = -12189.118277289568
Iteration 1370: Loss = -12189.118214656277
Iteration 1380: Loss = -12189.118151161463
Iteration 1390: Loss = -12189.11809923405
Iteration 1400: Loss = -12189.118075046135
Iteration 1410: Loss = -12189.11801837627
Iteration 1420: Loss = -12189.117964445455
Iteration 1430: Loss = -12189.117861245162
Iteration 1440: Loss = -12189.117883704588
1
Iteration 1450: Loss = -12189.117770533518
Iteration 1460: Loss = -12189.117785485334
1
Iteration 1470: Loss = -12189.117676181291
Iteration 1480: Loss = -12189.117654330135
Iteration 1490: Loss = -12189.117616259737
Iteration 1500: Loss = -12189.117550735009
Iteration 1510: Loss = -12189.117531161415
Iteration 1520: Loss = -12189.117535034877
1
Iteration 1530: Loss = -12189.117459292036
Iteration 1540: Loss = -12189.117400807114
Iteration 1550: Loss = -12189.117404172895
1
Iteration 1560: Loss = -12189.117325559646
Iteration 1570: Loss = -12189.117255955594
Iteration 1580: Loss = -12189.117295066595
1
Iteration 1590: Loss = -12189.117202392586
Iteration 1600: Loss = -12189.117159677491
Iteration 1610: Loss = -12189.117102846432
Iteration 1620: Loss = -12189.117084169346
Iteration 1630: Loss = -12189.117076910274
Iteration 1640: Loss = -12189.117006997825
Iteration 1650: Loss = -12189.116985012362
Iteration 1660: Loss = -12189.116938971049
Iteration 1670: Loss = -12189.116911282203
Iteration 1680: Loss = -12189.11688007175
Iteration 1690: Loss = -12189.1168312791
Iteration 1700: Loss = -12189.116817575417
Iteration 1710: Loss = -12189.116777136225
Iteration 1720: Loss = -12189.116744707131
Iteration 1730: Loss = -12189.116706345389
Iteration 1740: Loss = -12189.11672537115
1
Iteration 1750: Loss = -12189.116673590937
Iteration 1760: Loss = -12189.11663663921
Iteration 1770: Loss = -12189.116577175853
Iteration 1780: Loss = -12189.116575915137
Iteration 1790: Loss = -12189.116533349374
Iteration 1800: Loss = -12189.116512587447
Iteration 1810: Loss = -12189.116477505208
Iteration 1820: Loss = -12189.116485074195
1
Iteration 1830: Loss = -12189.116414098098
Iteration 1840: Loss = -12189.116416696901
1
Iteration 1850: Loss = -12189.116388715205
Iteration 1860: Loss = -12189.116370716401
Iteration 1870: Loss = -12189.11631919789
Iteration 1880: Loss = -12189.116289376145
Iteration 1890: Loss = -12189.11631877187
1
Iteration 1900: Loss = -12189.116267110869
Iteration 1910: Loss = -12189.116155355108
Iteration 1920: Loss = -12189.116181072817
1
Iteration 1930: Loss = -12189.116207814153
2
Iteration 1940: Loss = -12189.11612063068
Iteration 1950: Loss = -12189.116199030694
1
Iteration 1960: Loss = -12189.11604336708
Iteration 1970: Loss = -12189.116030724723
Iteration 1980: Loss = -12189.116024782901
Iteration 1990: Loss = -12189.11607238598
1
Iteration 2000: Loss = -12189.116017789307
Iteration 2010: Loss = -12189.115984377959
Iteration 2020: Loss = -12189.1159660705
Iteration 2030: Loss = -12189.115978687927
1
Iteration 2040: Loss = -12189.115973238788
2
Iteration 2050: Loss = -12189.115893671646
Iteration 2060: Loss = -12189.115930268597
1
Iteration 2070: Loss = -12189.115869338284
Iteration 2080: Loss = -12189.115882060647
1
Iteration 2090: Loss = -12189.115849903757
Iteration 2100: Loss = -12189.115826720887
Iteration 2110: Loss = -12189.11585327809
1
Iteration 2120: Loss = -12189.115788155501
Iteration 2130: Loss = -12189.115832079888
1
Iteration 2140: Loss = -12189.115755610068
Iteration 2150: Loss = -12189.115750663139
Iteration 2160: Loss = -12189.11571554702
Iteration 2170: Loss = -12189.115711152308
Iteration 2180: Loss = -12189.1157160781
1
Iteration 2190: Loss = -12189.11564372569
Iteration 2200: Loss = -12189.115670413494
1
Iteration 2210: Loss = -12189.115640783471
Iteration 2220: Loss = -12189.115593793742
Iteration 2230: Loss = -12189.115606507203
1
Iteration 2240: Loss = -12189.115580073021
Iteration 2250: Loss = -12189.11560589692
1
Iteration 2260: Loss = -12189.11555046599
Iteration 2270: Loss = -12189.11556488725
1
Iteration 2280: Loss = -12189.115568309817
2
Iteration 2290: Loss = -12189.115497925579
Iteration 2300: Loss = -12189.115490921595
Iteration 2310: Loss = -12189.115514295241
1
Iteration 2320: Loss = -12189.11547400907
Iteration 2330: Loss = -12189.115472623025
Iteration 2340: Loss = -12189.115469967557
Iteration 2350: Loss = -12189.115461689542
Iteration 2360: Loss = -12189.115447190587
Iteration 2370: Loss = -12189.11545662236
1
Iteration 2380: Loss = -12189.115385625033
Iteration 2390: Loss = -12189.115394708935
1
Iteration 2400: Loss = -12189.11537583838
Iteration 2410: Loss = -12189.115392710773
1
Iteration 2420: Loss = -12189.11538668774
2
Iteration 2430: Loss = -12189.115324942308
Iteration 2440: Loss = -12189.115324158163
Iteration 2450: Loss = -12189.11530738204
Iteration 2460: Loss = -12189.11533887351
1
Iteration 2470: Loss = -12189.115317636597
2
Iteration 2480: Loss = -12189.115341239683
3
Stopping early at iteration 2479 due to no improvement.
pi: tensor([[0.5346, 0.4654],
        [0.5475, 0.4525]], dtype=torch.float64)
alpha: tensor([0.5405, 0.4595])
beta: tensor([[[0.1923, 0.1906],
         [0.1360, 0.1926]],

        [[0.7906, 0.1856],
         [0.4212, 0.1682]],

        [[0.5099, 0.2005],
         [0.1036, 0.6552]],

        [[0.9221, 0.1950],
         [0.0110, 0.4545]],

        [[0.9840, 0.1906],
         [0.8449, 0.5927]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18663.821536177813
Iteration 100: Loss = -12294.057510971103
Iteration 200: Loss = -12219.103508148824
Iteration 300: Loss = -12191.556221587123
Iteration 400: Loss = -12189.917927480914
Iteration 500: Loss = -12189.497130019558
Iteration 600: Loss = -12189.33303567134
Iteration 700: Loss = -12189.244572913332
Iteration 800: Loss = -12189.194976055323
Iteration 900: Loss = -12189.164348580462
Iteration 1000: Loss = -12189.143175622115
Iteration 1100: Loss = -12189.127353755708
Iteration 1200: Loss = -12189.115154225792
Iteration 1300: Loss = -12189.10550794737
Iteration 1400: Loss = -12189.097539656987
Iteration 1500: Loss = -12189.090818149456
Iteration 1600: Loss = -12189.085234608698
Iteration 1700: Loss = -12189.080593946657
Iteration 1800: Loss = -12189.076671586505
Iteration 1900: Loss = -12189.073081071701
Iteration 2000: Loss = -12189.068251448545
Iteration 2100: Loss = -12189.030057370474
Iteration 2200: Loss = -12189.026515251871
Iteration 2300: Loss = -12189.023846394883
Iteration 2400: Loss = -12189.02146682293
Iteration 2500: Loss = -12189.01930657485
Iteration 2600: Loss = -12189.017244811066
Iteration 2700: Loss = -12189.015397806308
Iteration 2800: Loss = -12189.013805110084
Iteration 2900: Loss = -12189.01252960594
Iteration 3000: Loss = -12189.011316853204
Iteration 3100: Loss = -12189.010154876165
Iteration 3200: Loss = -12189.007534903247
Iteration 3300: Loss = -12188.94204696741
Iteration 3400: Loss = -12188.937254844375
Iteration 3500: Loss = -12188.931934158625
Iteration 3600: Loss = -12188.92926621476
Iteration 3700: Loss = -12188.927293815703
Iteration 3800: Loss = -12188.925588013832
Iteration 3900: Loss = -12188.923971799242
Iteration 4000: Loss = -12188.922526929608
Iteration 4100: Loss = -12188.92526327909
1
Iteration 4200: Loss = -12188.926194331338
2
Iteration 4300: Loss = -12188.930578625266
3
Iteration 4400: Loss = -12188.916014570217
Iteration 4500: Loss = -12188.914866212139
Iteration 4600: Loss = -12188.911204100079
Iteration 4700: Loss = -12188.920064036945
1
Iteration 4800: Loss = -12188.90328443374
Iteration 4900: Loss = -12188.897407447033
Iteration 5000: Loss = -12188.92772298451
1
Iteration 5100: Loss = -12188.881918811998
Iteration 5200: Loss = -12188.936468694827
1
Iteration 5300: Loss = -12188.870806691228
Iteration 5400: Loss = -12188.863077433307
Iteration 5500: Loss = -12188.894971618909
1
Iteration 5600: Loss = -12188.85000997378
Iteration 5700: Loss = -12188.83948399258
Iteration 5800: Loss = -12188.811049834883
Iteration 5900: Loss = -12188.7607523224
Iteration 6000: Loss = -12188.741502147044
Iteration 6100: Loss = -12188.711504690164
Iteration 6200: Loss = -12188.70453182588
Iteration 6300: Loss = -12188.70348795533
Iteration 6400: Loss = -12188.820958499247
1
Iteration 6500: Loss = -12188.700230536084
Iteration 6600: Loss = -12188.701984326904
1
Iteration 6700: Loss = -12188.699397810222
Iteration 6800: Loss = -12188.698602586108
Iteration 6900: Loss = -12188.703245600478
1
Iteration 7000: Loss = -12188.740283876637
2
Iteration 7100: Loss = -12188.709634075596
3
Iteration 7200: Loss = -12188.69765150424
Iteration 7300: Loss = -12188.698621754003
1
Iteration 7400: Loss = -12188.69784422233
2
Iteration 7500: Loss = -12188.697420135826
Iteration 7600: Loss = -12188.69745142064
1
Iteration 7700: Loss = -12188.704200147942
2
Iteration 7800: Loss = -12188.85346515503
3
Iteration 7900: Loss = -12188.697990358283
4
Iteration 8000: Loss = -12188.730283275638
5
Iteration 8100: Loss = -12188.704892630109
6
Iteration 8200: Loss = -12188.719183174922
7
Iteration 8300: Loss = -12188.718304143038
8
Iteration 8400: Loss = -12188.70742195943
9
Iteration 8500: Loss = -12188.70177530856
10
Stopping early at iteration 8500 due to no improvement.
tensor([[ 5.3266, -6.9803],
        [ 5.0465, -6.7779],
        [ 4.7608, -6.6751],
        [ 4.1431, -7.5846],
        [ 5.6339, -7.0231],
        [ 4.7397, -6.7790],
        [ 5.3829, -6.8218],
        [ 4.4626, -7.0746],
        [ 4.5547, -6.9668],
        [ 5.4004, -6.8933],
        [ 5.6442, -7.0781],
        [ 4.4440, -7.1101],
        [ 4.8820, -6.4341],
        [ 4.5368, -7.0866],
        [ 5.2617, -6.9822],
        [ 4.7174, -6.7219],
        [ 5.2410, -7.2943],
        [ 4.5594, -6.9408],
        [ 5.0093, -7.4464],
        [ 4.8259, -7.0668],
        [ 4.8051, -6.9206],
        [ 5.0058, -7.0064],
        [ 5.1600, -7.1703],
        [ 4.8028, -6.9919],
        [ 4.9164, -6.9308],
        [ 4.8194, -7.0026],
        [ 5.0711, -6.7661],
        [ 4.9993, -6.7303],
        [ 5.0184, -6.7479],
        [ 5.2732, -7.0275],
        [ 5.4795, -6.8948],
        [ 4.9532, -6.8918],
        [ 5.1039, -6.4911],
        [ 5.0926, -7.4985],
        [ 5.1604, -7.2652],
        [ 5.6001, -6.9908],
        [ 5.1897, -6.6350],
        [ 5.2864, -6.7738],
        [ 4.8492, -6.5233],
        [ 4.7266, -6.9809],
        [ 4.9293, -6.7553],
        [ 5.0856, -6.4800],
        [ 5.2010, -6.6022],
        [ 5.0348, -6.6741],
        [ 5.0843, -6.6786],
        [ 5.2712, -6.7152],
        [ 5.0631, -6.7008],
        [ 4.8785, -6.8417],
        [ 4.7925, -6.6802],
        [ 4.6846, -7.4420],
        [ 4.7648, -6.3293],
        [ 4.7655, -6.5702],
        [ 3.5237, -8.1389],
        [ 5.3454, -6.9107],
        [ 3.4829, -8.0982],
        [ 5.3088, -6.7997],
        [ 5.0319, -6.5879],
        [ 4.8145, -6.3658],
        [ 5.2078, -6.5955],
        [ 5.1380, -6.6807],
        [ 5.1110, -6.5417],
        [ 5.1432, -7.3943],
        [ 5.3253, -7.4377],
        [ 4.9333, -6.9113],
        [ 5.5436, -6.9931],
        [ 5.1870, -6.6256],
        [ 5.3713, -7.2220],
        [ 5.0893, -6.8441],
        [ 4.8586, -6.6672],
        [ 5.1668, -6.5799],
        [ 5.4656, -7.2857],
        [ 5.3243, -6.7108],
        [ 5.2650, -7.1768],
        [ 4.1294, -7.4649],
        [ 5.1564, -6.6485],
        [ 4.6312, -6.9500],
        [ 4.2562, -7.3506],
        [ 5.5062, -7.0281],
        [ 5.2163, -6.9952],
        [ 5.4904, -6.9761],
        [ 4.9291, -6.4346],
        [ 3.6025, -8.2177],
        [ 5.3222, -7.2894],
        [ 5.1665, -6.5543],
        [ 5.3007, -6.8463],
        [ 5.6375, -7.2612],
        [ 5.0618, -6.5311],
        [ 4.6749, -6.8266],
        [ 5.6349, -7.0293],
        [ 5.2134, -6.7477],
        [ 3.8598, -8.0369],
        [ 5.6672, -7.1577],
        [ 5.2274, -6.9215],
        [ 4.6075, -7.0036],
        [ 4.1134, -8.7287],
        [ 4.6528, -7.0815],
        [ 4.9643, -6.5256],
        [ 4.8995, -6.2864],
        [ 4.8413, -6.8828],
        [ 5.0091, -6.9575]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6451, 0.3549],
        [0.1510, 0.8490]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9999e-01, 7.6464e-06], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1918, 0.1990],
         [0.1360, 0.1982]],

        [[0.7906, 0.1878],
         [0.4212, 0.1682]],

        [[0.5099, 0.2023],
         [0.1036, 0.6552]],

        [[0.9221, 0.1961],
         [0.0110, 0.4545]],

        [[0.9840, 0.1912],
         [0.8449, 0.5927]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.004166919759460609
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0019972731615532165
Average Adjusted Rand Index: 0.0008333839518921218
Iteration 0: Loss = -18802.05006700048
Iteration 10: Loss = -12189.690097696479
Iteration 20: Loss = -12189.3983241373
Iteration 30: Loss = -12189.268768281807
Iteration 40: Loss = -12189.23675645354
Iteration 50: Loss = -12189.230579420126
Iteration 60: Loss = -12189.228014014063
Iteration 70: Loss = -12189.22571699703
Iteration 80: Loss = -12189.223390006508
Iteration 90: Loss = -12189.221055402244
Iteration 100: Loss = -12189.21869606222
Iteration 110: Loss = -12189.216367630854
Iteration 120: Loss = -12189.214029557195
Iteration 130: Loss = -12189.211734570059
Iteration 140: Loss = -12189.209521072149
Iteration 150: Loss = -12189.207290088036
Iteration 160: Loss = -12189.205152611963
Iteration 170: Loss = -12189.202920242944
Iteration 180: Loss = -12189.200875603392
Iteration 190: Loss = -12189.19886282538
Iteration 200: Loss = -12189.196848652398
Iteration 210: Loss = -12189.194891068504
Iteration 220: Loss = -12189.192997573595
Iteration 230: Loss = -12189.191155870381
Iteration 240: Loss = -12189.18939477752
Iteration 250: Loss = -12189.187635165143
Iteration 260: Loss = -12189.185937652743
Iteration 270: Loss = -12189.184280880092
Iteration 280: Loss = -12189.182639609962
Iteration 290: Loss = -12189.18109455074
Iteration 300: Loss = -12189.179564604712
Iteration 310: Loss = -12189.178090424093
Iteration 320: Loss = -12189.1766411424
Iteration 330: Loss = -12189.175270059879
Iteration 340: Loss = -12189.173902204679
Iteration 350: Loss = -12189.172572983982
Iteration 360: Loss = -12189.171275520068
Iteration 370: Loss = -12189.170037915206
Iteration 380: Loss = -12189.168854945088
Iteration 390: Loss = -12189.167673736649
Iteration 400: Loss = -12189.166529642189
Iteration 410: Loss = -12189.16541058465
Iteration 420: Loss = -12189.164353584609
Iteration 430: Loss = -12189.163224168575
Iteration 440: Loss = -12189.162286862702
Iteration 450: Loss = -12189.161269889119
Iteration 460: Loss = -12189.160258901231
Iteration 470: Loss = -12189.159330245458
Iteration 480: Loss = -12189.15841850884
Iteration 490: Loss = -12189.157521430365
Iteration 500: Loss = -12189.156653705668
Iteration 510: Loss = -12189.155783651673
Iteration 520: Loss = -12189.154944084856
Iteration 530: Loss = -12189.154127407652
Iteration 540: Loss = -12189.153416640786
Iteration 550: Loss = -12189.152563437267
Iteration 560: Loss = -12189.151858165445
Iteration 570: Loss = -12189.151158035731
Iteration 580: Loss = -12189.150449416444
Iteration 590: Loss = -12189.149692630133
Iteration 600: Loss = -12189.149054960226
Iteration 610: Loss = -12189.148412817774
Iteration 620: Loss = -12189.147706878795
Iteration 630: Loss = -12189.147164958376
Iteration 640: Loss = -12189.146444420361
Iteration 650: Loss = -12189.145911717358
Iteration 660: Loss = -12189.14531295051
Iteration 670: Loss = -12189.144706853462
Iteration 680: Loss = -12189.144168916617
Iteration 690: Loss = -12189.143609515835
Iteration 700: Loss = -12189.143149940608
Iteration 710: Loss = -12189.142605315421
Iteration 720: Loss = -12189.142103511938
Iteration 730: Loss = -12189.141569592075
Iteration 740: Loss = -12189.141087852453
Iteration 750: Loss = -12189.140571198997
Iteration 760: Loss = -12189.140174116412
Iteration 770: Loss = -12189.139658794787
Iteration 780: Loss = -12189.139242136476
Iteration 790: Loss = -12189.13880295119
Iteration 800: Loss = -12189.13838979231
Iteration 810: Loss = -12189.137961901233
Iteration 820: Loss = -12189.137600707722
Iteration 830: Loss = -12189.137178606448
Iteration 840: Loss = -12189.136765366524
Iteration 850: Loss = -12189.13635512331
Iteration 860: Loss = -12189.135966962742
Iteration 870: Loss = -12189.135626385349
Iteration 880: Loss = -12189.135237558361
Iteration 890: Loss = -12189.134965761948
Iteration 900: Loss = -12189.134559240063
Iteration 910: Loss = -12189.134248559318
Iteration 920: Loss = -12189.133888022821
Iteration 930: Loss = -12189.133533697872
Iteration 940: Loss = -12189.133246956035
Iteration 950: Loss = -12189.132929620078
Iteration 960: Loss = -12189.132646974767
Iteration 970: Loss = -12189.132340140375
Iteration 980: Loss = -12189.132018747203
Iteration 990: Loss = -12189.131766962057
Iteration 1000: Loss = -12189.131449821556
Iteration 1010: Loss = -12189.131158458023
Iteration 1020: Loss = -12189.130864688912
Iteration 1030: Loss = -12189.130681785931
Iteration 1040: Loss = -12189.130370803538
Iteration 1050: Loss = -12189.130105957545
Iteration 1060: Loss = -12189.129853995018
Iteration 1070: Loss = -12189.129609844429
Iteration 1080: Loss = -12189.12940120231
Iteration 1090: Loss = -12189.129134600784
Iteration 1100: Loss = -12189.128898119692
Iteration 1110: Loss = -12189.128629884828
Iteration 1120: Loss = -12189.128452397468
Iteration 1130: Loss = -12189.128213385819
Iteration 1140: Loss = -12189.127969376366
Iteration 1150: Loss = -12189.127758729524
Iteration 1160: Loss = -12189.127531856235
Iteration 1170: Loss = -12189.127394950026
Iteration 1180: Loss = -12189.12714191063
Iteration 1190: Loss = -12189.126961310722
Iteration 1200: Loss = -12189.1267854015
Iteration 1210: Loss = -12189.126575860198
Iteration 1220: Loss = -12189.126396894726
Iteration 1230: Loss = -12189.126233062503
Iteration 1240: Loss = -12189.126028861834
Iteration 1250: Loss = -12189.1258107528
Iteration 1260: Loss = -12189.125663222021
Iteration 1270: Loss = -12189.125461502756
Iteration 1280: Loss = -12189.125318939721
Iteration 1290: Loss = -12189.125158299115
Iteration 1300: Loss = -12189.124950613606
Iteration 1310: Loss = -12189.12480358648
Iteration 1320: Loss = -12189.124618292011
Iteration 1330: Loss = -12189.12453595859
Iteration 1340: Loss = -12189.124364563695
Iteration 1350: Loss = -12189.124228061646
Iteration 1360: Loss = -12189.124073762034
Iteration 1370: Loss = -12189.123884629867
Iteration 1380: Loss = -12189.123784593192
Iteration 1390: Loss = -12189.123600605286
Iteration 1400: Loss = -12189.123471754529
Iteration 1410: Loss = -12189.12338208363
Iteration 1420: Loss = -12189.123211863418
Iteration 1430: Loss = -12189.123081972177
Iteration 1440: Loss = -12189.122911054834
Iteration 1450: Loss = -12189.122813475324
Iteration 1460: Loss = -12189.12271410709
Iteration 1470: Loss = -12189.12260296992
Iteration 1480: Loss = -12189.12246642594
Iteration 1490: Loss = -12189.122329430245
Iteration 1500: Loss = -12189.122205792948
Iteration 1510: Loss = -12189.122123992738
Iteration 1520: Loss = -12189.122016236652
Iteration 1530: Loss = -12189.12185884553
Iteration 1540: Loss = -12189.12180284992
Iteration 1550: Loss = -12189.121667703932
Iteration 1560: Loss = -12189.121550733775
Iteration 1570: Loss = -12189.121464611804
Iteration 1580: Loss = -12189.121364957204
Iteration 1590: Loss = -12189.121278058114
Iteration 1600: Loss = -12189.121131666356
Iteration 1610: Loss = -12189.121065324012
Iteration 1620: Loss = -12189.120991583446
Iteration 1630: Loss = -12189.120891082404
Iteration 1640: Loss = -12189.120782207077
Iteration 1650: Loss = -12189.120687993698
Iteration 1660: Loss = -12189.120569070426
Iteration 1670: Loss = -12189.120524613692
Iteration 1680: Loss = -12189.120432304902
Iteration 1690: Loss = -12189.120357131631
Iteration 1700: Loss = -12189.120263066268
Iteration 1710: Loss = -12189.12015501542
Iteration 1720: Loss = -12189.12007349114
Iteration 1730: Loss = -12189.120038030087
Iteration 1740: Loss = -12189.119944142352
Iteration 1750: Loss = -12189.119832867525
Iteration 1760: Loss = -12189.119756592117
Iteration 1770: Loss = -12189.119694453782
Iteration 1780: Loss = -12189.119607704748
Iteration 1790: Loss = -12189.119498985512
Iteration 1800: Loss = -12189.119477366814
Iteration 1810: Loss = -12189.119370119553
Iteration 1820: Loss = -12189.119330483312
Iteration 1830: Loss = -12189.119282302481
Iteration 1840: Loss = -12189.119175777874
Iteration 1850: Loss = -12189.119104732259
Iteration 1860: Loss = -12189.119027819732
Iteration 1870: Loss = -12189.118986007974
Iteration 1880: Loss = -12189.118958175703
Iteration 1890: Loss = -12189.118838284692
Iteration 1900: Loss = -12189.118814615644
Iteration 1910: Loss = -12189.118755219986
Iteration 1920: Loss = -12189.118703440981
Iteration 1930: Loss = -12189.118625696863
Iteration 1940: Loss = -12189.118567504522
Iteration 1950: Loss = -12189.118524074522
Iteration 1960: Loss = -12189.118433722655
Iteration 1970: Loss = -12189.1183410283
Iteration 1980: Loss = -12189.11834785316
1
Iteration 1990: Loss = -12189.118249739782
Iteration 2000: Loss = -12189.118220798331
Iteration 2010: Loss = -12189.118153820944
Iteration 2020: Loss = -12189.118124526596
Iteration 2030: Loss = -12189.118095782685
Iteration 2040: Loss = -12189.11798744356
Iteration 2050: Loss = -12189.117982921769
Iteration 2060: Loss = -12189.117896623738
Iteration 2070: Loss = -12189.117889541898
Iteration 2080: Loss = -12189.117850519371
Iteration 2090: Loss = -12189.117802231603
Iteration 2100: Loss = -12189.117806723763
1
Iteration 2110: Loss = -12189.117673862844
Iteration 2120: Loss = -12189.117650398308
Iteration 2130: Loss = -12189.117612296399
Iteration 2140: Loss = -12189.117539295501
Iteration 2150: Loss = -12189.117521136024
Iteration 2160: Loss = -12189.117456730186
Iteration 2170: Loss = -12189.117435168504
Iteration 2180: Loss = -12189.117417575846
Iteration 2190: Loss = -12189.117379331998
Iteration 2200: Loss = -12189.117322323478
Iteration 2210: Loss = -12189.117279950076
Iteration 2220: Loss = -12189.11723858636
Iteration 2230: Loss = -12189.117208096122
Iteration 2240: Loss = -12189.117162783148
Iteration 2250: Loss = -12189.117119911356
Iteration 2260: Loss = -12189.11705695354
Iteration 2270: Loss = -12189.117013644303
Iteration 2280: Loss = -12189.116995601304
Iteration 2290: Loss = -12189.11695272774
Iteration 2300: Loss = -12189.116915610011
Iteration 2310: Loss = -12189.11688444365
Iteration 2320: Loss = -12189.11686590263
Iteration 2330: Loss = -12189.116855854332
Iteration 2340: Loss = -12189.116845834742
Iteration 2350: Loss = -12189.116775649876
Iteration 2360: Loss = -12189.11678410336
1
Iteration 2370: Loss = -12189.116734341922
Iteration 2380: Loss = -12189.116670000536
Iteration 2390: Loss = -12189.11665341362
Iteration 2400: Loss = -12189.116611507976
Iteration 2410: Loss = -12189.116603468445
Iteration 2420: Loss = -12189.116576905857
Iteration 2430: Loss = -12189.116530441936
Iteration 2440: Loss = -12189.116511453498
Iteration 2450: Loss = -12189.116477131736
Iteration 2460: Loss = -12189.116500267406
1
Iteration 2470: Loss = -12189.116462997848
Iteration 2480: Loss = -12189.116388107535
Iteration 2490: Loss = -12189.116473293907
1
Iteration 2500: Loss = -12189.116389649322
2
Iteration 2510: Loss = -12189.116319802033
Iteration 2520: Loss = -12189.116288130124
Iteration 2530: Loss = -12189.11628042132
Iteration 2540: Loss = -12189.116261154166
Iteration 2550: Loss = -12189.116271919129
1
Iteration 2560: Loss = -12189.116192094394
Iteration 2570: Loss = -12189.1162339047
1
Iteration 2580: Loss = -12189.116167797267
Iteration 2590: Loss = -12189.116147649273
Iteration 2600: Loss = -12189.116149625781
1
Iteration 2610: Loss = -12189.116064644213
Iteration 2620: Loss = -12189.116074957896
1
Iteration 2630: Loss = -12189.11608186181
2
Iteration 2640: Loss = -12189.11605070108
Iteration 2650: Loss = -12189.11599855231
Iteration 2660: Loss = -12189.115990657157
Iteration 2670: Loss = -12189.116006864688
1
Iteration 2680: Loss = -12189.115950095684
Iteration 2690: Loss = -12189.115980402712
1
Iteration 2700: Loss = -12189.115947744094
Iteration 2710: Loss = -12189.115877416514
Iteration 2720: Loss = -12189.11589275775
1
Iteration 2730: Loss = -12189.115847950818
Iteration 2740: Loss = -12189.115881958178
1
Iteration 2750: Loss = -12189.115810949295
Iteration 2760: Loss = -12189.11580743177
Iteration 2770: Loss = -12189.115785349166
Iteration 2780: Loss = -12189.115742843773
Iteration 2790: Loss = -12189.115793357192
1
Iteration 2800: Loss = -12189.115718845724
Iteration 2810: Loss = -12189.115783819883
1
Iteration 2820: Loss = -12189.115693550579
Iteration 2830: Loss = -12189.115722576353
1
Iteration 2840: Loss = -12189.115710496564
2
Iteration 2850: Loss = -12189.115683737697
Iteration 2860: Loss = -12189.115608784508
Iteration 2870: Loss = -12189.11561465787
1
Iteration 2880: Loss = -12189.115645937176
2
Iteration 2890: Loss = -12189.115628367872
3
Stopping early at iteration 2889 due to no improvement.
pi: tensor([[0.4318, 0.5682],
        [0.4679, 0.5321]], dtype=torch.float64)
alpha: tensor([0.4516, 0.5484])
beta: tensor([[[0.1927, 0.1906],
         [0.3003, 0.1922]],

        [[0.1060, 0.1856],
         [0.7779, 0.3755]],

        [[0.9340, 0.2005],
         [0.4805, 0.8799]],

        [[0.7409, 0.1951],
         [0.8498, 0.2979]],

        [[0.8817, 0.1906],
         [0.5825, 0.8220]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18801.44256820829
Iteration 100: Loss = -12191.330064529697
Iteration 200: Loss = -12190.114914136324
Iteration 300: Loss = -12189.783981259452
Iteration 400: Loss = -12189.597810221667
Iteration 500: Loss = -12189.466217025207
Iteration 600: Loss = -12189.332339137096
Iteration 700: Loss = -12189.205681187954
Iteration 800: Loss = -12189.117988695762
Iteration 900: Loss = -12189.03992821405
Iteration 1000: Loss = -12189.005617191307
Iteration 1100: Loss = -12188.98768180921
Iteration 1200: Loss = -12188.956690517032
Iteration 1300: Loss = -12188.946184099905
Iteration 1400: Loss = -12188.941045382904
Iteration 1500: Loss = -12188.937267128498
Iteration 1600: Loss = -12188.934192443565
Iteration 1700: Loss = -12188.931671333237
Iteration 1800: Loss = -12188.929484279028
Iteration 1900: Loss = -12188.927615337008
Iteration 2000: Loss = -12188.925926405398
Iteration 2100: Loss = -12188.924511539277
Iteration 2200: Loss = -12188.923267145567
Iteration 2300: Loss = -12188.922184880139
Iteration 2400: Loss = -12188.921157136596
Iteration 2500: Loss = -12188.920247861428
Iteration 2600: Loss = -12188.919382029026
Iteration 2700: Loss = -12188.918575302701
Iteration 2800: Loss = -12188.91771900851
Iteration 2900: Loss = -12188.916955583094
Iteration 3000: Loss = -12188.916056767692
Iteration 3100: Loss = -12188.950035125848
1
Iteration 3200: Loss = -12188.91411103374
Iteration 3300: Loss = -12188.92297376214
1
Iteration 3400: Loss = -12188.911607204884
Iteration 3500: Loss = -12188.910156193031
Iteration 3600: Loss = -12188.908144346768
Iteration 3700: Loss = -12188.90566680831
Iteration 3800: Loss = -12188.902578603396
Iteration 3900: Loss = -12188.898333910183
Iteration 4000: Loss = -12188.893236313428
Iteration 4100: Loss = -12188.887643501417
Iteration 4200: Loss = -12188.881731613386
Iteration 4300: Loss = -12188.875983815982
Iteration 4400: Loss = -12188.871092674131
Iteration 4500: Loss = -12188.867022395814
Iteration 4600: Loss = -12188.863651520296
Iteration 4700: Loss = -12188.861106078326
Iteration 4800: Loss = -12188.858555706649
Iteration 4900: Loss = -12188.856598517328
Iteration 5000: Loss = -12188.854672239726
Iteration 5100: Loss = -12188.853541074099
Iteration 5200: Loss = -12188.851271657566
Iteration 5300: Loss = -12188.853863564027
1
Iteration 5400: Loss = -12188.847358987423
Iteration 5500: Loss = -12188.843495273055
Iteration 5600: Loss = -12188.850072997098
1
Iteration 5700: Loss = -12188.80076194302
Iteration 5800: Loss = -12188.732771336206
Iteration 5900: Loss = -12188.709632842952
Iteration 6000: Loss = -12188.702396737055
Iteration 6100: Loss = -12188.699699791565
Iteration 6200: Loss = -12188.698888462994
Iteration 6300: Loss = -12188.698328298067
Iteration 6400: Loss = -12188.697921618408
Iteration 6500: Loss = -12188.69803474626
1
Iteration 6600: Loss = -12188.700948372894
2
Iteration 6700: Loss = -12188.699421752211
3
Iteration 6800: Loss = -12188.698818575378
4
Iteration 6900: Loss = -12188.697903988645
Iteration 7000: Loss = -12188.697620328354
Iteration 7100: Loss = -12188.702738315718
1
Iteration 7200: Loss = -12188.698578847405
2
Iteration 7300: Loss = -12188.69858915897
3
Iteration 7400: Loss = -12188.69898476435
4
Iteration 7500: Loss = -12188.699004859012
5
Iteration 7600: Loss = -12188.699971959415
6
Iteration 7700: Loss = -12188.707789980355
7
Iteration 7800: Loss = -12188.697925218914
8
Iteration 7900: Loss = -12188.697487347657
Iteration 8000: Loss = -12188.700646334508
1
Iteration 8100: Loss = -12188.697869982003
2
Iteration 8200: Loss = -12188.703286394277
3
Iteration 8300: Loss = -12188.98294970173
4
Iteration 8400: Loss = -12188.698876748289
5
Iteration 8500: Loss = -12188.74063395373
6
Iteration 8600: Loss = -12188.69816539767
7
Iteration 8700: Loss = -12188.699114253383
8
Iteration 8800: Loss = -12188.69909259642
9
Iteration 8900: Loss = -12188.697476678188
Iteration 9000: Loss = -12188.698887270413
1
Iteration 9100: Loss = -12188.701705325291
2
Iteration 9200: Loss = -12188.698005512979
3
Iteration 9300: Loss = -12188.697329240393
Iteration 9400: Loss = -12188.697709610891
1
Iteration 9500: Loss = -12188.69946446933
2
Iteration 9600: Loss = -12188.960549172665
3
Iteration 9700: Loss = -12188.700826436894
4
Iteration 9800: Loss = -12188.715111293275
5
Iteration 9900: Loss = -12188.701622893146
6
Iteration 10000: Loss = -12188.69816781337
7
Iteration 10100: Loss = -12188.698743003606
8
Iteration 10200: Loss = -12188.697248522596
Iteration 10300: Loss = -12188.697217278272
Iteration 10400: Loss = -12188.703518973572
1
Iteration 10500: Loss = -12188.730546979405
2
Iteration 10600: Loss = -12188.698598362917
3
Iteration 10700: Loss = -12188.727324487185
4
Iteration 10800: Loss = -12188.69708319341
Iteration 10900: Loss = -12188.699263033774
1
Iteration 11000: Loss = -12188.697161743134
2
Iteration 11100: Loss = -12188.70001310454
3
Iteration 11200: Loss = -12188.713816762722
4
Iteration 11300: Loss = -12188.697654396065
5
Iteration 11400: Loss = -12188.702699599457
6
Iteration 11500: Loss = -12188.697405496061
7
Iteration 11600: Loss = -12188.721803701981
8
Iteration 11700: Loss = -12188.706263442684
9
Iteration 11800: Loss = -12188.701755149068
10
Stopping early at iteration 11800 due to no improvement.
tensor([[-3.7794,  2.3232],
        [-3.8360,  2.3219],
        [-3.7233,  2.3364],
        [-3.7694,  2.3704],
        [-3.7766,  2.3894],
        [-4.9755,  1.1079],
        [-5.3591,  0.7439],
        [-3.8002,  2.2737],
        [-3.7978,  2.3337],
        [-3.8521,  2.2901],
        [-4.0398,  2.1812],
        [-4.0728,  2.0256],
        [-5.3445,  0.7293],
        [-3.8382,  2.2581],
        [-3.7736,  2.3682],
        [-4.1056,  2.0039],
        [-4.0080,  2.0883],
        [-3.8222,  2.3314],
        [-5.4113,  0.8089],
        [-3.9490,  2.2172],
        [-4.0966,  2.0592],
        [-3.8957,  2.4047],
        [-3.8388,  2.2485],
        [-3.7735,  2.3338],
        [-3.8632,  2.3291],
        [-3.8145,  2.3296],
        [-4.0527,  1.9680],
        [-3.7871,  2.4007],
        [-4.0981,  2.0411],
        [-4.0579,  2.0893],
        [-3.7518,  2.3615],
        [-4.0493,  2.1193],
        [-5.1086,  1.0008],
        [-4.6079,  1.5084],
        [-4.1699,  1.9888],
        [-4.0417,  2.0264],
        [-3.7734,  2.3384],
        [-4.2623,  1.8591],
        [-3.7885,  2.3803],
        [-3.7366,  2.3283],
        [-3.8452,  2.2715],
        [-4.0418,  1.9675],
        [-4.3003,  1.7839],
        [-4.2999,  1.8158],
        [-3.7612,  2.3743],
        [-4.6007,  1.4312],
        [-3.7545,  2.3424],
        [-4.0991,  1.9901],
        [-3.7606,  2.3694],
        [-3.7567,  2.3702],
        [-3.8254,  2.2166],
        [-3.7348,  2.3372],
        [-3.9527,  2.1420],
        [-3.7699,  2.3820],
        [-3.7456,  2.3409],
        [-3.8143,  2.2973],
        [-4.3172,  1.7521],
        [-4.3640,  1.7724],
        [-4.2474,  1.8965],
        [-4.1816,  2.0131],
        [-4.3382,  1.8235],
        [-3.9361,  2.1652],
        [-3.7854,  2.3901],
        [-3.7850,  2.3915],
        [-4.8310,  1.3708],
        [-3.8734,  2.2851],
        [-3.8692,  2.2706],
        [-3.9827,  2.2001],
        [-3.7982,  2.2259],
        [-4.5684,  1.4959],
        [-3.8606,  2.2220],
        [-4.1112,  2.0724],
        [-3.7877,  2.4013],
        [-4.3583,  1.7559],
        [-3.7801,  2.3865],
        [-3.9508,  2.1730],
        [-4.1369,  1.9523],
        [-3.9795,  2.1121],
        [-4.3413,  1.6909],
        [-4.0036,  2.0992],
        [-4.2077,  1.8624],
        [-4.3640,  1.8464],
        [-4.5638,  1.5927],
        [-3.8449,  2.3118],
        [-3.9245,  2.1640],
        [-3.9500,  2.1501],
        [-3.7594,  2.3731],
        [-4.2653,  1.8070],
        [-3.7883,  2.3985],
        [-3.7173,  2.2917],
        [-4.3847,  1.8073],
        [-4.1788,  2.0337],
        [-3.7434,  2.3571],
        [-3.9801,  2.1178],
        [-3.7728,  2.3861],
        [-4.1769,  1.9649],
        [-3.9876,  2.0898],
        [-4.4013,  1.6667],
        [-4.0188,  2.1809],
        [-3.8984,  2.2080]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.8580, 0.1420],
        [0.3563, 0.6437]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0022, 0.9978], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1982, 0.1940],
         [0.3003, 0.1917]],

        [[0.1060, 0.1878],
         [0.7779, 0.3755]],

        [[0.9340, 0.2020],
         [0.4805, 0.8799]],

        [[0.7409, 0.1959],
         [0.8498, 0.2979]],

        [[0.8817, 0.1910],
         [0.5825, 0.8220]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.008125898971292667
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0020137506733031198
Average Adjusted Rand Index: 0.0016251797942585335
11685.494931225014
new:  [0.0002930142699234603, 0.0002930142699234603, -0.0019972731615532165, -0.0020137506733031198] [0.0, 0.0, 0.0008333839518921218, 0.0016251797942585335] [12188.722962036652, 12188.722255176112, 12188.70177530856, 12188.701755149068]
prior:  [0.0, 0.0, 0.0, 0.0] [0.0, 0.0, 0.0, 0.0] [12190.108269292667, 12189.115677361991, 12189.115341239683, 12189.115628367872]
-----------------------------------------------------------------------------------------
This iteration is 7
True Objective function: Loss = -11740.28893810901
Iteration 0: Loss = -19159.069900028047
Iteration 10: Loss = -12200.041432808002
Iteration 20: Loss = -12200.015045513042
Iteration 30: Loss = -12199.922207389347
Iteration 40: Loss = -12199.374611133631
Iteration 50: Loss = -12199.052980613402
Iteration 60: Loss = -12198.791874562952
Iteration 70: Loss = -12198.639790764471
Iteration 80: Loss = -12198.572536746551
Iteration 90: Loss = -12198.549495181389
Iteration 100: Loss = -12198.544614109149
Iteration 110: Loss = -12198.546062058365
1
Iteration 120: Loss = -12198.549262027303
2
Iteration 130: Loss = -12198.55257696759
3
Stopping early at iteration 129 due to no improvement.
pi: tensor([[9.5839e-13, 1.0000e+00],
        [3.5497e-02, 9.6450e-01]], dtype=torch.float64)
alpha: tensor([0.0335, 0.9665])
beta: tensor([[[0.2015, 0.1043],
         [0.6145, 0.1934]],

        [[0.1428, 0.2514],
         [0.5136, 0.1413]],

        [[0.9244, 0.1890],
         [0.1859, 0.5372]],

        [[0.1324, 0.1974],
         [0.6196, 0.3345]],

        [[0.8073, 0.1657],
         [0.0653, 0.2809]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 61
Adjusted Rand Index: 0.02523388575915466
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -9.428816943229404e-05
Average Adjusted Rand Index: 0.005046777151830932
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19386.290598408454
Iteration 100: Loss = -12202.02478159874
Iteration 200: Loss = -12200.746270262181
Iteration 300: Loss = -12200.321305101475
Iteration 400: Loss = -12200.10854516504
Iteration 500: Loss = -12199.98522065496
Iteration 600: Loss = -12199.907117791146
Iteration 700: Loss = -12199.855111823494
Iteration 800: Loss = -12199.819885590314
Iteration 900: Loss = -12199.79622051688
Iteration 1000: Loss = -12199.780581145294
Iteration 1100: Loss = -12199.770688791843
Iteration 1200: Loss = -12199.764436592382
Iteration 1300: Loss = -12199.760566682782
Iteration 1400: Loss = -12199.75797489062
Iteration 1500: Loss = -12199.7560166116
Iteration 1600: Loss = -12199.754347348637
Iteration 1700: Loss = -12199.75274374305
Iteration 1800: Loss = -12199.751028291157
Iteration 1900: Loss = -12199.748958592083
Iteration 2000: Loss = -12199.746206575466
Iteration 2100: Loss = -12199.74297364249
Iteration 2200: Loss = -12199.73865380452
Iteration 2300: Loss = -12199.731705697628
Iteration 2400: Loss = -12199.71756662619
Iteration 2500: Loss = -12199.657767866627
Iteration 2600: Loss = -12198.643525321742
Iteration 2700: Loss = -12198.585652809556
Iteration 2800: Loss = -12198.562242391334
Iteration 2900: Loss = -12198.543407193652
Iteration 3000: Loss = -12198.508856405087
Iteration 3100: Loss = -12198.383367319771
Iteration 3200: Loss = -12197.64011909928
Iteration 3300: Loss = -12197.600668185682
Iteration 3400: Loss = -12197.57381301887
Iteration 3500: Loss = -12197.547496677667
Iteration 3600: Loss = -12197.530662251174
Iteration 3700: Loss = -12197.519663202796
Iteration 3800: Loss = -12197.51020923756
Iteration 3900: Loss = -12197.504807492727
Iteration 4000: Loss = -12197.500700901852
Iteration 4100: Loss = -12197.497650114623
Iteration 4200: Loss = -12197.494802272515
Iteration 4300: Loss = -12197.492602020588
Iteration 4400: Loss = -12197.490679060977
Iteration 4500: Loss = -12197.48915441513
Iteration 4600: Loss = -12197.49352576267
1
Iteration 4700: Loss = -12197.485988851817
Iteration 4800: Loss = -12197.487733642738
1
Iteration 4900: Loss = -12197.483794958722
Iteration 5000: Loss = -12197.482888742972
Iteration 5100: Loss = -12197.483076280654
1
Iteration 5200: Loss = -12197.481377496923
Iteration 5300: Loss = -12197.527737319035
1
Iteration 5400: Loss = -12197.480264190352
Iteration 5500: Loss = -12197.479729737486
Iteration 5600: Loss = -12197.483877156445
1
Iteration 5700: Loss = -12197.478873591892
Iteration 5800: Loss = -12197.478508722954
Iteration 5900: Loss = -12197.478156405838
Iteration 6000: Loss = -12197.481379414417
1
Iteration 6100: Loss = -12197.477505126504
Iteration 6200: Loss = -12197.477196684187
Iteration 6300: Loss = -12197.508503145937
1
Iteration 6400: Loss = -12197.476466476384
Iteration 6500: Loss = -12197.475938493693
Iteration 6600: Loss = -12197.475858191234
Iteration 6700: Loss = -12197.475160525453
Iteration 6800: Loss = -12197.4746687181
Iteration 6900: Loss = -12197.474268448324
Iteration 7000: Loss = -12197.614907724159
1
Iteration 7100: Loss = -12197.472724094981
Iteration 7200: Loss = -12197.471288479319
Iteration 7300: Loss = -12197.47082552239
Iteration 7400: Loss = -12197.532388724885
1
Iteration 7500: Loss = -12197.470191231914
Iteration 7600: Loss = -12197.470193109171
1
Iteration 7700: Loss = -12197.469902438437
Iteration 7800: Loss = -12197.469844116407
Iteration 7900: Loss = -12197.469713433933
Iteration 8000: Loss = -12197.469692813125
Iteration 8100: Loss = -12197.471510209643
1
Iteration 8200: Loss = -12197.514337503551
2
Iteration 8300: Loss = -12197.469694066329
3
Iteration 8400: Loss = -12197.489919265008
4
Iteration 8500: Loss = -12197.46919157586
Iteration 8600: Loss = -12197.470675593244
1
Iteration 8700: Loss = -12197.46903044059
Iteration 8800: Loss = -12197.470740908291
1
Iteration 8900: Loss = -12197.471869621406
2
Iteration 9000: Loss = -12197.46901692051
Iteration 9100: Loss = -12197.468783127088
Iteration 9200: Loss = -12197.486365633318
1
Iteration 9300: Loss = -12197.47384733571
2
Iteration 9400: Loss = -12197.468507218133
Iteration 9500: Loss = -12197.468739761469
1
Iteration 9600: Loss = -12197.579043311973
2
Iteration 9700: Loss = -12197.468248690822
Iteration 9800: Loss = -12197.56700991555
1
Iteration 9900: Loss = -12197.468067506044
Iteration 10000: Loss = -12197.844191455835
1
Iteration 10100: Loss = -12197.469866957948
2
Iteration 10200: Loss = -12197.468194691213
3
Iteration 10300: Loss = -12197.46785028144
Iteration 10400: Loss = -12197.475913654467
1
Iteration 10500: Loss = -12197.467640379959
Iteration 10600: Loss = -12197.468760761187
1
Iteration 10700: Loss = -12197.468159979027
2
Iteration 10800: Loss = -12197.467425851175
Iteration 10900: Loss = -12197.629608687796
1
Iteration 11000: Loss = -12197.467285188759
Iteration 11100: Loss = -12197.467246022956
Iteration 11200: Loss = -12197.592160864308
1
Iteration 11300: Loss = -12197.467225487748
Iteration 11400: Loss = -12197.4739200541
1
Iteration 11500: Loss = -12197.46712890813
Iteration 11600: Loss = -12197.48272117211
1
Iteration 11700: Loss = -12197.467116767151
Iteration 11800: Loss = -12197.467004573085
Iteration 11900: Loss = -12197.704738275941
1
Iteration 12000: Loss = -12197.466933500082
Iteration 12100: Loss = -12197.474126310106
1
Iteration 12200: Loss = -12197.466881014148
Iteration 12300: Loss = -12197.543136255481
1
Iteration 12400: Loss = -12197.466844815966
Iteration 12500: Loss = -12197.467161420027
1
Iteration 12600: Loss = -12197.550111785999
2
Iteration 12700: Loss = -12197.680122626693
3
Iteration 12800: Loss = -12197.470440419303
4
Iteration 12900: Loss = -12197.494720725706
5
Iteration 13000: Loss = -12197.466766335237
Iteration 13100: Loss = -12197.46667461255
Iteration 13200: Loss = -12197.466798278263
1
Iteration 13300: Loss = -12197.471340430693
2
Iteration 13400: Loss = -12197.467133007955
3
Iteration 13500: Loss = -12197.470743169497
4
Iteration 13600: Loss = -12197.466620062114
Iteration 13700: Loss = -12197.467660682429
1
Iteration 13800: Loss = -12197.476081769959
2
Iteration 13900: Loss = -12197.466563222484
Iteration 14000: Loss = -12197.473647439954
1
Iteration 14100: Loss = -12197.70434399023
2
Iteration 14200: Loss = -12197.466565607017
3
Iteration 14300: Loss = -12197.472724369238
4
Iteration 14400: Loss = -12197.466519190542
Iteration 14500: Loss = -12197.469537102515
1
Iteration 14600: Loss = -12197.467884342368
2
Iteration 14700: Loss = -12197.466489143015
Iteration 14800: Loss = -12197.471244046927
1
Iteration 14900: Loss = -12197.466505397171
2
Iteration 15000: Loss = -12197.469094579223
3
Iteration 15100: Loss = -12197.466548864182
4
Iteration 15200: Loss = -12197.466540975047
5
Iteration 15300: Loss = -12197.466480247183
Iteration 15400: Loss = -12197.466517021272
1
Iteration 15500: Loss = -12197.466422952164
Iteration 15600: Loss = -12197.466612532819
1
Iteration 15700: Loss = -12197.466459425334
2
Iteration 15800: Loss = -12197.466698611437
3
Iteration 15900: Loss = -12197.467054952302
4
Iteration 16000: Loss = -12197.741178253937
5
Iteration 16100: Loss = -12197.473786809383
6
Iteration 16200: Loss = -12197.46646014286
7
Iteration 16300: Loss = -12197.466531993798
8
Iteration 16400: Loss = -12197.466495866933
9
Iteration 16500: Loss = -12197.466777656557
10
Stopping early at iteration 16500 due to no improvement.
tensor([[-2.0276e+00, -2.5876e+00],
        [-7.0410e+00,  2.4258e+00],
        [-5.5648e+00,  9.4959e-01],
        [-5.5703e+00,  9.5512e-01],
        [-4.6016e+00, -1.3604e-02],
        [-9.4521e+00,  4.8369e+00],
        [-6.4738e+00,  1.8586e+00],
        [-6.7551e+00,  2.1398e+00],
        [-6.9032e+00,  2.2880e+00],
        [-2.1102e+00, -2.5050e+00],
        [-4.6236e+00,  8.4164e-03],
        [-5.5120e+00,  8.9676e-01],
        [-7.9434e+00,  3.3282e+00],
        [-3.9720e+00, -6.4317e-01],
        [-7.8697e+00,  3.2545e+00],
        [-4.9404e+00,  3.2517e-01],
        [-6.4186e+00,  1.8033e+00],
        [-7.1150e+00,  2.4997e+00],
        [-4.9178e+00,  3.0262e-01],
        [-6.5641e+00,  1.9489e+00],
        [-7.8566e+00,  3.2414e+00],
        [-9.1811e+00,  4.5659e+00],
        [-5.1913e+00,  5.7610e-01],
        [-9.5137e+00,  4.8985e+00],
        [-4.6625e+00,  4.7316e-02],
        [-7.4963e+00,  2.8810e+00],
        [-7.4509e+00,  2.8357e+00],
        [-7.1238e+00,  2.5086e+00],
        [-6.0479e+00,  1.4327e+00],
        [-7.8812e+00,  3.2660e+00],
        [-2.5151e+00, -2.1002e+00],
        [-5.6564e+00,  1.0411e+00],
        [-4.8684e+00,  2.5321e-01],
        [-1.2842e+00, -3.3310e+00],
        [-6.9651e+00,  2.3499e+00],
        [-8.2447e+00,  3.6295e+00],
        [-8.8949e+00,  4.2796e+00],
        [-8.2844e+00,  3.6692e+00],
        [-7.9366e+00,  3.3214e+00],
        [-6.5086e+00,  1.8934e+00],
        [-6.0173e+00,  1.4021e+00],
        [-5.2471e+00,  6.3186e-01],
        [-5.9797e+00,  1.3644e+00],
        [-3.2962e+00, -1.3190e+00],
        [-4.7622e+00,  1.4694e-01],
        [-5.7324e+00,  1.1172e+00],
        [-4.8755e+00,  2.6026e-01],
        [-5.2946e+00,  6.7936e-01],
        [-6.2337e+00,  1.6185e+00],
        [-5.4414e+00,  8.2613e-01],
        [-6.0744e+00,  1.4592e+00],
        [-5.2967e+00,  6.8151e-01],
        [-5.0399e+00,  4.2471e-01],
        [-6.5874e+00,  1.9722e+00],
        [-9.7212e+00,  5.1060e+00],
        [-5.9237e+00,  1.3085e+00],
        [-5.8462e+00,  1.2310e+00],
        [-3.5989e+00, -1.0163e+00],
        [-6.7611e+00,  2.1459e+00],
        [-6.9709e+00,  2.3557e+00],
        [-5.3411e+00,  7.2583e-01],
        [-5.5761e+00,  9.6086e-01],
        [-8.0303e+00,  3.4151e+00],
        [-5.4977e+00,  8.8247e-01],
        [-8.9920e+00,  4.3768e+00],
        [-4.7191e+00,  1.0390e-01],
        [-6.6831e+00,  2.0678e+00],
        [-5.6588e+00,  1.0436e+00],
        [-5.4872e+00,  8.7198e-01],
        [-3.7370e+00, -8.7823e-01],
        [-5.4228e+00,  8.0759e-01],
        [-5.4980e+00,  8.8275e-01],
        [-6.2344e+00,  1.6192e+00],
        [-7.3744e+00,  2.7592e+00],
        [-6.3582e+00,  1.7430e+00],
        [-7.1428e+00,  2.5276e+00],
        [-4.2139e+00, -4.0135e-01],
        [-7.1591e+00,  2.5439e+00],
        [-5.1952e+00,  5.7996e-01],
        [-3.9924e+00, -6.2285e-01],
        [-7.0373e+00,  2.4220e+00],
        [-6.4950e+00,  1.8798e+00],
        [-6.9036e+00,  2.2883e+00],
        [-6.7801e+00,  2.1649e+00],
        [-5.7030e+00,  1.0878e+00],
        [-2.2432e+00, -2.3721e+00],
        [-7.1214e+00,  2.5062e+00],
        [-2.4677e+00, -2.1475e+00],
        [-6.6778e+00,  2.0626e+00],
        [-4.4689e+00, -1.4627e-01],
        [-6.2437e+00,  1.6285e+00],
        [-9.0683e+00,  4.4531e+00],
        [-7.3006e+00,  2.6854e+00],
        [-5.2179e+00,  6.0268e-01],
        [-6.8364e+00,  2.2211e+00],
        [-5.7321e+00,  1.1169e+00],
        [-7.6166e+00,  3.0014e+00],
        [-7.2954e+00,  2.6802e+00],
        [-1.2016e+00, -3.4136e+00],
        [-6.3676e+00,  1.7524e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 2.0662e-06],
        [5.1714e-03, 9.9483e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0484, 0.9516], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3766, 0.2014],
         [0.6145, 0.1931]],

        [[0.1428, 0.2424],
         [0.5136, 0.1413]],

        [[0.9244, 0.2168],
         [0.1859, 0.5372]],

        [[0.1324, 0.2253],
         [0.6196, 0.3345]],

        [[0.8073, 0.1339],
         [0.0653, 0.2809]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: -0.012897625100762696
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.005661239846689405
time is 2
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.007231852971857895
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.018289900270691713
time is 4
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.006214218236452756
Global Adjusted Rand Index: -0.0004995199731330595
Average Adjusted Rand Index: -0.004680538801966632
Iteration 0: Loss = -22308.104216513664
Iteration 10: Loss = -12200.05274201698
Iteration 20: Loss = -12200.048359396309
Iteration 30: Loss = -12200.014262944043
Iteration 40: Loss = -12199.868099846215
Iteration 50: Loss = -12199.675969407232
Iteration 60: Loss = -12199.39065647174
Iteration 70: Loss = -12199.064519986761
Iteration 80: Loss = -12198.797770661396
Iteration 90: Loss = -12198.642580221316
Iteration 100: Loss = -12198.573947648147
Iteration 110: Loss = -12198.550039857491
Iteration 120: Loss = -12198.544717580733
Iteration 130: Loss = -12198.54598178859
1
Iteration 140: Loss = -12198.54911417239
2
Iteration 150: Loss = -12198.552398774733
3
Stopping early at iteration 149 due to no improvement.
pi: tensor([[9.6455e-01, 3.5454e-02],
        [1.0000e+00, 5.4065e-17]], dtype=torch.float64)
alpha: tensor([0.9665, 0.0335])
beta: tensor([[[0.1934, 0.1043],
         [0.5587, 0.2015]],

        [[0.7761, 0.2514],
         [0.6924, 0.8892]],

        [[0.3102, 0.1890],
         [0.6065, 0.6554]],

        [[0.5748, 0.1974],
         [0.8843, 0.7068]],

        [[0.8645, 0.1657],
         [0.6079, 0.8825]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 39
Adjusted Rand Index: 0.02523388575915466
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -9.428816943229404e-05
Average Adjusted Rand Index: 0.005046777151830932
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22307.717363767842
Iteration 100: Loss = -12220.252365587117
Iteration 200: Loss = -12201.945673335993
Iteration 300: Loss = -12200.494238002813
Iteration 400: Loss = -12200.04491392287
Iteration 500: Loss = -12199.850673395962
Iteration 600: Loss = -12199.749692959023
Iteration 700: Loss = -12199.680522114786
Iteration 800: Loss = -12199.629419135847
Iteration 900: Loss = -12199.586562666911
Iteration 1000: Loss = -12199.548340509595
Iteration 1100: Loss = -12199.507294963403
Iteration 1200: Loss = -12199.478442738682
Iteration 1300: Loss = -12199.453900022718
Iteration 1400: Loss = -12199.430080366817
Iteration 1500: Loss = -12199.406277613267
Iteration 1600: Loss = -12199.382422461389
Iteration 1700: Loss = -12199.358671136692
Iteration 1800: Loss = -12199.336057641018
Iteration 1900: Loss = -12199.314413027589
Iteration 2000: Loss = -12199.29422187518
Iteration 2100: Loss = -12199.276269917964
Iteration 2200: Loss = -12199.261056305008
Iteration 2300: Loss = -12199.248606487057
Iteration 2400: Loss = -12199.238818040723
Iteration 2500: Loss = -12199.231370753161
Iteration 2600: Loss = -12199.225029804635
Iteration 2700: Loss = -12199.214449538222
Iteration 2800: Loss = -12199.195540872051
Iteration 2900: Loss = -12199.19113435625
Iteration 3000: Loss = -12199.188363902576
Iteration 3100: Loss = -12199.186244024277
Iteration 3200: Loss = -12199.184597769015
Iteration 3300: Loss = -12199.183248599706
Iteration 3400: Loss = -12199.182176336473
Iteration 3500: Loss = -12199.181312928595
Iteration 3600: Loss = -12199.180733497455
Iteration 3700: Loss = -12199.181047733146
1
Iteration 3800: Loss = -12199.18011616713
Iteration 3900: Loss = -12199.179946478675
Iteration 4000: Loss = -12199.267302403636
1
Iteration 4100: Loss = -12199.179632934012
Iteration 4200: Loss = -12199.179470481977
Iteration 4300: Loss = -12199.217398708914
1
Iteration 4400: Loss = -12199.179344037857
Iteration 4500: Loss = -12199.179249797779
Iteration 4600: Loss = -12199.179186681124
Iteration 4700: Loss = -12199.1792449182
1
Iteration 4800: Loss = -12199.17908497649
Iteration 4900: Loss = -12199.179020252883
Iteration 5000: Loss = -12199.187197943966
1
Iteration 5100: Loss = -12199.178979763026
Iteration 5200: Loss = -12199.178917727873
Iteration 5300: Loss = -12199.25938581172
1
Iteration 5400: Loss = -12199.17887050896
Iteration 5500: Loss = -12199.178767150273
Iteration 5600: Loss = -12199.17877675862
1
Iteration 5700: Loss = -12199.178804118426
2
Iteration 5800: Loss = -12199.178701299319
Iteration 5900: Loss = -12199.17863431011
Iteration 6000: Loss = -12199.237533711168
1
Iteration 6100: Loss = -12199.17858555135
Iteration 6200: Loss = -12199.17855055741
Iteration 6300: Loss = -12199.178466415571
Iteration 6400: Loss = -12199.178463482152
Iteration 6500: Loss = -12199.17834534467
Iteration 6600: Loss = -12199.178327283887
Iteration 6700: Loss = -12199.178577518644
1
Iteration 6800: Loss = -12199.178234383375
Iteration 6900: Loss = -12199.178180624102
Iteration 7000: Loss = -12199.178978701664
1
Iteration 7100: Loss = -12199.178076538838
Iteration 7200: Loss = -12199.17802969806
Iteration 7300: Loss = -12199.397608931768
1
Iteration 7400: Loss = -12199.177953086313
Iteration 7500: Loss = -12199.177912793657
Iteration 7600: Loss = -12199.470993478806
1
Iteration 7700: Loss = -12199.177812188485
Iteration 7800: Loss = -12199.177779248179
Iteration 7900: Loss = -12199.180798265581
1
Iteration 8000: Loss = -12199.177676325275
Iteration 8100: Loss = -12199.177595984731
Iteration 8200: Loss = -12199.17758212658
Iteration 8300: Loss = -12199.177623769063
1
Iteration 8400: Loss = -12199.177462000089
Iteration 8500: Loss = -12199.177387263768
Iteration 8600: Loss = -12199.177588205766
1
Iteration 8700: Loss = -12199.17727001219
Iteration 8800: Loss = -12199.177201924187
Iteration 8900: Loss = -12199.180364911836
1
Iteration 9000: Loss = -12199.176597519272
Iteration 9100: Loss = -12199.17148056105
Iteration 9200: Loss = -12198.21629099954
Iteration 9300: Loss = -12198.208188028766
Iteration 9400: Loss = -12198.206560799303
Iteration 9500: Loss = -12198.205587045022
Iteration 9600: Loss = -12198.47227262607
1
Iteration 9700: Loss = -12198.204492039604
Iteration 9800: Loss = -12198.204117399888
Iteration 9900: Loss = -12198.204227410864
1
Iteration 10000: Loss = -12198.20574019324
2
Iteration 10100: Loss = -12198.218936891608
3
Iteration 10200: Loss = -12198.20330888552
Iteration 10300: Loss = -12198.219236519042
1
Iteration 10400: Loss = -12198.204893158047
2
Iteration 10500: Loss = -12198.203015266738
Iteration 10600: Loss = -12198.203244313085
1
Iteration 10700: Loss = -12198.202885164048
Iteration 10800: Loss = -12198.20276952834
Iteration 10900: Loss = -12198.209447964682
1
Iteration 11000: Loss = -12198.202699267273
Iteration 11100: Loss = -12198.202634495743
Iteration 11200: Loss = -12198.202642608421
1
Iteration 11300: Loss = -12198.202702687255
2
Iteration 11400: Loss = -12198.207098880643
3
Iteration 11500: Loss = -12198.202485418129
Iteration 11600: Loss = -12198.204274251018
1
Iteration 11700: Loss = -12198.20786790539
2
Iteration 11800: Loss = -12198.20238694543
Iteration 11900: Loss = -12198.203261035005
1
Iteration 12000: Loss = -12198.202319563025
Iteration 12100: Loss = -12198.222415755396
1
Iteration 12200: Loss = -12198.202339293242
2
Iteration 12300: Loss = -12198.45139173641
3
Iteration 12400: Loss = -12198.202314713513
Iteration 12500: Loss = -12198.2022851653
Iteration 12600: Loss = -12198.204200946035
1
Iteration 12700: Loss = -12198.202579489314
2
Iteration 12800: Loss = -12198.216141506206
3
Iteration 12900: Loss = -12198.20224437362
Iteration 13000: Loss = -12198.202207535987
Iteration 13100: Loss = -12198.214091908645
1
Iteration 13200: Loss = -12198.20218571535
Iteration 13300: Loss = -12198.202187659339
1
Iteration 13400: Loss = -12198.202563636305
2
Iteration 13500: Loss = -12198.202220066434
3
Iteration 13600: Loss = -12198.283718636503
4
Iteration 13700: Loss = -12198.202200715976
5
Iteration 13800: Loss = -12198.202261519578
6
Iteration 13900: Loss = -12198.231264886907
7
Iteration 14000: Loss = -12198.24143398609
8
Iteration 14100: Loss = -12198.208715153265
9
Iteration 14200: Loss = -12198.202200832724
10
Stopping early at iteration 14200 due to no improvement.
tensor([[ 3.1741, -4.6899],
        [ 4.7635, -6.1805],
        [ 4.2337, -6.7243],
        [ 3.3427, -5.1588],
        [ 3.4758, -4.9184],
        [ 3.9500, -6.1956],
        [ 1.7869, -4.5055],
        [ 3.8566, -5.2433],
        [ 0.7095, -2.2677],
        [ 4.7375, -6.1643],
        [ 2.3509, -3.8632],
        [ 3.3921, -5.2463],
        [ 2.7810, -5.0818],
        [ 1.2657, -2.7416],
        [ 3.0073, -4.4017],
        [ 2.6109, -4.4527],
        [ 1.0969, -2.5331],
        [ 2.3854, -4.6016],
        [ 4.5168, -5.9573],
        [ 6.1117, -8.2625],
        [ 1.6424, -3.1420],
        [ 0.2023, -3.0003],
        [ 1.0167, -2.8718],
        [ 4.4921, -6.2568],
        [ 2.9785, -5.6778],
        [ 0.7376, -4.3160],
        [ 2.4776, -3.8746],
        [ 1.5701, -3.1461],
        [ 1.1937, -3.0982],
        [-0.8966, -0.6263],
        [ 2.2693, -3.9245],
        [ 3.5868, -8.2020],
        [ 4.3455, -5.8517],
        [ 3.4704, -5.8760],
        [ 3.1188, -4.5250],
        [ 1.9822, -4.2842],
        [-0.1312, -1.4084],
        [ 1.3737, -3.0025],
        [ 3.9168, -7.0904],
        [ 2.0068, -4.2508],
        [ 1.8040, -5.2793],
        [ 1.1635, -2.7153],
        [ 3.9171, -6.1814],
        [ 0.4231, -2.7221],
        [ 3.6873, -5.0736],
        [-1.6147,  0.2276],
        [ 2.7804, -4.3503],
        [ 2.4628, -3.8510],
        [ 5.5417, -6.9288],
        [ 4.7191, -6.1771],
        [ 1.5944, -3.0180],
        [ 2.8346, -4.4283],
        [ 4.1285, -6.0202],
        [ 3.9037, -5.3007],
        [ 2.6747, -5.5804],
        [ 3.4176, -5.0592],
        [ 5.0761, -6.6592],
        [ 3.1976, -4.6561],
        [ 4.3590, -6.2278],
        [ 1.0786, -2.4650],
        [ 2.7396, -4.2218],
        [ 1.9604, -3.6642],
        [-0.3674, -1.7757],
        [ 2.9895, -6.4203],
        [ 1.5070, -2.9219],
        [ 3.0478, -4.7733],
        [ 5.4569, -6.9395],
        [ 2.4504, -3.8844],
        [ 4.1304, -6.0111],
        [ 3.2432, -7.2600],
        [ 3.3925, -5.1433],
        [ 1.3107, -4.2071],
        [ 2.2524, -3.6771],
        [ 2.7886, -4.2345],
        [-1.3002, -0.7876],
        [ 4.3046, -5.6931],
        [ 3.2270, -4.6385],
        [ 1.7435, -4.8392],
        [ 5.1248, -6.5874],
        [ 2.4559, -3.9034],
        [ 2.5090, -4.4805],
        [ 5.2712, -6.6849],
        [ 2.6963, -7.3116],
        [ 0.3848, -3.5344],
        [ 4.6243, -6.3419],
        [ 3.8622, -5.4555],
        [ 2.0379, -3.5085],
        [ 2.9818, -5.4874],
        [ 1.4118, -2.9591],
        [ 2.6562, -5.2999],
        [ 3.7553, -5.1445],
        [ 4.1040, -6.0479],
        [ 1.2166, -3.2327],
        [ 3.8528, -5.2424],
        [ 3.9205, -5.3481],
        [ 3.9010, -5.5671],
        [ 5.3922, -8.6234],
        [ 1.5142, -3.2537],
        [ 2.8575, -5.2603],
        [ 3.0023, -7.6175]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.6267e-01, 3.7325e-02],
        [9.9996e-01, 3.5972e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9712, 0.0288], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1952, 0.1008],
         [0.5587, 0.2174]],

        [[0.7761, 0.2504],
         [0.6924, 0.8892]],

        [[0.3102, 0.1955],
         [0.6065, 0.6554]],

        [[0.5748, 0.2012],
         [0.8843, 0.7068]],

        [[0.8645, 0.1687],
         [0.6079, 0.8825]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 39
Adjusted Rand Index: 0.02523388575915466
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -9.428816943229404e-05
Average Adjusted Rand Index: 0.005046777151830932
Iteration 0: Loss = -19277.501804212872
Iteration 10: Loss = -12200.054900343584
Iteration 20: Loss = -12200.054900435212
1
Iteration 30: Loss = -12200.054900709834
2
Iteration 40: Loss = -12200.054901651878
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[1.0000e+00, 4.6479e-11],
        [1.0000e+00, 1.4061e-15]], dtype=torch.float64)
alpha: tensor([1.0000e+00, 6.2543e-11])
beta: tensor([[[0.1927, 0.0875],
         [0.6704, 0.1286]],

        [[0.9769, 0.2695],
         [0.1090, 0.0983]],

        [[0.7459, 0.2274],
         [0.2093, 0.0448]],

        [[0.7807, 0.2025],
         [0.8770, 0.1036]],

        [[0.8690, 0.1549],
         [0.2993, 0.8689]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19277.420097202652
Iteration 100: Loss = -12202.374106938254
Iteration 200: Loss = -12200.929583806981
Iteration 300: Loss = -12200.426032510215
Iteration 400: Loss = -12200.167487402654
Iteration 500: Loss = -12199.987687233892
Iteration 600: Loss = -12199.840550675684
Iteration 700: Loss = -12199.72230297033
Iteration 800: Loss = -12199.631527111671
Iteration 900: Loss = -12199.561682494641
Iteration 1000: Loss = -12199.505876346559
Iteration 1100: Loss = -12199.460095723201
Iteration 1200: Loss = -12199.421864328464
Iteration 1300: Loss = -12199.389714419944
Iteration 1400: Loss = -12199.362439169767
Iteration 1500: Loss = -12199.338633812995
Iteration 1600: Loss = -12199.317896154203
Iteration 1700: Loss = -12199.299555217562
Iteration 1800: Loss = -12199.282941495952
Iteration 1900: Loss = -12199.267814332046
Iteration 2000: Loss = -12199.253967141964
Iteration 2100: Loss = -12199.241392371618
Iteration 2200: Loss = -12199.230134642907
Iteration 2300: Loss = -12199.220378154636
Iteration 2400: Loss = -12199.212056830442
Iteration 2500: Loss = -12199.205260955448
Iteration 2600: Loss = -12199.199894799622
Iteration 2700: Loss = -12199.195781323586
Iteration 2800: Loss = -12199.192761210095
Iteration 2900: Loss = -12199.190478417411
Iteration 3000: Loss = -12199.188809912772
Iteration 3100: Loss = -12199.187587272514
Iteration 3200: Loss = -12199.186645654005
Iteration 3300: Loss = -12199.185918867595
Iteration 3400: Loss = -12199.185346417848
Iteration 3500: Loss = -12199.184929557037
Iteration 3600: Loss = -12199.184565942734
Iteration 3700: Loss = -12199.184267860963
Iteration 3800: Loss = -12199.184002576843
Iteration 3900: Loss = -12199.183842825882
Iteration 4000: Loss = -12199.183668175325
Iteration 4100: Loss = -12199.183543732855
Iteration 4200: Loss = -12199.183410195978
Iteration 4300: Loss = -12199.183260752143
Iteration 4400: Loss = -12199.183160356315
Iteration 4500: Loss = -12199.183048428755
Iteration 4600: Loss = -12199.182917143606
Iteration 4700: Loss = -12199.182855358864
Iteration 4800: Loss = -12199.182790829693
Iteration 4900: Loss = -12199.182640781659
Iteration 5000: Loss = -12199.182562993552
Iteration 5100: Loss = -12199.182613491577
1
Iteration 5200: Loss = -12199.184336618004
2
Iteration 5300: Loss = -12199.184040235063
3
Iteration 5400: Loss = -12199.18211844827
Iteration 5500: Loss = -12199.277596206139
1
Iteration 5600: Loss = -12199.181935273094
Iteration 5700: Loss = -12199.181825565727
Iteration 5800: Loss = -12199.181725540133
Iteration 5900: Loss = -12199.182135205745
1
Iteration 6000: Loss = -12199.181503736003
Iteration 6100: Loss = -12199.181354324259
Iteration 6200: Loss = -12199.26786257798
1
Iteration 6300: Loss = -12199.18118301696
Iteration 6400: Loss = -12199.18103181193
Iteration 6500: Loss = -12199.181542578346
1
Iteration 6600: Loss = -12199.18084838452
Iteration 6700: Loss = -12199.180631595676
Iteration 6800: Loss = -12199.18061065396
Iteration 6900: Loss = -12199.180495461726
Iteration 7000: Loss = -12199.180288799787
Iteration 7100: Loss = -12199.18016640787
Iteration 7200: Loss = -12199.180435560249
1
Iteration 7300: Loss = -12199.179932758303
Iteration 7400: Loss = -12199.179809732557
Iteration 7500: Loss = -12199.182250862912
1
Iteration 7600: Loss = -12199.17957433636
Iteration 7700: Loss = -12199.179461583186
Iteration 7800: Loss = -12199.20303145436
1
Iteration 7900: Loss = -12199.179217015564
Iteration 8000: Loss = -12199.17910386032
Iteration 8100: Loss = -12199.18230681767
1
Iteration 8200: Loss = -12199.178914838816
Iteration 8300: Loss = -12199.178791993196
Iteration 8400: Loss = -12199.178687386602
Iteration 8500: Loss = -12199.178825992727
1
Iteration 8600: Loss = -12199.17844648391
Iteration 8700: Loss = -12199.178360757664
Iteration 8800: Loss = -12199.179713022615
1
Iteration 8900: Loss = -12199.178154072675
Iteration 9000: Loss = -12199.178014120265
Iteration 9100: Loss = -12199.18263633746
1
Iteration 9200: Loss = -12199.172987386857
Iteration 9300: Loss = -12198.21520091914
Iteration 9400: Loss = -12198.212275706126
Iteration 9500: Loss = -12198.208411382077
Iteration 9600: Loss = -12198.208924787798
1
Iteration 9700: Loss = -12198.206202293433
Iteration 9800: Loss = -12198.330020515592
1
Iteration 9900: Loss = -12198.205076794617
Iteration 10000: Loss = -12198.204698604068
Iteration 10100: Loss = -12198.212449515706
1
Iteration 10200: Loss = -12198.204126534401
Iteration 10300: Loss = -12198.203944094921
Iteration 10400: Loss = -12198.210911183176
1
Iteration 10500: Loss = -12198.203595557168
Iteration 10600: Loss = -12198.20341786107
Iteration 10700: Loss = -12198.393594194433
1
Iteration 10800: Loss = -12198.203240206129
Iteration 10900: Loss = -12198.20312047428
Iteration 11000: Loss = -12198.206182426435
1
Iteration 11100: Loss = -12198.207067304069
2
Iteration 11200: Loss = -12198.345912707162
3
Iteration 11300: Loss = -12198.202823298041
Iteration 11400: Loss = -12198.205093154787
1
Iteration 11500: Loss = -12198.202705475655
Iteration 11600: Loss = -12198.242541166526
1
Iteration 11700: Loss = -12198.20263902739
Iteration 11800: Loss = -12198.24756620892
1
Iteration 11900: Loss = -12198.202588792485
Iteration 12000: Loss = -12198.206794608177
1
Iteration 12100: Loss = -12198.203734698813
2
Iteration 12200: Loss = -12198.202728998785
3
Iteration 12300: Loss = -12198.202547093688
Iteration 12400: Loss = -12198.202445540404
Iteration 12500: Loss = -12198.20378289342
1
Iteration 12600: Loss = -12198.206116663037
2
Iteration 12700: Loss = -12198.219927180675
3
Iteration 12800: Loss = -12198.20236509664
Iteration 12900: Loss = -12198.241854518821
1
Iteration 13000: Loss = -12198.202328094689
Iteration 13100: Loss = -12198.202303811193
Iteration 13200: Loss = -12198.202669625069
1
Iteration 13300: Loss = -12198.202329448839
2
Iteration 13400: Loss = -12198.202459817878
3
Iteration 13500: Loss = -12198.202356191065
4
Iteration 13600: Loss = -12198.20450760784
5
Iteration 13700: Loss = -12198.202312105594
6
Iteration 13800: Loss = -12198.202189511088
Iteration 13900: Loss = -12198.20435720323
1
Iteration 14000: Loss = -12198.202218831446
2
Iteration 14100: Loss = -12198.237102187764
3
Iteration 14200: Loss = -12198.208386870561
4
Iteration 14300: Loss = -12198.202225618637
5
Iteration 14400: Loss = -12198.206959829771
6
Iteration 14500: Loss = -12198.20221988193
7
Iteration 14600: Loss = -12198.202549065809
8
Iteration 14700: Loss = -12198.202147040272
Iteration 14800: Loss = -12198.206401837579
1
Iteration 14900: Loss = -12198.205874710562
2
Iteration 15000: Loss = -12198.256644180732
3
Iteration 15100: Loss = -12198.20247486847
4
Iteration 15200: Loss = -12198.205934894897
5
Iteration 15300: Loss = -12198.202190107064
6
Iteration 15400: Loss = -12198.384659806088
7
Iteration 15500: Loss = -12198.202170030512
8
Iteration 15600: Loss = -12198.210707821676
9
Iteration 15700: Loss = -12198.202138556453
Iteration 15800: Loss = -12198.238813400412
1
Iteration 15900: Loss = -12198.203920783972
2
Iteration 16000: Loss = -12198.20208116968
Iteration 16100: Loss = -12198.202271658865
1
Iteration 16200: Loss = -12198.202079364706
Iteration 16300: Loss = -12198.20560037546
1
Iteration 16400: Loss = -12198.314592741966
2
Iteration 16500: Loss = -12198.202104304606
3
Iteration 16600: Loss = -12198.203443609045
4
Iteration 16700: Loss = -12198.20208877908
5
Iteration 16800: Loss = -12198.211357580762
6
Iteration 16900: Loss = -12198.202085156734
7
Iteration 17000: Loss = -12198.207220534227
8
Iteration 17100: Loss = -12198.42410611901
9
Iteration 17200: Loss = -12198.206714612907
10
Stopping early at iteration 17200 due to no improvement.
tensor([[ 3.1882, -4.6756],
        [ 4.6845, -6.2602],
        [ 4.7721, -6.1957],
        [ 3.3775, -5.1251],
        [ 3.5037, -4.8905],
        [ 4.1875, -5.9610],
        [ 2.0564, -4.2378],
        [ 3.6848, -5.4140],
        [ 0.1529, -2.8248],
        [ 4.5198, -6.3784],
        [ 1.0609, -5.1544],
        [ 3.5399, -5.0982],
        [ 2.7823, -5.0831],
        [ 1.1972, -2.8110],
        [ 2.5973, -4.8128],
        [ 2.8355, -4.2302],
        [ 1.1134, -2.5173],
        [ 2.6984, -4.2897],
        [ 4.5360, -5.9345],
        [ 6.3858, -8.0220],
        [ 1.6452, -3.1405],
        [-0.0128, -3.2156],
        [-0.1691, -4.0592],
        [ 4.6227, -6.1261],
        [ 2.6082, -6.0479],
        [ 1.5451, -3.5094],
        [ 2.4818, -3.8722],
        [ 1.6597, -3.0582],
        [ 0.9879, -3.3041],
        [-0.8474, -0.5757],
        [ 2.3133, -3.8823],
        [ 4.6300, -7.1500],
        [ 3.9426, -6.2546],
        [ 3.9776, -5.3672],
        [ 2.9864, -4.6594],
        [ 2.2275, -4.0405],
        [-0.1297, -1.4073],
        [ 1.4957, -2.8821],
        [ 4.7475, -6.2629],
        [ 2.3490, -3.9103],
        [ 2.4630, -4.6226],
        [ 1.2434, -2.6342],
        [ 4.3540, -5.7461],
        [ 0.6021, -2.5440],
        [ 3.5364, -5.2239],
        [-1.8179,  0.0257],
        [ 2.3871, -4.7453],
        [ 1.5536, -4.7619],
        [ 5.5347, -6.9425],
        [ 4.7486, -6.1402],
        [ 1.5580, -3.0537],
        [ 2.7762, -4.4854],
        [ 3.8821, -6.2604],
        [ 3.3314, -5.8737],
        [ 3.4343, -4.8224],
        [ 3.4907, -4.9872],
        [ 4.6152, -7.1220],
        [ 2.9677, -4.8881],
        [ 4.5588, -6.0271],
        [ 1.0535, -2.4914],
        [ 2.5115, -4.4515],
        [ 1.3957, -4.2292],
        [-0.5652, -1.9729],
        [ 3.9957, -5.4156],
        [ 0.9864, -3.4424],
        [ 3.1500, -4.6716],
        [ 5.4952, -6.9066],
        [ 2.4087, -3.9280],
        [ 4.3525, -5.7894],
        [ 4.5177, -5.9874],
        [ 2.1311, -6.4068],
        [ 2.0573, -3.4618],
        [ 1.5119, -4.4176],
        [ 2.5468, -4.4784],
        [-1.0637, -0.5515],
        [ 4.3009, -5.6967],
        [ 2.9226, -4.9435],
        [ 2.5984, -3.9858],
        [ 5.0826, -6.6340],
        [ 1.6394, -4.7211],
        [ 2.4167, -4.5743],
        [ 4.9954, -6.9376],
        [ 3.8146, -6.1930],
        [ 1.2580, -2.6626],
        [ 4.5734, -6.3885],
        [ 3.5900, -5.7282],
        [ 1.4779, -4.0698],
        [ 2.8232, -5.6464],
        [ 1.4865, -2.8858],
        [ 3.2726, -4.6820],
        [ 3.2905, -5.6097],
        [ 4.3438, -5.8074],
        [ 1.3477, -3.1025],
        [ 3.1915, -5.9045],
        [ 3.9422, -5.3285],
        [ 3.7397, -5.7293],
        [ 5.8696, -8.1633],
        [ 1.4698, -3.2998],
        [ 2.5648, -5.5527],
        [ 4.3013, -6.3141]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.6359e-01, 3.6409e-02],
        [9.9999e-01, 1.1194e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9712, 0.0288], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1952, 0.1008],
         [0.6704, 0.2176]],

        [[0.9769, 0.2518],
         [0.1090, 0.0983]],

        [[0.7459, 0.1954],
         [0.2093, 0.0448]],

        [[0.7807, 0.2013],
         [0.8770, 0.1036]],

        [[0.8690, 0.1683],
         [0.2993, 0.8689]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 39
Adjusted Rand Index: 0.02523388575915466
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -9.428816943229404e-05
Average Adjusted Rand Index: 0.005046777151830932
Iteration 0: Loss = -30677.27662123395
Iteration 10: Loss = -12200.054841393701
Iteration 20: Loss = -12200.054949916148
1
Iteration 30: Loss = -12200.054804150712
Iteration 40: Loss = -12200.053997835377
Iteration 50: Loss = -12200.052542973606
Iteration 60: Loss = -12200.048652746727
Iteration 70: Loss = -12200.038523105075
Iteration 80: Loss = -12200.012755723048
Iteration 90: Loss = -12199.951726423651
Iteration 100: Loss = -12199.824743840718
Iteration 110: Loss = -12199.603931855512
Iteration 120: Loss = -12199.295889445873
Iteration 130: Loss = -12198.974393815455
Iteration 140: Loss = -12198.739188034819
Iteration 150: Loss = -12198.614988068108
Iteration 160: Loss = -12198.563985480992
Iteration 170: Loss = -12198.547541321477
Iteration 180: Loss = -12198.544763405069
Iteration 190: Loss = -12198.546805482345
1
Iteration 200: Loss = -12198.550047384542
2
Iteration 210: Loss = -12198.553280419103
3
Stopping early at iteration 209 due to no improvement.
pi: tensor([[6.5507e-37, 1.0000e+00],
        [3.5645e-02, 9.6435e-01]], dtype=torch.float64)
alpha: tensor([0.0336, 0.9664])
beta: tensor([[[0.2014, 0.1043],
         [0.9048, 0.1934]],

        [[0.7701, 0.2513],
         [0.2544, 0.9661]],

        [[0.5095, 0.1890],
         [0.1856, 0.9777]],

        [[0.0223, 0.1974],
         [0.2557, 0.2427]],

        [[0.8338, 0.1658],
         [0.8377, 0.2167]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 61
Adjusted Rand Index: 0.02523388575915466
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -9.428816943229404e-05
Average Adjusted Rand Index: 0.005046777151830932
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -30677.192640072833
Iteration 100: Loss = -12259.720474077138
Iteration 200: Loss = -12232.36427876226
Iteration 300: Loss = -12216.819844046098
Iteration 400: Loss = -12204.395702162552
Iteration 500: Loss = -12202.268227152537
Iteration 600: Loss = -12201.547425320774
Iteration 700: Loss = -12201.126576829412
Iteration 800: Loss = -12200.848906211624
Iteration 900: Loss = -12200.65229771699
Iteration 1000: Loss = -12200.506411919372
Iteration 1100: Loss = -12200.39417163052
Iteration 1200: Loss = -12200.30533981837
Iteration 1300: Loss = -12200.233467251466
Iteration 1400: Loss = -12200.174295967532
Iteration 1500: Loss = -12200.1249780605
Iteration 1600: Loss = -12200.083203898788
Iteration 1700: Loss = -12200.04741005396
Iteration 1800: Loss = -12200.016541063396
Iteration 1900: Loss = -12199.989598315966
Iteration 2000: Loss = -12199.96595732711
Iteration 2100: Loss = -12199.945112387615
Iteration 2200: Loss = -12199.92654710229
Iteration 2300: Loss = -12199.909891409281
Iteration 2400: Loss = -12199.8946511401
Iteration 2500: Loss = -12199.879609757782
Iteration 2600: Loss = -12199.865880642712
Iteration 2700: Loss = -12199.85367935438
Iteration 2800: Loss = -12199.84265014058
Iteration 2900: Loss = -12199.832678038605
Iteration 3000: Loss = -12199.823285337954
Iteration 3100: Loss = -12199.814361992341
Iteration 3200: Loss = -12199.805993540194
Iteration 3300: Loss = -12199.799036524573
Iteration 3400: Loss = -12199.793319417313
Iteration 3500: Loss = -12199.788333548144
Iteration 3600: Loss = -12199.783953609849
Iteration 3700: Loss = -12199.780139311219
Iteration 3800: Loss = -12199.77693252653
Iteration 3900: Loss = -12199.77439378436
Iteration 4000: Loss = -12199.772321400997
Iteration 4100: Loss = -12199.770605622754
Iteration 4200: Loss = -12199.769000452594
Iteration 4300: Loss = -12199.767419742266
Iteration 4400: Loss = -12199.765785770303
Iteration 4500: Loss = -12199.764191396796
Iteration 4600: Loss = -12199.76252786352
Iteration 4700: Loss = -12199.76080301719
Iteration 4800: Loss = -12199.759042774998
Iteration 4900: Loss = -12199.75720844366
Iteration 5000: Loss = -12199.755217956912
Iteration 5100: Loss = -12199.753148150343
Iteration 5200: Loss = -12199.750841603927
Iteration 5300: Loss = -12199.74822015091
Iteration 5400: Loss = -12199.74524949838
Iteration 5500: Loss = -12199.74197181591
Iteration 5600: Loss = -12199.738410215654
Iteration 5700: Loss = -12199.734265870999
Iteration 5800: Loss = -12199.728830912785
Iteration 5900: Loss = -12199.718357774744
Iteration 6000: Loss = -12198.943861895557
Iteration 6100: Loss = -12198.692786920457
Iteration 6200: Loss = -12198.677873249977
Iteration 6300: Loss = -12198.667995478712
Iteration 6400: Loss = -12198.659758220047
Iteration 6500: Loss = -12198.651277303281
Iteration 6600: Loss = -12198.641646296248
Iteration 6700: Loss = -12198.630144510715
Iteration 6800: Loss = -12198.616316938032
Iteration 6900: Loss = -12198.600089446014
Iteration 7000: Loss = -12198.58159815302
Iteration 7100: Loss = -12198.561241460016
Iteration 7200: Loss = -12198.541941707046
Iteration 7300: Loss = -12198.524416971206
Iteration 7400: Loss = -12198.61908979383
1
Iteration 7500: Loss = -12198.482699139427
Iteration 7600: Loss = -12198.461965455019
Iteration 7700: Loss = -12198.441775889196
Iteration 7800: Loss = -12198.401853004096
Iteration 7900: Loss = -12198.369216304123
Iteration 8000: Loss = -12198.352459288617
Iteration 8100: Loss = -12198.488391750008
1
Iteration 8200: Loss = -12198.276265997976
Iteration 8300: Loss = -12198.229648095135
Iteration 8400: Loss = -12198.207444525959
Iteration 8500: Loss = -12198.207681379941
1
Iteration 8600: Loss = -12198.205917523934
Iteration 8700: Loss = -12198.20659746938
1
Iteration 8800: Loss = -12198.205358232346
Iteration 8900: Loss = -12198.20530421944
Iteration 9000: Loss = -12198.205038182752
Iteration 9100: Loss = -12198.205384138393
1
Iteration 9200: Loss = -12198.204731894244
Iteration 9300: Loss = -12198.204546917379
Iteration 9400: Loss = -12198.205547293817
1
Iteration 9500: Loss = -12198.204231454149
Iteration 9600: Loss = -12198.452179879969
1
Iteration 9700: Loss = -12198.203969616698
Iteration 9800: Loss = -12198.20860893287
1
Iteration 9900: Loss = -12198.20378765052
Iteration 10000: Loss = -12198.203709321573
Iteration 10100: Loss = -12198.20362624028
Iteration 10200: Loss = -12198.203592390078
Iteration 10300: Loss = -12198.203581243473
Iteration 10400: Loss = -12198.20359702718
1
Iteration 10500: Loss = -12198.203466202818
Iteration 10600: Loss = -12198.203389022718
Iteration 10700: Loss = -12198.20332363319
Iteration 10800: Loss = -12198.213966688514
1
Iteration 10900: Loss = -12198.203284645258
Iteration 11000: Loss = -12198.20322197055
Iteration 11100: Loss = -12198.20339311167
1
Iteration 11200: Loss = -12198.204030690855
2
Iteration 11300: Loss = -12198.20315643129
Iteration 11400: Loss = -12198.203092407628
Iteration 11500: Loss = -12198.204894490304
1
Iteration 11600: Loss = -12198.203001375969
Iteration 11700: Loss = -12198.20340273705
1
Iteration 11800: Loss = -12198.208707162028
2
Iteration 11900: Loss = -12198.203417076416
3
Iteration 12000: Loss = -12198.313182388496
4
Iteration 12100: Loss = -12198.203290773763
5
Iteration 12200: Loss = -12198.20504769776
6
Iteration 12300: Loss = -12198.202877214017
Iteration 12400: Loss = -12198.275291518643
1
Iteration 12500: Loss = -12198.269121458654
2
Iteration 12600: Loss = -12198.202870927096
Iteration 12700: Loss = -12198.20297402464
1
Iteration 12800: Loss = -12198.206163293446
2
Iteration 12900: Loss = -12198.208780182578
3
Iteration 13000: Loss = -12198.202991371025
4
Iteration 13100: Loss = -12198.20898861997
5
Iteration 13200: Loss = -12198.203012731678
6
Iteration 13300: Loss = -12198.249983578307
7
Iteration 13400: Loss = -12198.202878705893
8
Iteration 13500: Loss = -12198.20474119755
9
Iteration 13600: Loss = -12198.202667843074
Iteration 13700: Loss = -12198.202895264776
1
Iteration 13800: Loss = -12198.202650201396
Iteration 13900: Loss = -12198.202710751659
1
Iteration 14000: Loss = -12198.202628645779
Iteration 14100: Loss = -12198.203268238458
1
Iteration 14200: Loss = -12198.20259281532
Iteration 14300: Loss = -12198.202629990774
1
Iteration 14400: Loss = -12198.20410529108
2
Iteration 14500: Loss = -12198.204256400706
3
Iteration 14600: Loss = -12198.203155057485
4
Iteration 14700: Loss = -12198.202590081972
Iteration 14800: Loss = -12198.20254266867
Iteration 14900: Loss = -12198.203097017833
1
Iteration 15000: Loss = -12198.202541955618
Iteration 15100: Loss = -12198.407258980451
1
Iteration 15200: Loss = -12198.207175431618
2
Iteration 15300: Loss = -12198.202536339213
Iteration 15400: Loss = -12198.20292884615
1
Iteration 15500: Loss = -12198.202757217854
2
Iteration 15600: Loss = -12198.210971578548
3
Iteration 15700: Loss = -12198.202582293674
4
Iteration 15800: Loss = -12198.437995999575
5
Iteration 15900: Loss = -12198.203211988672
6
Iteration 16000: Loss = -12198.202485022985
Iteration 16100: Loss = -12198.202797222386
1
Iteration 16200: Loss = -12198.202490734298
2
Iteration 16300: Loss = -12198.203064435285
3
Iteration 16400: Loss = -12198.20307408695
4
Iteration 16500: Loss = -12198.202530268365
5
Iteration 16600: Loss = -12198.231268278436
6
Iteration 16700: Loss = -12198.203464467628
7
Iteration 16800: Loss = -12198.205813858027
8
Iteration 16900: Loss = -12198.206394911189
9
Iteration 17000: Loss = -12198.202533551153
10
Stopping early at iteration 17000 due to no improvement.
tensor([[-4.6410,  3.2267],
        [-6.1918,  4.7559],
        [-6.5738,  4.3897],
        [-5.6293,  2.8759],
        [-4.9913,  3.4079],
        [-6.2141,  3.9354],
        [-4.2950,  2.0005],
        [-5.9014,  3.2005],
        [-2.1914,  0.7879],
        [-6.7305,  4.1722],
        [-3.8026,  2.4149],
        [-5.3680,  3.2736],
        [-4.9225,  2.9448],
        [-2.6987,  1.3112],
        [-5.6327,  1.7804],
        [-4.2809,  2.7863],
        [-2.5499,  1.0827],
        [-4.2215,  2.7691],
        [-6.1535,  4.3267],
        [-7.9793,  6.4604],
        [-3.9238,  0.8633],
        [-2.2979,  0.9068],
        [-3.0046,  0.8866],
        [-6.1304,  4.6244],
        [-5.0234,  3.6368],
        [-3.4997,  1.5568],
        [-4.2246,  2.1309],
        [-3.3028,  1.4162],
        [-4.3816, -0.0866],
        [-1.0699, -1.3394],
        [-3.8017,  2.3964],
        [-6.6570,  5.1394],
        [-5.7952,  4.4086],
        [-5.3676,  3.9806],
        [-4.5235,  3.1237],
        [-4.1809,  2.0896],
        [-1.3538, -0.0750],
        [-2.8977,  1.4814],
        [-6.3327,  4.6811],
        [-3.9233,  2.3375],
        [-4.2418,  2.8463],
        [-2.6933,  1.1883],
        [-5.7567,  4.3466],
        [-2.2671,  0.8804],
        [-5.3194,  3.4462],
        [-0.7977, -2.6400],
        [-4.4039,  2.7300],
        [-4.0916,  2.2253],
        [-7.0650,  5.4444],
        [-6.2560,  4.6398],
        [-3.5058,  1.1096],
        [-5.9411,  1.3259],
        [-5.8140,  4.3330],
        [-5.9881,  3.2203],
        [-4.8549,  3.4050],
        [-5.6135,  2.8672],
        [-6.5611,  5.1729],
        [-9.1112,  7.1142],
        [-7.1910,  3.3984],
        [-3.7497, -0.2034],
        [-5.7897,  1.1745],
        [-3.5652,  2.0626],
        [-1.5276, -0.1178],
        [-5.6557,  3.7587],
        [-3.1015,  1.3302],
        [-4.8970,  2.9278],
        [-6.9153,  5.4843],
        [-3.9367,  2.4012],
        [-6.3880,  3.7640],
        [-6.6042,  3.9052],
        [-5.1823,  3.3589],
        [-3.5761,  1.9454],
        [-4.2310,  1.7019],
        [-4.3088,  2.7178],
        [-1.5701, -2.0817],
        [-5.9618,  4.0439],
        [-5.0278,  2.8414],
        [-4.2441,  2.3418],
        [-6.6452,  5.0745],
        [-3.9087,  2.4535],
        [-4.5420,  2.4511],
        [-6.9177,  5.0186],
        [-5.7018,  4.3120],
        [-2.6545,  1.2672],
        [-6.1990,  4.7732],
        [-5.4745,  3.8486],
        [-3.5323,  2.0170],
        [-5.1545,  3.3193],
        [-2.9859,  1.3880],
        [-4.6959,  3.2632],
        [-5.1606,  3.7430],
        [-5.7861,  4.3702],
        [-3.3683,  1.0838],
        [-5.2461,  3.8523],
        [-6.3564,  2.9154],
        [-5.4664,  4.0059],
        [-7.8582,  6.1167],
        [-3.1185,  1.6519],
        [-5.1103,  3.0118],
        [-6.0039,  4.6176]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.4898e-05, 9.9999e-01],
        [3.7333e-02, 9.6267e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0288, 0.9712], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2174, 0.1008],
         [0.9048, 0.1952]],

        [[0.7701, 0.2503],
         [0.2544, 0.9661]],

        [[0.5095, 0.1954],
         [0.1856, 0.9777]],

        [[0.0223, 0.2012],
         [0.2557, 0.2427]],

        [[0.8338, 0.1687],
         [0.8377, 0.2167]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 61
Adjusted Rand Index: 0.02523388575915466
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -9.428816943229404e-05
Average Adjusted Rand Index: 0.005046777151830932
Iteration 0: Loss = -19055.23526228868
Iteration 10: Loss = -12199.954980647064
Iteration 20: Loss = -12199.767549295559
Iteration 30: Loss = -12199.736996418624
Iteration 40: Loss = -12199.728336115691
Iteration 50: Loss = -12199.725246601194
Iteration 60: Loss = -12199.72405111422
Iteration 70: Loss = -12199.723573294943
Iteration 80: Loss = -12199.72341284578
Iteration 90: Loss = -12199.723410662384
Iteration 100: Loss = -12199.723453059123
1
Iteration 110: Loss = -12199.723554931486
2
Iteration 120: Loss = -12199.723627825404
3
Stopping early at iteration 119 due to no improvement.
pi: tensor([[0.5078, 0.4922],
        [0.5075, 0.4925]], dtype=torch.float64)
alpha: tensor([0.5074, 0.4926])
beta: tensor([[[0.2016, 0.1909],
         [0.3994, 0.1839]],

        [[0.6165, 0.1968],
         [0.5822, 0.5180]],

        [[0.3775, 0.1920],
         [0.6113, 0.6730]],

        [[0.5648, 0.1940],
         [0.8050, 0.6131]],

        [[0.5579, 0.1885],
         [0.6094, 0.9491]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 36
Adjusted Rand Index: 0.06902570954464152
time is 1
tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 30
Adjusted Rand Index: 0.15148897156055505
time is 2
tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 71
Adjusted Rand Index: 0.16814925052998395
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 65
Adjusted Rand Index: 0.08048023978674736
time is 4
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.008571288632022334
Global Adjusted Rand Index: -0.0017363106047358565
Average Adjusted Rand Index: 0.09211457655798111
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19054.88720998247
Iteration 100: Loss = -12248.12471253066
Iteration 200: Loss = -12204.480068528357
Iteration 300: Loss = -12201.820401129968
Iteration 400: Loss = -12200.88250698544
Iteration 500: Loss = -12200.40233311877
Iteration 600: Loss = -12200.116960184483
Iteration 700: Loss = -12199.932679372176
Iteration 800: Loss = -12199.806245437638
Iteration 900: Loss = -12199.715081893099
Iteration 1000: Loss = -12199.646983494402
Iteration 1100: Loss = -12199.594485185908
Iteration 1200: Loss = -12199.55301733839
Iteration 1300: Loss = -12199.519622777778
Iteration 1400: Loss = -12199.492330060728
Iteration 1500: Loss = -12199.469749984906
Iteration 1600: Loss = -12199.450873735403
Iteration 1700: Loss = -12199.434925308518
Iteration 1800: Loss = -12199.421257846252
Iteration 1900: Loss = -12199.409325289476
Iteration 2000: Loss = -12199.39864215578
Iteration 2100: Loss = -12199.388230340448
Iteration 2200: Loss = -12199.37304825685
Iteration 2300: Loss = -12198.424730566134
Iteration 2400: Loss = -12198.334631268648
Iteration 2500: Loss = -12198.304981145035
Iteration 2600: Loss = -12198.283337897105
Iteration 2700: Loss = -12198.268751083631
Iteration 2800: Loss = -12198.257439188237
Iteration 2900: Loss = -12198.247329451257
Iteration 3000: Loss = -12198.236220194754
Iteration 3100: Loss = -12198.228687526962
Iteration 3200: Loss = -12198.223089711486
Iteration 3300: Loss = -12198.218419325158
Iteration 3400: Loss = -12198.214450048768
Iteration 3500: Loss = -12198.2109866568
Iteration 3600: Loss = -12198.207905250472
Iteration 3700: Loss = -12198.20517641352
Iteration 3800: Loss = -12198.20279386514
Iteration 3900: Loss = -12198.200542579949
Iteration 4000: Loss = -12198.198578998697
Iteration 4100: Loss = -12198.19674274121
Iteration 4200: Loss = -12198.195165490297
Iteration 4300: Loss = -12198.193645090927
Iteration 4400: Loss = -12198.192254624686
Iteration 4500: Loss = -12198.191010256243
Iteration 4600: Loss = -12198.189895525325
Iteration 4700: Loss = -12198.188797594083
Iteration 4800: Loss = -12198.18784259944
Iteration 4900: Loss = -12198.18690135017
Iteration 5000: Loss = -12198.186086535992
Iteration 5100: Loss = -12198.185369387204
Iteration 5200: Loss = -12198.184558624282
Iteration 5300: Loss = -12198.185406791643
1
Iteration 5400: Loss = -12198.183201973923
Iteration 5500: Loss = -12198.205893977753
1
Iteration 5600: Loss = -12198.208756311653
2
Iteration 5700: Loss = -12198.256744782631
3
Iteration 5800: Loss = -12198.120663585069
Iteration 5900: Loss = -12198.127188208313
1
Iteration 6000: Loss = -12198.120224208202
Iteration 6100: Loss = -12198.120323550598
1
Iteration 6200: Loss = -12198.125008104262
2
Iteration 6300: Loss = -12198.117880367457
Iteration 6400: Loss = -12198.117615796578
Iteration 6500: Loss = -12198.118332086251
1
Iteration 6600: Loss = -12198.120826363844
2
Iteration 6700: Loss = -12198.116679847724
Iteration 6800: Loss = -12198.11639739532
Iteration 6900: Loss = -12198.116130549139
Iteration 7000: Loss = -12198.116056735498
Iteration 7100: Loss = -12198.115697263791
Iteration 7200: Loss = -12198.11554114304
Iteration 7300: Loss = -12198.155567091046
1
Iteration 7400: Loss = -12198.115139248292
Iteration 7500: Loss = -12198.114971877389
Iteration 7600: Loss = -12198.126230264674
1
Iteration 7700: Loss = -12198.114681055305
Iteration 7800: Loss = -12198.122060376289
1
Iteration 7900: Loss = -12198.11972171102
2
Iteration 8000: Loss = -12198.116523357581
3
Iteration 8100: Loss = -12198.114164638255
Iteration 8200: Loss = -12198.114589078861
1
Iteration 8300: Loss = -12198.113971041399
Iteration 8400: Loss = -12198.114075591659
1
Iteration 8500: Loss = -12198.113821033896
Iteration 8600: Loss = -12198.113643802102
Iteration 8700: Loss = -12198.113576080283
Iteration 8800: Loss = -12198.11477130836
1
Iteration 8900: Loss = -12198.113393948086
Iteration 9000: Loss = -12198.11332844893
Iteration 9100: Loss = -12198.126257237958
1
Iteration 9200: Loss = -12198.113197294888
Iteration 9300: Loss = -12198.113091090563
Iteration 9400: Loss = -12198.408341853714
1
Iteration 9500: Loss = -12198.113046604152
Iteration 9600: Loss = -12198.112965955253
Iteration 9700: Loss = -12198.112877722715
Iteration 9800: Loss = -12198.13353334065
1
Iteration 9900: Loss = -12198.11278579193
Iteration 10000: Loss = -12198.113079853325
1
Iteration 10100: Loss = -12198.114586575624
2
Iteration 10200: Loss = -12198.112658309044
Iteration 10300: Loss = -12198.113530678127
1
Iteration 10400: Loss = -12198.11256145212
Iteration 10500: Loss = -12198.14053316319
1
Iteration 10600: Loss = -12198.112512370079
Iteration 10700: Loss = -12198.113006577889
1
Iteration 10800: Loss = -12198.111920985228
Iteration 10900: Loss = -12198.113929863504
1
Iteration 11000: Loss = -12198.111943360142
2
Iteration 11100: Loss = -12198.122417174827
3
Iteration 11200: Loss = -12198.11195279826
4
Iteration 11300: Loss = -12198.11179527517
Iteration 11400: Loss = -12198.112046977945
1
Iteration 11500: Loss = -12198.111730656035
Iteration 11600: Loss = -12198.11460717731
1
Iteration 11700: Loss = -12198.111704930108
Iteration 11800: Loss = -12198.247201925878
1
Iteration 11900: Loss = -12198.111668232792
Iteration 12000: Loss = -12198.111621411124
Iteration 12100: Loss = -12198.111687103168
1
Iteration 12200: Loss = -12198.111581690253
Iteration 12300: Loss = -12198.135133295347
1
Iteration 12400: Loss = -12198.111542624934
Iteration 12500: Loss = -12198.113562887556
1
Iteration 12600: Loss = -12198.11181383355
2
Iteration 12700: Loss = -12198.111818063071
3
Iteration 12800: Loss = -12198.112674881262
4
Iteration 12900: Loss = -12198.112836668066
5
Iteration 13000: Loss = -12198.124361700478
6
Iteration 13100: Loss = -12198.113502334163
7
Iteration 13200: Loss = -12198.111472333841
Iteration 13300: Loss = -12198.112558665876
1
Iteration 13400: Loss = -12198.111435600966
Iteration 13500: Loss = -12198.146191652353
1
Iteration 13600: Loss = -12198.112102162142
2
Iteration 13700: Loss = -12198.111407747185
Iteration 13800: Loss = -12198.114317841098
1
Iteration 13900: Loss = -12198.111109085261
Iteration 14000: Loss = -12198.111541452296
1
Iteration 14100: Loss = -12198.12346333054
2
Iteration 14200: Loss = -12198.114905394683
3
Iteration 14300: Loss = -12198.11167406604
4
Iteration 14400: Loss = -12198.129886198338
5
Iteration 14500: Loss = -12198.112245389471
6
Iteration 14600: Loss = -12198.11108525787
Iteration 14700: Loss = -12198.115581770131
1
Iteration 14800: Loss = -12198.187074883192
2
Iteration 14900: Loss = -12198.133821584226
3
Iteration 15000: Loss = -12198.103930372765
Iteration 15100: Loss = -12198.114167974882
1
Iteration 15200: Loss = -12198.103927295197
Iteration 15300: Loss = -12198.108384314384
1
Iteration 15400: Loss = -12198.103994542818
2
Iteration 15500: Loss = -12198.10691993553
3
Iteration 15600: Loss = -12198.140999637262
4
Iteration 15700: Loss = -12198.104072789141
5
Iteration 15800: Loss = -12198.148298702043
6
Iteration 15900: Loss = -12198.10445551212
7
Iteration 16000: Loss = -12198.104178148049
8
Iteration 16100: Loss = -12198.103974836715
9
Iteration 16200: Loss = -12198.130855421174
10
Stopping early at iteration 16200 due to no improvement.
tensor([[  8.0627,  -9.5333],
        [  8.3318,  -9.7792],
        [  8.0821, -10.0402],
        [  3.1022,  -5.5123],
        [  1.6273,  -3.0452],
        [  7.7273,  -9.7137],
        [  8.0163,  -9.4463],
        [  8.3672,  -9.9670],
        [  8.0017, -11.1675],
        [  8.0297,  -9.5870],
        [  7.8256, -10.5895],
        [  8.2393,  -9.6831],
        [  8.3004,  -9.7604],
        [  8.7428, -10.6621],
        [  3.9551,  -6.2836],
        [  8.0612, -10.4339],
        [  8.1933, -10.4174],
        [  8.3884,  -9.8839],
        [  3.5820,  -4.9728],
        [  7.5098,  -8.9664],
        [  8.5989, -10.0906],
        [  8.0820, -10.1655],
        [  2.1914,  -4.1083],
        [  1.0800,  -3.0626],
        [  7.9832,  -9.5450],
        [  4.0051,  -6.6076],
        [  8.6905, -10.1688],
        [  8.0211,  -9.8887],
        [  8.7333, -10.4196],
        [  7.6327,  -9.8275],
        [  8.1993,  -9.6123],
        [  7.6625,  -9.3860],
        [  7.9729,  -9.3592],
        [  8.2626, -10.0973],
        [  8.0734,  -9.9052],
        [  8.0453,  -9.4362],
        [  8.3337, -10.3328],
        [  8.3903, -10.2622],
        [  0.3920,  -1.9742],
        [  8.4206,  -9.8088],
        [  4.8191,  -6.3808],
        [  7.3067, -11.3905],
        [  1.6890,  -3.1113],
        [  8.1460,  -9.7653],
        [  5.4924,  -6.8828],
        [  5.4415,  -7.0701],
        [  4.4434,  -7.8216],
        [  7.5991, -11.1766],
        [  7.8794,  -9.9980],
        [  7.7354,  -9.1327],
        [  7.7772, -10.9450],
        [  8.7822, -10.2035],
        [  7.9661,  -9.3611],
        [  3.3929,  -4.7800],
        [  8.3416,  -9.7320],
        [  8.3178,  -9.9706],
        [  7.9504,  -9.3540],
        [  8.2165,  -9.6692],
        [  1.7400,  -3.1567],
        [  4.5977,  -6.1477],
        [  8.5611, -10.6637],
        [  6.0741,  -8.1029],
        [  8.8406, -10.3567],
        [  1.7208,  -3.1762],
        [  8.3384, -10.4234],
        [  8.1028, -10.0922],
        [  2.4077,  -5.0334],
        [  7.9465, -10.8025],
        [  8.2585,  -9.6791],
        [  4.8084,  -6.8509],
        [  6.9362, -11.5514],
        [  8.0165, -10.5193],
        [  8.0628, -10.1108],
        [  7.8112, -10.1873],
        [  8.3846,  -9.9625],
        [  8.0720,  -9.6320],
        [  8.4707,  -9.9569],
        [  8.1938, -10.0132],
        [  2.1871,  -4.1190],
        [  8.2635,  -9.7012],
        [  8.6000, -10.1178],
        [  2.4798,  -4.6623],
        [  1.2971,  -4.7517],
        [  8.4959,  -9.9227],
        [  7.1333, -10.5451],
        [  2.0912,  -5.9405],
        [  3.3867,  -6.1444],
        [  4.6243,  -6.1325],
        [  4.8199,  -6.2062],
        [  2.7799,  -4.1791],
        [  1.9837,  -4.3984],
        [  4.6191,  -6.0062],
        [  8.4001,  -9.8042],
        [  7.8730,  -9.2870],
        [  8.3475,  -9.7985],
        [  8.3606,  -9.7650],
        [ -2.4049,  -0.2707],
        [  8.3842,  -9.8078],
        [  7.9895, -11.2275],
        [  8.4076,  -9.7944]], dtype=torch.float64, requires_grad=True)
pi: tensor([[6.5075e-08, 1.0000e+00],
        [1.0000e+00, 7.6437e-08]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9896, 0.0104], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1912, 0.2690],
         [0.3994, 0.1966]],

        [[0.6165, 0.1780],
         [0.5822, 0.5180]],

        [[0.3775, 0.2578],
         [0.6113, 0.6730]],

        [[0.5648, 0.2534],
         [0.8050, 0.6131]],

        [[0.5579, 0.2696],
         [0.6094, 0.9491]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
Difference count: 43
Adjusted Rand Index: -0.005513710431743218
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0])
Difference count: 41
Adjusted Rand Index: 0.007614171548597778
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
Global Adjusted Rand Index: -0.0015323137193970294
Average Adjusted Rand Index: 0.0008566122936134793
11740.28893810901
new:  [-9.428816943229404e-05, -9.428816943229404e-05, -9.428816943229404e-05, -0.0015323137193970294] [0.005046777151830932, 0.005046777151830932, 0.005046777151830932, 0.0008566122936134793] [12198.202200832724, 12198.206714612907, 12198.202533551153, 12198.130855421174]
prior:  [-9.428816943229404e-05, 0.0, -9.428816943229404e-05, -0.0017363106047358565] [0.005046777151830932, 0.0, 0.005046777151830932, 0.09211457655798111] [12198.552398774733, 12200.054901651878, 12198.553280419103, 12199.723627825404]
-----------------------------------------------------------------------------------------
This iteration is 8
True Objective function: Loss = -11865.947284900833
Iteration 0: Loss = -30985.304365500597
Iteration 10: Loss = -12364.4468961087
Iteration 20: Loss = -12364.446896108775
1
Iteration 30: Loss = -12364.446896110287
2
Iteration 40: Loss = -12364.446896139203
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[1.0000e+00, 9.9488e-13],
        [1.0000e+00, 1.0914e-20]], dtype=torch.float64)
alpha: tensor([1.0000e+00, 1.1354e-12])
beta: tensor([[[0.1974, 0.3074],
         [0.9508, 0.2328]],

        [[0.0117, 0.2135],
         [0.7693, 0.9393]],

        [[0.6283, 0.2879],
         [0.4500, 0.9168]],

        [[0.1162, 0.1091],
         [0.1341, 0.7246]],

        [[0.1410, 0.2256],
         [0.8907, 0.1892]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -30806.340315494283
Iteration 100: Loss = -12369.552255290233
Iteration 200: Loss = -12366.191629152927
Iteration 300: Loss = -12365.210712087228
Iteration 400: Loss = -12364.819460994231
Iteration 500: Loss = -12364.612329253912
Iteration 600: Loss = -12364.474774245158
Iteration 700: Loss = -12364.36627698122
Iteration 800: Loss = -12364.268292736635
Iteration 900: Loss = -12364.18576635009
Iteration 1000: Loss = -12364.132997626517
Iteration 1100: Loss = -12364.104515974217
Iteration 1200: Loss = -12364.081364647674
Iteration 1300: Loss = -12364.047851122286
Iteration 1400: Loss = -12363.979261304748
Iteration 1500: Loss = -12363.737843815483
Iteration 1600: Loss = -12362.6855447723
Iteration 1700: Loss = -12362.067091168869
Iteration 1800: Loss = -12361.879926901851
Iteration 1900: Loss = -12361.371245601838
Iteration 2000: Loss = -12360.747650778769
Iteration 2100: Loss = -12360.64212042149
Iteration 2200: Loss = -12360.576843806599
Iteration 2300: Loss = -12360.526649190213
Iteration 2400: Loss = -12360.485281693449
Iteration 2500: Loss = -12360.449294056036
Iteration 2600: Loss = -12360.416947398517
Iteration 2700: Loss = -12360.387280776697
Iteration 2800: Loss = -12360.359798466374
Iteration 2900: Loss = -12360.334076078123
Iteration 3000: Loss = -12360.309996485485
Iteration 3100: Loss = -12360.28748743418
Iteration 3200: Loss = -12360.266395178503
Iteration 3300: Loss = -12360.24659949192
Iteration 3400: Loss = -12360.227982917866
Iteration 3500: Loss = -12360.210450776885
Iteration 3600: Loss = -12360.193861127327
Iteration 3700: Loss = -12360.177625461674
Iteration 3800: Loss = -12360.162106173562
Iteration 3900: Loss = -12360.146490859166
Iteration 4000: Loss = -12360.13035145203
Iteration 4100: Loss = -12360.113412153374
Iteration 4200: Loss = -12360.095380018069
Iteration 4300: Loss = -12360.07395202185
Iteration 4400: Loss = -12360.05409047089
Iteration 4500: Loss = -12360.018722155728
Iteration 4600: Loss = -12359.981648667099
Iteration 4700: Loss = -12359.941746121029
Iteration 4800: Loss = -12359.888582991201
Iteration 4900: Loss = -12359.836936610865
Iteration 5000: Loss = -12359.838969038272
1
Iteration 5100: Loss = -12359.737253736024
Iteration 5200: Loss = -12359.692529005024
Iteration 5300: Loss = -12359.651991318624
Iteration 5400: Loss = -12359.61630920047
Iteration 5500: Loss = -12359.583152515976
Iteration 5600: Loss = -12359.554248384868
Iteration 5700: Loss = -12359.529090767168
Iteration 5800: Loss = -12359.505424950059
Iteration 5900: Loss = -12359.484722149196
Iteration 6000: Loss = -12359.466209752203
Iteration 6100: Loss = -12359.46560960562
Iteration 6200: Loss = -12359.434458353056
Iteration 6300: Loss = -12359.420852753738
Iteration 6400: Loss = -12359.408494234727
Iteration 6500: Loss = -12359.397598423937
Iteration 6600: Loss = -12359.387025313345
Iteration 6700: Loss = -12359.377603556162
Iteration 6800: Loss = -12359.38615644861
1
Iteration 6900: Loss = -12359.361141272195
Iteration 7000: Loss = -12359.353842465856
Iteration 7100: Loss = -12359.347165314477
Iteration 7200: Loss = -12359.359551281284
1
Iteration 7300: Loss = -12359.335284205863
Iteration 7400: Loss = -12359.329967198782
Iteration 7500: Loss = -12359.325072285314
Iteration 7600: Loss = -12359.321677094771
Iteration 7700: Loss = -12359.316266659229
Iteration 7800: Loss = -12359.312317519465
Iteration 7900: Loss = -12359.30863685194
Iteration 8000: Loss = -12359.306508947617
Iteration 8100: Loss = -12359.301991883829
Iteration 8200: Loss = -12359.29895050984
Iteration 8300: Loss = -12359.296170973046
Iteration 8400: Loss = -12359.293524961122
Iteration 8500: Loss = -12359.291073460385
Iteration 8600: Loss = -12359.288739204427
Iteration 8700: Loss = -12359.287097762452
Iteration 8800: Loss = -12359.284530959661
Iteration 8900: Loss = -12359.282592003621
Iteration 9000: Loss = -12359.90050265804
1
Iteration 9100: Loss = -12359.279076475872
Iteration 9200: Loss = -12359.277506659671
Iteration 9300: Loss = -12359.275972773254
Iteration 9400: Loss = -12359.277722738216
1
Iteration 9500: Loss = -12359.273226712103
Iteration 9600: Loss = -12359.27197527667
Iteration 9700: Loss = -12359.270749529062
Iteration 9800: Loss = -12359.269762941261
Iteration 9900: Loss = -12359.26858875734
Iteration 10000: Loss = -12359.267548622205
Iteration 10100: Loss = -12359.374652672022
1
Iteration 10200: Loss = -12359.26566857367
Iteration 10300: Loss = -12359.264825349372
Iteration 10400: Loss = -12359.264039877151
Iteration 10500: Loss = -12359.272094957287
1
Iteration 10600: Loss = -12359.26257538123
Iteration 10700: Loss = -12359.261896222633
Iteration 10800: Loss = -12359.261223285077
Iteration 10900: Loss = -12359.260635548946
Iteration 11000: Loss = -12359.2600392593
Iteration 11100: Loss = -12359.259475356204
Iteration 11200: Loss = -12359.289450543614
1
Iteration 11300: Loss = -12359.258443058017
Iteration 11400: Loss = -12359.257969558857
Iteration 11500: Loss = -12359.257512861699
Iteration 11600: Loss = -12359.257040485374
Iteration 11700: Loss = -12359.256638609966
Iteration 11800: Loss = -12359.256153496086
Iteration 11900: Loss = -12359.255806316796
Iteration 12000: Loss = -12359.255454641101
Iteration 12100: Loss = -12359.255841306547
1
Iteration 12200: Loss = -12359.2547519276
Iteration 12300: Loss = -12359.25444071459
Iteration 12400: Loss = -12359.254192597171
Iteration 12500: Loss = -12359.255323313315
1
Iteration 12600: Loss = -12359.253635618328
Iteration 12700: Loss = -12359.253402048465
Iteration 12800: Loss = -12359.255937726706
1
Iteration 12900: Loss = -12359.25301883307
Iteration 13000: Loss = -12359.252748392171
Iteration 13100: Loss = -12359.252491973448
Iteration 13200: Loss = -12359.456551534666
1
Iteration 13300: Loss = -12359.251219962043
Iteration 13400: Loss = -12359.250987587468
Iteration 13500: Loss = -12359.250809851132
Iteration 13600: Loss = -12359.254758562332
1
Iteration 13700: Loss = -12359.250536233081
Iteration 13800: Loss = -12359.250406509196
Iteration 13900: Loss = -12359.250530799385
1
Iteration 14000: Loss = -12359.250197610627
Iteration 14100: Loss = -12359.254486482276
1
Iteration 14200: Loss = -12359.24997693865
Iteration 14300: Loss = -12359.27670226593
1
Iteration 14400: Loss = -12359.250844251757
2
Iteration 14500: Loss = -12359.385819856889
3
Iteration 14600: Loss = -12359.249725672258
Iteration 14700: Loss = -12359.251368647354
1
Iteration 14800: Loss = -12359.249467691618
Iteration 14900: Loss = -12359.249346292858
Iteration 15000: Loss = -12359.249365722497
1
Iteration 15100: Loss = -12359.249313987257
Iteration 15200: Loss = -12359.256005065754
1
Iteration 15300: Loss = -12359.249052197261
Iteration 15400: Loss = -12359.248979519098
Iteration 15500: Loss = -12359.249419868871
1
Iteration 15600: Loss = -12359.24899143967
2
Iteration 15700: Loss = -12359.249511834161
3
Iteration 15800: Loss = -12359.248870440953
Iteration 15900: Loss = -12359.248769123564
Iteration 16000: Loss = -12359.263018445286
1
Iteration 16100: Loss = -12359.248808111966
2
Iteration 16200: Loss = -12359.264747562924
3
Iteration 16300: Loss = -12359.249648926483
4
Iteration 16400: Loss = -12359.259460290748
5
Iteration 16500: Loss = -12359.248538889475
Iteration 16600: Loss = -12359.248851117853
1
Iteration 16700: Loss = -12359.248791346434
2
Iteration 16800: Loss = -12359.249197421907
3
Iteration 16900: Loss = -12359.248396781448
Iteration 17000: Loss = -12359.249219215022
1
Iteration 17100: Loss = -12359.249856705866
2
Iteration 17200: Loss = -12359.257073088858
3
Iteration 17300: Loss = -12359.248464821665
4
Iteration 17400: Loss = -12359.25037345629
5
Iteration 17500: Loss = -12359.248853586963
6
Iteration 17600: Loss = -12359.248296699285
Iteration 17700: Loss = -12359.248412228304
1
Iteration 17800: Loss = -12359.248843501024
2
Iteration 17900: Loss = -12359.249028625727
3
Iteration 18000: Loss = -12359.54968285312
4
Iteration 18100: Loss = -12359.248199348647
Iteration 18200: Loss = -12359.331670422685
1
Iteration 18300: Loss = -12359.248860218058
2
Iteration 18400: Loss = -12359.248864835949
3
Iteration 18500: Loss = -12359.248508437231
4
Iteration 18600: Loss = -12359.24815688044
Iteration 18700: Loss = -12359.248454487912
1
Iteration 18800: Loss = -12359.399402822593
2
Iteration 18900: Loss = -12359.249797665421
3
Iteration 19000: Loss = -12359.252326976817
4
Iteration 19100: Loss = -12359.248110697394
Iteration 19200: Loss = -12359.258593491786
1
Iteration 19300: Loss = -12359.248023794567
Iteration 19400: Loss = -12359.476745165417
1
Iteration 19500: Loss = -12359.249683479873
2
Iteration 19600: Loss = -12359.271147750365
3
Iteration 19700: Loss = -12359.2494198628
4
Iteration 19800: Loss = -12359.591144776254
5
Iteration 19900: Loss = -12359.249208708485
6
tensor([[  2.8849,  -7.5001],
        [  7.4356, -12.0508],
        [  6.8718, -11.4870],
        [  6.9050, -11.5202],
        [  6.6423, -11.2575],
        [  6.4868, -11.1020],
        [  6.8890, -11.5042],
        [  7.1610, -11.7762],
        [  6.9332, -11.5484],
        [  5.9357, -10.5510],
        [  6.8176, -11.4328],
        [  6.6490, -11.2642],
        [  7.3944, -12.0096],
        [  6.8071, -11.4223],
        [  7.5931, -12.2083],
        [  5.2808,  -9.8960],
        [  6.5206, -11.1358],
        [  6.8283, -11.4435],
        [  6.0054, -10.6207],
        [  7.0926, -11.7078],
        [  6.8509, -11.4662],
        [  6.1313, -10.7465],
        [  7.1724, -11.7876],
        [  3.1386,  -7.7538],
        [  6.0025, -10.6178],
        [  6.2712, -10.8864],
        [  6.8420, -11.4572],
        [  6.7925, -11.4077],
        [  6.2990, -10.9142],
        [  6.1594, -10.7746],
        [  6.2928, -10.9081],
        [  6.7371, -11.3523],
        [  6.3448, -10.9600],
        [  6.7600, -11.3752],
        [  7.1328, -11.7480],
        [  7.1078, -11.7230],
        [  6.5790, -11.1943],
        [  7.1277, -11.7429],
        [  6.5538, -11.1690],
        [  3.9482,  -8.5634],
        [  6.7988, -11.4140],
        [  5.9453, -10.5605],
        [  6.8971, -11.5123],
        [  7.7674, -12.3827],
        [  6.8954, -11.5107],
        [  6.8324, -11.4476],
        [  4.5809,  -9.1962],
        [  5.0248,  -9.6400],
        [  7.6739, -12.2891],
        [  6.3667, -10.9819],
        [  6.4578, -11.0730],
        [  4.4094,  -9.0246],
        [  6.4726, -11.0878],
        [  6.9467, -11.5620],
        [  0.9146,  -5.5298],
        [  1.0907,  -5.7059],
        [  6.7575, -11.3727],
        [  7.4292, -12.0444],
        [  7.2632, -11.8784],
        [  6.9579, -11.5731],
        [  4.4832,  -9.0984],
        [  6.6465, -11.2618],
        [  6.5943, -11.2096],
        [  6.8169, -11.4321],
        [  7.8059, -12.4211],
        [  6.9278, -11.5430],
        [  5.1185,  -9.7337],
        [  7.7020, -12.3172],
        [  1.6517,  -6.2670],
        [  6.5434, -11.1586],
        [  6.8663, -11.4815],
        [  7.7020, -12.3172],
        [  6.9454, -11.5606],
        [  6.1394, -10.7546],
        [  4.2329,  -8.8481],
        [  6.3222, -10.9374],
        [  6.8969, -11.5121],
        [  6.2343, -10.8495],
        [  5.5566, -10.1718],
        [  7.0661, -11.6813],
        [  6.2872, -10.9025],
        [  7.0585, -11.6737],
        [  6.4743, -11.0896],
        [  6.8769, -11.4921],
        [ -3.6358,  -0.9794],
        [  5.7728, -10.3880],
        [  6.2829, -10.8981],
        [ -5.2135,   0.5983],
        [  7.1378, -11.7530],
        [  6.3916, -11.0068],
        [  5.9329, -10.5482],
        [  5.6315, -10.2467],
        [  6.3170, -10.9322],
        [  5.3102,  -9.9254],
        [  4.0404,  -8.6556],
        [  6.7778, -11.3931],
        [  7.0576, -11.6729],
        [  3.5069,  -8.1222],
        [  5.6641, -10.2793],
        [  5.8572, -10.4724]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 4.0044e-09],
        [1.0824e-06, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9807, 0.0193], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[1.9835e-01, 2.8589e-01],
         [9.5076e-01, 2.8508e-05]],

        [[1.1729e-02, 1.7791e-01],
         [7.6933e-01, 9.3934e-01]],

        [[6.2833e-01, 3.1244e-01],
         [4.5001e-01, 9.1682e-01]],

        [[1.1618e-01, 2.4514e-01],
         [1.3411e-01, 7.2461e-01]],

        [[1.4104e-01, 1.6397e-01],
         [8.9072e-01, 1.8921e-01]]], dtype=torch.float64,
       grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: -0.005119313988496187
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.0005089183229907909
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: -0.010213452814961178
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.0007748402262652058
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
Global Adjusted Rand Index: -0.0007361729337332871
Average Adjusted Rand Index: -0.0031655940829074603
Iteration 0: Loss = -35867.905621363134
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.9340,    nan]],

        [[0.2682,    nan],
         [0.8028, 0.3357]],

        [[0.8183,    nan],
         [0.0962, 0.7961]],

        [[0.5687,    nan],
         [0.9016, 0.0063]],

        [[0.1673,    nan],
         [0.2626, 0.7473]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -35869.352673658956
Iteration 100: Loss = -12381.714426038256
Iteration 200: Loss = -12374.576457821497
Iteration 300: Loss = -12368.034789306796
Iteration 400: Loss = -12366.9969527195
Iteration 500: Loss = -12366.349133315387
Iteration 600: Loss = -12365.90577105567
Iteration 700: Loss = -12365.584077762021
Iteration 800: Loss = -12365.341825522843
Iteration 900: Loss = -12365.154347314994
Iteration 1000: Loss = -12365.005764796582
Iteration 1100: Loss = -12364.885877777122
Iteration 1200: Loss = -12364.78758109199
Iteration 1300: Loss = -12364.705870061583
Iteration 1400: Loss = -12364.637177691826
Iteration 1500: Loss = -12364.578829823931
Iteration 1600: Loss = -12364.528790808312
Iteration 1700: Loss = -12364.485531652965
Iteration 1800: Loss = -12364.44774987281
Iteration 1900: Loss = -12364.414470926995
Iteration 2000: Loss = -12364.385094642505
Iteration 2100: Loss = -12364.358875969681
Iteration 2200: Loss = -12364.335298993059
Iteration 2300: Loss = -12364.31404690894
Iteration 2400: Loss = -12364.2946392878
Iteration 2500: Loss = -12364.276832633528
Iteration 2600: Loss = -12364.260396236128
Iteration 2700: Loss = -12364.244946845443
Iteration 2800: Loss = -12364.230432208993
Iteration 2900: Loss = -12364.216495327693
Iteration 3000: Loss = -12364.202976977058
Iteration 3100: Loss = -12364.18971092742
Iteration 3200: Loss = -12364.176324034876
Iteration 3300: Loss = -12364.16247238597
Iteration 3400: Loss = -12364.1477621369
Iteration 3500: Loss = -12364.131320017023
Iteration 3600: Loss = -12364.111294394186
Iteration 3700: Loss = -12364.080989907918
Iteration 3800: Loss = -12363.972663268642
Iteration 3900: Loss = -12363.756164778843
Iteration 4000: Loss = -12363.659323826449
Iteration 4100: Loss = -12363.595736934576
Iteration 4200: Loss = -12363.538034163805
Iteration 4300: Loss = -12363.480172710242
Iteration 4400: Loss = -12363.419798988674
Iteration 4500: Loss = -12363.385048643131
Iteration 4600: Loss = -12363.272457124951
Iteration 4700: Loss = -12363.173746115404
Iteration 4800: Loss = -12363.04628830733
Iteration 4900: Loss = -12362.884325379246
Iteration 5000: Loss = -12362.644747021322
Iteration 5100: Loss = -12362.351278285241
Iteration 5200: Loss = -12362.050987223643
Iteration 5300: Loss = -12361.824765938321
Iteration 5400: Loss = -12361.667608637841
Iteration 5500: Loss = -12361.54811841425
Iteration 5600: Loss = -12361.465066334222
Iteration 5700: Loss = -12361.363787651551
Iteration 5800: Loss = -12361.539424792409
1
Iteration 5900: Loss = -12361.25069125248
Iteration 6000: Loss = -12361.232497902316
Iteration 6100: Loss = -12361.179138447269
Iteration 6200: Loss = -12361.153463791758
Iteration 6300: Loss = -12361.133652192511
Iteration 6400: Loss = -12361.14411001823
1
Iteration 6500: Loss = -12361.10607100381
Iteration 6600: Loss = -12361.097822742959
Iteration 6700: Loss = -12361.120895128945
1
Iteration 6800: Loss = -12361.074974744763
Iteration 6900: Loss = -12361.07099286643
Iteration 7000: Loss = -12361.065103513816
Iteration 7100: Loss = -12361.093056266394
1
Iteration 7200: Loss = -12361.059276096774
Iteration 7300: Loss = -12361.059150182491
Iteration 7400: Loss = -12361.055729721344
Iteration 7500: Loss = -12361.054445677151
Iteration 7600: Loss = -12361.054247998745
Iteration 7700: Loss = -12361.052278913836
Iteration 7800: Loss = -12361.051384458977
Iteration 7900: Loss = -12361.081208207688
1
Iteration 8000: Loss = -12361.049746430639
Iteration 8100: Loss = -12361.048926021866
Iteration 8200: Loss = -12361.220536533961
1
Iteration 8300: Loss = -12361.047568886566
Iteration 8400: Loss = -12361.04694543863
Iteration 8500: Loss = -12361.641455175497
1
Iteration 8600: Loss = -12361.045840673307
Iteration 8700: Loss = -12361.045297854147
Iteration 8800: Loss = -12361.045496663322
1
Iteration 8900: Loss = -12361.044351427057
Iteration 9000: Loss = -12361.04389051045
Iteration 9100: Loss = -12361.043488768573
Iteration 9200: Loss = -12361.043100968214
Iteration 9300: Loss = -12361.0427157599
Iteration 9400: Loss = -12361.042335880169
Iteration 9500: Loss = -12361.042637470973
1
Iteration 9600: Loss = -12361.041671151093
Iteration 9700: Loss = -12361.04144446095
Iteration 9800: Loss = -12361.041671376028
1
Iteration 9900: Loss = -12361.04084053325
Iteration 10000: Loss = -12361.040562571117
Iteration 10100: Loss = -12361.062866071334
1
Iteration 10200: Loss = -12361.040082743892
Iteration 10300: Loss = -12361.03984001059
Iteration 10400: Loss = -12361.039906930364
1
Iteration 10500: Loss = -12361.039438439633
Iteration 10600: Loss = -12361.039234458669
Iteration 10700: Loss = -12361.039074820212
Iteration 10800: Loss = -12361.039145829005
1
Iteration 10900: Loss = -12361.038720857956
Iteration 11000: Loss = -12361.038578288364
Iteration 11100: Loss = -12361.066460675214
1
Iteration 11200: Loss = -12361.038250031326
Iteration 11300: Loss = -12361.038123938024
Iteration 11400: Loss = -12361.066263305695
1
Iteration 11500: Loss = -12361.037851512407
Iteration 11600: Loss = -12361.037813090423
Iteration 11700: Loss = -12361.03896043276
1
Iteration 11800: Loss = -12361.037617392176
Iteration 11900: Loss = -12361.0374997171
Iteration 12000: Loss = -12361.037376360711
Iteration 12100: Loss = -12361.037268620064
Iteration 12200: Loss = -12361.03722527368
Iteration 12300: Loss = -12361.037119626244
Iteration 12400: Loss = -12361.0375116968
1
Iteration 12500: Loss = -12361.03696825607
Iteration 12600: Loss = -12361.03703293806
1
Iteration 12700: Loss = -12361.036904283943
Iteration 12800: Loss = -12361.036756604333
Iteration 12900: Loss = -12361.042636317257
1
Iteration 13000: Loss = -12361.036900617262
2
Iteration 13100: Loss = -12361.045476935626
3
Iteration 13200: Loss = -12361.036924531149
4
Iteration 13300: Loss = -12361.036751073243
Iteration 13400: Loss = -12361.041085706875
1
Iteration 13500: Loss = -12361.036451638403
Iteration 13600: Loss = -12361.03714416422
1
Iteration 13700: Loss = -12361.036356450419
Iteration 13800: Loss = -12361.036932644947
1
Iteration 13900: Loss = -12361.036951467639
2
Iteration 14000: Loss = -12361.036589528076
3
Iteration 14100: Loss = -12361.115618005264
4
Iteration 14200: Loss = -12361.042828158268
5
Iteration 14300: Loss = -12361.038141931851
6
Iteration 14400: Loss = -12361.036085097498
Iteration 14500: Loss = -12361.036162635008
1
Iteration 14600: Loss = -12361.036219899293
2
Iteration 14700: Loss = -12361.039098921308
3
Iteration 14800: Loss = -12361.07610104928
4
Iteration 14900: Loss = -12361.035978107071
Iteration 15000: Loss = -12361.063531652691
1
Iteration 15100: Loss = -12361.035923560383
Iteration 15200: Loss = -12361.03599593848
1
Iteration 15300: Loss = -12361.035947108743
2
Iteration 15400: Loss = -12361.035882221251
Iteration 15500: Loss = -12361.04484395897
1
Iteration 15600: Loss = -12361.03584652435
Iteration 15700: Loss = -12361.042749058173
1
Iteration 15800: Loss = -12361.035929254067
2
Iteration 15900: Loss = -12361.0360215825
3
Iteration 16000: Loss = -12361.044842363375
4
Iteration 16100: Loss = -12361.267829805902
5
Iteration 16200: Loss = -12361.035791944634
Iteration 16300: Loss = -12361.03646879353
1
Iteration 16400: Loss = -12361.124975332013
2
Iteration 16500: Loss = -12361.035735909643
Iteration 16600: Loss = -12361.036397700202
1
Iteration 16700: Loss = -12361.03767753946
2
Iteration 16800: Loss = -12361.035968394237
3
Iteration 16900: Loss = -12361.042200273378
4
Iteration 17000: Loss = -12361.324159117881
5
Iteration 17100: Loss = -12361.035683107972
Iteration 17200: Loss = -12361.168237842456
1
Iteration 17300: Loss = -12361.041943136845
2
Iteration 17400: Loss = -12361.035728763918
3
Iteration 17500: Loss = -12361.051656750768
4
Iteration 17600: Loss = -12361.037264438395
5
Iteration 17700: Loss = -12361.035670522431
Iteration 17800: Loss = -12361.062561500377
1
Iteration 17900: Loss = -12361.035649895499
Iteration 18000: Loss = -12361.036386202499
1
Iteration 18100: Loss = -12361.095586794703
2
Iteration 18200: Loss = -12361.047003817128
3
Iteration 18300: Loss = -12361.041133860286
4
Iteration 18400: Loss = -12361.035793375822
5
Iteration 18500: Loss = -12361.035810379271
6
Iteration 18600: Loss = -12361.039315274149
7
Iteration 18700: Loss = -12361.036449082594
8
Iteration 18800: Loss = -12361.035709059139
9
Iteration 18900: Loss = -12361.037056831407
10
Stopping early at iteration 18900 due to no improvement.
tensor([[-4.7544,  0.1392],
        [-3.0734,  1.0275],
        [-3.6665,  1.6242],
        [-3.9369,  0.2378],
        [-1.8663,  0.2374],
        [-2.7757,  0.5718],
        [-3.8722,  1.6972],
        [-3.4320,  1.9153],
        [-3.7251,  2.2211],
        [-1.8320,  0.3244],
        [-3.3456,  1.7837],
        [-3.0409,  1.6539],
        [-1.2525, -0.5563],
        [-2.9985,  0.2129],
        [-2.3081,  0.2401],
        [-2.0090, -0.1186],
        [-2.7689,  1.0409],
        [-2.5180,  1.1038],
        [-2.7875,  1.3607],
        [-1.7949,  0.3962],
        [-3.6800,  0.3617],
        [-1.9047,  0.5138],
        [-3.0927,  1.6959],
        [-1.9628,  0.3866],
        [-4.5176, -0.0976],
        [-2.7246,  0.7997],
        [-3.3369,  1.7532],
        [-3.1427,  1.4027],
        [-3.2411,  1.2191],
        [-2.5121,  1.1095],
        [-3.3313,  1.7189],
        [-3.7256,  0.1548],
        [-3.4928,  2.0606],
        [-1.9285,  0.5407],
        [-3.2845,  0.6116],
        [-1.4861, -0.0840],
        [-1.4072, -0.4161],
        [-3.6242,  1.4879],
        [-2.8827,  1.2070],
        [-1.5218, -0.0337],
        [-2.6432,  1.2539],
        [-3.1109,  0.5615],
        [-3.3707,  1.8681],
        [-2.5612,  1.1063],
        [-3.1187,  1.7197],
        [-3.0973,  1.7107],
        [-2.7267,  1.2052],
        [-1.9010, -0.6794],
        [-2.4857,  0.9605],
        [-3.1474,  1.4350],
        [-2.1685,  0.7074],
        [-3.1152,  1.0396],
        [-0.2126, -1.2851],
        [-2.4892,  1.0603],
        [-2.8964, -0.8030],
        [-3.6125,  2.0143],
        [-2.6330,  1.2417],
        [-1.2986, -0.4188],
        [-3.9721,  1.8719],
        [-3.3819,  1.1581],
        [-1.0977, -0.3492],
        [-1.5275, -0.5956],
        [-1.2536, -0.1330],
        [-0.9496, -0.9795],
        [-3.1386,  0.8327],
        [-3.9745,  1.0191],
        [-1.7420,  0.3367],
        [-2.8302,  1.0822],
        [-3.1279,  1.4959],
        [-2.8592,  1.3000],
        [-3.6307, -0.5174],
        [-2.9453,  1.2060],
        [-3.2481,  1.8199],
        [-2.9465,  1.4687],
        [-4.5727, -0.0080],
        [-3.2260,  1.8270],
        [-3.1850,  1.7117],
        [-3.1192,  1.6157],
        [-2.0594,  0.4967],
        [-2.9348,  0.9081],
        [-3.6822,  1.9394],
        [-1.9786, -0.3844],
        [-2.9110,  1.4093],
        [-2.7181,  0.6643],
        [-1.4100, -0.3713],
        [-2.3853,  0.9813],
        [-2.8857,  1.3878],
        [-1.5065, -1.0616],
        [-4.1189,  1.1099],
        [-2.1974,  0.0611],
        [-1.7019,  0.2795],
        [-2.0348,  0.6349],
        [-3.1714,  1.5805],
        [-3.3800,  0.8117],
        [-1.6242,  0.0971],
        [-3.4789,  1.3428],
        [-1.9237,  0.3649],
        [-3.3652,  1.9753],
        [-2.3538, -0.1240],
        [-2.0735,  0.5719]], dtype=torch.float64, requires_grad=True)
pi: tensor([[2.0023e-05, 9.9998e-01],
        [4.5048e-02, 9.5495e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0718, 0.9282], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2916, 0.2600],
         [0.9340, 0.1963]],

        [[0.2682, 0.2288],
         [0.8028, 0.3357]],

        [[0.8183, 0.2582],
         [0.0962, 0.7961]],

        [[0.5687, 0.1273],
         [0.9016, 0.0063]],

        [[0.1673, 0.2276],
         [0.2626, 0.7473]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.00028700959432072443
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: -0.005513710431743218
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.006509078451524495
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00042302607439050034
Average Adjusted Rand Index: 0.00014167168509211046
Iteration 0: Loss = -24219.503420426317
Iteration 10: Loss = -12216.568319143818
Iteration 20: Loss = -11907.363936352409
Iteration 30: Loss = -11907.363445826844
Iteration 40: Loss = -11907.36344489923
Iteration 50: Loss = -11907.36344489923
1
Iteration 60: Loss = -11907.36344489923
2
Iteration 70: Loss = -11907.36344489923
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[0.6677, 0.3323],
        [0.4017, 0.5983]], dtype=torch.float64)
alpha: tensor([0.5476, 0.4524])
beta: tensor([[[0.2913, 0.1050],
         [0.3680, 0.2924]],

        [[0.9239, 0.1027],
         [0.3011, 0.0449]],

        [[0.4205, 0.0999],
         [0.6011, 0.2703]],

        [[0.4258, 0.0965],
         [0.7872, 0.2622]],

        [[0.0461, 0.0998],
         [0.2837, 0.9023]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
time is 3
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 4
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.03649746627853757
Average Adjusted Rand Index: 0.9759973667477564
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24219.440854967044
Iteration 100: Loss = -12484.866654512443
Iteration 200: Loss = -12431.483243093413
Iteration 300: Loss = -12403.686951946904
Iteration 400: Loss = -12373.205728766454
Iteration 500: Loss = -12366.954486910623
Iteration 600: Loss = -12365.341867711808
Iteration 700: Loss = -12364.60768167623
Iteration 800: Loss = -12364.186997645764
Iteration 900: Loss = -12363.916735180683
Iteration 1000: Loss = -12363.729490721316
Iteration 1100: Loss = -12363.592691321724
Iteration 1200: Loss = -12363.488919598183
Iteration 1300: Loss = -12363.407940647587
Iteration 1400: Loss = -12363.343215936095
Iteration 1500: Loss = -12363.290622439054
Iteration 1600: Loss = -12363.2471666044
Iteration 1700: Loss = -12363.210757556146
Iteration 1800: Loss = -12363.179952157569
Iteration 1900: Loss = -12363.153579540955
Iteration 2000: Loss = -12363.130817143388
Iteration 2100: Loss = -12363.111091689792
Iteration 2200: Loss = -12363.093794805827
Iteration 2300: Loss = -12363.078519108458
Iteration 2400: Loss = -12363.065050282677
Iteration 2500: Loss = -12363.053066195089
Iteration 2600: Loss = -12363.042340311626
Iteration 2700: Loss = -12363.0326195623
Iteration 2800: Loss = -12363.02395799495
Iteration 2900: Loss = -12363.016038544847
Iteration 3000: Loss = -12363.008875723976
Iteration 3100: Loss = -12363.002314182995
Iteration 3200: Loss = -12362.996271105852
Iteration 3300: Loss = -12362.990686706728
Iteration 3400: Loss = -12362.985546537706
Iteration 3500: Loss = -12362.980915865244
Iteration 3600: Loss = -12362.976651009963
Iteration 3700: Loss = -12362.972842335439
Iteration 3800: Loss = -12362.969262974137
Iteration 3900: Loss = -12362.966047936972
Iteration 4000: Loss = -12362.963073521563
Iteration 4100: Loss = -12362.960366029709
Iteration 4200: Loss = -12362.957835642668
Iteration 4300: Loss = -12362.955500182867
Iteration 4400: Loss = -12362.953351357326
Iteration 4500: Loss = -12362.95132846404
Iteration 4600: Loss = -12362.949443360436
Iteration 4700: Loss = -12362.947725029764
Iteration 4800: Loss = -12362.946079839534
Iteration 4900: Loss = -12362.944612737807
Iteration 5000: Loss = -12362.94320043582
Iteration 5100: Loss = -12362.941934928884
Iteration 5200: Loss = -12362.940654207581
Iteration 5300: Loss = -12362.939509859989
Iteration 5400: Loss = -12362.938429425656
Iteration 5500: Loss = -12362.937369957539
Iteration 5600: Loss = -12362.936477098554
Iteration 5700: Loss = -12362.93552744282
Iteration 5800: Loss = -12362.934736286492
Iteration 5900: Loss = -12362.933931684014
Iteration 6000: Loss = -12362.933158536158
Iteration 6100: Loss = -12362.932444114536
Iteration 6200: Loss = -12362.93179507726
Iteration 6300: Loss = -12362.93115797818
Iteration 6400: Loss = -12362.93058479002
Iteration 6500: Loss = -12362.929982881793
Iteration 6600: Loss = -12362.92949928007
Iteration 6700: Loss = -12362.928988685479
Iteration 6800: Loss = -12362.9284687536
Iteration 6900: Loss = -12362.928037685338
Iteration 7000: Loss = -12362.927682310425
Iteration 7100: Loss = -12362.92720083244
Iteration 7200: Loss = -12363.07415297015
1
Iteration 7300: Loss = -12362.941256947772
2
Iteration 7400: Loss = -12362.926083093596
Iteration 7500: Loss = -12362.92572299218
Iteration 7600: Loss = -12362.926937094539
1
Iteration 7700: Loss = -12362.92507843228
Iteration 7800: Loss = -12362.924847663347
Iteration 7900: Loss = -12363.358373350884
1
Iteration 8000: Loss = -12362.93000191587
2
Iteration 8100: Loss = -12362.923989972867
Iteration 8200: Loss = -12362.923721734465
Iteration 8300: Loss = -12362.930649548953
1
Iteration 8400: Loss = -12362.923219420965
Iteration 8500: Loss = -12362.922992218644
Iteration 8600: Loss = -12362.922745399397
Iteration 8700: Loss = -12362.925357473368
1
Iteration 8800: Loss = -12362.922003878695
Iteration 8900: Loss = -12362.921404623488
Iteration 9000: Loss = -12362.919970238989
Iteration 9100: Loss = -12362.910143310353
Iteration 9200: Loss = -12362.628882139448
Iteration 9300: Loss = -12362.489625428343
Iteration 9400: Loss = -12362.442619008643
Iteration 9500: Loss = -12362.433340978245
Iteration 9600: Loss = -12362.444789360121
1
Iteration 9700: Loss = -12362.426876995018
Iteration 9800: Loss = -12362.410321387955
Iteration 9900: Loss = -12362.406848072136
Iteration 10000: Loss = -12362.406638331888
Iteration 10100: Loss = -12362.405483770815
Iteration 10200: Loss = -12362.404852924914
Iteration 10300: Loss = -12362.40225186054
Iteration 10400: Loss = -12362.451134822379
1
Iteration 10500: Loss = -12362.41725281938
2
Iteration 10600: Loss = -12362.48620896872
3
Iteration 10700: Loss = -12362.399208518893
Iteration 10800: Loss = -12362.394146711638
Iteration 10900: Loss = -12362.389878552736
Iteration 11000: Loss = -12362.396878683641
1
Iteration 11100: Loss = -12362.380737384821
Iteration 11200: Loss = -12362.389553241275
1
Iteration 11300: Loss = -12362.42555499086
2
Iteration 11400: Loss = -12362.375136293651
Iteration 11500: Loss = -12362.382137773293
1
Iteration 11600: Loss = -12362.376418901124
2
Iteration 11700: Loss = -12362.370828713025
Iteration 11800: Loss = -12362.370334258327
Iteration 11900: Loss = -12362.394648156413
1
Iteration 12000: Loss = -12362.420520250871
2
Iteration 12100: Loss = -12362.368939895141
Iteration 12200: Loss = -12362.367847969426
Iteration 12300: Loss = -12362.367849616145
1
Iteration 12400: Loss = -12362.372873840699
2
Iteration 12500: Loss = -12362.365447822127
Iteration 12600: Loss = -12362.3655282602
1
Iteration 12700: Loss = -12362.365299545092
Iteration 12800: Loss = -12362.365962853228
1
Iteration 12900: Loss = -12362.365937781351
2
Iteration 13000: Loss = -12362.46806819897
3
Iteration 13100: Loss = -12362.364209814854
Iteration 13200: Loss = -12362.3642693059
1
Iteration 13300: Loss = -12362.363964892646
Iteration 13400: Loss = -12362.36570989641
1
Iteration 13500: Loss = -12362.36379238522
Iteration 13600: Loss = -12362.3820397266
1
Iteration 13700: Loss = -12362.36338974265
Iteration 13800: Loss = -12362.363178005005
Iteration 13900: Loss = -12362.364617640362
1
Iteration 14000: Loss = -12362.363364412331
2
Iteration 14100: Loss = -12362.365632541047
3
Iteration 14200: Loss = -12362.365074728046
4
Iteration 14300: Loss = -12362.365812477301
5
Iteration 14400: Loss = -12362.374448355002
6
Iteration 14500: Loss = -12362.363881079305
7
Iteration 14600: Loss = -12362.363445573079
8
Iteration 14700: Loss = -12362.404506423338
9
Iteration 14800: Loss = -12362.363942140599
10
Stopping early at iteration 14800 due to no improvement.
tensor([[  7.7599,  -9.2271],
        [  7.7983, -10.0034],
        [  7.3164,  -9.0792],
        [  7.8301,  -9.2722],
        [  7.5628,  -9.0070],
        [  8.3624,  -9.8672],
        [  7.8738,  -9.3238],
        [  7.3550,  -8.7430],
        [  7.8168,  -9.2030],
        [  8.4232,  -9.8176],
        [  7.8636,  -9.2941],
        [  8.0852, -11.5361],
        [  7.8339,  -9.3168],
        [  7.2948, -10.5959],
        [  7.6098,  -9.6319],
        [  7.5198,  -9.2435],
        [  7.6009,  -9.0250],
        [  8.0405,  -9.5319],
        [  7.7475,  -9.1355],
        [  8.6242, -10.1680],
        [  6.7709, -11.3861],
        [  7.3639,  -9.5224],
        [  8.1327, -10.7986],
        [  7.8042,  -9.4870],
        [  7.8132,  -9.4808],
        [  7.5218, -10.8108],
        [  6.4846, -10.9165],
        [  7.7095, -10.1375],
        [  7.7028, -10.4361],
        [  8.1886,  -9.9879],
        [  8.4041, -11.1074],
        [  7.2705,  -9.6786],
        [  7.7103, -11.1201],
        [  7.7768,  -9.1702],
        [  8.1455, -10.2881],
        [  7.7473,  -9.5818],
        [  7.0692,  -9.5342],
        [  7.2917, -10.5531],
        [  8.2701,  -9.6700],
        [  7.4723,  -9.6646],
        [  7.9474,  -9.9895],
        [  7.5579,  -8.9443],
        [  7.8550,  -9.3368],
        [  8.0473,  -9.9250],
        [  7.9914,  -9.5119],
        [  7.9446,  -9.7638],
        [  7.8129,  -9.2392],
        [  7.5741, -10.1524],
        [  7.8309,  -9.2841],
        [  8.1597,  -9.7157],
        [  7.9966, -11.2628],
        [  8.2414,  -9.9188],
        [  7.3905,  -8.8675],
        [  7.5653,  -9.0179],
        [  8.1998,  -9.8622],
        [  7.6791,  -9.0658],
        [  8.1685,  -9.5553],
        [  7.7525,  -9.4392],
        [  8.0389, -10.1107],
        [  7.2315, -10.8510],
        [  7.6524, -10.4212],
        [  7.2060,  -9.0415],
        [  7.3676,  -8.8815],
        [  7.0485,  -8.9015],
        [  7.9021,  -9.3145],
        [  8.2025,  -9.5963],
        [  7.5396,  -9.3780],
        [  7.4026,  -9.4782],
        [  7.2757, -10.2376],
        [  7.9843,  -9.3965],
        [  8.1726,  -9.6439],
        [  8.1666, -10.7152],
        [  8.0650,  -9.4672],
        [  8.1995, -10.0899],
        [  7.5711,  -9.4419],
        [  8.1118, -10.5706],
        [  7.8679,  -9.3815],
        [  8.1222, -10.1124],
        [  7.7149,  -9.7260],
        [  8.6391, -11.5839],
        [  7.9011,  -9.2874],
        [  7.5211,  -9.7781],
        [  7.1887,  -9.7558],
        [  7.6133,  -9.4996],
        [  8.4497, -10.9244],
        [  7.6227,  -9.7766],
        [  7.5071, -10.3999],
        [  6.5205,  -9.9201],
        [  7.2851,  -8.7879],
        [  8.2472,  -9.8024],
        [  7.7229,  -9.1163],
        [  8.0082,  -9.4264],
        [  8.0286,  -9.4542],
        [  8.0158,  -9.8775],
        [  7.3834,  -9.5582],
        [  8.2492, -10.3612],
        [  7.9336,  -9.4109],
        [  7.9830,  -9.6531],
        [  7.6398, -10.9950],
        [  7.8056,  -9.1920]], dtype=torch.float64, requires_grad=True)
pi: tensor([[2.2413e-07, 1.0000e+00],
        [9.8646e-01, 1.3544e-02]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 3.7958e-08], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2020, 0.2096],
         [0.3680, 0.1937]],

        [[0.9239, 0.2058],
         [0.3011, 0.0449]],

        [[0.4205, 0.2939],
         [0.6011, 0.2703]],

        [[0.4258, 0.2485],
         [0.7872, 0.2622]],

        [[0.0461, 0.1823],
         [0.2837, 0.9023]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 43
Adjusted Rand Index: -0.005513710431743218
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.006509078451524495
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
Global Adjusted Rand Index: 0.001182683487797338
Average Adjusted Rand Index: 0.0003624016531468207
Iteration 0: Loss = -15903.31738000454
Iteration 10: Loss = -12361.787752886072
Iteration 20: Loss = -12361.737155940229
Iteration 30: Loss = -12361.725156376653
Iteration 40: Loss = -12361.717904760695
Iteration 50: Loss = -12361.708181860722
Iteration 60: Loss = -12361.693030996292
Iteration 70: Loss = -12361.672215839671
Iteration 80: Loss = -12361.648058731986
Iteration 90: Loss = -12361.623578598274
Iteration 100: Loss = -12361.60012179526
Iteration 110: Loss = -12361.57687880967
Iteration 120: Loss = -12361.551814823439
Iteration 130: Loss = -12361.522585596578
Iteration 140: Loss = -12361.48789002957
Iteration 150: Loss = -12361.448354518752
Iteration 160: Loss = -12361.407035505988
Iteration 170: Loss = -12361.367097444685
Iteration 180: Loss = -12361.330163397442
Iteration 190: Loss = -12361.296580588434
Iteration 200: Loss = -12361.265813298858
Iteration 210: Loss = -12361.237564388437
Iteration 220: Loss = -12361.211326589317
Iteration 230: Loss = -12361.186855105507
Iteration 240: Loss = -12361.163944378453
Iteration 250: Loss = -12361.142347573525
Iteration 260: Loss = -12361.121870238821
Iteration 270: Loss = -12361.102338143717
Iteration 280: Loss = -12361.083619055764
Iteration 290: Loss = -12361.065537758537
Iteration 300: Loss = -12361.048091225288
Iteration 310: Loss = -12361.031058616698
Iteration 320: Loss = -12361.014344822852
Iteration 330: Loss = -12360.997947595914
Iteration 340: Loss = -12360.981726877553
Iteration 350: Loss = -12360.965644628459
Iteration 360: Loss = -12360.949533334755
Iteration 370: Loss = -12360.933372293262
Iteration 380: Loss = -12360.917031795796
Iteration 390: Loss = -12360.900431642163
Iteration 400: Loss = -12360.883450576277
Iteration 410: Loss = -12360.865822281756
Iteration 420: Loss = -12360.847481373665
Iteration 430: Loss = -12360.82815175991
Iteration 440: Loss = -12360.807340267547
Iteration 450: Loss = -12360.784482297897
Iteration 460: Loss = -12360.758493092764
Iteration 470: Loss = -12360.727329600055
Iteration 480: Loss = -12360.685449982744
Iteration 490: Loss = -12360.610050438494
Iteration 500: Loss = -12359.661357236442
Iteration 510: Loss = -12231.046809166994
Iteration 520: Loss = -11907.43796246293
Iteration 530: Loss = -11907.36342614903
Iteration 540: Loss = -11907.363422620252
Iteration 550: Loss = -11907.363422620252
1
Iteration 560: Loss = -11907.363422620252
2
Iteration 570: Loss = -11907.363422620252
3
Stopping early at iteration 569 due to no improvement.
pi: tensor([[0.6677, 0.3323],
        [0.4017, 0.5983]], dtype=torch.float64)
alpha: tensor([0.5476, 0.4524])
beta: tensor([[[0.2913, 0.1050],
         [0.4400, 0.2924]],

        [[0.0878, 0.1027],
         [0.1191, 0.2043]],

        [[0.1727, 0.0999],
         [0.9786, 0.0213]],

        [[0.6861, 0.0965],
         [0.2113, 0.3250]],

        [[0.8022, 0.0998],
         [0.7197, 0.8919]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
time is 3
tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 4
tensor([0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.03649746627853757
Average Adjusted Rand Index: 0.9759973667477564
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -15902.887843402526
Iteration 100: Loss = -12366.874582177184
Iteration 200: Loss = -12363.384986343995
Iteration 300: Loss = -12362.753115788746
Iteration 400: Loss = -12362.224418790585
Iteration 500: Loss = -12361.749400284249
Iteration 600: Loss = -12361.485119920344
Iteration 700: Loss = -12361.315775873034
Iteration 800: Loss = -12361.206050308236
Iteration 900: Loss = -12361.15710442182
Iteration 1000: Loss = -12361.129613958738
Iteration 1100: Loss = -12361.111167149185
Iteration 1200: Loss = -12361.09803674936
Iteration 1300: Loss = -12361.088421030978
Iteration 1400: Loss = -12361.08112667366
Iteration 1500: Loss = -12361.075480050884
Iteration 1600: Loss = -12361.070966506633
Iteration 1700: Loss = -12361.067286774918
Iteration 1800: Loss = -12361.064127757994
Iteration 1900: Loss = -12361.061483573469
Iteration 2000: Loss = -12361.059168987696
Iteration 2100: Loss = -12361.05711477171
Iteration 2200: Loss = -12361.055353026995
Iteration 2300: Loss = -12361.053773419657
Iteration 2400: Loss = -12361.052342853063
Iteration 2500: Loss = -12361.051100130378
Iteration 2600: Loss = -12361.049942360254
Iteration 2700: Loss = -12361.048895183787
Iteration 2800: Loss = -12361.047961373035
Iteration 2900: Loss = -12361.047083809455
Iteration 3000: Loss = -12361.046292459574
Iteration 3100: Loss = -12361.045561871577
Iteration 3200: Loss = -12361.044917346337
Iteration 3300: Loss = -12361.044331162377
Iteration 3400: Loss = -12361.043752823287
Iteration 3500: Loss = -12361.043229019735
Iteration 3600: Loss = -12361.04279903347
Iteration 3700: Loss = -12361.042355162937
Iteration 3800: Loss = -12361.0418771154
Iteration 3900: Loss = -12361.041508197843
Iteration 4000: Loss = -12361.041193555988
Iteration 4100: Loss = -12361.040908407398
Iteration 4200: Loss = -12361.040567893833
Iteration 4300: Loss = -12361.04024248928
Iteration 4400: Loss = -12361.040020282268
Iteration 4500: Loss = -12361.039738547004
Iteration 4600: Loss = -12361.039591875367
Iteration 4700: Loss = -12361.039325484371
Iteration 4800: Loss = -12361.039123627015
Iteration 4900: Loss = -12361.038891198508
Iteration 5000: Loss = -12361.038804317159
Iteration 5100: Loss = -12361.03863981697
Iteration 5200: Loss = -12361.038420701832
Iteration 5300: Loss = -12361.038232700877
Iteration 5400: Loss = -12361.03848800184
1
Iteration 5500: Loss = -12361.037962007567
Iteration 5600: Loss = -12361.037832704642
Iteration 5700: Loss = -12361.039174024896
1
Iteration 5800: Loss = -12361.037607651151
Iteration 5900: Loss = -12361.037492037884
Iteration 6000: Loss = -12361.038276734507
1
Iteration 6100: Loss = -12361.037301550352
Iteration 6200: Loss = -12361.037202638166
Iteration 6300: Loss = -12361.037150002048
Iteration 6400: Loss = -12361.037036648891
Iteration 6500: Loss = -12361.036966044583
Iteration 6600: Loss = -12361.04743290165
1
Iteration 6700: Loss = -12361.036845647917
Iteration 6800: Loss = -12361.036780617926
Iteration 6900: Loss = -12361.037783299016
1
Iteration 7000: Loss = -12361.03664864176
Iteration 7100: Loss = -12361.036602047927
Iteration 7200: Loss = -12361.036613249962
1
Iteration 7300: Loss = -12361.036608070477
2
Iteration 7400: Loss = -12361.03645748443
Iteration 7500: Loss = -12361.03637013471
Iteration 7600: Loss = -12361.036922603244
1
Iteration 7700: Loss = -12361.03631978443
Iteration 7800: Loss = -12361.036294762502
Iteration 7900: Loss = -12361.037318376266
1
Iteration 8000: Loss = -12361.036181982363
Iteration 8100: Loss = -12361.036184683544
1
Iteration 8200: Loss = -12361.036791489572
2
Iteration 8300: Loss = -12361.036079942201
Iteration 8400: Loss = -12361.036077076418
Iteration 8500: Loss = -12361.03639282303
1
Iteration 8600: Loss = -12361.036020157926
Iteration 8700: Loss = -12361.036013516883
Iteration 8800: Loss = -12361.03628384566
1
Iteration 8900: Loss = -12361.035945478496
Iteration 9000: Loss = -12361.063178870123
1
Iteration 9100: Loss = -12361.035966231584
2
Iteration 9200: Loss = -12361.036786538958
3
Iteration 9300: Loss = -12361.038718098778
4
Iteration 9400: Loss = -12361.035955557056
5
Iteration 9500: Loss = -12361.396030370146
6
Iteration 9600: Loss = -12361.035801676057
Iteration 9700: Loss = -12361.036007754994
1
Iteration 9800: Loss = -12361.03588264086
2
Iteration 9900: Loss = -12361.037971192254
3
Iteration 10000: Loss = -12361.072014186315
4
Iteration 10100: Loss = -12361.035747456437
Iteration 10200: Loss = -12361.038306039836
1
Iteration 10300: Loss = -12361.078503098342
2
Iteration 10400: Loss = -12361.038551749774
3
Iteration 10500: Loss = -12361.035844302665
4
Iteration 10600: Loss = -12361.038865827042
5
Iteration 10700: Loss = -12361.264749945616
6
Iteration 10800: Loss = -12361.048855873594
7
Iteration 10900: Loss = -12361.146179808975
8
Iteration 11000: Loss = -12361.040622886943
9
Iteration 11100: Loss = -12361.113703782079
10
Stopping early at iteration 11100 due to no improvement.
tensor([[-3.1446e+00,  1.7579e+00],
        [-3.0476e+00,  1.0672e+00],
        [-4.4861e+00,  8.2113e-01],
        [-2.7880e+00,  1.4017e+00],
        [-2.4901e+00, -3.7471e-01],
        [-2.3952e+00,  9.6585e-01],
        [-3.4842e+00,  2.0883e+00],
        [-3.3841e+00,  1.9730e+00],
        [-3.6696e+00,  2.2793e+00],
        [-1.7856e+00,  3.8277e-01],
        [-3.7720e+00,  1.3708e+00],
        [-3.4725e+00,  1.2296e+00],
        [-1.0481e+00, -3.4865e-01],
        [-2.3129e+00,  9.1606e-01],
        [-2.3109e+00,  2.5182e-01],
        [-1.6636e+00,  2.3687e-01],
        [-2.7568e+00,  1.0713e+00],
        [-2.5702e+00,  1.0656e+00],
        [-3.0512e+00,  1.1175e+00],
        [-2.7109e+00, -5.0770e-01],
        [-2.7333e+00,  1.3249e+00],
        [-2.0042e+00,  4.2796e-01],
        [-3.0965e+00,  1.7029e+00],
        [-1.8775e+00,  4.8504e-01],
        [-2.9283e+00,  1.5007e+00],
        [-3.4744e+00,  6.9517e-02],
        [-3.5740e+00,  1.5199e+00],
        [-2.9899e+00,  1.5647e+00],
        [-2.9394e+00,  1.5377e+00],
        [-3.0029e+00,  6.3892e-01],
        [-3.2229e+00,  1.8340e+00],
        [-4.2578e+00, -3.5740e-01],
        [-3.9775e+00,  1.5850e+00],
        [-2.0315e+00,  4.5187e-01],
        [-2.7794e+00,  1.1306e+00],
        [-2.0551e+00, -6.4528e-01],
        [-1.2111e+00, -2.1452e-01],
        [-3.2548e+00,  1.8622e+00],
        [-2.8106e+00,  1.2900e+00],
        [-1.4417e+00,  5.4191e-02],
        [-2.7743e+00,  1.1329e+00],
        [-2.6876e+00,  1.0010e+00],
        [-3.5987e+00,  1.6442e+00],
        [-2.5932e+00,  1.0918e+00],
        [-3.3237e+00,  1.5282e+00],
        [-3.6411e+00,  1.1866e+00],
        [-2.7232e+00,  1.2236e+00],
        [-1.3372e+00, -1.1033e-01],
        [-2.4349e+00,  1.0308e+00],
        [-3.0931e+00,  1.5024e+00],
        [-3.2207e+00, -3.2760e-01],
        [-2.9253e+00,  1.2421e+00],
        [-3.5962e-01, -1.4333e+00],
        [-3.0781e+00,  4.9054e-01],
        [-2.1038e+00,  5.2377e-04],
        [-4.0677e+00,  1.5620e+00],
        [-2.7742e+00,  1.1195e+00],
        [-1.3821e+00, -4.9811e-01],
        [-4.0719e+00,  1.7770e+00],
        [-3.3701e+00,  1.1882e+00],
        [-1.1367e+00, -3.8401e-01],
        [-1.1729e+00, -2.3608e-01],
        [-1.3371e+00, -2.1055e-01],
        [-7.2198e-01, -7.5015e-01],
        [-2.9009e+00,  1.0869e+00],
        [-4.3447e+00,  6.6497e-01],
        [-1.8368e+00,  2.5326e-01],
        [-2.9634e+00,  9.6646e-01],
        [-3.4660e+00,  1.1748e+00],
        [-3.4962e+00,  6.8350e-01],
        [-2.4486e+00,  6.8269e-01],
        [-2.8338e+00,  1.3325e+00],
        [-3.2530e+00,  1.8205e+00],
        [-2.9653e+00,  1.4601e+00],
        [-3.4598e+00,  1.1177e+00],
        [-3.6905e+00,  1.3695e+00],
        [-3.5969e+00,  1.3172e+00],
        [-3.8453e+00,  9.0769e-01],
        [-2.0068e+00,  5.6371e-01],
        [-2.8168e+00,  1.0419e+00],
        [-4.8512e+00,  7.8486e-01],
        [-1.7944e+00, -1.9132e-01],
        [-2.8633e+00,  1.4664e+00],
        [-2.4464e+00,  9.5171e-01],
        [-1.2238e+00, -1.7993e-01],
        [-2.9976e+00,  3.8891e-01],
        [-2.8913e+00,  1.3965e+00],
        [-1.4999e+00, -1.0513e+00],
        [-3.3204e+00,  1.9140e+00],
        [-1.9094e+00,  3.6164e-01],
        [-1.9475e+00,  4.3859e-02],
        [-3.6500e+00, -9.6526e-01],
        [-3.2532e+00,  1.5116e+00],
        [-2.9048e+00,  1.2988e+00],
        [-1.5628e+00,  1.6653e-01],
        [-3.1357e+00,  1.6943e+00],
        [-1.9511e+00,  3.5034e-01],
        [-3.5252e+00,  1.8190e+00],
        [-1.8759e+00,  3.6658e-01],
        [-2.0759e+00,  5.8475e-01]], dtype=torch.float64, requires_grad=True)
pi: tensor([[5.0353e-05, 9.9995e-01],
        [4.4635e-02, 9.5537e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0723, 0.9277], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2922, 0.2583],
         [0.4400, 0.1960]],

        [[0.0878, 0.2289],
         [0.1191, 0.2043]],

        [[0.1727, 0.2591],
         [0.9786, 0.0213]],

        [[0.6861, 0.1269],
         [0.2113, 0.3250]],

        [[0.8022, 0.2277],
         [0.7197, 0.8919]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.00028700959432072443
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: -0.005513710431743218
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.006509078451524495
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00042302607439050034
Average Adjusted Rand Index: 0.00014167168509211046
Iteration 0: Loss = -18623.402378442814
Iteration 10: Loss = -12362.489528878139
Iteration 20: Loss = -12361.773871096713
Iteration 30: Loss = -12361.496481601118
Iteration 40: Loss = -12361.482768425765
Iteration 50: Loss = -12361.480749111166
Iteration 60: Loss = -12361.47993121706
Iteration 70: Loss = -12361.479600238194
Iteration 80: Loss = -12361.479376977657
Iteration 90: Loss = -12361.479294585371
Iteration 100: Loss = -12361.479272356592
Iteration 110: Loss = -12361.479266693426
Iteration 120: Loss = -12361.479227802418
Iteration 130: Loss = -12361.47917663527
Iteration 140: Loss = -12361.479221759784
1
Iteration 150: Loss = -12361.479182108264
2
Iteration 160: Loss = -12361.479251737639
3
Stopping early at iteration 159 due to no improvement.
pi: tensor([[9.5509e-01, 4.4911e-02],
        [1.0000e+00, 3.1381e-06]], dtype=torch.float64)
alpha: tensor([0.9565, 0.0435])
beta: tensor([[[0.1950, 0.2715],
         [0.8212, 0.2588]],

        [[0.2738, 0.2206],
         [0.3293, 0.6119]],

        [[0.7169, 0.2540],
         [0.9332, 0.7145]],

        [[0.5261, 0.1306],
         [0.6547, 0.8292]],

        [[0.9976, 0.2227],
         [0.6680, 0.5021]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.006509078451524495
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00040374138271456197
Average Adjusted Rand Index: 0.0007091372022792591
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18453.493845097742
Iteration 100: Loss = -12367.557059230812
Iteration 200: Loss = -12363.60565721536
Iteration 300: Loss = -12362.87530373989
Iteration 400: Loss = -12362.185863998766
Iteration 500: Loss = -12361.677096108464
Iteration 600: Loss = -12361.397908174617
Iteration 700: Loss = -12361.284629818992
Iteration 800: Loss = -12361.229125265025
Iteration 900: Loss = -12361.194455639297
Iteration 1000: Loss = -12361.170106515432
Iteration 1100: Loss = -12361.152068096486
Iteration 1200: Loss = -12361.138280243105
Iteration 1300: Loss = -12361.127390217523
Iteration 1400: Loss = -12361.118569424843
Iteration 1500: Loss = -12361.11124709873
Iteration 1600: Loss = -12361.105063297222
Iteration 1700: Loss = -12361.099680370986
Iteration 1800: Loss = -12361.09495904904
Iteration 1900: Loss = -12361.09080007004
Iteration 2000: Loss = -12361.087041757335
Iteration 2100: Loss = -12361.083625821484
Iteration 2200: Loss = -12361.08054732787
Iteration 2300: Loss = -12361.077746431114
Iteration 2400: Loss = -12361.07507394251
Iteration 2500: Loss = -12361.072671145028
Iteration 2600: Loss = -12361.070440044696
Iteration 2700: Loss = -12361.068359691228
Iteration 2800: Loss = -12361.066413953595
Iteration 2900: Loss = -12361.064606358783
Iteration 3000: Loss = -12361.062905107925
Iteration 3100: Loss = -12361.061338993242
Iteration 3200: Loss = -12361.05990515876
Iteration 3300: Loss = -12361.058538069643
Iteration 3400: Loss = -12361.057223176256
Iteration 3500: Loss = -12361.056029088519
Iteration 3600: Loss = -12361.054886853211
Iteration 3700: Loss = -12361.053880953365
Iteration 3800: Loss = -12361.052875381205
Iteration 3900: Loss = -12361.051883488994
Iteration 4000: Loss = -12361.0553149533
1
Iteration 4100: Loss = -12361.050171365829
Iteration 4200: Loss = -12361.049400097794
Iteration 4300: Loss = -12361.065034493467
1
Iteration 4400: Loss = -12361.047981446323
Iteration 4500: Loss = -12361.047272550917
Iteration 4600: Loss = -12361.07397734248
1
Iteration 4700: Loss = -12361.046165016063
Iteration 4800: Loss = -12361.045528654116
Iteration 4900: Loss = -12361.062432426765
1
Iteration 5000: Loss = -12361.044548852144
Iteration 5100: Loss = -12361.044108840322
Iteration 5200: Loss = -12361.443357926039
1
Iteration 5300: Loss = -12361.043193391082
Iteration 5400: Loss = -12361.042844118561
Iteration 5500: Loss = -12361.175635005136
1
Iteration 5600: Loss = -12361.042169524497
Iteration 5700: Loss = -12361.041803443644
Iteration 5800: Loss = -12361.591985837404
1
Iteration 5900: Loss = -12361.041196838476
Iteration 6000: Loss = -12361.040901467215
Iteration 6100: Loss = -12361.041270889717
1
Iteration 6200: Loss = -12361.040432394471
Iteration 6300: Loss = -12361.040106854925
Iteration 6400: Loss = -12361.040609877167
1
Iteration 6500: Loss = -12361.039661151497
Iteration 6600: Loss = -12361.039468135164
Iteration 6700: Loss = -12361.039305872617
Iteration 6800: Loss = -12361.039147194095
Iteration 6900: Loss = -12361.03892958649
Iteration 7000: Loss = -12361.03874266319
Iteration 7100: Loss = -12361.038685746506
Iteration 7200: Loss = -12361.038392089835
Iteration 7300: Loss = -12361.038311706836
Iteration 7400: Loss = -12361.063894023937
1
Iteration 7500: Loss = -12361.038009533844
Iteration 7600: Loss = -12361.037897193342
Iteration 7700: Loss = -12361.055769051904
1
Iteration 7800: Loss = -12361.037639370612
Iteration 7900: Loss = -12361.037563259872
Iteration 8000: Loss = -12361.080634087459
1
Iteration 8100: Loss = -12361.037345035515
Iteration 8200: Loss = -12361.03729326936
Iteration 8300: Loss = -12361.053647285711
1
Iteration 8400: Loss = -12361.037099606905
Iteration 8500: Loss = -12361.037039171048
Iteration 8600: Loss = -12361.421934384332
1
Iteration 8700: Loss = -12361.036899004625
Iteration 8800: Loss = -12361.036803028926
Iteration 8900: Loss = -12361.036759423208
Iteration 9000: Loss = -12361.036707047577
Iteration 9100: Loss = -12361.036642086736
Iteration 9200: Loss = -12361.036614795954
Iteration 9300: Loss = -12361.036990092236
1
Iteration 9400: Loss = -12361.036473138605
Iteration 9500: Loss = -12361.03642491878
Iteration 9600: Loss = -12361.037210350021
1
Iteration 9700: Loss = -12361.036342680973
Iteration 9800: Loss = -12361.03632811938
Iteration 9900: Loss = -12361.039859568125
1
Iteration 10000: Loss = -12361.03627764952
Iteration 10100: Loss = -12361.036236859143
Iteration 10200: Loss = -12361.090706733425
1
Iteration 10300: Loss = -12361.036149786485
Iteration 10400: Loss = -12361.036095908776
Iteration 10500: Loss = -12361.039692235907
1
Iteration 10600: Loss = -12361.036067457459
Iteration 10700: Loss = -12361.036010902202
Iteration 10800: Loss = -12361.073077126419
1
Iteration 10900: Loss = -12361.035953045033
Iteration 11000: Loss = -12361.035993631807
1
Iteration 11100: Loss = -12361.073492586265
2
Iteration 11200: Loss = -12361.0359098258
Iteration 11300: Loss = -12361.03588511589
Iteration 11400: Loss = -12361.044957573855
1
Iteration 11500: Loss = -12361.03591302183
2
Iteration 11600: Loss = -12361.037279368453
3
Iteration 11700: Loss = -12361.035922762116
4
Iteration 11800: Loss = -12361.039287762484
5
Iteration 11900: Loss = -12361.150035203886
6
Iteration 12000: Loss = -12361.035818158569
Iteration 12100: Loss = -12361.04098716358
1
Iteration 12200: Loss = -12361.068689833033
2
Iteration 12300: Loss = -12361.155907615737
3
Iteration 12400: Loss = -12361.042116350767
4
Iteration 12500: Loss = -12361.035797769047
Iteration 12600: Loss = -12361.082828257382
1
Iteration 12700: Loss = -12361.037484811519
2
Iteration 12800: Loss = -12361.035829330905
3
Iteration 12900: Loss = -12361.036210429385
4
Iteration 13000: Loss = -12361.042413334324
5
Iteration 13100: Loss = -12361.03599285014
6
Iteration 13200: Loss = -12361.04640867968
7
Iteration 13300: Loss = -12361.035682855816
Iteration 13400: Loss = -12361.035837406973
1
Iteration 13500: Loss = -12361.036225949232
2
Iteration 13600: Loss = -12361.125356873448
3
Iteration 13700: Loss = -12361.036348748836
4
Iteration 13800: Loss = -12361.035699192049
5
Iteration 13900: Loss = -12361.036407343832
6
Iteration 14000: Loss = -12361.042025527951
7
Iteration 14100: Loss = -12361.035678172815
Iteration 14200: Loss = -12361.038585932485
1
Iteration 14300: Loss = -12361.036437866644
2
Iteration 14400: Loss = -12361.035638237356
Iteration 14500: Loss = -12361.040305186527
1
Iteration 14600: Loss = -12361.035642749262
2
Iteration 14700: Loss = -12361.03561724232
Iteration 14800: Loss = -12361.038857447193
1
Iteration 14900: Loss = -12361.035667095863
2
Iteration 15000: Loss = -12361.036495658278
3
Iteration 15100: Loss = -12361.072551964304
4
Iteration 15200: Loss = -12361.055035277594
5
Iteration 15300: Loss = -12361.037292782803
6
Iteration 15400: Loss = -12361.036722886973
7
Iteration 15500: Loss = -12361.216670001513
8
Iteration 15600: Loss = -12361.039234013562
9
Iteration 15700: Loss = -12361.041265067517
10
Stopping early at iteration 15700 due to no improvement.
tensor([[ 1.7528, -3.1485],
        [ 1.3586, -2.7500],
        [ 1.2479, -4.0485],
        [ 1.1272, -3.0557],
        [ 0.2567, -1.8490],
        [ 0.4766, -2.8771],
        [ 1.9783, -3.5983],
        [ 0.6945, -4.6600],
        [ 0.6689, -5.2841],
        [-1.2223, -3.3809],
        [ 1.0364, -4.0998],
        [ 0.7588, -3.9434],
        [-0.3466, -1.0414],
        [-0.1741, -3.3910],
        [ 0.3774, -2.1749],
        [ 0.2479, -1.6446],
        [ 1.1498, -2.6669],
        [ 1.1211, -2.5076],
        [ 1.3597, -2.7958],
        [ 0.3975, -1.7961],
        [ 1.3190, -2.7302],
        [ 0.4717, -1.9495],
        [ 1.2829, -3.5132],
        [ 0.1375, -2.2141],
        [ 1.5156, -2.9123],
        [-0.0955, -3.6262],
        [ 1.6340, -3.4638],
        [ 1.2469, -3.3065],
        [ 1.5201, -2.9482],
        [ 0.3908, -3.2383],
        [ 1.5754, -3.4821],
        [ 1.1322, -2.7553],
        [ 1.7511, -3.8107],
        [-0.2437, -2.7161],
        [ 1.1669, -2.7364],
        [-0.3387, -1.7427],
        [-0.8379, -1.8304],
        [ 1.6309, -3.4883],
        [ 1.3550, -2.7419],
        [-0.8668, -2.3555],
        [ 1.2592, -2.6455],
        [ 0.8685, -2.8114],
        [ 1.8724, -3.3721],
        [ 1.0180, -2.6571],
        [ 1.4204, -3.4262],
        [ 1.3597, -3.4551],
        [ 1.2527, -2.6863],
        [-0.4328, -1.6530],
        [-0.3785, -3.8308],
        [ 1.6006, -2.9899],
        [ 0.6717, -2.2090],
        [ 1.3875, -2.7749],
        [-1.3516, -0.2749],
        [ 1.0440, -2.5119],
        [-0.0889, -2.1838],
        [ 2.0789, -3.5537],
        [ 1.2355, -2.6460],
        [-0.9253, -1.8052],
        [ 1.7971, -4.0536],
        [ 1.2043, -3.3435],
        [-0.5040, -1.2514],
        [-0.4583, -1.3902],
        [-0.1609, -1.2816],
        [-1.2412, -1.2081],
        [ 0.3230, -3.6557],
        [ 1.2231, -3.7771],
        [ 0.2083, -1.8721],
        [ 1.1215, -2.7982],
        [ 0.2430, -4.3882],
        [ 1.3484, -2.8183],
        [ 0.6820, -2.4365],
        [ 0.9380, -3.2213],
        [ 1.7885, -3.2859],
        [ 1.4761, -2.9464],
        [ 1.5292, -3.0435],
        [ 1.7201, -3.3402],
        [ 1.5892, -3.3161],
        [ 1.2816, -3.4611],
        [ 0.1863, -2.3737],
        [-0.3821, -4.2331],
        [ 2.1223, -3.5086],
        [-0.4094, -2.0057],
        [ 1.3842, -2.9441],
        [ 0.8876, -2.5013],
        [-0.5252, -1.5634],
        [ 0.5067, -2.8671],
        [ 1.4392, -2.8423],
        [-0.5030, -0.9462],
        [ 1.3396, -3.8962],
        [ 0.3727, -1.8891],
        [-0.0539, -2.0361],
        [ 0.4101, -2.2626],
        [ 0.4063, -4.3522],
        [ 1.3330, -2.8669],
        [ 0.0632, -1.6579],
        [ 1.7213, -3.1083],
        [ 0.4369, -1.8551],
        [ 1.6985, -3.6478],
        [-1.1611, -3.3938],
        [ 0.2410, -2.4074]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.5512e-01, 4.4882e-02],
        [9.9998e-01, 1.5203e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9282, 0.0718], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1964, 0.2595],
         [0.8212, 0.2916]],

        [[0.2738, 0.2288],
         [0.3293, 0.6119]],

        [[0.7169, 0.2579],
         [0.9332, 0.7145]],

        [[0.5261, 0.1269],
         [0.6547, 0.8292]],

        [[0.9976, 0.2276],
         [0.6680, 0.5021]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.00028700959432072443
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 43
Adjusted Rand Index: -0.005513710431743218
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.006509078451524495
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00042302607439050034
Average Adjusted Rand Index: 0.00014167168509211046
11865.947284900833
new:  [-0.00042302607439050034, 0.001182683487797338, -0.00042302607439050034, -0.00042302607439050034] [0.00014167168509211046, 0.0003624016531468207, 0.00014167168509211046, 0.00014167168509211046] [12361.037056831407, 12362.363942140599, 12361.113703782079, 12361.041265067517]
prior:  [0.0, 0.03649746627853757, 0.03649746627853757, -0.00040374138271456197] [0.0, 0.9759973667477564, 0.9759973667477564, 0.0007091372022792591] [nan, 11907.36344489923, 11907.363422620252, 12361.479251737639]
-----------------------------------------------------------------------------------------
This iteration is 9
True Objective function: Loss = -11911.501377209708
Iteration 0: Loss = -25494.258795566162
Iteration 10: Loss = -12450.049815418899
Iteration 20: Loss = -12449.655596107466
Iteration 30: Loss = -12449.077162706215
Iteration 40: Loss = -12448.744402557932
Iteration 50: Loss = -12448.600947551577
Iteration 60: Loss = -12448.524446948177
Iteration 70: Loss = -12448.469271779528
Iteration 80: Loss = -12448.417049451822
Iteration 90: Loss = -12448.356225129794
Iteration 100: Loss = -12448.278603502926
Iteration 110: Loss = -12448.191696330321
Iteration 120: Loss = -12448.126028360273
Iteration 130: Loss = -12448.09593225774
Iteration 140: Loss = -12448.08641988786
Iteration 150: Loss = -12448.085184001913
Iteration 160: Loss = -12448.087919642081
1
Iteration 170: Loss = -12448.092849871704
2
Iteration 180: Loss = -12448.099147676829
3
Stopping early at iteration 179 due to no improvement.
pi: tensor([[0.8417, 0.1583],
        [0.8791, 0.1209]], dtype=torch.float64)
alpha: tensor([0.8486, 0.1514])
beta: tensor([[[0.2086, 0.1850],
         [0.7689, 0.1547]],

        [[0.9839, 0.1743],
         [0.0171, 0.3628]],

        [[0.1786, 0.1951],
         [0.7095, 0.6306]],

        [[0.9403, 0.1619],
         [0.0217, 0.0424]],

        [[0.7116, 0.1829],
         [0.3575, 0.6448]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.001737130633455728
Average Adjusted Rand Index: 1.427637013092014e-05
tensor([[-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25534.754705590094
Iteration 100: Loss = -12454.723007896993
Iteration 200: Loss = -12452.2759029656
Iteration 300: Loss = -12451.38414786792
Iteration 400: Loss = -12450.92686004824
Iteration 500: Loss = -12450.63612588779
Iteration 600: Loss = -12450.423434621243
Iteration 700: Loss = -12450.252898309096
Iteration 800: Loss = -12450.121587364616
Iteration 900: Loss = -12450.035349998134
Iteration 1000: Loss = -12449.98502924094
Iteration 1100: Loss = -12449.95694031845
Iteration 1200: Loss = -12449.938182654025
Iteration 1300: Loss = -12449.924132013528
Iteration 1400: Loss = -12449.912788640591
Iteration 1500: Loss = -12449.903157643144
Iteration 1600: Loss = -12449.894700848456
Iteration 1700: Loss = -12449.88714001181
Iteration 1800: Loss = -12449.880372263355
Iteration 1900: Loss = -12449.882411071369
1
Iteration 2000: Loss = -12449.86851688977
Iteration 2100: Loss = -12449.863246364153
Iteration 2200: Loss = -12449.85846839286
Iteration 2300: Loss = -12449.853917063889
Iteration 2400: Loss = -12449.849632654075
Iteration 2500: Loss = -12449.845623141797
Iteration 2600: Loss = -12449.842124078092
Iteration 2700: Loss = -12449.838130527629
Iteration 2800: Loss = -12449.834532969237
Iteration 2900: Loss = -12449.831024638463
Iteration 3000: Loss = -12449.82762948673
Iteration 3100: Loss = -12449.823982709286
Iteration 3200: Loss = -12449.82024007075
Iteration 3300: Loss = -12449.816385682223
Iteration 3400: Loss = -12449.81218954736
Iteration 3500: Loss = -12449.80763510148
Iteration 3600: Loss = -12449.802403743073
Iteration 3700: Loss = -12449.796440385822
Iteration 3800: Loss = -12449.789236119348
Iteration 3900: Loss = -12449.780647692636
Iteration 4000: Loss = -12449.779372205521
Iteration 4100: Loss = -12449.756728803895
Iteration 4200: Loss = -12449.740892349615
Iteration 4300: Loss = -12449.72325610617
Iteration 4400: Loss = -12449.70735165476
Iteration 4500: Loss = -12449.685628756357
Iteration 4600: Loss = -12449.665379347134
Iteration 4700: Loss = -12449.644213343661
Iteration 4800: Loss = -12449.623174572274
Iteration 4900: Loss = -12449.604123723011
Iteration 5000: Loss = -12449.587973348338
Iteration 5100: Loss = -12449.862421962072
1
Iteration 5200: Loss = -12449.56466212725
Iteration 5300: Loss = -12449.556878355159
Iteration 5400: Loss = -12449.550901618948
Iteration 5500: Loss = -12449.548687100412
Iteration 5600: Loss = -12449.542939949673
Iteration 5700: Loss = -12449.538577349287
Iteration 5800: Loss = -12449.566137541478
1
Iteration 5900: Loss = -12449.533187661358
Iteration 6000: Loss = -12449.576432800506
1
Iteration 6100: Loss = -12449.528945815526
Iteration 6200: Loss = -12449.535681584704
1
Iteration 6300: Loss = -12449.525480446488
Iteration 6400: Loss = -12449.524020597413
Iteration 6500: Loss = -12449.52290410903
Iteration 6600: Loss = -12449.522434384018
Iteration 6700: Loss = -12449.537129454577
1
Iteration 6800: Loss = -12449.571017947157
2
Iteration 6900: Loss = -12449.517711683704
Iteration 7000: Loss = -12449.518537323496
1
Iteration 7100: Loss = -12449.529242207278
2
Iteration 7200: Loss = -12449.51578103836
Iteration 7300: Loss = -12449.514063472898
Iteration 7400: Loss = -12449.513252200797
Iteration 7500: Loss = -12449.521409151028
1
Iteration 7600: Loss = -12449.511604182871
Iteration 7700: Loss = -12449.51094674523
Iteration 7800: Loss = -12449.51043702205
Iteration 7900: Loss = -12449.509805736683
Iteration 8000: Loss = -12449.50918246401
Iteration 8100: Loss = -12449.508731340426
Iteration 8200: Loss = -12449.508219953745
Iteration 8300: Loss = -12449.50805538593
Iteration 8400: Loss = -12449.507385112494
Iteration 8500: Loss = -12449.509283748917
1
Iteration 8600: Loss = -12449.506710155785
Iteration 8700: Loss = -12449.528734838541
1
Iteration 8800: Loss = -12449.506115997923
Iteration 8900: Loss = -12449.50581717078
Iteration 9000: Loss = -12449.507022251224
1
Iteration 9100: Loss = -12449.50535833088
Iteration 9200: Loss = -12449.511517429819
1
Iteration 9300: Loss = -12449.50495498071
Iteration 9400: Loss = -12449.571132321464
1
Iteration 9500: Loss = -12449.54164269307
2
Iteration 9600: Loss = -12449.50449348073
Iteration 9700: Loss = -12449.504252999179
Iteration 9800: Loss = -12449.504106388313
Iteration 9900: Loss = -12449.5083582808
1
Iteration 10000: Loss = -12449.50378568942
Iteration 10100: Loss = -12449.515125772716
1
Iteration 10200: Loss = -12449.503613384644
Iteration 10300: Loss = -12449.503404341138
Iteration 10400: Loss = -12449.506168598911
1
Iteration 10500: Loss = -12449.503213912467
Iteration 10600: Loss = -12449.505912494766
1
Iteration 10700: Loss = -12449.509115038496
2
Iteration 10800: Loss = -12449.642879470552
3
Iteration 10900: Loss = -12449.505426693539
4
Iteration 11000: Loss = -12449.503450382303
5
Iteration 11100: Loss = -12449.515775638316
6
Iteration 11200: Loss = -12449.503492029071
7
Iteration 11300: Loss = -12449.502763100374
Iteration 11400: Loss = -12449.502888136432
1
Iteration 11500: Loss = -12449.502424070228
Iteration 11600: Loss = -12449.504174884274
1
Iteration 11700: Loss = -12449.502283705046
Iteration 11800: Loss = -12449.503945880702
1
Iteration 11900: Loss = -12449.502060374738
Iteration 12000: Loss = -12449.510341766256
1
Iteration 12100: Loss = -12449.50191466596
Iteration 12200: Loss = -12449.509384977715
1
Iteration 12300: Loss = -12449.50173453121
Iteration 12400: Loss = -12449.505746966404
1
Iteration 12500: Loss = -12449.501717634143
Iteration 12600: Loss = -12449.50239962604
1
Iteration 12700: Loss = -12449.541641683312
2
Iteration 12800: Loss = -12449.501404643192
Iteration 12900: Loss = -12449.513443168056
1
Iteration 13000: Loss = -12449.505703228648
2
Iteration 13100: Loss = -12449.501142916206
Iteration 13200: Loss = -12449.528358896032
1
Iteration 13300: Loss = -12449.500911051988
Iteration 13400: Loss = -12449.505447227533
1
Iteration 13500: Loss = -12449.500732039834
Iteration 13600: Loss = -12449.511427439777
1
Iteration 13700: Loss = -12449.500632574958
Iteration 13800: Loss = -12449.501320788644
1
Iteration 13900: Loss = -12449.774749337215
2
Iteration 14000: Loss = -12449.500449310945
Iteration 14100: Loss = -12449.565744596906
1
Iteration 14200: Loss = -12449.500242590666
Iteration 14300: Loss = -12449.50026569689
1
Iteration 14400: Loss = -12449.500161943213
Iteration 14500: Loss = -12449.500097893739
Iteration 14600: Loss = -12449.500134361784
1
Iteration 14700: Loss = -12449.500000269509
Iteration 14800: Loss = -12449.499960835825
Iteration 14900: Loss = -12449.504180569957
1
Iteration 15000: Loss = -12449.52690289361
2
Iteration 15100: Loss = -12449.503970102944
3
Iteration 15200: Loss = -12449.510289078306
4
Iteration 15300: Loss = -12449.499950125311
Iteration 15400: Loss = -12449.500029256877
1
Iteration 15500: Loss = -12449.501204536444
2
Iteration 15600: Loss = -12449.502179577376
3
Iteration 15700: Loss = -12449.503967173638
4
Iteration 15800: Loss = -12449.501218455747
5
Iteration 15900: Loss = -12449.499510367496
Iteration 16000: Loss = -12449.499372656472
Iteration 16100: Loss = -12449.50383567683
1
Iteration 16200: Loss = -12449.499278965824
Iteration 16300: Loss = -12449.500327298745
1
Iteration 16400: Loss = -12449.49962192771
2
Iteration 16500: Loss = -12449.499306767788
3
Iteration 16600: Loss = -12449.499861154909
4
Iteration 16700: Loss = -12449.500912906578
5
Iteration 16800: Loss = -12449.49925548512
Iteration 16900: Loss = -12449.499576062071
1
Iteration 17000: Loss = -12449.500153170115
2
Iteration 17100: Loss = -12449.554504995778
3
Iteration 17200: Loss = -12449.286904347837
Iteration 17300: Loss = -12449.293826155399
1
Iteration 17400: Loss = -12449.341331546273
2
Iteration 17500: Loss = -12449.287578929107
3
Iteration 17600: Loss = -12449.286340168435
Iteration 17700: Loss = -12449.288263143053
1
Iteration 17800: Loss = -12449.286344288535
2
Iteration 17900: Loss = -12449.286292146851
Iteration 18000: Loss = -12449.286249125682
Iteration 18100: Loss = -12449.28650263178
1
Iteration 18200: Loss = -12449.286183297423
Iteration 18300: Loss = -12449.28797102359
1
Iteration 18400: Loss = -12449.286187820406
2
Iteration 18500: Loss = -12449.295274085294
3
Iteration 18600: Loss = -12449.282063069253
Iteration 18700: Loss = -12449.280632989858
Iteration 18800: Loss = -12449.279515555001
Iteration 18900: Loss = -12449.517873691086
1
Iteration 19000: Loss = -12449.2821651284
2
Iteration 19100: Loss = -12449.301979627451
3
Iteration 19200: Loss = -12449.273126736794
Iteration 19300: Loss = -12449.272640212272
Iteration 19400: Loss = -12449.274718743332
1
Iteration 19500: Loss = -12449.279461892924
2
Iteration 19600: Loss = -12449.347296126334
3
Iteration 19700: Loss = -12449.267817872373
Iteration 19800: Loss = -12449.266270015793
Iteration 19900: Loss = -12449.318633675173
1
tensor([[-6.0272e+00,  1.4119e+00],
        [-4.8594e+00,  2.4415e-01],
        [-4.0361e+00, -5.7909e-01],
        [-3.7501e+00, -8.6516e-01],
        [-4.1714e+00, -4.4378e-01],
        [-6.3359e+00,  1.7207e+00],
        [-6.1853e+00,  1.5700e+00],
        [-5.6599e+00,  1.0447e+00],
        [-3.6523e+00, -9.6294e-01],
        [-5.6435e+00,  1.0283e+00],
        [-5.2703e+00,  6.5509e-01],
        [-6.4330e+00,  1.8178e+00],
        [-4.2803e+00, -3.3495e-01],
        [-5.9658e+00,  1.3505e+00],
        [-5.8528e+00,  1.2375e+00],
        [-4.6172e+00,  2.0104e-03],
        [-5.0841e+00,  4.6887e-01],
        [-5.3020e+00,  6.8680e-01],
        [-4.4402e+00, -1.7502e-01],
        [-4.4757e+00, -1.3948e-01],
        [-5.9326e+00,  1.3174e+00],
        [-4.1582e+00, -4.5703e-01],
        [-4.8854e+00,  2.7018e-01],
        [-3.3296e+00, -1.2857e+00],
        [-4.8021e+00,  1.8688e-01],
        [-5.1575e+00,  5.4224e-01],
        [-5.2070e+00,  5.9180e-01],
        [-4.6034e+00, -1.1830e-02],
        [-3.1775e+00, -1.4377e+00],
        [-5.7301e+00,  1.1149e+00],
        [-5.3493e+00,  7.3409e-01],
        [-4.4240e+00, -1.9126e-01],
        [-4.7208e+00,  1.0558e-01],
        [-4.3283e+00, -2.8696e-01],
        [-3.8511e+00, -7.6417e-01],
        [-5.6042e+00,  9.8894e-01],
        [-4.1928e+00, -4.2241e-01],
        [-6.8391e+00,  2.2238e+00],
        [-4.3880e+00, -2.2720e-01],
        [-3.0122e+00, -1.6030e+00],
        [-4.7317e+00,  1.1652e-01],
        [-4.9223e+00,  3.0708e-01],
        [-4.4906e+00, -1.2466e-01],
        [-4.9724e+00,  3.5722e-01],
        [-6.4109e+00,  1.7957e+00],
        [-3.3977e+00, -1.2175e+00],
        [-3.4210e+00, -1.1942e+00],
        [-5.4889e+00,  8.7371e-01],
        [-3.2149e+00, -1.4003e+00],
        [-4.1489e+00, -4.6636e-01],
        [-5.0284e+00,  4.1322e-01],
        [-3.5553e+00, -1.0599e+00],
        [-4.8445e+00,  2.2926e-01],
        [-4.7281e+00,  1.1285e-01],
        [-4.6038e+00, -1.1382e-02],
        [-4.2817e+00, -3.3353e-01],
        [-5.7226e+00,  1.1073e+00],
        [-5.3021e+00,  6.8692e-01],
        [-5.1005e+00,  4.8525e-01],
        [-4.7451e+00,  1.2984e-01],
        [-6.1162e+00,  1.5010e+00],
        [-4.9124e+00,  2.9721e-01],
        [-5.1538e+00,  5.3854e-01],
        [-3.9856e+00, -6.2962e-01],
        [-4.8805e+00,  2.6529e-01],
        [-4.5965e+00, -1.8676e-02],
        [-2.7262e+00, -1.8891e+00],
        [-5.4697e+00,  8.5448e-01],
        [-4.8380e+00,  2.2273e-01],
        [-4.9863e+00,  3.7107e-01],
        [-6.7858e+00,  2.1705e+00],
        [-6.7538e+00,  2.1386e+00],
        [-5.6649e+00,  1.0497e+00],
        [-5.4942e+00,  8.7897e-01],
        [-4.7771e+00,  1.6185e-01],
        [-5.4634e+00,  8.4815e-01],
        [-4.8732e+00,  2.5798e-01],
        [-5.2599e+00,  6.4463e-01],
        [-5.7002e+00,  1.0849e+00],
        [-5.1879e+00,  5.7271e-01],
        [-4.4424e+00, -1.7281e-01],
        [-4.8692e+00,  2.5399e-01],
        [-3.8427e+00, -7.7252e-01],
        [-2.9807e+00, -1.6345e+00],
        [-3.8130e+00, -8.0221e-01],
        [-4.2726e+00, -3.4261e-01],
        [-4.5124e+00, -1.0285e-01],
        [-2.7355e+00, -1.8798e+00],
        [-4.5669e+00, -4.8273e-02],
        [-5.7370e+00,  1.1217e+00],
        [-5.0631e+00,  4.4783e-01],
        [-4.5840e+00, -3.1262e-02],
        [-4.8505e+00,  2.3523e-01],
        [-4.9884e+00,  3.7315e-01],
        [-4.3792e+00, -2.3600e-01],
        [-5.7130e+00,  1.0978e+00],
        [-3.0745e+00, -1.5407e+00],
        [-4.9943e+00,  3.7910e-01],
        [-3.4188e+00, -1.1964e+00],
        [-5.7308e+00,  1.1156e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 3.2259e-08],
        [9.7970e-01, 2.0300e-02]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0281, 0.9719], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2006, 0.1439],
         [0.7689, 0.2095]],

        [[0.9839, 0.1663],
         [0.0171, 0.3628]],

        [[0.1786, 0.2503],
         [0.7095, 0.6306]],

        [[0.9403, 0.1024],
         [0.0217, 0.0424]],

        [[0.7116, 0.2610],
         [0.3575, 0.6448]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0026952888326661237
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -20832.444464949946
Iteration 10: Loss = -12448.40628214835
Iteration 20: Loss = -12448.18710440751
Iteration 30: Loss = -12448.142656970924
Iteration 40: Loss = -12448.135866338684
Iteration 50: Loss = -12448.139524229393
1
Iteration 60: Loss = -12448.145638833306
2
Iteration 70: Loss = -12448.150215383122
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[0.9301, 0.0699],
        [0.9348, 0.0652]], dtype=torch.float64)
alpha: tensor([0.9316, 0.0684])
beta: tensor([[[0.2059, 0.1633],
         [0.7548, 0.1286]],

        [[0.9747, 0.1612],
         [0.3696, 0.6750]],

        [[0.5721, 0.1940],
         [0.0481, 0.5970]],

        [[0.7532, 0.1456],
         [0.7864, 0.2493]],

        [[0.5567, 0.1487],
         [0.5387, 0.5092]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.0005089183229907909
Global Adjusted Rand Index: 0.0005804508253739887
Average Adjusted Rand Index: -0.0002587066922518387
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20832.05942307483
Iteration 100: Loss = -12516.058372894739
Iteration 200: Loss = -12481.167951090354
Iteration 300: Loss = -12460.661038556329
Iteration 400: Loss = -12456.099766915859
Iteration 500: Loss = -12451.69538135261
Iteration 600: Loss = -12451.049458471565
Iteration 700: Loss = -12450.700761760472
Iteration 800: Loss = -12450.475111061614
Iteration 900: Loss = -12450.308081160403
Iteration 1000: Loss = -12450.18238563953
Iteration 1100: Loss = -12450.080909507871
Iteration 1200: Loss = -12450.001607065054
Iteration 1300: Loss = -12449.936990299328
Iteration 1400: Loss = -12449.882889289043
Iteration 1500: Loss = -12449.840472148799
Iteration 1600: Loss = -12449.806354722354
Iteration 1700: Loss = -12449.780344172505
Iteration 1800: Loss = -12449.760493728541
Iteration 1900: Loss = -12449.75592980842
Iteration 2000: Loss = -12449.732979818798
Iteration 2100: Loss = -12449.72300254263
Iteration 2200: Loss = -12449.714748690358
Iteration 2300: Loss = -12449.708097945282
Iteration 2400: Loss = -12449.701682548934
Iteration 2500: Loss = -12449.696494229267
Iteration 2600: Loss = -12449.719905483387
1
Iteration 2700: Loss = -12449.6884487114
Iteration 2800: Loss = -12449.685322615946
Iteration 2900: Loss = -12449.682531808197
Iteration 3000: Loss = -12449.681076157878
Iteration 3100: Loss = -12449.677919970549
Iteration 3200: Loss = -12449.67595999841
Iteration 3300: Loss = -12450.006015081957
1
Iteration 3400: Loss = -12449.67240735908
Iteration 3500: Loss = -12449.670814482626
Iteration 3600: Loss = -12449.669228868972
Iteration 3700: Loss = -12449.667665003555
Iteration 3800: Loss = -12449.666270073903
Iteration 3900: Loss = -12449.665023509246
Iteration 4000: Loss = -12449.76059525085
1
Iteration 4100: Loss = -12449.662820555872
Iteration 4200: Loss = -12449.661760356767
Iteration 4300: Loss = -12449.660769031054
Iteration 4400: Loss = -12449.659876187798
Iteration 4500: Loss = -12449.658705893778
Iteration 4600: Loss = -12449.657736748135
Iteration 4700: Loss = -12449.661972440641
1
Iteration 4800: Loss = -12449.655557531416
Iteration 4900: Loss = -12449.6543750913
Iteration 5000: Loss = -12449.653189256638
Iteration 5100: Loss = -12449.65160968945
Iteration 5200: Loss = -12449.649365791602
Iteration 5300: Loss = -12449.645271304445
Iteration 5400: Loss = -12449.642830772433
Iteration 5500: Loss = -12449.631207847206
Iteration 5600: Loss = -12449.62280850963
Iteration 5700: Loss = -12449.707766492236
1
Iteration 5800: Loss = -12449.573483685017
Iteration 5900: Loss = -12449.388923177548
Iteration 6000: Loss = -12449.053630179209
Iteration 6100: Loss = -12448.832064139693
Iteration 6200: Loss = -12448.629732823123
Iteration 6300: Loss = -12448.214351774508
Iteration 6400: Loss = -12448.040993750856
Iteration 6500: Loss = -12448.025547925137
Iteration 6600: Loss = -12448.03770016984
1
Iteration 6700: Loss = -12448.014390746184
Iteration 6800: Loss = -12448.011999066253
Iteration 6900: Loss = -12448.01029897095
Iteration 7000: Loss = -12448.008473542388
Iteration 7100: Loss = -12448.796211660054
1
Iteration 7200: Loss = -12448.000816506024
Iteration 7300: Loss = -12447.999791528968
Iteration 7400: Loss = -12447.999204716842
Iteration 7500: Loss = -12447.998739200673
Iteration 7600: Loss = -12447.998297767552
Iteration 7700: Loss = -12447.997903301879
Iteration 7800: Loss = -12448.058810256682
1
Iteration 7900: Loss = -12447.994283018392
Iteration 8000: Loss = -12447.938387454913
Iteration 8100: Loss = -12447.975813089693
1
Iteration 8200: Loss = -12447.919281122462
Iteration 8300: Loss = -12447.919098034843
Iteration 8400: Loss = -12448.626092237662
1
Iteration 8500: Loss = -12447.891792284168
Iteration 8600: Loss = -12447.891259852111
Iteration 8700: Loss = -12447.891178585516
Iteration 8800: Loss = -12447.891543397098
1
Iteration 8900: Loss = -12447.891057554983
Iteration 9000: Loss = -12447.891254625509
1
Iteration 9100: Loss = -12447.891160938954
2
Iteration 9200: Loss = -12447.890937036336
Iteration 9300: Loss = -12447.89088875045
Iteration 9400: Loss = -12447.890761797218
Iteration 9500: Loss = -12447.891692735398
1
Iteration 9600: Loss = -12447.89066608565
Iteration 9700: Loss = -12447.929856618592
1
Iteration 9800: Loss = -12447.890550370745
Iteration 9900: Loss = -12447.89467300188
1
Iteration 10000: Loss = -12447.890256014052
Iteration 10100: Loss = -12447.878058337954
Iteration 10200: Loss = -12447.878006868352
Iteration 10300: Loss = -12448.405043037892
1
Iteration 10400: Loss = -12447.877959805983
Iteration 10500: Loss = -12447.87790901561
Iteration 10600: Loss = -12447.878327802919
1
Iteration 10700: Loss = -12447.877836381853
Iteration 10800: Loss = -12447.877716988225
Iteration 10900: Loss = -12447.872041190312
Iteration 11000: Loss = -12447.868024577545
Iteration 11100: Loss = -12447.86799829961
Iteration 11200: Loss = -12447.871300025809
1
Iteration 11300: Loss = -12447.86794212795
Iteration 11400: Loss = -12447.867937812483
Iteration 11500: Loss = -12447.872126804396
1
Iteration 11600: Loss = -12447.86790139249
Iteration 11700: Loss = -12447.867885612317
Iteration 11800: Loss = -12448.00482117797
1
Iteration 11900: Loss = -12447.86787895057
Iteration 12000: Loss = -12447.867860542723
Iteration 12100: Loss = -12447.87037842103
1
Iteration 12200: Loss = -12447.867834770623
Iteration 12300: Loss = -12447.867805948197
Iteration 12400: Loss = -12447.890501736669
1
Iteration 12500: Loss = -12447.867800115595
Iteration 12600: Loss = -12447.867834445708
1
Iteration 12700: Loss = -12447.872173820244
2
Iteration 12800: Loss = -12447.867767632477
Iteration 12900: Loss = -12447.867784894497
1
Iteration 13000: Loss = -12447.86841313113
2
Iteration 13100: Loss = -12447.867811174976
3
Iteration 13200: Loss = -12447.870566720876
4
Iteration 13300: Loss = -12447.867767984551
5
Iteration 13400: Loss = -12447.869668725389
6
Iteration 13500: Loss = -12447.867766346733
Iteration 13600: Loss = -12447.87468956672
1
Iteration 13700: Loss = -12447.868849322096
2
Iteration 13800: Loss = -12447.867805192422
3
Iteration 13900: Loss = -12447.871348672823
4
Iteration 14000: Loss = -12447.874338569472
5
Iteration 14100: Loss = -12447.867795254557
6
Iteration 14200: Loss = -12447.921761344696
7
Iteration 14300: Loss = -12447.867759777164
Iteration 14400: Loss = -12447.893285854901
1
Iteration 14500: Loss = -12447.86788374555
2
Iteration 14600: Loss = -12447.867915266352
3
Iteration 14700: Loss = -12447.881959797105
4
Iteration 14800: Loss = -12447.86779791539
5
Iteration 14900: Loss = -12447.868259591389
6
Iteration 15000: Loss = -12447.86775028058
Iteration 15100: Loss = -12447.869657921246
1
Iteration 15200: Loss = -12447.889835077673
2
Iteration 15300: Loss = -12447.867778096743
3
Iteration 15400: Loss = -12447.87104727021
4
Iteration 15500: Loss = -12447.868150848042
5
Iteration 15600: Loss = -12447.867888255814
6
Iteration 15700: Loss = -12447.871353006509
7
Iteration 15800: Loss = -12447.88057204262
8
Iteration 15900: Loss = -12447.867771449828
9
Iteration 16000: Loss = -12447.868710137358
10
Stopping early at iteration 16000 due to no improvement.
tensor([[  4.4678,  -5.8584],
        [  4.6997,  -6.1328],
        [  4.8246,  -6.2733],
        [  4.9114,  -6.3863],
        [  4.8312,  -6.3978],
        [  3.9469,  -6.2857],
        [  3.3406,  -7.0325],
        [  4.5033,  -6.0480],
        [  4.6619,  -6.7462],
        [  4.4665,  -6.0542],
        [  4.6182,  -6.1001],
        [  4.4194,  -5.8066],
        [  4.8240,  -6.2829],
        [  4.2012,  -6.0345],
        [  3.8578,  -6.5927],
        [  4.0355,  -6.9873],
        [  4.1389,  -6.8088],
        [  4.4017,  -6.3796],
        [  4.7825,  -6.2199],
        [  3.7467,  -7.2598],
        [  4.4724,  -5.8707],
        [  4.2223,  -7.0005],
        [  6.0829, -10.1869],
        [  4.9921,  -6.6821],
        [  4.6385,  -6.2790],
        [  4.2352,  -6.6481],
        [  3.7308,  -6.9388],
        [  4.8260,  -6.2153],
        [  4.9493,  -6.6574],
        [  4.5672,  -5.9544],
        [  4.5364,  -6.2491],
        [  4.7758,  -6.2550],
        [  4.5232,  -6.4239],
        [  4.3035,  -6.8697],
        [  4.7472,  -6.5724],
        [  3.8995,  -6.6571],
        [  4.9417,  -6.3305],
        [  4.1809,  -5.7732],
        [  3.9811,  -7.1323],
        [  5.1295,  -6.5673],
        [  4.7796,  -6.1704],
        [  4.7132,  -6.1070],
        [  4.7481,  -6.2763],
        [  4.7057,  -6.2082],
        [  4.4680,  -5.9613],
        [  4.1327,  -7.3768],
        [  4.2411,  -7.2629],
        [  4.5984,  -5.9957],
        [  4.8528,  -6.8471],
        [  4.7184,  -6.3873],
        [  4.6417,  -6.2383],
        [  4.9137,  -6.5273],
        [  3.9687,  -6.8533],
        [  4.7905,  -6.2154],
        [  3.7296,  -7.2994],
        [  4.8510,  -6.2933],
        [  4.1182,  -6.4723],
        [  4.5786,  -6.1410],
        [  4.5907,  -6.0201],
        [  3.8431,  -7.0708],
        [  4.3775,  -5.9429],
        [  4.7135,  -6.1005],
        [  4.5559,  -6.2147],
        [  4.6850,  -6.4250],
        [  4.5702,  -6.3717],
        [  4.2744,  -6.7798],
        [  4.8526,  -6.9370],
        [  4.4672,  -6.2205],
        [  4.7002,  -6.2302],
        [  4.8544,  -6.2429],
        [  4.2047,  -5.8737],
        [  4.1638,  -5.8650],
        [  3.9632,  -6.5573],
        [  4.5337,  -6.1367],
        [  4.6499,  -6.2614],
        [  4.3905,  -6.2407],
        [  3.6218,  -7.4212],
        [  3.9127,  -6.8148],
        [  3.3269,  -7.1967],
        [  4.0416,  -6.5800],
        [  4.6707,  -6.3596],
        [  4.4530,  -6.4202],
        [  4.8398,  -6.5405],
        [  4.7437,  -7.0273],
        [  4.9193,  -6.3282],
        [  4.3693,  -6.6487],
        [  4.6483,  -6.3596],
        [  5.1087,  -6.7098],
        [  4.7163,  -6.2751],
        [  4.5473,  -5.9774],
        [  3.9799,  -6.8234],
        [  4.8155,  -6.2714],
        [  4.5219,  -6.4526],
        [  4.7152,  -6.1350],
        [  4.8662,  -6.2869],
        [  4.4306,  -5.8873],
        [  5.2096,  -6.6626],
        [  4.5440,  -6.2755],
        [  5.0420,  -6.4822],
        [  4.2813,  -6.3376]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.9476, 0.0524],
        [0.9931, 0.0069]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9998e-01, 1.9546e-05], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2055, 0.2222],
         [0.7548, 0.1361]],

        [[0.9747, 0.1622],
         [0.3696, 0.6750]],

        [[0.5721, 0.2031],
         [0.0481, 0.5970]],

        [[0.7532, 0.1421],
         [0.7864, 0.2493]],

        [[0.5567, 0.1327],
         [0.5387, 0.5092]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.0005089183229907909
Global Adjusted Rand Index: 0.0005804508253739887
Average Adjusted Rand Index: -0.0002587066922518387
Iteration 0: Loss = -29785.527213353587
Iteration 10: Loss = -12448.795145261874
Iteration 20: Loss = -12448.628318674122
Iteration 30: Loss = -12448.532076236246
Iteration 40: Loss = -12448.474310443486
Iteration 50: Loss = -12448.448800102258
Iteration 60: Loss = -12448.441837938957
Iteration 70: Loss = -12448.442884522608
1
Iteration 80: Loss = -12448.446952568056
2
Iteration 90: Loss = -12448.452134586714
3
Stopping early at iteration 89 due to no improvement.
pi: tensor([[0.6531, 0.3469],
        [0.7082, 0.2918]], dtype=torch.float64)
alpha: tensor([0.6725, 0.3275])
beta: tensor([[[0.2126, 0.1972],
         [0.4461, 0.1751]],

        [[0.5005, 0.1897],
         [0.0898, 0.8808]],

        [[0.8052, 0.2009],
         [0.1543, 0.0431]],

        [[0.4336, 0.1811],
         [0.9164, 0.9368]],

        [[0.3546, 0.1957],
         [0.8670, 0.3436]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.023323334729290386
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.002236184874153835
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: -0.00491035742834304
time is 3
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: -0.0020244543554292383
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0015793426848825644
Global Adjusted Rand Index: 0.0016551584352660462
Average Adjusted Rand Index: 0.0031463361512493676
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -29785.584139219456
Iteration 100: Loss = -12592.512908779521
Iteration 200: Loss = -12506.684307221854
Iteration 300: Loss = -12486.18263340894
Iteration 400: Loss = -12469.510511494644
Iteration 500: Loss = -12459.413061110454
Iteration 600: Loss = -12452.810829213226
Iteration 700: Loss = -12451.870510878955
Iteration 800: Loss = -12451.34660545081
Iteration 900: Loss = -12450.995655980269
Iteration 1000: Loss = -12450.739902664058
Iteration 1100: Loss = -12450.546986917312
Iteration 1200: Loss = -12450.39684700637
Iteration 1300: Loss = -12450.276799999117
Iteration 1400: Loss = -12450.178563888256
Iteration 1500: Loss = -12450.096598279151
Iteration 1600: Loss = -12450.027515065121
Iteration 1700: Loss = -12449.968503262646
Iteration 1800: Loss = -12449.91769734963
Iteration 1900: Loss = -12449.87358636808
Iteration 2000: Loss = -12449.835265681391
Iteration 2100: Loss = -12449.801974663427
Iteration 2200: Loss = -12449.7729635112
Iteration 2300: Loss = -12449.747582187318
Iteration 2400: Loss = -12449.725295347433
Iteration 2500: Loss = -12449.705660083455
Iteration 2600: Loss = -12449.688208153166
Iteration 2700: Loss = -12449.672652185673
Iteration 2800: Loss = -12449.658693387946
Iteration 2900: Loss = -12449.646188188646
Iteration 3000: Loss = -12449.634875896285
Iteration 3100: Loss = -12449.624671437357
Iteration 3200: Loss = -12449.618417043943
Iteration 3300: Loss = -12449.606848488616
Iteration 3400: Loss = -12449.599090003472
Iteration 3500: Loss = -12449.594818668515
Iteration 3600: Loss = -12449.5855366942
Iteration 3700: Loss = -12449.579554937516
Iteration 3800: Loss = -12449.57404695366
Iteration 3900: Loss = -12449.56942783865
Iteration 4000: Loss = -12449.56434110007
Iteration 4100: Loss = -12449.56001187266
Iteration 4200: Loss = -12449.838557449171
1
Iteration 4300: Loss = -12449.55229028693
Iteration 4400: Loss = -12449.548779419722
Iteration 4500: Loss = -12449.545564287335
Iteration 4600: Loss = -12449.56675070415
1
Iteration 4700: Loss = -12449.539765049267
Iteration 4800: Loss = -12449.53714775649
Iteration 4900: Loss = -12449.562008487472
1
Iteration 5000: Loss = -12449.533093293378
Iteration 5100: Loss = -12449.530365012986
Iteration 5200: Loss = -12449.528387828232
Iteration 5300: Loss = -12449.646795548655
1
Iteration 5400: Loss = -12449.524796254978
Iteration 5500: Loss = -12449.523090120394
Iteration 5600: Loss = -12449.55562603815
1
Iteration 5700: Loss = -12449.52014908484
Iteration 5800: Loss = -12449.51876903958
Iteration 5900: Loss = -12449.517958418457
Iteration 6000: Loss = -12449.516453256021
Iteration 6100: Loss = -12449.515180770208
Iteration 6200: Loss = -12449.514105340253
Iteration 6300: Loss = -12449.515028947995
1
Iteration 6400: Loss = -12449.512210093606
Iteration 6500: Loss = -12449.511367804804
Iteration 6600: Loss = -12449.531897533292
1
Iteration 6700: Loss = -12449.509802944296
Iteration 6800: Loss = -12449.509063113966
Iteration 6900: Loss = -12449.516607011032
1
Iteration 7000: Loss = -12449.507880288606
Iteration 7100: Loss = -12449.709797104088
1
Iteration 7200: Loss = -12449.50676825875
Iteration 7300: Loss = -12449.506266060198
Iteration 7400: Loss = -12449.541982953808
1
Iteration 7500: Loss = -12449.505429624614
Iteration 7600: Loss = -12449.505284780651
Iteration 7700: Loss = -12449.504634652676
Iteration 7800: Loss = -12449.50430590957
Iteration 7900: Loss = -12449.512099395824
1
Iteration 8000: Loss = -12449.503653980593
Iteration 8100: Loss = -12449.506630826872
1
Iteration 8200: Loss = -12449.503089145634
Iteration 8300: Loss = -12449.502809096517
Iteration 8400: Loss = -12449.771119681209
1
Iteration 8500: Loss = -12449.502352302832
Iteration 8600: Loss = -12449.502519688627
1
Iteration 8700: Loss = -12449.501948201452
Iteration 8800: Loss = -12449.501656321756
Iteration 8900: Loss = -12449.513440240613
1
Iteration 9000: Loss = -12449.501164335745
Iteration 9100: Loss = -12449.50098812672
Iteration 9200: Loss = -12449.500961966565
Iteration 9300: Loss = -12449.502898270344
1
Iteration 9400: Loss = -12449.508810622117
2
Iteration 9500: Loss = -12449.498991825149
Iteration 9600: Loss = -12449.497476639446
Iteration 9700: Loss = -12449.496448707747
Iteration 9800: Loss = -12449.492787341607
Iteration 9900: Loss = -12449.493018133913
1
Iteration 10000: Loss = -12449.49181916413
Iteration 10100: Loss = -12449.491698559284
Iteration 10200: Loss = -12449.489313299195
Iteration 10300: Loss = -12449.4941326611
1
Iteration 10400: Loss = -12449.477256238946
Iteration 10500: Loss = -12449.470520851812
Iteration 10600: Loss = -12449.463887234135
Iteration 10700: Loss = -12449.461297287564
Iteration 10800: Loss = -12449.456308271456
Iteration 10900: Loss = -12449.43823838554
Iteration 11000: Loss = -12449.40740061954
Iteration 11100: Loss = -12449.363928053419
Iteration 11200: Loss = -12449.347352986628
Iteration 11300: Loss = -12449.322893290506
Iteration 11400: Loss = -12449.317540968992
Iteration 11500: Loss = -12449.290565204994
Iteration 11600: Loss = -12449.262539498204
Iteration 11700: Loss = -12449.199946931196
Iteration 11800: Loss = -12449.184132434633
Iteration 11900: Loss = -12449.136380064347
Iteration 12000: Loss = -12449.036382700971
Iteration 12100: Loss = -12449.009074700274
Iteration 12200: Loss = -12448.89876138136
Iteration 12300: Loss = -12448.855803136206
Iteration 12400: Loss = -12448.778786491768
Iteration 12500: Loss = -12448.741030618667
Iteration 12600: Loss = -12448.688731549837
Iteration 12700: Loss = -12448.612637311378
Iteration 12800: Loss = -12448.534706262502
Iteration 12900: Loss = -12448.5451127277
1
Iteration 13000: Loss = -12448.652317945749
2
Iteration 13100: Loss = -12448.446205922599
Iteration 13200: Loss = -12448.340975765226
Iteration 13300: Loss = -12448.33784234038
Iteration 13400: Loss = -12448.257689892329
Iteration 13500: Loss = -12448.188208398251
Iteration 13600: Loss = -12448.187235136063
Iteration 13700: Loss = -12448.182908655972
Iteration 13800: Loss = -12448.203627926876
1
Iteration 13900: Loss = -12448.182244496113
Iteration 14000: Loss = -12448.187030723566
1
Iteration 14100: Loss = -12448.181752109536
Iteration 14200: Loss = -12448.182227831912
1
Iteration 14300: Loss = -12448.181415304341
Iteration 14400: Loss = -12448.183771974916
1
Iteration 14500: Loss = -12448.194434656056
2
Iteration 14600: Loss = -12448.248269004216
3
Iteration 14700: Loss = -12448.181323027397
Iteration 14800: Loss = -12448.216520810767
1
Iteration 14900: Loss = -12448.181290989496
Iteration 15000: Loss = -12448.320999412555
1
Iteration 15100: Loss = -12448.185219988689
2
Iteration 15200: Loss = -12448.182808307733
3
Iteration 15300: Loss = -12448.181262743476
Iteration 15400: Loss = -12448.181284825581
1
Iteration 15500: Loss = -12448.181487525593
2
Iteration 15600: Loss = -12448.215332567375
3
Iteration 15700: Loss = -12448.198954776331
4
Iteration 15800: Loss = -12448.181302864614
5
Iteration 15900: Loss = -12448.181271378484
6
Iteration 16000: Loss = -12448.181429229982
7
Iteration 16100: Loss = -12448.182758704233
8
Iteration 16200: Loss = -12448.181355967767
9
Iteration 16300: Loss = -12448.182954982925
10
Stopping early at iteration 16300 due to no improvement.
tensor([[ 4.2283, -7.5145],
        [ 4.9847, -7.2089],
        [ 5.4207, -6.8432],
        [ 5.3021, -7.3550],
        [ 4.7127, -7.5681],
        [ 4.4759, -7.1835],
        [ 4.6480, -7.0370],
        [ 5.2139, -6.6032],
        [ 6.3299, -9.2794],
        [ 5.4923, -8.5884],
        [ 4.5341, -7.4358],
        [ 5.1266, -6.5130],
        [ 5.2985, -7.0269],
        [ 5.0085, -6.6016],
        [ 4.5809, -7.1849],
        [ 5.4456, -6.8497],
        [ 4.5396, -7.3821],
        [ 5.2410, -6.7103],
        [ 4.5774, -7.8167],
        [ 5.1646, -7.0729],
        [ 5.1302, -6.5737],
        [ 6.5825, -7.9883],
        [ 5.1522, -7.0411],
        [ 5.5692, -6.9565],
        [ 4.8100, -7.3255],
        [ 5.1120, -6.9038],
        [ 5.2675, -6.6873],
        [ 4.7807, -7.3666],
        [ 5.5125, -7.1777],
        [ 5.2316, -6.6180],
        [ 5.1642, -6.7111],
        [ 5.2919, -7.0318],
        [ 5.6238, -7.0108],
        [ 5.3136, -6.9017],
        [ 5.9886, -7.4355],
        [ 5.2290, -6.6240],
        [ 5.4227, -7.0741],
        [ 4.5991, -6.7602],
        [ 5.4306, -7.0171],
        [ 5.0266, -7.9658],
        [ 5.4032, -6.8042],
        [ 5.3660, -6.7904],
        [ 5.0685, -7.3048],
        [ 5.5622, -7.9319],
        [ 4.8819, -6.6482],
        [ 5.1169, -9.0838],
        [ 4.5967, -7.8560],
        [ 5.1066, -6.7208],
        [ 5.6534, -7.1161],
        [ 5.2674, -7.1772],
        [ 5.2134, -6.8037],
        [ 5.6197, -7.0062],
        [ 5.3213, -6.8342],
        [ 4.8795, -7.1968],
        [ 4.8850, -7.4925],
        [ 5.3878, -7.0412],
        [ 5.2333, -6.6333],
        [ 5.3141, -6.7671],
        [ 5.2978, -6.6849],
        [ 6.0736, -7.5541],
        [ 5.1057, -6.6427],
        [ 5.2337, -6.8089],
        [ 4.3736, -7.5608],
        [ 5.4475, -6.8343],
        [ 4.7646, -7.3666],
        [ 4.9099, -7.4253],
        [ 5.5061, -7.5215],
        [ 5.2563, -6.6533],
        [ 4.1737, -7.9319],
        [ 5.2416, -6.8044],
        [ 4.1651, -7.2185],
        [ 4.9507, -6.4809],
        [ 6.5571, -8.1221],
        [ 5.2619, -6.6899],
        [ 6.4855, -7.8719],
        [ 5.2954, -6.6818],
        [ 4.7004, -7.3410],
        [ 5.3275, -6.7177],
        [ 5.2469, -6.6332],
        [ 5.1603, -6.7538],
        [ 5.4330, -6.8470],
        [ 5.3568, -6.7448],
        [ 5.5411, -7.0061],
        [ 5.5757, -7.0057],
        [ 4.9813, -7.5524],
        [ 5.1636, -7.0620],
        [ 4.7763, -7.3399],
        [ 6.1161, -7.8716],
        [ 6.6095, -8.2129],
        [ 5.2493, -6.6483],
        [ 5.3724, -6.7832],
        [ 5.4419, -6.8591],
        [ 6.2647, -7.7028],
        [ 3.7194, -8.3346],
        [ 5.3885, -7.0067],
        [ 5.1101, -6.6027],
        [ 5.3894, -7.1833],
        [ 4.9111, -7.2546],
        [ 4.2445, -8.6018],
        [ 4.7467, -7.1228]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.0305, 0.9695],
        [0.0678, 0.9322]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9999e-01, 5.1144e-06], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2056, 0.2211],
         [0.4461, 0.2021]],

        [[0.5005, 0.1670],
         [0.0898, 0.8808]],

        [[0.8052, 0.2184],
         [0.1543, 0.0431]],

        [[0.4336, 0.1514],
         [0.9164, 0.9368]],

        [[0.3546, 0.2106],
         [0.8670, 0.3436]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0026952888326661237
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -24942.935459995213
Iteration 10: Loss = -12450.15218956176
Iteration 20: Loss = -12450.15218956176
1
Iteration 30: Loss = -12450.15218956176
2
Iteration 40: Loss = -12450.15218956176
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[1.0000e+00, 7.5877e-18],
        [1.0000e+00, 4.1601e-16]], dtype=torch.float64)
alpha: tensor([1.0000e+00, 7.3188e-18])
beta: tensor([[[0.1999, 0.2391],
         [0.9228, 0.1436]],

        [[0.6642, 0.1530],
         [0.9885, 0.2999]],

        [[0.7318, 0.2319],
         [0.7182, 0.0127]],

        [[0.9095, 0.1126],
         [0.8629, 0.6041]],

        [[0.6760, 0.2294],
         [0.1640, 0.7745]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24942.13580020815
Iteration 100: Loss = -12469.541121326962
Iteration 200: Loss = -12457.345116043762
Iteration 300: Loss = -12452.77806146332
Iteration 400: Loss = -12451.680413558603
Iteration 500: Loss = -12450.99275374423
Iteration 600: Loss = -12450.547684745998
Iteration 700: Loss = -12450.318856158572
Iteration 800: Loss = -12450.163312703808
Iteration 900: Loss = -12450.04579014406
Iteration 1000: Loss = -12449.958120015597
Iteration 1100: Loss = -12449.888444194246
Iteration 1200: Loss = -12449.82804755191
Iteration 1300: Loss = -12449.774252427858
Iteration 1400: Loss = -12449.724849164166
Iteration 1500: Loss = -12449.678630709364
Iteration 1600: Loss = -12449.63595071916
Iteration 1700: Loss = -12449.596689521066
Iteration 1800: Loss = -12449.55989851207
Iteration 1900: Loss = -12449.52510251346
Iteration 2000: Loss = -12449.49160685519
Iteration 2100: Loss = -12449.45899137043
Iteration 2200: Loss = -12449.426471542229
Iteration 2300: Loss = -12449.393878548171
Iteration 2400: Loss = -12449.360278876859
Iteration 2500: Loss = -12449.325062353862
Iteration 2600: Loss = -12449.288158819623
Iteration 2700: Loss = -12449.24919267901
Iteration 2800: Loss = -12449.207948094023
Iteration 2900: Loss = -12449.164487347054
Iteration 3000: Loss = -12449.118791761086
Iteration 3100: Loss = -12449.071014129106
Iteration 3200: Loss = -12449.02182314341
Iteration 3300: Loss = -12448.972729022633
Iteration 3400: Loss = -12448.92332823923
Iteration 3500: Loss = -12448.87396296786
Iteration 3600: Loss = -12448.824379185096
Iteration 3700: Loss = -12448.774377275944
Iteration 3800: Loss = -12448.724401023417
Iteration 3900: Loss = -12448.672055877068
Iteration 4000: Loss = -12448.618064184742
Iteration 4100: Loss = -12449.106275794138
1
Iteration 4200: Loss = -12448.500717236508
Iteration 4300: Loss = -12448.434877448777
Iteration 4400: Loss = -12448.354689487114
Iteration 4500: Loss = -12448.25852222424
Iteration 4600: Loss = -12448.12945163378
Iteration 4700: Loss = -12447.975830592555
Iteration 4800: Loss = -12447.849982403615
Iteration 4900: Loss = -12447.786937967498
Iteration 5000: Loss = -12447.690718740248
Iteration 5100: Loss = -12447.387618050874
Iteration 5200: Loss = -12447.338120724226
Iteration 5300: Loss = -12447.316520081227
Iteration 5400: Loss = -12447.307148032234
Iteration 5500: Loss = -12447.405124247285
1
Iteration 5600: Loss = -12447.28141998362
Iteration 5700: Loss = -12447.274414546746
Iteration 5800: Loss = -12447.269644718039
Iteration 5900: Loss = -12447.268563511336
Iteration 6000: Loss = -12447.2615736293
Iteration 6100: Loss = -12447.258778220477
Iteration 6200: Loss = -12447.260326152016
1
Iteration 6300: Loss = -12447.256558333625
Iteration 6400: Loss = -12447.25022009302
Iteration 6500: Loss = -12447.248522777787
Iteration 6600: Loss = -12447.312581748807
1
Iteration 6700: Loss = -12447.334221647026
2
Iteration 6800: Loss = -12447.241841491072
Iteration 6900: Loss = -12447.314806572089
1
Iteration 7000: Loss = -12447.229913187019
Iteration 7100: Loss = -12447.29806592785
1
Iteration 7200: Loss = -12447.193341447042
Iteration 7300: Loss = -12447.166990078404
Iteration 7400: Loss = -12447.038565932575
Iteration 7500: Loss = -11976.139125976662
Iteration 7600: Loss = -11976.102577674117
Iteration 7700: Loss = -11976.089940781287
Iteration 7800: Loss = -11963.04979262345
Iteration 7900: Loss = -11955.772545191017
Iteration 8000: Loss = -11955.670127447142
Iteration 8100: Loss = -11944.342882240337
Iteration 8200: Loss = -11944.35532647619
1
Iteration 8300: Loss = -11944.340174734567
Iteration 8400: Loss = -11944.338045618599
Iteration 8500: Loss = -11944.337160452844
Iteration 8600: Loss = -11944.334547513909
Iteration 8700: Loss = -11944.327641867734
Iteration 8800: Loss = -11928.233035875377
Iteration 8900: Loss = -11928.229363392948
Iteration 9000: Loss = -11928.226517316894
Iteration 9100: Loss = -11928.245994542252
1
Iteration 9200: Loss = -11928.22519208436
Iteration 9300: Loss = -11928.224906406982
Iteration 9400: Loss = -11928.223985113804
Iteration 9500: Loss = -11928.221309037557
Iteration 9600: Loss = -11928.222272416197
1
Iteration 9700: Loss = -11928.28696724439
2
Iteration 9800: Loss = -11928.227603722895
3
Iteration 9900: Loss = -11920.941627668815
Iteration 10000: Loss = -11920.942481671686
1
Iteration 10100: Loss = -11920.939779356846
Iteration 10200: Loss = -11920.954314838978
1
Iteration 10300: Loss = -11920.95274445278
2
Iteration 10400: Loss = -11920.937873003972
Iteration 10500: Loss = -11920.407866037485
Iteration 10600: Loss = -11920.42512297792
1
Iteration 10700: Loss = -11920.412948478028
2
Iteration 10800: Loss = -11920.409736564372
3
Iteration 10900: Loss = -11920.420348915086
4
Iteration 11000: Loss = -11920.436966198069
5
Iteration 11100: Loss = -11920.405453495736
Iteration 11200: Loss = -11920.421573734111
1
Iteration 11300: Loss = -11920.399937402528
Iteration 11400: Loss = -11913.408914859407
Iteration 11500: Loss = -11913.405315235008
Iteration 11600: Loss = -11913.570569854628
1
Iteration 11700: Loss = -11913.403748911618
Iteration 11800: Loss = -11913.410568355483
1
Iteration 11900: Loss = -11913.496010688767
2
Iteration 12000: Loss = -11913.401917706271
Iteration 12100: Loss = -11913.401399667518
Iteration 12200: Loss = -11913.404410843787
1
Iteration 12300: Loss = -11913.409923671197
2
Iteration 12400: Loss = -11913.42430305668
3
Iteration 12500: Loss = -11913.401867138766
4
Iteration 12600: Loss = -11913.401081416672
Iteration 12700: Loss = -11913.426601584826
1
Iteration 12800: Loss = -11913.399969738753
Iteration 12900: Loss = -11913.41117971748
1
Iteration 13000: Loss = -11913.399965103607
Iteration 13100: Loss = -11913.418545893026
1
Iteration 13200: Loss = -11913.397325976075
Iteration 13300: Loss = -11913.394110846122
Iteration 13400: Loss = -11913.390799688028
Iteration 13500: Loss = -11913.403835381627
1
Iteration 13600: Loss = -11913.391622298283
2
Iteration 13700: Loss = -11913.39072139791
Iteration 13800: Loss = -11913.391688659078
1
Iteration 13900: Loss = -11913.437571415598
2
Iteration 14000: Loss = -11913.39723944283
3
Iteration 14100: Loss = -11913.44411250123
4
Iteration 14200: Loss = -11913.42625077882
5
Iteration 14300: Loss = -11913.406367861655
6
Iteration 14400: Loss = -11911.996662706397
Iteration 14500: Loss = -11912.103489189216
1
Iteration 14600: Loss = -11911.791689774689
Iteration 14700: Loss = -11911.562578163222
Iteration 14800: Loss = -11911.561911011602
Iteration 14900: Loss = -11911.569443002105
1
Iteration 15000: Loss = -11911.56276156532
2
Iteration 15100: Loss = -11911.57917646871
3
Iteration 15200: Loss = -11911.560249285967
Iteration 15300: Loss = -11911.570627285704
1
Iteration 15400: Loss = -11911.564135112882
2
Iteration 15500: Loss = -11911.582236844111
3
Iteration 15600: Loss = -11903.340876312406
Iteration 15700: Loss = -11903.320715035574
Iteration 15800: Loss = -11903.310642583814
Iteration 15900: Loss = -11903.308482115028
Iteration 16000: Loss = -11903.307967990626
Iteration 16100: Loss = -11903.311209519308
1
Iteration 16200: Loss = -11903.324125408222
2
Iteration 16300: Loss = -11903.335903168845
3
Iteration 16400: Loss = -11903.313112246675
4
Iteration 16500: Loss = -11903.311823034426
5
Iteration 16600: Loss = -11903.330664272742
6
Iteration 16700: Loss = -11903.31575424147
7
Iteration 16800: Loss = -11903.319833647685
8
Iteration 16900: Loss = -11903.324614915735
9
Iteration 17000: Loss = -11903.309512813075
10
Stopping early at iteration 17000 due to no improvement.
tensor([[  6.6216,  -8.6647],
        [ -9.4119,   7.6771],
        [ -6.1127,   4.7099],
        [ -4.4368,   2.7128],
        [  7.5649,  -9.0492],
        [  8.2308, -11.4428],
        [  7.9375, -11.3519],
        [  7.8323,  -9.4435],
        [ -6.3915,   4.8763],
        [  7.7257,  -9.6699],
        [  6.6653,  -8.0680],
        [  7.3598, -11.7599],
        [ -4.8725,   3.1890],
        [ -8.5470,   7.0498],
        [  8.1480, -11.6416],
        [  7.3578,  -8.8379],
        [  7.6200, -11.3011],
        [  6.3662,  -8.5163],
        [ -7.7331,   5.9435],
        [  7.8109,  -9.7549],
        [  7.2628,  -8.6587],
        [  3.0344,  -4.5020],
        [ -8.8302,   6.7366],
        [-11.1855,   8.1268],
        [  8.1347, -10.3370],
        [  7.3083,  -9.2399],
        [ -8.8149,   6.7767],
        [  4.3628,  -5.9081],
        [ -8.6822,   7.2253],
        [  6.2836, -10.7835],
        [  5.5969,  -8.4967],
        [ -8.5172,   7.1283],
        [ -8.7722,   4.1929],
        [ -9.7077,   8.3146],
        [ -6.7908,   4.0076],
        [  5.5678,  -6.9768],
        [  7.8861,  -9.4355],
        [  8.0006,  -9.6483],
        [  5.5707,  -6.9571],
        [  7.0973,  -8.4908],
        [  7.3002,  -8.7058],
        [  7.1209,  -8.5204],
        [  7.8961,  -9.9538],
        [  6.0289,  -7.9072],
        [  7.0764,  -8.5000],
        [ -9.8691,   8.0498],
        [ -4.7089,   2.4404],
        [-10.7767,   6.7089],
        [ -8.4137,   6.8921],
        [ -7.7076,   5.2013],
        [  6.0066,  -7.5038],
        [ -5.6800,   3.7457],
        [ -8.9395,   7.4694],
        [  7.1839,  -8.6115],
        [  6.4665,  -9.7846],
        [ -5.8743,   4.2036],
        [  7.7806,  -9.2712],
        [  8.0823,  -9.5516],
        [ -7.8014,   6.3752],
        [ -4.2689,   2.8825],
        [  8.4399, -10.3245],
        [ -8.5714,   6.1796],
        [  4.1197,  -5.6882],
        [ -8.7465,   7.1372],
        [  7.9145,  -9.6704],
        [  7.7666,  -9.2318],
        [ -6.8979,   5.3766],
        [ -8.9534,   6.5778],
        [  8.0230, -10.0202],
        [  5.8366,  -7.3755],
        [  7.8857,  -9.5488],
        [  8.3558, -10.1220],
        [  8.0483,  -9.7561],
        [  7.1266, -11.7048],
        [  7.6145, -11.4362],
        [  6.2690,  -7.6555],
        [  8.4270, -10.5413],
        [  7.9755,  -9.3738],
        [  8.1454, -10.5187],
        [ -8.8040,   7.0600],
        [ -8.9663,   7.5686],
        [ -9.0532,   7.2269],
        [ -5.7188,   4.1991],
        [ -6.2727,   4.5358],
        [ -8.9116,   7.4460],
        [ -9.7374,   7.8194],
        [ -7.9162,   6.5070],
        [ -6.3230,   4.9358],
        [ -7.9413,   3.3260],
        [  7.6214,  -9.3685],
        [  7.8491,  -9.2489],
        [  1.6399,  -4.0582],
        [  7.7640,  -9.9679],
        [ -8.5882,   7.1657],
        [  5.5971,  -6.9922],
        [ -4.8576,   3.2050],
        [  7.1001,  -9.0903],
        [ -4.9683,   3.5654],
        [ -6.1008,   3.7983],
        [  7.4613,  -8.8601]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.3306, 0.6694],
        [0.7558, 0.2442]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5682, 0.4318], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3053, 0.1003],
         [0.9228, 0.3052]],

        [[0.6642, 0.0937],
         [0.9885, 0.2999]],

        [[0.7318, 0.0972],
         [0.7182, 0.0127]],

        [[0.9095, 0.0945],
         [0.8629, 0.6041]],

        [[0.6760, 0.1086],
         [0.1640, 0.7745]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.03806970694940257
Average Adjusted Rand Index: 1.0
Iteration 0: Loss = -22234.156311776158
Iteration 10: Loss = -12448.200395275044
Iteration 20: Loss = -12448.139615628355
Iteration 30: Loss = -12448.136387185654
Iteration 40: Loss = -12448.141253870703
1
Iteration 50: Loss = -12448.14735756638
2
Iteration 60: Loss = -12448.150759150094
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.0662, 0.9338],
        [0.0718, 0.9282]], dtype=torch.float64)
alpha: tensor([0.0703, 0.9297])
beta: tensor([[[0.1292, 0.1640],
         [0.9954, 0.2059]],

        [[0.9378, 0.1615],
         [0.7575, 0.4370]],

        [[0.7608, 0.1938],
         [0.8064, 0.0922]],

        [[0.9660, 0.1460],
         [0.1742, 0.0659]],

        [[0.2564, 0.1504],
         [0.4278, 0.1185]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 4
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.0005089183229907909
Global Adjusted Rand Index: 0.0005804508253739887
Average Adjusted Rand Index: -0.0002587066922518387
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22233.618913842372
Iteration 100: Loss = -12474.679711123821
Iteration 200: Loss = -12454.127835150899
Iteration 300: Loss = -12451.315920037317
Iteration 400: Loss = -12450.750592667611
Iteration 500: Loss = -12450.461606425617
Iteration 600: Loss = -12450.293183646934
Iteration 700: Loss = -12450.184446346626
Iteration 800: Loss = -12450.109335612387
Iteration 900: Loss = -12450.05454788225
Iteration 1000: Loss = -12450.013564199275
Iteration 1100: Loss = -12449.981464700955
Iteration 1200: Loss = -12449.95565305318
Iteration 1300: Loss = -12449.934334112551
Iteration 1400: Loss = -12449.91627121498
Iteration 1500: Loss = -12449.900644221856
Iteration 1600: Loss = -12449.88683944066
Iteration 1700: Loss = -12449.87435918114
Iteration 1800: Loss = -12449.862849712863
Iteration 1900: Loss = -12449.851954699398
Iteration 2000: Loss = -12449.841682587414
Iteration 2100: Loss = -12449.83171752618
Iteration 2200: Loss = -12449.82183806558
Iteration 2300: Loss = -12449.811291542042
Iteration 2400: Loss = -12449.798140083612
Iteration 2500: Loss = -12449.777202569407
Iteration 2600: Loss = -12449.757503593875
Iteration 2700: Loss = -12449.728660967254
Iteration 2800: Loss = -12449.679377113926
 10%|         | 10/100 [7:25:17<70:00:37, 2800.41s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 11%|         | 11/100 [8:00:50<64:10:57, 2596.15s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 12%|        | 12/100 [8:54:41<68:11:04, 2789.37s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 13%|        | 13/100 [9:46:35<69:47:04, 2887.64s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 14%|        | 14/100 [10:37:15<70:04:40, 2933.50s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 15%|        | 15/100 [11:26:59<69:37:22, 2948.74s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 16%|        | 16/100 [12:08:09<65:26:41, 2804.78s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 17%|        | 17/100 [12:49:05<62:14:40, 2699.76s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 18%|        | 18/100 [13:37:58<63:05:33, 2769.92s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 19%|        | 19/100 [14:22:23<61:37:02, 2738.55s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
Iteration 2900: Loss = -12449.578259728436
Iteration 3000: Loss = -12449.365787613162
Iteration 3100: Loss = -12449.156132992843
Iteration 3200: Loss = -12449.011471103933
Iteration 3300: Loss = -12448.850042393253
Iteration 3400: Loss = -12448.672203050619
Iteration 3500: Loss = -12448.51153904561
Iteration 3600: Loss = -12448.378297322584
Iteration 3700: Loss = -12448.24276949504
Iteration 3800: Loss = -12448.1058186227
Iteration 3900: Loss = -12448.025851479453
Iteration 4000: Loss = -12447.991435966574
Iteration 4100: Loss = -12447.962645185758
Iteration 4200: Loss = -12447.867648409465
Iteration 4300: Loss = -12447.848296248327
Iteration 4400: Loss = -12447.835786907961
Iteration 4500: Loss = -12447.827996118425
Iteration 4600: Loss = -12447.822695192635
Iteration 4700: Loss = -12447.818353890001
Iteration 4800: Loss = -12447.781068257307
Iteration 4900: Loss = -12447.76483954836
Iteration 5000: Loss = -12447.741274822849
Iteration 5100: Loss = -12447.726166441831
Iteration 5200: Loss = -12447.728511062529
1
Iteration 5300: Loss = -12447.717103009063
Iteration 5400: Loss = -12447.713901965119
Iteration 5500: Loss = -12447.669607994781
Iteration 5600: Loss = -12447.318523550828
Iteration 5700: Loss = -12447.279272702239
Iteration 5800: Loss = -12447.264158273225
Iteration 5900: Loss = -12447.254972680505
Iteration 6000: Loss = -12447.25162593678
Iteration 6100: Loss = -12447.245794580722
Iteration 6200: Loss = -12447.242724493775
Iteration 6300: Loss = -12447.243682500964
1
Iteration 6400: Loss = -12447.246108620606
2
Iteration 6500: Loss = -12447.282995002592
3
Iteration 6600: Loss = -12447.230080137639
Iteration 6700: Loss = -12447.222979029784
Iteration 6800: Loss = -12447.208070254519
Iteration 6900: Loss = -12447.156903969926
Iteration 7000: Loss = -12433.611903055382
Iteration 7100: Loss = -11979.487807317164
Iteration 7200: Loss = -11970.92563022247
Iteration 7300: Loss = -11970.919702612924
Iteration 7400: Loss = -11970.90417096397
Iteration 7500: Loss = -11970.885996739733
Iteration 7600: Loss = -11970.883171603857
Iteration 7700: Loss = -11970.877860808294
Iteration 7800: Loss = -11970.876624447808
Iteration 7900: Loss = -11970.917140273861
1
Iteration 8000: Loss = -11963.342408161274
Iteration 8100: Loss = -11963.338582215238
Iteration 8200: Loss = -11963.33623211823
Iteration 8300: Loss = -11963.332667658193
Iteration 8400: Loss = -11953.916099777722
Iteration 8500: Loss = -11947.338266715282
Iteration 8600: Loss = -11947.285198947162
Iteration 8700: Loss = -11947.28554811167
1
Iteration 8800: Loss = -11946.140692236664
Iteration 8900: Loss = -11939.557871695153
Iteration 9000: Loss = -11939.555118432585
Iteration 9100: Loss = -11939.562937109285
1
Iteration 9200: Loss = -11939.58133567788
2
Iteration 9300: Loss = -11939.550947571706
Iteration 9400: Loss = -11939.584083525258
1
Iteration 9500: Loss = -11939.55030380645
Iteration 9600: Loss = -11939.556023770456
1
Iteration 9700: Loss = -11939.550025204
Iteration 9800: Loss = -11939.55002706077
1
Iteration 9900: Loss = -11939.558729452076
2
Iteration 10000: Loss = -11939.55089868853
3
Iteration 10100: Loss = -11939.547581802413
Iteration 10200: Loss = -11939.547273480426
Iteration 10300: Loss = -11939.544592453462
Iteration 10400: Loss = -11939.545322318258
1
Iteration 10500: Loss = -11939.545272173662
2
Iteration 10600: Loss = -11939.543957365711
Iteration 10700: Loss = -11939.546574022452
1
Iteration 10800: Loss = -11939.546204980696
2
Iteration 10900: Loss = -11939.54581400016
3
Iteration 11000: Loss = -11939.54926591995
4
Iteration 11100: Loss = -11939.555049865554
5
Iteration 11200: Loss = -11939.568381629595
6
Iteration 11300: Loss = -11939.559814325967
7
Iteration 11400: Loss = -11939.547894413798
8
Iteration 11500: Loss = -11939.553658360895
9
Iteration 11600: Loss = -11939.540827580346
Iteration 11700: Loss = -11939.539198636943
Iteration 11800: Loss = -11939.540134247662
1
Iteration 11900: Loss = -11939.543111768686
2
Iteration 12000: Loss = -11939.5603145414
3
Iteration 12100: Loss = -11939.54897960176
4
Iteration 12200: Loss = -11939.547172188168
5
Iteration 12300: Loss = -11939.560309912009
6
Iteration 12400: Loss = -11928.140335690117
Iteration 12500: Loss = -11928.117886265623
Iteration 12600: Loss = -11928.118764862624
1
Iteration 12700: Loss = -11928.113657297943
Iteration 12800: Loss = -11928.105419942853
Iteration 12900: Loss = -11928.131862958104
1
Iteration 13000: Loss = -11928.104354240582
Iteration 13100: Loss = -11928.105628053525
1
Iteration 13200: Loss = -11928.11051844984
2
Iteration 13300: Loss = -11928.146110495603
3
Iteration 13400: Loss = -11928.109643360214
4
Iteration 13500: Loss = -11927.861194217447
Iteration 13600: Loss = -11926.004155317076
Iteration 13700: Loss = -11926.004026022114
Iteration 13800: Loss = -11925.993015950753
Iteration 13900: Loss = -11910.39210395458
Iteration 14000: Loss = -11910.378605001266
Iteration 14100: Loss = -11910.405750557313
1
Iteration 14200: Loss = -11910.388029947064
2
Iteration 14300: Loss = -11910.399890991739
3
Iteration 14400: Loss = -11910.385904716995
4
Iteration 14500: Loss = -11910.378479107856
Iteration 14600: Loss = -11910.392777849256
1
Iteration 14700: Loss = -11910.432258205143
2
Iteration 14800: Loss = -11903.77883361594
Iteration 14900: Loss = -11903.548478289156
Iteration 15000: Loss = -11903.545992065636
Iteration 15100: Loss = -11903.572351483966
1
Iteration 15200: Loss = -11903.558133039709
2
Iteration 15300: Loss = -11903.555015364698
3
Iteration 15400: Loss = -11903.57187741094
4
Iteration 15500: Loss = -11903.581683896478
5
Iteration 15600: Loss = -11903.541319481632
Iteration 15700: Loss = -11903.54530869155
1
Iteration 15800: Loss = -11903.55282389596
2
Iteration 15900: Loss = -11903.598510802572
3
Iteration 16000: Loss = -11903.566578905708
4
Iteration 16100: Loss = -11903.574023168514
5
Iteration 16200: Loss = -11903.56094005565
6
Iteration 16300: Loss = -11903.554193050535
7
Iteration 16400: Loss = -11903.543432072453
8
Iteration 16500: Loss = -11903.544538311353
9
Iteration 16600: Loss = -11903.369601065024
Iteration 16700: Loss = -11903.345471397037
Iteration 16800: Loss = -11903.345381461082
Iteration 16900: Loss = -11903.346526723804
1
Iteration 17000: Loss = -11903.344002492098
Iteration 17100: Loss = -11903.342964184032
Iteration 17200: Loss = -11903.34317017997
1
Iteration 17300: Loss = -11903.34254767231
Iteration 17400: Loss = -11903.343655844095
1
Iteration 17500: Loss = -11903.351225697768
2
Iteration 17600: Loss = -11903.343097584051
3
Iteration 17700: Loss = -11903.343412289987
4
Iteration 17800: Loss = -11903.357984328433
5
Iteration 17900: Loss = -11903.345655942281
6
Iteration 18000: Loss = -11903.353944648434
7
Iteration 18100: Loss = -11903.362032768333
8
Iteration 18200: Loss = -11903.533266686283
9
Iteration 18300: Loss = -11903.342922177855
10
Stopping early at iteration 18300 due to no improvement.
tensor([[ -8.2996,   6.8789],
        [  7.1799,  -9.4394],
        [  4.2971,  -6.5024],
        [  2.5891,  -4.5587],
        [-10.1413,   8.4103],
        [-10.5582,   8.3777],
        [ -9.5244,   8.0246],
        [ -9.7090,   7.7988],
        [  4.7917,  -6.4701],
        [ -9.8017,   8.3307],
        [ -8.9639,   6.3354],
        [-11.0301,   9.4002],
        [  2.9396,  -5.1181],
        [  7.5800,  -8.9712],
        [-10.1391,   8.7380],
        [-10.4043,   9.0148],
        [ -7.2106,   5.8079],
        [ -8.3236,   6.8896],
        [  7.3541,  -8.8985],
        [-10.2997,   8.3235],
        [ -9.4682,   7.6652],
        [-10.2095,   8.6514],
        [  7.3551,  -8.7504],
        [  8.4655, -10.2227],
        [-10.5291,   8.8513],
        [ -9.2740,   7.8877],
        [  6.9058,  -9.4977],
        [ -6.4379,   3.8435],
        [  7.2989,  -9.1096],
        [ -9.4190,   7.4462],
        [ -7.7748,   6.1628],
        [  4.9289,  -6.3403],
        [  5.4957,  -7.1627],
        [  8.3425,  -9.8588],
        [  4.1340,  -6.6591],
        [ -8.2421,   4.3369],
        [-10.0613,   8.4680],
        [ -9.8496,   8.4554],
        [ -7.2602,   5.2847],
        [ -9.6569,   6.9135],
        [ -9.4824,   8.0855],
        [ -8.6938,   7.2784],
        [-10.4148,   8.5885],
        [ -4.9354,   3.5050],
        [ -9.4509,   7.7989],
        [  8.3412, -10.0127],
        [  2.7333,  -4.4137],
        [  6.6793,  -8.4314],
        [  6.0479, -10.6386],
        [  6.2202,  -8.3030],
        [ -7.6603,   6.2637],
        [  3.9858,  -5.4330],
        [  8.4650, -10.7023],
        [ -9.8761,   7.9973],
        [ -9.8443,   8.4368],
        [  4.1287,  -5.7625],
        [ -9.9323,   8.2571],
        [-10.3650,   8.7602],
        [  6.4883,  -7.8791],
        [  2.8806,  -4.2696],
        [-10.4065,   8.6130],
        [  7.0865,  -8.5496],
        [ -5.6404,   4.1689],
        [  6.8213,  -9.4861],
        [-10.2548,   8.7502],
        [-10.1808,   8.7890],
        [  5.0927,  -7.0724],
        [  5.9379, -10.1496],
        [-10.3279,   8.9293],
        [ -6.1523,   4.1162],
        [ -9.9302,   8.4810],
        [-10.7164,   8.0818],
        [-10.6692,   9.1735],
        [ -9.8331,   7.7742],
        [-10.2681,   8.8559],
        [ -8.1338,   5.7834],
        [ -9.3894,   7.8947],
        [-10.2807,   8.8440],
        [-10.3577,   8.8030],
        [  6.8107,  -8.2211],
        [  8.2937,  -9.6983],
        [  7.6332,  -9.1500],
        [  4.1557,  -5.7349],
        [  4.6388,  -6.1501],
        [  7.9969,  -9.4633],
        [  7.8473,  -9.3535],
        [  5.9622,  -8.1181],
        [  4.8792,  -6.3758],
        [  3.3240,  -7.9393],
        [ -9.7195,   8.3137],
        [-10.7651,   8.7026],
        [ -3.6805,   2.0183],
        [-10.4373,   8.6189],
        [  7.1430,  -9.3820],
        [ -7.0121,   5.5497],
        [  3.2903,  -4.7677],
        [-10.0149,   8.3311],
        [  3.5620,  -4.9688],
        [  4.2467,  -5.6476],
        [ -9.2560,   7.6578]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.2435, 0.7565],
        [0.6695, 0.3305]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4300, 0.5700], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3055, 0.1004],
         [0.9954, 0.3050]],

        [[0.9378, 0.0934],
         [0.7575, 0.4370]],

        [[0.7608, 0.0973],
         [0.8064, 0.0922]],

        [[0.9660, 0.0944],
         [0.1742, 0.0659]],

        [[0.2564, 0.1086],
         [0.4278, 0.1185]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.03806970694940257
Average Adjusted Rand Index: 1.0
11911.501377209708
new:  [0.0005804508253739887, -0.0026952888326661237, 0.03806970694940257, 0.03806970694940257] [-0.0002587066922518387, 0.0, 1.0, 1.0] [12447.868710137358, 12448.182954982925, 11903.309512813075, 11903.342922177855]
prior:  [0.0005804508253739887, 0.0016551584352660462, 0.0, 0.0005804508253739887] [-0.0002587066922518387, 0.0031463361512493676, 0.0, -0.0002587066922518387] [12448.150215383122, 12448.452134586714, 12450.15218956176, 12448.150759150094]
-----------------------------------------------------------------------------------------
This iteration is 10
True Objective function: Loss = -11829.937437180504
Iteration 0: Loss = -20581.23854929683
Iteration 10: Loss = -11822.618484308068
Iteration 20: Loss = -11822.618181835413
Iteration 30: Loss = -11822.618178683624
Iteration 40: Loss = -11822.618178683624
1
Iteration 50: Loss = -11822.618178683624
2
Iteration 60: Loss = -11822.618178683624
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.7886, 0.2114],
        [0.2509, 0.7491]], dtype=torch.float64)
alpha: tensor([0.5302, 0.4698])
beta: tensor([[[0.2940, 0.0894],
         [0.0575, 0.2981]],

        [[0.2538, 0.1034],
         [0.8818, 0.1464]],

        [[0.1888, 0.1002],
         [0.3721, 0.4025]],

        [[0.3353, 0.1058],
         [0.9766, 0.7856]],

        [[0.6878, 0.0893],
         [0.0785, 0.2297]]], dtype=torch.float64)
time is 0
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919999730634713
Average Adjusted Rand Index: 0.9919992163297293
tensor([[-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20458.958816740065
Iteration 100: Loss = -12385.546384290728
Iteration 200: Loss = -12379.309131535665
Iteration 300: Loss = -12378.59620155501
Iteration 400: Loss = -12378.486551603299
Iteration 500: Loss = -12378.418445522886
Iteration 600: Loss = -12378.350136839712
Iteration 700: Loss = -12378.285109257988
Iteration 800: Loss = -12378.212926409633
Iteration 900: Loss = -12378.16699737752
Iteration 1000: Loss = -12378.129694264579
Iteration 1100: Loss = -12378.11419922363
Iteration 1200: Loss = -12378.0715957332
Iteration 1300: Loss = -12378.05475542271
Iteration 1400: Loss = -12378.03522347034
Iteration 1500: Loss = -12378.025064925803
Iteration 1600: Loss = -12378.010718078564
Iteration 1700: Loss = -12378.001549144094
Iteration 1800: Loss = -12377.993784913284
Iteration 1900: Loss = -12377.987143572074
Iteration 2000: Loss = -12378.023920842634
1
Iteration 2100: Loss = -12377.976775455301
Iteration 2200: Loss = -12377.972639922971
Iteration 2300: Loss = -12378.017459573122
1
Iteration 2400: Loss = -12377.9659006891
Iteration 2500: Loss = -12377.9631395054
Iteration 2600: Loss = -12377.962003248133
Iteration 2700: Loss = -12377.958547245951
Iteration 2800: Loss = -12377.956649204181
Iteration 2900: Loss = -12377.954923918485
Iteration 3000: Loss = -12377.953787323155
Iteration 3100: Loss = -12377.951984981064
Iteration 3200: Loss = -12377.950726714098
Iteration 3300: Loss = -12377.95051743528
Iteration 3400: Loss = -12377.948541451182
Iteration 3500: Loss = -12377.947651475868
Iteration 3600: Loss = -12377.951188123536
1
Iteration 3700: Loss = -12377.946028329052
Iteration 3800: Loss = -12377.94524521923
Iteration 3900: Loss = -12377.946008482795
1
Iteration 4000: Loss = -12377.943984948619
Iteration 4100: Loss = -12377.94343969626
Iteration 4200: Loss = -12377.944601691965
1
Iteration 4300: Loss = -12377.942468975549
Iteration 4400: Loss = -12377.941984249928
Iteration 4500: Loss = -12377.984456497934
1
Iteration 4600: Loss = -12377.941186733973
Iteration 4700: Loss = -12377.940818815776
Iteration 4800: Loss = -12377.940899289326
1
Iteration 4900: Loss = -12377.940156275718
Iteration 5000: Loss = -12377.939905437768
Iteration 5100: Loss = -12378.147890802536
1
Iteration 5200: Loss = -12377.939355252101
Iteration 5300: Loss = -12377.939140586772
Iteration 5400: Loss = -12377.939289825306
1
Iteration 5500: Loss = -12377.938705699773
Iteration 5600: Loss = -12377.938500680768
Iteration 5700: Loss = -12378.0596851515
1
Iteration 5800: Loss = -12377.938161569453
Iteration 5900: Loss = -12377.937971789268
Iteration 6000: Loss = -12377.938113088607
1
Iteration 6100: Loss = -12377.937704620002
Iteration 6200: Loss = -12377.93758032037
Iteration 6300: Loss = -12377.937552186215
Iteration 6400: Loss = -12377.937389959852
Iteration 6500: Loss = -12377.937296814125
Iteration 6600: Loss = -12377.937100835095
Iteration 6700: Loss = -12377.936971390904
Iteration 6800: Loss = -12377.938102034093
1
Iteration 6900: Loss = -12377.937294925207
2
Iteration 7000: Loss = -12377.944939499344
3
Iteration 7100: Loss = -12377.936641120146
Iteration 7200: Loss = -12377.942800793615
1
Iteration 7300: Loss = -12377.936466308727
Iteration 7400: Loss = -12377.93720363553
1
Iteration 7500: Loss = -12377.936475496712
2
Iteration 7600: Loss = -12377.95331820016
3
Iteration 7700: Loss = -12377.936266424345
Iteration 7800: Loss = -12377.965457021848
1
Iteration 7900: Loss = -12377.936131610182
Iteration 8000: Loss = -12377.962406835715
1
Iteration 8100: Loss = -12377.936054077916
Iteration 8200: Loss = -12377.951823728468
1
Iteration 8300: Loss = -12377.941820069724
2
Iteration 8400: Loss = -12377.944170808209
3
Iteration 8500: Loss = -12377.935835557722
Iteration 8600: Loss = -12377.937714865215
1
Iteration 8700: Loss = -12377.935802951608
Iteration 8800: Loss = -12377.935908920857
1
Iteration 8900: Loss = -12377.935736010217
Iteration 9000: Loss = -12377.935796742353
1
Iteration 9100: Loss = -12377.935945466965
2
Iteration 9200: Loss = -12377.941522332503
3
Iteration 9300: Loss = -12377.938144361678
4
Iteration 9400: Loss = -12377.935850378606
5
Iteration 9500: Loss = -12377.973827048749
6
Iteration 9600: Loss = -12377.935578499622
Iteration 9700: Loss = -12377.936340910162
1
Iteration 9800: Loss = -12377.935490173892
Iteration 9900: Loss = -12377.93711948994
1
Iteration 10000: Loss = -12377.935606093164
2
Iteration 10100: Loss = -12377.945984650045
3
Iteration 10200: Loss = -12377.935699973059
4
Iteration 10300: Loss = -12377.940240696591
5
Iteration 10400: Loss = -12377.935412398458
Iteration 10500: Loss = -12377.943464055921
1
Iteration 10600: Loss = -12377.935397392213
Iteration 10700: Loss = -12377.93553372647
1
Iteration 10800: Loss = -12377.945770313681
2
Iteration 10900: Loss = -12377.935373205399
Iteration 11000: Loss = -12377.93544800915
1
Iteration 11100: Loss = -12377.941911054424
2
Iteration 11200: Loss = -12377.940630742236
3
Iteration 11300: Loss = -12377.93609089546
4
Iteration 11400: Loss = -12377.9354440155
5
Iteration 11500: Loss = -12377.995013748252
6
Iteration 11600: Loss = -12377.935366784519
Iteration 11700: Loss = -12377.935512561446
1
Iteration 11800: Loss = -12377.93615162889
2
Iteration 11900: Loss = -12377.94318803296
3
Iteration 12000: Loss = -12377.935477856017
4
Iteration 12100: Loss = -12377.955653642384
5
Iteration 12200: Loss = -12377.939408808325
6
Iteration 12300: Loss = -12377.935549951639
7
Iteration 12400: Loss = -12377.971251095383
8
Iteration 12500: Loss = -12377.935527323983
9
Iteration 12600: Loss = -12377.935263270414
Iteration 12700: Loss = -12377.936040000988
1
Iteration 12800: Loss = -12377.954559301137
2
Iteration 12900: Loss = -12377.968657172481
3
Iteration 13000: Loss = -12377.978869615494
4
Iteration 13100: Loss = -12377.948478549426
5
Iteration 13200: Loss = -12377.935286735707
6
Iteration 13300: Loss = -12377.940151014582
7
Iteration 13400: Loss = -12377.93890406936
8
Iteration 13500: Loss = -12377.935302150512
9
Iteration 13600: Loss = -12377.936046415416
10
Stopping early at iteration 13600 due to no improvement.
tensor([[-1.8989, -2.7164],
        [-1.3097, -3.3055],
        [-1.9934, -2.6218],
        [-2.3786, -2.2366],
        [ 0.1309, -4.7461],
        [-1.5603, -3.0549],
        [-0.7272, -3.8881],
        [-2.1558, -2.4594],
        [-2.1361, -2.4791],
        [-0.2892, -4.3260],
        [-0.0898, -4.5255],
        [ 0.9479, -5.5631],
        [-1.3432, -3.2720],
        [-1.1047, -3.5105],
        [-1.1827, -3.4325],
        [-0.7064, -3.9088],
        [-1.7246, -2.8906],
        [-0.7693, -3.8459],
        [-0.7801, -3.8351],
        [ 0.9790, -5.5943],
        [-3.0766, -1.5387],
        [-0.0485, -4.5667],
        [-2.3490, -2.2662],
        [ 0.3327, -4.9480],
        [-0.8016, -3.8136],
        [-1.7275, -2.8877],
        [-1.2925, -3.3228],
        [-0.8490, -3.7662],
        [-1.4599, -3.1553],
        [-0.6106, -4.0047],
        [-1.1597, -3.4555],
        [-1.2807, -3.3345],
        [-1.1931, -3.4221],
        [-1.4858, -3.1294],
        [ 1.0770, -5.6922],
        [-1.2114, -3.4038],
        [ 0.5311, -5.1463],
        [-1.2204, -3.3948],
        [-0.5683, -4.0469],
        [-0.0463, -4.5690],
        [-0.9213, -3.6939],
        [ 0.8810, -5.4962],
        [ 1.0808, -5.6960],
        [-1.3557, -3.2595],
        [ 0.2819, -4.8972],
        [ 0.0572, -4.6724],
        [-0.5574, -4.0578],
        [-2.5405, -2.0748],
        [-3.1754, -1.4399],
        [-1.1981, -3.4171],
        [ 0.2196, -4.8348],
        [-0.6432, -3.9720],
        [ 0.0885, -4.7037],
        [-1.0819, -3.5333],
        [-0.1894, -4.4258],
        [-0.8487, -3.7665],
        [-0.4874, -4.1278],
        [ 0.0880, -4.7032],
        [-0.3650, -4.2502],
        [-0.3301, -4.2851],
        [-0.7557, -3.8595],
        [-0.0970, -4.5182],
        [-1.1359, -3.4793],
        [-0.3622, -4.2530],
        [-0.7488, -3.8664],
        [-3.4739, -1.1414],
        [ 0.8733, -5.4885],
        [-2.3010, -2.3142],
        [-2.1132, -2.5020],
        [ 1.1997, -5.8149],
        [ 0.3494, -4.9646],
        [-0.7506, -3.8646],
        [-1.9749, -2.6403],
        [-0.0578, -4.5575],
        [-0.3990, -4.2163],
        [-2.0006, -2.6146],
        [-2.9390, -1.6763],
        [-1.0359, -3.5793],
        [ 0.4769, -5.0922],
        [ 0.0274, -4.6426],
        [-0.3832, -4.2320],
        [-1.7143, -2.9009],
        [ 0.5399, -5.1552],
        [-0.6650, -3.9502],
        [ 0.2767, -4.8919],
        [-1.8629, -2.7523],
        [-3.1856, -1.4296],
        [-1.1336, -3.4816],
        [ 0.2036, -4.8189],
        [-0.5762, -4.0391],
        [-0.1526, -4.4626],
        [-0.6689, -3.9463],
        [-0.3248, -4.2905],
        [-1.4594, -3.1558],
        [-3.5207, -1.0946],
        [-0.6091, -4.0061],
        [-0.8308, -3.7844],
        [-0.8961, -3.7192],
        [-1.1442, -3.4711],
        [ 0.6529, -5.2681]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.9999e-01, 6.2398e-06],
        [5.4228e-06, 9.9999e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8555, 0.1445], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1935, 0.2070],
         [0.0575, 0.2240]],

        [[0.2538, 0.2014],
         [0.8818, 0.1464]],

        [[0.1888, 0.2635],
         [0.3721, 0.4025]],

        [[0.3353, 0.2297],
         [0.9766, 0.7856]],

        [[0.6878, 0.1842],
         [0.0785, 0.2297]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.001983822866076629
time is 1
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.004021803333230736
time is 2
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: -0.02322047458059384
time is 3
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0028334070515487393
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.002918749315918129
Global Adjusted Rand Index: 0.0012201840070994912
Average Adjusted Rand Index: -0.005862288608854119
Iteration 0: Loss = -28974.210733579162
Iteration 10: Loss = -12383.884418098634
Iteration 20: Loss = -12383.884418098642
1
Iteration 30: Loss = -12383.884418098874
2
Iteration 40: Loss = -12383.884418107757
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[1.0000e+00, 3.6000e-13],
        [1.0000e+00, 6.2442e-14]], dtype=torch.float64)
alpha: tensor([1.0000e+00, 3.4500e-13])
beta: tensor([[[0.1980, 0.1463],
         [0.1544, 0.2638]],

        [[0.7135, 0.1900],
         [0.0867, 0.1913]],

        [[0.5732, 0.2985],
         [0.7037, 0.2861]],

        [[0.5162, 0.2677],
         [0.9871, 0.1703]],

        [[0.6330, 0.1480],
         [0.7916, 0.6519]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 62
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -28975.272180893026
Iteration 100: Loss = -12393.02862782744
Iteration 200: Loss = -12387.563019050978
Iteration 300: Loss = -12383.840073140902
Iteration 400: Loss = -12383.244102726609
Iteration 500: Loss = -12382.852554761646
Iteration 600: Loss = -12382.58207112839
Iteration 700: Loss = -12382.351793289095
Iteration 800: Loss = -12382.118539083858
Iteration 900: Loss = -12381.87790934213
Iteration 1000: Loss = -12381.627000989403
Iteration 1100: Loss = -12381.370753195219
Iteration 1200: Loss = -12381.111470414522
Iteration 1300: Loss = -12380.858804084362
Iteration 1400: Loss = -12380.6235024847
Iteration 1500: Loss = -12380.410005108868
Iteration 1600: Loss = -12380.21266844081
Iteration 1700: Loss = -12380.02088206561
Iteration 1800: Loss = -12379.846014731753
Iteration 1900: Loss = -12379.680580108461
Iteration 2000: Loss = -12379.523711628324
Iteration 2100: Loss = -12379.377235626616
Iteration 2200: Loss = -12379.241586461929
Iteration 2300: Loss = -12379.092497394213
Iteration 2400: Loss = -12378.95535885382
Iteration 2500: Loss = -12378.848479032024
Iteration 2600: Loss = -12378.758037980431
Iteration 2700: Loss = -12378.668894362734
Iteration 2800: Loss = -12378.584966107315
Iteration 2900: Loss = -12378.508000865851
Iteration 3000: Loss = -12378.423236493085
Iteration 3100: Loss = -12378.380993709703
Iteration 3200: Loss = -12378.337879757319
Iteration 3300: Loss = -12378.3027195016
Iteration 3400: Loss = -12378.289160589205
Iteration 3500: Loss = -12378.277971827989
Iteration 3600: Loss = -12378.26353821853
Iteration 3700: Loss = -12378.257363498087
Iteration 3800: Loss = -12378.253922551103
Iteration 3900: Loss = -12378.25061275048
Iteration 4000: Loss = -12378.251369475085
1
Iteration 4100: Loss = -12378.245887116964
Iteration 4200: Loss = -12378.244018306628
Iteration 4300: Loss = -12378.242243447876
Iteration 4400: Loss = -12378.240816164809
Iteration 4500: Loss = -12378.239366332686
Iteration 4600: Loss = -12378.237931998268
Iteration 4700: Loss = -12378.236615477761
Iteration 4800: Loss = -12378.235428258391
Iteration 4900: Loss = -12378.234549342067
Iteration 5000: Loss = -12378.233781894261
Iteration 5100: Loss = -12378.242042768945
1
Iteration 5200: Loss = -12378.232817256025
Iteration 5300: Loss = -12378.232329345503
Iteration 5400: Loss = -12378.232065082431
Iteration 5500: Loss = -12378.231535358876
Iteration 5600: Loss = -12378.231297144548
Iteration 5700: Loss = -12378.2309705875
Iteration 5800: Loss = -12378.230476433682
Iteration 5900: Loss = -12378.23017549097
Iteration 6000: Loss = -12378.230919749358
1
Iteration 6100: Loss = -12378.229971159468
Iteration 6200: Loss = -12378.229243256157
Iteration 6300: Loss = -12378.229982679619
1
Iteration 6400: Loss = -12378.228881070158
Iteration 6500: Loss = -12378.228899818414
1
Iteration 6600: Loss = -12378.228606382365
Iteration 6700: Loss = -12378.264478005325
1
Iteration 6800: Loss = -12378.228417828637
Iteration 6900: Loss = -12378.228632695424
1
Iteration 7000: Loss = -12378.22826287225
Iteration 7100: Loss = -12378.23119534056
1
Iteration 7200: Loss = -12378.228137494356
Iteration 7300: Loss = -12378.36338475418
1
Iteration 7400: Loss = -12378.22805061348
Iteration 7500: Loss = -12378.230686394078
1
Iteration 7600: Loss = -12378.227925703502
Iteration 7700: Loss = -12378.233672226108
1
Iteration 7800: Loss = -12378.227824643685
Iteration 7900: Loss = -12378.228116896034
1
Iteration 8000: Loss = -12378.227802354968
Iteration 8100: Loss = -12378.227759395766
Iteration 8200: Loss = -12378.229778758456
1
Iteration 8300: Loss = -12378.227682214028
Iteration 8400: Loss = -12378.22780288077
1
Iteration 8500: Loss = -12378.227739475107
2
Iteration 8600: Loss = -12378.227635166879
Iteration 8700: Loss = -12378.28607978529
1
Iteration 8800: Loss = -12378.227642680098
2
Iteration 8900: Loss = -12378.227672946468
3
Iteration 9000: Loss = -12378.282142812202
4
Iteration 9100: Loss = -12378.227601914834
Iteration 9200: Loss = -12378.227611551445
1
Iteration 9300: Loss = -12378.279420161107
2
Iteration 9400: Loss = -12378.227569945166
Iteration 9500: Loss = -12378.22758982722
1
Iteration 9600: Loss = -12378.229460274373
2
Iteration 9700: Loss = -12378.227579590315
3
Iteration 9800: Loss = -12378.228096289811
4
Iteration 9900: Loss = -12378.227572778274
5
Iteration 10000: Loss = -12378.227597348678
6
Iteration 10100: Loss = -12378.638625201116
7
Iteration 10200: Loss = -12378.227575447394
8
Iteration 10300: Loss = -12378.229504223602
9
Iteration 10400: Loss = -12378.227560044414
Iteration 10500: Loss = -12378.255444180535
1
Iteration 10600: Loss = -12378.24785706516
2
Iteration 10700: Loss = -12378.246650749606
3
Iteration 10800: Loss = -12378.239376858732
4
Iteration 10900: Loss = -12378.229556684746
5
Iteration 11000: Loss = -12378.245570836525
6
Iteration 11100: Loss = -12378.241849221762
7
Iteration 11200: Loss = -12378.230472674151
8
Iteration 11300: Loss = -12378.233681457283
9
Iteration 11400: Loss = -12378.23727774438
10
Stopping early at iteration 11400 due to no improvement.
tensor([[ 2.0982, -4.0725],
        [ 2.4423, -3.9171],
        [ 2.0457, -4.0892],
        [ 1.6952, -4.4279],
        [ 2.0643, -4.2880],
        [ 2.2343, -3.9786],
        [ 2.4485, -3.8471],
        [ 2.3495, -3.7471],
        [ 1.8751, -4.0680],
        [ 2.3737, -3.9026],
        [ 2.4047, -3.9708],
        [ 2.0044, -4.3529],
        [ 2.4856, -3.9811],
        [ 2.1217, -4.1574],
        [ 2.0696, -4.0944],
        [ 2.3448, -3.7572],
        [ 2.1845, -4.1387],
        [ 2.2202, -4.1950],
        [ 0.9585, -5.5737],
        [ 2.6001, -4.0123],
        [ 2.3545, -3.7624],
        [ 2.2206, -3.9161],
        [ 1.3832, -4.8139],
        [ 2.0466, -4.4232],
        [ 1.1647, -5.1933],
        [ 2.2726, -3.9315],
        [ 2.5138, -3.9698],
        [ 2.4324, -3.8527],
        [ 1.9220, -4.2890],
        [ 2.3907, -3.8047],
        [ 2.1227, -3.8534],
        [ 1.8757, -4.3801],
        [ 0.8373, -5.4525],
        [ 2.1686, -3.9586],
        [ 2.3048, -4.2406],
        [ 2.2562, -3.8421],
        [ 2.3828, -4.1700],
        [ 1.4830, -4.5652],
        [ 1.2608, -4.8364],
        [ 2.3072, -4.1226],
        [ 1.5788, -4.6267],
        [ 1.7525, -4.5885],
        [ 2.3420, -3.8752],
        [ 1.3228, -4.8726],
        [ 2.4676, -3.8606],
        [ 1.9820, -4.0984],
        [ 2.2928, -4.1769],
        [ 2.1981, -3.7363],
        [ 2.1664, -3.9209],
        [ 2.1021, -3.8979],
        [ 2.4141, -4.0799],
        [ 2.2524, -4.0024],
        [ 2.4570, -4.0064],
        [ 2.2346, -4.0859],
        [ 1.8441, -4.8487],
        [ 2.6047, -4.0325],
        [ 1.5394, -4.9367],
        [ 1.3724, -5.1017],
        [ 2.3266, -3.8851],
        [ 2.3347, -3.9941],
        [ 2.5159, -3.9194],
        [ 1.3824, -4.9084],
        [ 2.4844, -3.9558],
        [ 2.2798, -3.9717],
        [ 2.3894, -3.8086],
        [ 2.1860, -3.6973],
        [ 2.3380, -4.1908],
        [ 2.3573, -3.7446],
        [ 2.3240, -3.7831],
        [ 1.4098, -4.9628],
        [ 2.5743, -4.0632],
        [ 2.0054, -4.4040],
        [ 2.3656, -3.7719],
        [ 2.2820, -4.0643],
        [ 1.8994, -4.1713],
        [ 1.6171, -4.2592],
        [ 1.9524, -4.3194],
        [ 2.3453, -3.7409],
        [ 2.0166, -4.1826],
        [ 2.4481, -3.9244],
        [ 2.3111, -3.8291],
        [ 2.3866, -3.7730],
        [ 2.1922, -3.7463],
        [ 2.4075, -3.8067],
        [ 2.0777, -4.2868],
        [ 2.2943, -3.8039],
        [ 2.4351, -3.8826],
        [ 2.4700, -3.8584],
        [ 2.2282, -4.1715],
        [ 2.2022, -4.2473],
        [ 0.9083, -5.5235],
        [ 2.4301, -3.8194],
        [ 2.3423, -4.1376],
        [ 2.0704, -4.0595],
        [ 1.9807, -4.3473],
        [ 1.9442, -4.2667],
        [ 1.8658, -4.3265],
        [ 2.2672, -4.2756],
        [ 1.3453, -5.0378],
        [ 2.2482, -4.0559]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.8429, 0.1571],
        [0.6217, 0.3783]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9981, 0.0019], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1908, 0.1955],
         [0.1544, 0.2565]],

        [[0.7135, 0.2146],
         [0.0867, 0.1913]],

        [[0.5732, 0.2395],
         [0.7037, 0.2861]],

        [[0.5162, 0.2236],
         [0.9871, 0.1703]],

        [[0.6330, 0.2009],
         [0.7916, 0.6519]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: -0.02322047458059384
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0017137835707030447
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0011838553159455114
Average Adjusted Rand Index: -0.00430133820197816
Iteration 0: Loss = -20243.861587790045
Iteration 10: Loss = -12380.106288778697
Iteration 20: Loss = -12379.699657208477
Iteration 30: Loss = -12379.563336621946
Iteration 40: Loss = -12379.479621710232
Iteration 50: Loss = -12379.431149219472
Iteration 60: Loss = -12379.409832807733
Iteration 70: Loss = -12379.406805207356
Iteration 80: Loss = -12379.415512049125
1
Iteration 90: Loss = -12379.431651613593
2
Iteration 100: Loss = -12379.45269184374
3
Stopping early at iteration 99 due to no improvement.
pi: tensor([[0.7390, 0.2610],
        [0.7205, 0.2795]], dtype=torch.float64)
alpha: tensor([0.7372, 0.2628])
beta: tensor([[[0.1841, 0.1979],
         [0.1424, 0.2393]],

        [[0.2493, 0.2067],
         [0.0819, 0.6704]],

        [[0.6233, 0.2301],
         [0.7014, 0.1093]],

        [[0.1366, 0.2161],
         [0.2051, 0.3711]],

        [[0.4546, 0.1977],
         [0.5991, 0.4381]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.006738692547152536
time is 2
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.029044706928388838
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.0028641099491338354
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0035057643168196814
Average Adjusted Rand Index: -0.007886424912588723
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20243.71198300484
Iteration 100: Loss = -12412.343546801525
Iteration 200: Loss = -12304.724601277849
Iteration 300: Loss = -12250.298217909805
Iteration 400: Loss = -12229.371239147209
Iteration 500: Loss = -12206.95307002724
Iteration 600: Loss = -12126.14762907884
Iteration 700: Loss = -11889.231385996422
Iteration 800: Loss = -11835.415606109827
Iteration 900: Loss = -11834.96608816982
Iteration 1000: Loss = -11834.760458637986
Iteration 1100: Loss = -11820.937350006476
Iteration 1200: Loss = -11820.858455055873
Iteration 1300: Loss = -11820.802874817276
Iteration 1400: Loss = -11820.761646038545
Iteration 1500: Loss = -11820.729838417923
Iteration 1600: Loss = -11820.704668453185
Iteration 1700: Loss = -11820.684338414947
Iteration 1800: Loss = -11820.667591556707
Iteration 1900: Loss = -11820.653553069817
Iteration 2000: Loss = -11820.641704100852
Iteration 2100: Loss = -11820.631609542752
Iteration 2200: Loss = -11820.622876303652
Iteration 2300: Loss = -11820.615312188302
Iteration 2400: Loss = -11820.608584214926
Iteration 2500: Loss = -11820.602539002593
Iteration 2600: Loss = -11820.596887969066
Iteration 2700: Loss = -11820.59018444398
Iteration 2800: Loss = -11820.58265652693
Iteration 2900: Loss = -11820.578995929334
Iteration 3000: Loss = -11820.57567349921
Iteration 3100: Loss = -11820.572735531847
Iteration 3200: Loss = -11820.570048664553
Iteration 3300: Loss = -11820.567578964698
Iteration 3400: Loss = -11820.56532918206
Iteration 3500: Loss = -11820.563335918045
Iteration 3600: Loss = -11820.56139053679
Iteration 3700: Loss = -11820.559714571149
Iteration 3800: Loss = -11820.55813823694
Iteration 3900: Loss = -11820.556700133224
Iteration 4000: Loss = -11820.555331784853
Iteration 4100: Loss = -11820.554107443593
Iteration 4200: Loss = -11820.552915986847
Iteration 4300: Loss = -11820.551815434153
Iteration 4400: Loss = -11820.55084049835
Iteration 4500: Loss = -11820.549892535166
Iteration 4600: Loss = -11820.549039981202
Iteration 4700: Loss = -11820.548235110313
Iteration 4800: Loss = -11820.547474820509
Iteration 4900: Loss = -11820.546718473539
Iteration 5000: Loss = -11820.546069125192
Iteration 5100: Loss = -11820.54539758963
Iteration 5200: Loss = -11820.544807543063
Iteration 5300: Loss = -11820.544170325422
Iteration 5400: Loss = -11820.543631861437
Iteration 5500: Loss = -11820.5431277268
Iteration 5600: Loss = -11820.54263902428
Iteration 5700: Loss = -11820.54222123246
Iteration 5800: Loss = -11820.54193073652
Iteration 5900: Loss = -11820.541543502375
Iteration 6000: Loss = -11820.541066458345
Iteration 6100: Loss = -11820.540819230962
Iteration 6200: Loss = -11820.540498393442
Iteration 6300: Loss = -11820.546712888725
1
Iteration 6400: Loss = -11820.539905631353
Iteration 6500: Loss = -11820.539788956867
Iteration 6600: Loss = -11820.540477054261
1
Iteration 6700: Loss = -11820.543056207096
2
Iteration 6800: Loss = -11820.542616025583
3
Iteration 6900: Loss = -11820.547262863158
4
Iteration 7000: Loss = -11820.542015062718
5
Iteration 7100: Loss = -11820.538822167166
Iteration 7200: Loss = -11820.540415375082
1
Iteration 7300: Loss = -11820.54067361316
2
Iteration 7400: Loss = -11820.538850933784
3
Iteration 7500: Loss = -11820.552598614306
4
Iteration 7600: Loss = -11820.692693425524
5
Iteration 7700: Loss = -11820.54660665453
6
Iteration 7800: Loss = -11820.53536509662
Iteration 7900: Loss = -11820.528154801003
Iteration 8000: Loss = -11820.526701864914
Iteration 8100: Loss = -11820.526653211384
Iteration 8200: Loss = -11820.526441118473
Iteration 8300: Loss = -11820.52656741148
1
Iteration 8400: Loss = -11820.526253719321
Iteration 8500: Loss = -11820.526569061867
1
Iteration 8600: Loss = -11820.535258813781
2
Iteration 8700: Loss = -11820.52580692277
Iteration 8800: Loss = -11820.526234795268
1
Iteration 8900: Loss = -11820.676650323732
2
Iteration 9000: Loss = -11820.534698036814
3
Iteration 9100: Loss = -11820.532909215257
4
Iteration 9200: Loss = -11820.563431170625
5
Iteration 9300: Loss = -11820.585905201948
6
Iteration 9400: Loss = -11820.528022285416
7
Iteration 9500: Loss = -11820.520737259898
Iteration 9600: Loss = -11820.5208154501
1
Iteration 9700: Loss = -11820.531578165257
2
Iteration 9800: Loss = -11820.529972806395
3
Iteration 9900: Loss = -11820.52086959661
4
Iteration 10000: Loss = -11820.522682376932
5
Iteration 10100: Loss = -11820.522017038767
6
Iteration 10200: Loss = -11820.523109821479
7
Iteration 10300: Loss = -11820.523691665276
8
Iteration 10400: Loss = -11820.5539935022
9
Iteration 10500: Loss = -11820.522205500343
10
Stopping early at iteration 10500 due to no improvement.
tensor([[  6.2388,  -7.8532],
        [  6.8531,  -9.2362],
        [ -7.8036,   5.4671],
        [  5.3715,  -7.2389],
        [ -7.2691,   4.1326],
        [ -8.1944,   6.6522],
        [  7.0027,  -8.5495],
        [ -7.7330,   5.7244],
        [  1.1381,  -3.4066],
        [ -7.3446,   5.8898],
        [  6.5434,  -9.7809],
        [  6.5504,  -8.0527],
        [ -6.4542,   4.4775],
        [ -6.1879,   4.7288],
        [  4.8319,  -6.5054],
        [  6.0963,  -9.4269],
        [ -7.4230,   5.6779],
        [ -7.9850,   6.5987],
        [  3.1367,  -5.2533],
        [ -8.4202,   6.4792],
        [ -9.5217,   6.2570],
        [ -7.2199,   4.8909],
        [  6.7243,  -8.3591],
        [  5.9557,  -7.3420],
        [ -6.8293,   5.4348],
        [  6.4456,  -7.8835],
        [  5.6951,  -8.0622],
        [  5.6872,  -9.2004],
        [ -7.0222,   5.2584],
        [  5.7903,  -9.6689],
        [ -6.1525,   4.7627],
        [ -8.1502,   6.0546],
        [  6.7171,  -8.7833],
        [ -7.3684,   5.9463],
        [  6.5439,  -8.3116],
        [ -8.1279,   6.7192],
        [  2.9616,  -4.4404],
        [ -7.1149,   4.9940],
        [ -5.4170,   4.0230],
        [  3.4007,  -4.9497],
        [ -3.2632,   1.7097],
        [  5.1399,  -9.7551],
        [ -8.0289,   4.4005],
        [  6.9455,  -9.1266],
        [ -8.8297,   6.9704],
        [ -7.7225,   4.6202],
        [ -7.3888,   5.8182],
        [  7.2252,  -8.9182],
        [ -7.8184,   6.3860],
        [  7.1024,  -8.6244],
        [ -7.4897,   5.8378],
        [  6.7844,  -8.4380],
        [ -8.1881,   6.7118],
        [  5.7921,  -7.8335],
        [  4.8585,  -6.4197],
        [ -3.9114,   1.6963],
        [ -8.7514,   5.1516],
        [  5.4605,  -8.1118],
        [ -9.4042,   4.7890],
        [ -8.2620,   6.4242],
        [  2.4404,  -4.9943],
        [ -8.4344,   7.0479],
        [ -7.2662,   5.7664],
        [  3.6129,  -5.3354],
        [  7.2312,  -8.6198],
        [  7.7425,  -9.2416],
        [  4.2277,  -5.6490],
        [ -8.1946,   6.6581],
        [  6.9033,  -8.8229],
        [  6.7519,  -8.3008],
        [ -6.1101,   3.8792],
        [-11.4972,   6.9308],
        [ -7.8442,   5.7391],
        [  6.1955,  -7.5870],
        [  6.3803,  -8.2467],
        [-11.2239,   6.8194],
        [  5.7588,  -8.2747],
        [ -6.2938,   4.6188],
        [  6.0651,  -8.9431],
        [  4.0756,  -7.8030],
        [  6.7778,  -8.1992],
        [  5.8997,  -8.3282],
        [ -7.9224,   6.2788],
        [ -6.4193,   5.0222],
        [  6.5753,  -9.3071],
        [ -6.2596,   1.6872],
        [  7.0023,  -8.4389],
        [ -8.7218,   6.0818],
        [  1.0535,  -2.8536],
        [ -7.8358,   6.3427],
        [  6.7410,  -8.1275],
        [  6.3154,  -7.7097],
        [ -7.8168,   6.3111],
        [ -8.7241,   6.5251],
        [ -7.9495,   6.2494],
        [  7.4625,  -8.8620],
        [ -6.5131,   4.9486],
        [  6.3230,  -8.3895],
        [  6.3227,  -7.7126],
        [  4.1571,  -5.7106]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7534, 0.2466],
        [0.2089, 0.7911]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5093, 0.4907], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3040, 0.0893],
         [0.1424, 0.3003]],

        [[0.2493, 0.1035],
         [0.0819, 0.6704]],

        [[0.6233, 0.1004],
         [0.7014, 0.1093]],

        [[0.1366, 0.1057],
         [0.2051, 0.3711]],

        [[0.4546, 0.0893],
         [0.5991, 0.4381]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 4
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320259550761
Average Adjusted Rand Index: 0.9841601267189862
Iteration 0: Loss = -14011.815538735387
Iteration 10: Loss = -12379.964087111963
Iteration 20: Loss = -12379.615259043392
Iteration 30: Loss = -12379.509150725644
Iteration 40: Loss = -12379.447797092618
Iteration 50: Loss = -12379.416276202797
Iteration 60: Loss = -12379.406321455657
Iteration 70: Loss = -12379.41061769919
1
Iteration 80: Loss = -12379.423995037107
2
Iteration 90: Loss = -12379.443076620337
3
Stopping early at iteration 89 due to no improvement.
pi: tensor([[0.7459, 0.2541],
        [0.7263, 0.2737]], dtype=torch.float64)
alpha: tensor([0.7439, 0.2561])
beta: tensor([[[0.1844, 0.1981],
         [0.5128, 0.2398]],

        [[0.3445, 0.2070],
         [0.7943, 0.5160]],

        [[0.1204, 0.2310],
         [0.1553, 0.1866]],

        [[0.1782, 0.2165],
         [0.7975, 0.0323]],

        [[0.1410, 0.1979],
         [0.3674, 0.1788]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0070073061126442035
time is 2
tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.029044706928388838
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.0028641099491338354
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0034083469664167175
Average Adjusted Rand Index: -0.007940147625687056
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -14011.518440027792
Iteration 100: Loss = -12380.01084831793
Iteration 200: Loss = -12379.797452370212
Iteration 300: Loss = -12379.653314289844
Iteration 400: Loss = -12379.119810227645
Iteration 500: Loss = -12378.44515991189
Iteration 600: Loss = -12378.31589599465
Iteration 700: Loss = -12378.271859002265
Iteration 800: Loss = -12378.255725609595
Iteration 900: Loss = -12378.24854785927
Iteration 1000: Loss = -12378.244345962907
Iteration 1100: Loss = -12378.241319739556
Iteration 1200: Loss = -12378.238951256697
Iteration 1300: Loss = -12378.240624461096
1
Iteration 1400: Loss = -12378.235687371502
Iteration 1500: Loss = -12378.264014949982
1
Iteration 1600: Loss = -12378.233415391818
Iteration 1700: Loss = -12378.232652967046
Iteration 1800: Loss = -12378.23191535846
Iteration 1900: Loss = -12378.231898401133
Iteration 2000: Loss = -12378.230824898452
Iteration 2100: Loss = -12378.232646798366
1
Iteration 2200: Loss = -12378.230010583915
Iteration 2300: Loss = -12378.314509633594
1
Iteration 2400: Loss = -12378.229413351819
Iteration 2500: Loss = -12378.229151298374
Iteration 2600: Loss = -12378.255832211682
1
Iteration 2700: Loss = -12378.228814250147
Iteration 2800: Loss = -12378.22859596483
Iteration 2900: Loss = -12378.235971576096
1
Iteration 3000: Loss = -12378.228337562135
Iteration 3100: Loss = -12378.228186042426
Iteration 3200: Loss = -12378.228124658886
Iteration 3300: Loss = -12378.228017559304
Iteration 3400: Loss = -12378.227922588525
Iteration 3500: Loss = -12378.228207345763
1
Iteration 3600: Loss = -12378.22780255784
Iteration 3700: Loss = -12378.228769164363
1
Iteration 3800: Loss = -12378.227689606449
Iteration 3900: Loss = -12378.22760419471
Iteration 4000: Loss = -12378.477153890202
1
Iteration 4100: Loss = -12378.227553395574
Iteration 4200: Loss = -12378.227503321625
Iteration 4300: Loss = -12378.267210964725
1
Iteration 4400: Loss = -12378.227437060073
Iteration 4500: Loss = -12378.227435282342
Iteration 4600: Loss = -12378.228743507518
1
Iteration 4700: Loss = -12378.227381433264
Iteration 4800: Loss = -12378.227333448427
Iteration 4900: Loss = -12378.240668008353
1
Iteration 5000: Loss = -12378.22729705699
Iteration 5100: Loss = -12378.22731732772
1
Iteration 5200: Loss = -12378.228024603433
2
Iteration 5300: Loss = -12378.227276999352
Iteration 5400: Loss = -12378.227246987033
Iteration 5500: Loss = -12378.227266180662
1
Iteration 5600: Loss = -12378.227237734078
Iteration 5700: Loss = -12378.267750389352
1
Iteration 5800: Loss = -12378.227239520205
2
Iteration 5900: Loss = -12378.22721259578
Iteration 6000: Loss = -12378.227569112909
1
Iteration 6100: Loss = -12378.227246306422
2
Iteration 6200: Loss = -12378.229307983778
3
Iteration 6300: Loss = -12378.227275499026
4
Iteration 6400: Loss = -12378.227214970977
5
Iteration 6500: Loss = -12378.243527125704
6
Iteration 6600: Loss = -12378.227209233575
Iteration 6700: Loss = -12378.227217167318
1
Iteration 6800: Loss = -12378.22776334166
2
Iteration 6900: Loss = -12378.227217668753
3
Iteration 7000: Loss = -12378.227423581095
4
Iteration 7100: Loss = -12378.229047070477
5
Iteration 7200: Loss = -12378.26801861347
6
Iteration 7300: Loss = -12378.227207512116
Iteration 7400: Loss = -12378.228451808198
1
Iteration 7500: Loss = -12378.231541451443
2
Iteration 7600: Loss = -12378.228005747422
3
Iteration 7700: Loss = -12378.228331641629
4
Iteration 7800: Loss = -12378.289319009167
5
Iteration 7900: Loss = -12378.233753138551
6
Iteration 8000: Loss = -12378.2312470956
7
Iteration 8100: Loss = -12378.259012031043
8
Iteration 8200: Loss = -12378.228409903451
9
Iteration 8300: Loss = -12378.228142561667
10
Stopping early at iteration 8300 due to no improvement.
tensor([[ 1.9640, -3.4069],
        [ 1.4198, -4.1386],
        [ 1.9850, -3.3952],
        [ 1.8020, -3.5374],
        [ 1.9868, -3.5682],
        [ 2.0028, -3.3939],
        [ 1.8190, -3.6154],
        [ 0.9094, -4.3597],
        [ 1.0332, -4.0403],
        [ 1.2615, -4.1791],
        [ 1.7893, -3.7862],
        [ 1.7445, -3.7702],
        [ 1.5906, -4.0763],
        [ 1.8324, -3.6647],
        [ 1.9493, -3.3895],
        [ 1.2810, -3.9258],
        [ 1.9183, -3.6051],
        [ 0.9974, -4.6453],
        [ 2.1058, -3.6535],
        [ 1.9857, -3.8510],
        [ 1.1334, -4.1419],
        [ 1.5087, -3.8361],
        [ 1.7470, -3.6416],
        [ 2.1635, -3.5506],
        [ 2.0730, -3.4939],
        [ 0.4729, -4.9121],
        [ 2.1447, -3.5312],
        [ 1.9764, -3.4535],
        [ 1.9058, -3.4795],
        [ 1.9829, -3.3851],
        [ 1.8907, -3.2863],
        [ 1.3633, -4.0686],
        [ 1.8955, -3.5431],
        [ 1.3030, -4.0074],
        [ 1.7979, -3.9717],
        [ 1.8063, -3.4751],
        [ 1.6259, -4.1632],
        [ 1.6976, -3.5073],
        [ 1.5471, -3.7585],
        [ 1.7179, -3.9564],
        [ 1.2540, -4.1169],
        [ 1.2691, -4.2652],
        [ 1.4628, -3.9800],
        [ 1.5079, -3.8712],
        [ 2.0139, -3.5134],
        [ 1.5480, -3.7938],
        [ 1.9555, -3.7233],
        [ 1.7034, -3.3792],
        [ 0.3274, -4.9426],
        [ 1.8540, -3.3113],
        [ 1.8575, -3.8547],
        [ 1.8041, -3.5907],
        [ 2.0104, -3.6544],
        [ 2.0479, -3.4484],
        [ 1.9166, -4.0399],
        [ 1.9307, -3.9576],
        [ 1.5936, -4.1019],
        [ 1.8932, -3.8006],
        [ 1.6511, -3.7681],
        [ 2.0665, -3.4786],
        [ 1.8516, -3.7840],
        [ 2.0675, -3.4668],
        [ 1.9395, -3.6978],
        [ 1.6753, -3.7325],
        [ 1.9822, -3.4073],
        [ 1.7232, -3.2905],
        [ 2.1676, -3.5634],
        [ 1.9425, -3.3414],
        [ 1.2583, -3.9963],
        [ 2.0635, -3.4944],
        [ 2.1817, -3.6972],
        [ 0.5431, -5.0744],
        [ 1.4348, -3.8868],
        [ 2.0198, -3.5085],
        [ 1.4983, -3.7367],
        [ 1.8242, -3.2351],
        [ 1.5950, -3.8762],
        [ 1.9479, -3.3382],
        [ 1.9327, -3.3984],
        [ 2.0862, -3.4759],
        [ 1.2981, -3.9994],
        [ 1.8032, -3.5296],
        [ 1.7239, -3.3722],
        [ 1.9358, -3.4594],
        [ 2.0559, -3.4843],
        [ 1.9237, -3.3402],
        [ 0.7986, -4.6668],
        [ 1.9408, -3.5558],
        [ 2.0297, -3.6037],
        [ 1.7879, -3.9141],
        [ 2.0418, -3.6177],
        [ 1.7729, -3.6156],
        [ 1.8446, -3.8264],
        [ 1.9258, -3.3697],
        [ 2.0451, -3.5107],
        [ 1.3902, -3.9516],
        [ 1.9863, -3.4225],
        [ 1.9169, -3.8379],
        [ 2.0996, -3.4930],
        [ 2.0084, -3.5311]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.8426, 0.1574],
        [0.6196, 0.3804]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9957, 0.0043], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1907, 0.1967],
         [0.5128, 0.2564]],

        [[0.3445, 0.2148],
         [0.7943, 0.5160]],

        [[0.1204, 0.2396],
         [0.1553, 0.1866]],

        [[0.1782, 0.2232],
         [0.7975, 0.0323]],

        [[0.1410, 0.2008],
         [0.3674, 0.1788]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: -0.02322047458059384
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0017137835707030447
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0011838553159455114
Average Adjusted Rand Index: -0.00430133820197816
Iteration 0: Loss = -31683.805494295564
Iteration 10: Loss = -12381.285027272079
Iteration 20: Loss = -12380.256288379493
Iteration 30: Loss = -12379.921196290645
Iteration 40: Loss = -12379.742351305398
Iteration 50: Loss = -12379.60879519071
Iteration 60: Loss = -12379.508333130761
Iteration 70: Loss = -12379.445401033297
Iteration 80: Loss = -12379.414883599224
Iteration 90: Loss = -12379.406597493158
Iteration 100: Loss = -12379.412100377085
1
Iteration 110: Loss = -12379.426422963099
2
Iteration 120: Loss = -12379.44617018731
3
Stopping early at iteration 119 due to no improvement.
pi: tensor([[0.2757, 0.7243],
        [0.2564, 0.7436]], dtype=torch.float64)
alpha: tensor([0.2583, 0.7417])
beta: tensor([[[0.2396, 0.1980],
         [0.1070, 0.1843]],

        [[0.1977, 0.2069],
         [0.9939, 0.9183]],

        [[0.2018, 0.2307],
         [0.8976, 0.3386]],

        [[0.1984, 0.2164],
         [0.9106, 0.3119]],

        [[0.4213, 0.1979],
         [0.9959, 0.3175]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.006738692547152536
time is 2
tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: -0.029044706928388838
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0028641099491338354
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0035057643168196814
Average Adjusted Rand Index: -0.007886424912588723
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -31683.67783702809
Iteration 100: Loss = -12448.35425692121
Iteration 200: Loss = -12385.462466838773
Iteration 300: Loss = -12249.766474093276
Iteration 400: Loss = -12158.04877971526
Iteration 500: Loss = -12085.852311839877
Iteration 600: Loss = -12074.786593862666
Iteration 700: Loss = -12074.075405829035
Iteration 800: Loss = -12073.707758331248
Iteration 900: Loss = -12073.480478942945
Iteration 1000: Loss = -12073.326009387818
Iteration 1100: Loss = -12073.214593092463
Iteration 1200: Loss = -12073.130839948079
Iteration 1300: Loss = -12073.065797559191
Iteration 1400: Loss = -12073.01402807247
Iteration 1500: Loss = -12072.971976967812
Iteration 1600: Loss = -12072.937264753671
Iteration 1700: Loss = -12072.908311078176
Iteration 1800: Loss = -12072.883979582917
Iteration 1900: Loss = -12072.863364196292
Iteration 2000: Loss = -12072.845673330296
Iteration 2100: Loss = -12072.830326550462
Iteration 2200: Loss = -12072.816981478987
Iteration 2300: Loss = -12072.805262388187
Iteration 2400: Loss = -12072.794926166052
Iteration 2500: Loss = -12072.785767554014
Iteration 2600: Loss = -12072.777512997674
Iteration 2700: Loss = -12072.770167125505
Iteration 2800: Loss = -12072.763657592413
Iteration 2900: Loss = -12072.757567056971
Iteration 3000: Loss = -12072.752128452128
Iteration 3100: Loss = -12072.74717316136
Iteration 3200: Loss = -12072.742606178785
Iteration 3300: Loss = -12072.738424279034
Iteration 3400: Loss = -12072.734515798547
Iteration 3500: Loss = -12072.730889205515
Iteration 3600: Loss = -12072.727454807986
Iteration 3700: Loss = -12072.724077265735
Iteration 3800: Loss = -12072.720768167312
Iteration 3900: Loss = -12072.716952972129
Iteration 4000: Loss = -12072.711103766564
Iteration 4100: Loss = -12072.695280795095
Iteration 4200: Loss = -12072.539189082047
Iteration 4300: Loss = -12072.436799950261
Iteration 4400: Loss = -12072.428953232193
Iteration 4500: Loss = -12072.212801403628
Iteration 4600: Loss = -12072.198415799896
Iteration 4700: Loss = -12070.260239767786
Iteration 4800: Loss = -12068.58762864404
Iteration 4900: Loss = -12066.11122926679
Iteration 5000: Loss = -12063.064297774106
Iteration 5100: Loss = -12055.815554858571
Iteration 5200: Loss = -12053.000663221366
Iteration 5300: Loss = -12039.058223606742
Iteration 5400: Loss = -12007.24322463038
Iteration 5500: Loss = -11969.495454842132
Iteration 5600: Loss = -11886.743204816514
Iteration 5700: Loss = -11842.432172462633
Iteration 5800: Loss = -11831.571691443005
Iteration 5900: Loss = -11820.004921967948
Iteration 6000: Loss = -11819.993299581896
Iteration 6100: Loss = -11819.967680804657
Iteration 6200: Loss = -11819.911474752269
Iteration 6300: Loss = -11819.91494616235
1
Iteration 6400: Loss = -11819.919608868879
2
Iteration 6500: Loss = -11819.90415370988
Iteration 6600: Loss = -11819.913016410223
1
Iteration 6700: Loss = -11819.906434566825
2
Iteration 6800: Loss = -11819.899497231314
Iteration 6900: Loss = -11819.89807451262
Iteration 7000: Loss = -11819.896670245964
Iteration 7100: Loss = -11819.893328587732
Iteration 7200: Loss = -11819.885744145016
Iteration 7300: Loss = -11819.884724396905
Iteration 7400: Loss = -11819.884274910284
Iteration 7500: Loss = -11819.88404629263
Iteration 7600: Loss = -11819.886523359035
1
Iteration 7700: Loss = -11819.883248004227
Iteration 7800: Loss = -11819.883121411875
Iteration 7900: Loss = -11819.883570303715
1
Iteration 8000: Loss = -11819.883969424556
2
Iteration 8100: Loss = -11819.88331393687
3
Iteration 8200: Loss = -11819.88203544003
Iteration 8300: Loss = -11819.882034867887
Iteration 8400: Loss = -11819.88281701641
1
Iteration 8500: Loss = -11819.880032402834
Iteration 8600: Loss = -11819.879894423248
Iteration 8700: Loss = -11819.90755296543
1
Iteration 8800: Loss = -11819.878918136506
Iteration 8900: Loss = -11819.884007826025
1
Iteration 9000: Loss = -11819.878645359579
Iteration 9100: Loss = -11819.878621791207
Iteration 9200: Loss = -11819.93719586312
1
Iteration 9300: Loss = -11819.87867380121
2
Iteration 9400: Loss = -11819.878418043214
Iteration 9500: Loss = -11819.878564785879
1
Iteration 9600: Loss = -11819.878233436924
Iteration 9700: Loss = -11819.878097093
Iteration 9800: Loss = -11819.882852925091
1
Iteration 9900: Loss = -11819.878038578505
Iteration 10000: Loss = -11819.877983830742
Iteration 10100: Loss = -11819.885744739073
1
Iteration 10200: Loss = -11819.882147434055
2
Iteration 10300: Loss = -11819.885172374863
3
Iteration 10400: Loss = -11819.887655879176
4
Iteration 10500: Loss = -11819.882624196578
5
Iteration 10600: Loss = -11819.87783060094
Iteration 10700: Loss = -11819.878758057743
1
Iteration 10800: Loss = -11819.88523885659
2
Iteration 10900: Loss = -11819.877770898396
Iteration 11000: Loss = -11819.891799592284
1
Iteration 11100: Loss = -11819.878783250906
2
Iteration 11200: Loss = -11819.880352680852
3
Iteration 11300: Loss = -11819.883188992593
4
Iteration 11400: Loss = -11819.880694230062
5
Iteration 11500: Loss = -11819.907659680046
6
Iteration 11600: Loss = -11819.897874567214
7
Iteration 11700: Loss = -11819.979360648122
8
Iteration 11800: Loss = -11819.911428321528
9
Iteration 11900: Loss = -11819.92114134084
10
Stopping early at iteration 11900 due to no improvement.
tensor([[ -8.8551,   7.4687],
        [ -8.3971,   6.9363],
        [  6.3719,  -7.9107],
        [ -6.8103,   5.1462],
        [  7.4082,  -8.9892],
        [  6.2996,  -7.7283],
        [ -7.9507,   6.3442],
        [  5.9664,  -8.0468],
        [ -3.3183,   1.0368],
        [  5.7948,  -7.3570],
        [-11.0898,   7.1987],
        [ -8.7728,   7.2202],
        [  4.6849,  -6.4181],
        [  3.6392,  -7.4653],
        [ -8.8630,   7.4391],
        [ -8.9186,   7.3858],
        [  7.7821,  -9.3960],
        [  6.9317,  -9.5969],
        [ -8.5591,   6.7477],
        [  6.3749,  -8.0863],
        [  6.9138,  -8.3014],
        [  5.4948,  -6.8819],
        [ -8.2806,   6.7952],
        [ -8.4168,   6.3845],
        [  5.4992,  -6.8872],
        [ -8.4951,   6.9887],
        [ -9.4762,   7.5886],
        [ -8.1311,   6.0497],
        [  5.4680,  -7.0939],
        [ -8.8594,   7.3075],
        [  4.8560,  -6.2497],
        [  6.3236,  -8.7209],
        [ -8.6954,   7.1011],
        [  5.6998,  -7.5396],
        [ -8.3019,   6.7618],
        [  5.9328,  -8.3204],
        [ -4.5636,   2.6482],
        [  5.0674,  -6.7024],
        [  3.5834,  -6.0452],
        [ -7.9851,   5.9148],
        [  0.8412,  -4.3186],
        [-10.1550,   7.0035],
        [  5.4183,  -6.9325],
        [ -8.4018,   7.0121],
        [  6.5717,  -8.3786],
        [  5.1801,  -6.7312],
        [  5.8468,  -7.2339],
        [ -8.6922,   7.2496],
        [  6.7513,  -8.1387],
        [ -9.0644,   7.5475],
        [  5.7836,  -7.2046],
        [ -9.7862,   6.3447],
        [  6.8279,  -8.3514],
        [ -7.8582,   5.7590],
        [ -8.3818,   6.9851],
        [  6.6482,  -8.1152],
        [  5.4628,  -7.1644],
        [ -9.0310,   7.1929],
        [  6.7369,  -8.1250],
        [  5.7811,  -8.2764],
        [ -4.6623,   2.5845],
        [  5.2109,  -6.6098],
        [  6.1344,  -7.5364],
        [ -5.5366,   3.2162],
        [ -9.3050,   7.4163],
        [-10.4120,   5.9857],
        [ -7.6033,   5.9309],
        [  5.8024,  -7.4587],
        [ -8.4356,   7.0068],
        [ -8.8088,   7.4023],
        [  6.6679,  -8.1101],
        [  7.1134,  -8.6910],
        [  6.0930,  -7.8580],
        [ -8.5554,   5.8187],
        [-10.2175,   7.7855],
        [  6.5432,  -8.0492],
        [ -8.7623,   6.5678],
        [  4.7195,  -6.3815],
        [ -8.8230,   7.4365],
        [ -6.6255,   5.0778],
        [ -8.8406,   7.1464],
        [ -7.5749,   6.0710],
        [  5.9258,  -8.0840],
        [  7.5375,  -8.9618],
        [ -8.0929,   6.7046],
        [  3.2799,  -4.8581],
        [ -8.3618,   6.9196],
        [  6.6811,  -8.0752],
        [-11.0925,   6.4773],
        [  6.2222,  -7.7952],
        [ -8.0145,   6.5789],
        [-11.3032,   6.6880],
        [  6.0326,  -7.4738],
        [  6.2289,  -7.6592],
        [  5.0919,  -8.8955],
        [ -8.6204,   6.9796],
        [  6.6593,  -8.8216],
        [ -8.9134,   5.8026],
        [ -8.9092,   6.0549],
        [ -8.3433,   6.5293]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.2567, 0.7433],
        [0.8058, 0.1942]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4902, 0.5098], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2984, 0.0887],
         [0.1070, 0.3040]],

        [[0.1977, 0.1035],
         [0.9939, 0.9183]],

        [[0.2018, 0.1001],
         [0.8976, 0.3386]],

        [[0.1984, 0.1057],
         [0.9106, 0.3119]],

        [[0.4213, 0.0892],
         [0.9959, 0.3175]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 4
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.0413486623640134
Average Adjusted Rand Index: 0.9841601267189862
11829.937437180504
new:  [-0.0011838553159455114, 0.9840320259550761, -0.0011838553159455114, 0.0413486623640134] [-0.00430133820197816, 0.9841601267189862, -0.00430133820197816, 0.9841601267189862] [12378.23727774438, 11820.522205500343, 12378.228142561667, 11819.92114134084]
prior:  [0.0, -0.0035057643168196814, -0.0034083469664167175, -0.0035057643168196814] [0.0, -0.007886424912588723, -0.007940147625687056, -0.007886424912588723] [12383.884418107757, 12379.45269184374, 12379.443076620337, 12379.44617018731]
-----------------------------------------------------------------------------------------
This iteration is 11
True Objective function: Loss = -11938.822284900833
Iteration 0: Loss = -15404.127700263696
Iteration 10: Loss = -12397.003304826863
Iteration 20: Loss = -12396.742724762462
Iteration 30: Loss = -12396.70741725614
Iteration 40: Loss = -12396.70161164285
Iteration 50: Loss = -12396.700400708654
Iteration 60: Loss = -12396.699893497822
Iteration 70: Loss = -12396.699507480527
Iteration 80: Loss = -12396.699164858921
Iteration 90: Loss = -12396.698858564554
Iteration 100: Loss = -12396.698519801648
Iteration 110: Loss = -12396.69817371088
Iteration 120: Loss = -12396.69784168656
Iteration 130: Loss = -12396.697532074668
Iteration 140: Loss = -12396.69718593362
Iteration 150: Loss = -12396.696868803996
Iteration 160: Loss = -12396.69650106642
Iteration 170: Loss = -12396.696240464458
Iteration 180: Loss = -12396.695808771636
Iteration 190: Loss = -12396.695446561238
Iteration 200: Loss = -12396.695116650679
Iteration 210: Loss = -12396.694744666916
Iteration 220: Loss = -12396.694400054012
Iteration 230: Loss = -12396.693971135406
Iteration 240: Loss = -12396.693594008095
Iteration 250: Loss = -12396.693178354084
Iteration 260: Loss = -12396.692843879207
Iteration 270: Loss = -12396.692379864407
Iteration 280: Loss = -12396.691971265598
Iteration 290: Loss = -12396.691512524827
Iteration 300: Loss = -12396.69106734936
Iteration 310: Loss = -12396.690695951778
Iteration 320: Loss = -12396.690235587243
Iteration 330: Loss = -12396.689801750003
Iteration 340: Loss = -12396.689308114861
Iteration 350: Loss = -12396.688858533691
Iteration 360: Loss = -12396.688386003398
Iteration 370: Loss = -12396.687921498899
Iteration 380: Loss = -12396.687429577449
Iteration 390: Loss = -12396.686965332152
Iteration 400: Loss = -12396.68639805011
Iteration 410: Loss = -12396.685900797718
Iteration 420: Loss = -12396.685341929293
Iteration 430: Loss = -12396.684892233636
Iteration 440: Loss = -12396.684312997764
Iteration 450: Loss = -12396.683703593722
Iteration 460: Loss = -12396.683137123051
Iteration 470: Loss = -12396.682593511592
Iteration 480: Loss = -12396.682055177393
Iteration 490: Loss = -12396.681405060202
Iteration 500: Loss = -12396.680819540761
Iteration 510: Loss = -12396.680173093173
Iteration 520: Loss = -12396.679557600002
Iteration 530: Loss = -12396.678993163634
Iteration 540: Loss = -12396.678329612492
Iteration 550: Loss = -12396.677693776073
Iteration 560: Loss = -12396.677023996914
Iteration 570: Loss = -12396.676374872204
Iteration 580: Loss = -12396.675722912387
Iteration 590: Loss = -12396.675046222052
Iteration 600: Loss = -12396.67435269271
Iteration 610: Loss = -12396.673668795613
Iteration 620: Loss = -12396.673005963128
Iteration 630: Loss = -12396.67230243921
Iteration 640: Loss = -12396.671652423
Iteration 650: Loss = -12396.670899980532
Iteration 660: Loss = -12396.670179307135
Iteration 670: Loss = -12396.669499923424
Iteration 680: Loss = -12396.668814550692
Iteration 690: Loss = -12396.668092959704
Iteration 700: Loss = -12396.667310827099
Iteration 710: Loss = -12396.666601977306
Iteration 720: Loss = -12396.665913302077
Iteration 730: Loss = -12396.665195884716
Iteration 740: Loss = -12396.664446275094
Iteration 750: Loss = -12396.663751326605
Iteration 760: Loss = -12396.66303281315
Iteration 770: Loss = -12396.662358918282
Iteration 780: Loss = -12396.661622563524
Iteration 790: Loss = -12396.660930194888
Iteration 800: Loss = -12396.660239276212
Iteration 810: Loss = -12396.659525349742
Iteration 820: Loss = -12396.65883946841
Iteration 830: Loss = -12396.658168995515
Iteration 840: Loss = -12396.657455161701
Iteration 850: Loss = -12396.656805854791
Iteration 860: Loss = -12396.656171152796
Iteration 870: Loss = -12396.65550736735
Iteration 880: Loss = -12396.65482828029
Iteration 890: Loss = -12396.654211666404
Iteration 900: Loss = -12396.653579066415
Iteration 910: Loss = -12396.652944883994
Iteration 920: Loss = -12396.652328952785
Iteration 930: Loss = -12396.65173265727
Iteration 940: Loss = -12396.651177911079
Iteration 950: Loss = -12396.650575414173
Iteration 960: Loss = -12396.650028992826
Iteration 970: Loss = -12396.649494819807
Iteration 980: Loss = -12396.648916997961
Iteration 990: Loss = -12396.648412479217
Iteration 1000: Loss = -12396.6478964163
Iteration 1010: Loss = -12396.64737212365
Iteration 1020: Loss = -12396.646867379863
Iteration 1030: Loss = -12396.646418438166
Iteration 1040: Loss = -12396.645954866493
Iteration 1050: Loss = -12396.645466284455
Iteration 1060: Loss = -12396.645040665775
Iteration 1070: Loss = -12396.644645573893
Iteration 1080: Loss = -12396.644173811297
Iteration 1090: Loss = -12396.643836278345
Iteration 1100: Loss = -12396.643426945364
Iteration 1110: Loss = -12396.64303833723
Iteration 1120: Loss = -12396.642684813456
Iteration 1130: Loss = -12396.64235042555
Iteration 1140: Loss = -12396.642010355376
Iteration 1150: Loss = -12396.641666870195
Iteration 1160: Loss = -12396.641378139395
Iteration 1170: Loss = -12396.641093636785
Iteration 1180: Loss = -12396.640786693433
Iteration 1190: Loss = -12396.640554510832
Iteration 1200: Loss = -12396.640291756165
Iteration 1210: Loss = -12396.64005644991
Iteration 1220: Loss = -12396.639789804321
Iteration 1230: Loss = -12396.63957522772
Iteration 1240: Loss = -12396.639403062021
Iteration 1250: Loss = -12396.639139397304
Iteration 1260: Loss = -12396.638973485966
Iteration 1270: Loss = -12396.638809175878
Iteration 1280: Loss = -12396.638666809518
Iteration 1290: Loss = -12396.638509762712
Iteration 1300: Loss = -12396.638337735998
Iteration 1310: Loss = -12396.638190058378
Iteration 1320: Loss = -12396.638109949643
Iteration 1330: Loss = -12396.637974345156
Iteration 1340: Loss = -12396.637877993895
Iteration 1350: Loss = -12396.637779250272
Iteration 1360: Loss = -12396.637680443291
Iteration 1370: Loss = -12396.637568237582
Iteration 1380: Loss = -12396.637522830171
Iteration 1390: Loss = -12396.63748131934
Iteration 1400: Loss = -12396.637433999924
Iteration 1410: Loss = -12396.637397114126
Iteration 1420: Loss = -12396.637330678424
Iteration 1430: Loss = -12396.63735245037
1
Iteration 1440: Loss = -12396.637230716839
Iteration 1450: Loss = -12396.63728778515
1
Iteration 1460: Loss = -12396.637222707299
Iteration 1470: Loss = -12396.637262660628
1
Iteration 1480: Loss = -12396.637209590646
Iteration 1490: Loss = -12396.637242070059
1
Iteration 1500: Loss = -12396.637292568586
2
Iteration 1510: Loss = -12396.637277176886
3
Stopping early at iteration 1509 due to no improvement.
pi: tensor([[0.2007, 0.7993],
        [0.2185, 0.7815]], dtype=torch.float64)
alpha: tensor([0.2149, 0.7851])
beta: tensor([[[0.1940, 0.1916],
         [0.5286, 0.1996]],

        [[0.8302, 0.2056],
         [0.8363, 0.6744]],

        [[0.7917, 0.2068],
         [0.0017, 0.2593]],

        [[0.4305, 0.1904],
         [0.7531, 0.2954]],

        [[0.7174, 0.1894],
         [0.7355, 0.5003]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -15528.423027765919
Iteration 100: Loss = -12401.548689559186
Iteration 200: Loss = -12397.756173164995
Iteration 300: Loss = -12397.305818668974
Iteration 400: Loss = -12396.845333366224
Iteration 500: Loss = -12396.38847740362
Iteration 600: Loss = -12395.978055747915
Iteration 700: Loss = -12395.607054131573
Iteration 800: Loss = -12395.323203722734
Iteration 900: Loss = -12395.127913464159
Iteration 1000: Loss = -12394.994232106876
Iteration 1100: Loss = -12394.899679028083
Iteration 1200: Loss = -12394.830374836158
Iteration 1300: Loss = -12394.777882518778
Iteration 1400: Loss = -12394.737058923292
Iteration 1500: Loss = -12394.704581266222
Iteration 1600: Loss = -12394.678299146522
Iteration 1700: Loss = -12394.656594113403
Iteration 1800: Loss = -12394.63852051681
Iteration 1900: Loss = -12394.62323071928
Iteration 2000: Loss = -12394.610257757433
Iteration 2100: Loss = -12394.599071929595
Iteration 2200: Loss = -12394.589336639512
Iteration 2300: Loss = -12394.580882339496
Iteration 2400: Loss = -12394.573436936986
Iteration 2500: Loss = -12394.566823943656
Iteration 2600: Loss = -12394.560978284002
Iteration 2700: Loss = -12394.555730717342
Iteration 2800: Loss = -12394.551079990111
Iteration 2900: Loss = -12394.546908707598
Iteration 3000: Loss = -12394.543068567224
Iteration 3100: Loss = -12394.539660943687
Iteration 3200: Loss = -12394.536481718791
Iteration 3300: Loss = -12394.533672340162
Iteration 3400: Loss = -12394.531044755742
Iteration 3500: Loss = -12394.52869505325
Iteration 3600: Loss = -12394.526501600942
Iteration 3700: Loss = -12394.524495102063
Iteration 3800: Loss = -12394.522612309203
Iteration 3900: Loss = -12394.520905733623
Iteration 4000: Loss = -12394.519815990107
Iteration 4100: Loss = -12394.517865790398
Iteration 4200: Loss = -12394.516517633745
Iteration 4300: Loss = -12394.515259850266
Iteration 4400: Loss = -12394.514052501525
Iteration 4500: Loss = -12394.513010178318
Iteration 4600: Loss = -12394.511921622307
Iteration 4700: Loss = -12394.511006210982
Iteration 4800: Loss = -12394.510108236644
Iteration 4900: Loss = -12394.50926651323
Iteration 5000: Loss = -12394.508496308372
Iteration 5100: Loss = -12394.50777101035
Iteration 5200: Loss = -12394.507085196403
Iteration 5300: Loss = -12394.506961739784
Iteration 5400: Loss = -12394.505839734247
Iteration 5500: Loss = -12394.50567728688
Iteration 5600: Loss = -12394.518241745385
1
Iteration 5700: Loss = -12394.504590993334
Iteration 5800: Loss = -12394.503825018024
Iteration 5900: Loss = -12394.52065505465
1
Iteration 6000: Loss = -12394.502942232493
Iteration 6100: Loss = -12394.505021124334
1
Iteration 6200: Loss = -12394.502145253557
Iteration 6300: Loss = -12394.502025225347
Iteration 6400: Loss = -12394.501451496
Iteration 6500: Loss = -12394.501179311406
Iteration 6600: Loss = -12394.501507268458
1
Iteration 6700: Loss = -12394.500583850033
Iteration 6800: Loss = -12394.500336293428
Iteration 6900: Loss = -12394.500122064543
Iteration 7000: Loss = -12394.499956921514
Iteration 7100: Loss = -12394.499602194499
Iteration 7200: Loss = -12394.499389990553
Iteration 7300: Loss = -12394.528929265703
1
Iteration 7400: Loss = -12394.499014758374
Iteration 7500: Loss = -12394.498856814831
Iteration 7600: Loss = -12394.498985866712
1
Iteration 7700: Loss = -12394.498567186632
Iteration 7800: Loss = -12394.498397340092
Iteration 7900: Loss = -12394.498195978165
Iteration 8000: Loss = -12394.499080595971
1
Iteration 8100: Loss = -12394.497989805577
Iteration 8200: Loss = -12394.631081447382
1
Iteration 8300: Loss = -12394.497716719785
Iteration 8400: Loss = -12394.497601816487
Iteration 8500: Loss = -12394.533183935011
1
Iteration 8600: Loss = -12394.497419629713
Iteration 8700: Loss = -12394.497315793526
Iteration 8800: Loss = -12394.497667772972
1
Iteration 8900: Loss = -12394.50030474145
2
Iteration 9000: Loss = -12394.781403379968
3
Iteration 9100: Loss = -12394.497013954215
Iteration 9200: Loss = -12394.49714489088
1
Iteration 9300: Loss = -12394.497019469669
2
Iteration 9400: Loss = -12394.534464172242
3
Iteration 9500: Loss = -12394.496716984953
Iteration 9600: Loss = -12394.504655093666
1
Iteration 9700: Loss = -12394.49687398259
2
Iteration 9800: Loss = -12394.496490023139
Iteration 9900: Loss = -12394.503026646893
1
Iteration 10000: Loss = -12394.496455283414
Iteration 10100: Loss = -12394.496600018445
1
Iteration 10200: Loss = -12394.496445987716
Iteration 10300: Loss = -12394.49629453243
Iteration 10400: Loss = -12394.49624102809
Iteration 10500: Loss = -12394.49660886256
1
Iteration 10600: Loss = -12394.498838699294
2
Iteration 10700: Loss = -12394.509844053338
3
Iteration 10800: Loss = -12394.496467811172
4
Iteration 10900: Loss = -12394.503780468996
5
Iteration 11000: Loss = -12394.496223045791
Iteration 11100: Loss = -12394.531068081982
1
Iteration 11200: Loss = -12394.496073732851
Iteration 11300: Loss = -12394.496154722725
1
Iteration 11400: Loss = -12394.499867773515
2
Iteration 11500: Loss = -12394.496192936991
3
Iteration 11600: Loss = -12394.498628063524
4
Iteration 11700: Loss = -12394.496799562492
5
Iteration 11800: Loss = -12394.49596791779
Iteration 11900: Loss = -12394.582923337226
1
Iteration 12000: Loss = -12394.4959423629
Iteration 12100: Loss = -12394.49590630708
Iteration 12200: Loss = -12394.535600389918
1
Iteration 12300: Loss = -12394.496129546238
2
Iteration 12400: Loss = -12394.496082904803
3
Iteration 12500: Loss = -12394.5077705761
4
Iteration 12600: Loss = -12394.498557893547
5
Iteration 12700: Loss = -12394.495714413826
Iteration 12800: Loss = -12394.496988437957
1
Iteration 12900: Loss = -12394.496620124717
2
Iteration 13000: Loss = -12394.495819991644
3
Iteration 13100: Loss = -12394.497053622797
4
Iteration 13200: Loss = -12394.508082903118
5
Iteration 13300: Loss = -12394.495651954383
Iteration 13400: Loss = -12394.49940717132
1
Iteration 13500: Loss = -12394.498023482593
2
Iteration 13600: Loss = -12394.495773785704
3
Iteration 13700: Loss = -12394.49576655406
4
Iteration 13800: Loss = -12394.496767417211
5
Iteration 13900: Loss = -12394.509230310956
6
Iteration 14000: Loss = -12394.495607259913
Iteration 14100: Loss = -12394.496833686837
1
Iteration 14200: Loss = -12394.496220129717
2
Iteration 14300: Loss = -12394.495892920291
3
Iteration 14400: Loss = -12394.500868950285
4
Iteration 14500: Loss = -12394.495662335707
5
Iteration 14600: Loss = -12394.49578453025
6
Iteration 14700: Loss = -12394.515381121331
7
Iteration 14800: Loss = -12394.495609503401
8
Iteration 14900: Loss = -12394.498119474696
9
Iteration 15000: Loss = -12394.570227656912
10
Stopping early at iteration 15000 due to no improvement.
tensor([[ -6.3383,   1.7231],
        [ -7.7983,   3.1830],
        [ -6.0885,   1.4733],
        [ -6.7734,   2.1581],
        [ -6.4964,   1.8811],
        [ -7.8670,   3.2518],
        [ -7.3204,   2.7052],
        [ -7.2849,   2.6697],
        [ -7.0052,   2.3900],
        [ -5.7148,   1.0996],
        [ -4.7893,   0.1741],
        [ -7.8269,   3.2117],
        [ -5.6706,   1.0554],
        [ -6.2997,   1.6845],
        [ -9.0532,   4.4380],
        [ -6.3513,   1.7361],
        [ -6.5094,   1.8942],
        [ -8.8473,   4.2321],
        [ -6.7452,   2.1300],
        [ -5.9230,   1.3078],
        [ -4.6348,   0.0196],
        [ -9.2621,   4.6469],
        [ -5.6238,   1.0086],
        [ -5.2567,   0.6415],
        [ -0.9390,  -3.6762],
        [ -7.2313,   2.6161],
        [ -6.5988,   1.9836],
        [ -6.6115,   1.9962],
        [ -6.6577,   2.0425],
        [ -6.4915,   1.8763],
        [ -7.9853,   3.3701],
        [ -7.5151,   2.8999],
        [ -6.6147,   1.9995],
        [ -4.2770,  -0.3382],
        [ -7.4387,   2.8235],
        [ -6.5177,   1.9025],
        [ -6.3451,   1.7299],
        [ -5.9376,   1.3224],
        [ -4.3976,  -0.2176],
        [-10.7553,   6.1401],
        [ -6.5495,   1.9343],
        [ -5.9992,   1.3840],
        [ -4.3890,  -0.2263],
        [ -7.3114,   2.6961],
        [ -5.9472,   1.3320],
        [ -7.0585,   2.4433],
        [ -5.3262,   0.7110],
        [ -8.9619,   4.3467],
        [ -5.5656,   0.9504],
        [ -4.0807,  -0.5345],
        [ -5.4555,   0.8402],
        [ -9.9121,   5.2969],
        [ -7.2438,   2.6285],
        [ -8.2347,   3.6195],
        [ -0.9184,  -3.6968],
        [ -7.4588,   2.8436],
        [ -4.2132,  -0.4020],
        [ -6.2896,   1.6744],
        [ -6.2723,   1.6571],
        [ -6.7691,   2.1539],
        [ -7.4791,   2.8639],
        [ -8.4207,   3.8055],
        [ -8.0132,   3.3980],
        [ -6.1363,   1.5211],
        [ -7.0812,   2.4660],
        [ -7.9921,   3.3769],
        [ -6.6439,   2.0287],
        [ -6.5075,   1.8923],
        [ -6.0882,   1.4729],
        [ -7.4957,   2.8805],
        [ -7.3432,   2.7280],
        [ -4.3485,  -0.2667],
        [ -8.4233,   3.8081],
        [ -7.3639,   2.7486],
        [ -6.2109,   1.5957],
        [ -1.2536,  -3.3617],
        [ -8.4988,   3.8836],
        [ -8.3494,   3.7342],
        [ -6.3339,   1.7186],
        [ -6.7081,   2.0929],
        [ -8.2486,   3.6334],
        [ -5.7633,   1.1481],
        [ -7.3816,   2.7663],
        [ -7.4997,   2.8845],
        [ -9.2126,   4.5974],
        [ -6.5442,   1.9290],
        [ -4.8321,   0.2169],
        [ -5.3438,   0.7286],
        [ -7.4882,   2.8730],
        [ -5.6809,   1.0656],
        [ -5.0787,   0.4635],
        [ -4.6880,   0.0728],
        [ -8.4229,   3.8077],
        [ -6.6445,   2.0293],
        [ -8.2901,   3.6749],
        [ -7.3418,   2.7266],
        [ -7.7918,   3.1766],
        [ -7.3159,   2.7007],
        [ -6.8343,   2.2191],
        [ -8.1102,   3.4949]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 3.0972e-07],
        [3.7898e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0299, 0.9701], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1480, 0.2170],
         [0.5286, 0.1966]],

        [[0.8302, 0.2939],
         [0.8363, 0.6744]],

        [[0.7917, 0.1875],
         [0.0017, 0.2593]],

        [[0.4305, 0.2499],
         [0.7531, 0.2954]],

        [[0.7174, 0.2706],
         [0.7355, 0.5003]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.0032454170932134756
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.002764517368444376
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0010149900615556472
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.002193311857154732
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: -0.0011519735416892346
Average Adjusted Rand Index: -0.002164923978724115
Iteration 0: Loss = -18572.011728012174
Iteration 10: Loss = -12397.591676425265
Iteration 20: Loss = -12397.275548075657
Iteration 30: Loss = -12397.05510372368
Iteration 40: Loss = -12396.982860702083
Iteration 50: Loss = -12396.938034186784
Iteration 60: Loss = -12396.903080135557
Iteration 70: Loss = -12396.872938700335
Iteration 80: Loss = -12396.847364095487
Iteration 90: Loss = -12396.82760733605
Iteration 100: Loss = -12396.813267999858
Iteration 110: Loss = -12396.803028423725
Iteration 120: Loss = -12396.79532944121
Iteration 130: Loss = -12396.789122056509
Iteration 140: Loss = -12396.783959985212
Iteration 150: Loss = -12396.779460960674
Iteration 160: Loss = -12396.77545684785
Iteration 170: Loss = -12396.77184741582
Iteration 180: Loss = -12396.768627232557
Iteration 190: Loss = -12396.765638610503
Iteration 200: Loss = -12396.762938641428
Iteration 210: Loss = -12396.760389863806
Iteration 220: Loss = -12396.758053662801
Iteration 230: Loss = -12396.755902933235
Iteration 240: Loss = -12396.753824899299
Iteration 250: Loss = -12396.751963105402
Iteration 260: Loss = -12396.750100437946
Iteration 270: Loss = -12396.748443253779
Iteration 280: Loss = -12396.746817793703
Iteration 290: Loss = -12396.745353963528
Iteration 300: Loss = -12396.743902325601
Iteration 310: Loss = -12396.742524854055
Iteration 320: Loss = -12396.741224780439
Iteration 330: Loss = -12396.739957687238
Iteration 340: Loss = -12396.738759447264
Iteration 350: Loss = -12396.737608063879
Iteration 360: Loss = -12396.736549398867
Iteration 370: Loss = -12396.735492033065
Iteration 380: Loss = -12396.734457918996
Iteration 390: Loss = -12396.733500136761
Iteration 400: Loss = -12396.73254166678
Iteration 410: Loss = -12396.731614559601
Iteration 420: Loss = -12396.730744271721
Iteration 430: Loss = -12396.729913627849
Iteration 440: Loss = -12396.729097323898
Iteration 450: Loss = -12396.72830643031
Iteration 460: Loss = -12396.727534714786
Iteration 470: Loss = -12396.726785829322
Iteration 480: Loss = -12396.726092820292
Iteration 490: Loss = -12396.725363245989
Iteration 500: Loss = -12396.724700221312
Iteration 510: Loss = -12396.724058208925
Iteration 520: Loss = -12396.723378408606
Iteration 530: Loss = -12396.722741257985
Iteration 540: Loss = -12396.722187588952
Iteration 550: Loss = -12396.721575469372
Iteration 560: Loss = -12396.720976433675
Iteration 570: Loss = -12396.720430398533
Iteration 580: Loss = -12396.719886165136
Iteration 590: Loss = -12396.719347260723
Iteration 600: Loss = -12396.71886810847
Iteration 610: Loss = -12396.718352583683
Iteration 620: Loss = -12396.717873261967
Iteration 630: Loss = -12396.717284750444
Iteration 640: Loss = -12396.716851823512
Iteration 650: Loss = -12396.716396698603
Iteration 660: Loss = -12396.715955528862
Iteration 670: Loss = -12396.715508910826
Iteration 680: Loss = -12396.71507625929
Iteration 690: Loss = -12396.714657081187
Iteration 700: Loss = -12396.714233682369
Iteration 710: Loss = -12396.713848214478
Iteration 720: Loss = -12396.713408419379
Iteration 730: Loss = -12396.713045898308
Iteration 740: Loss = -12396.712679421455
Iteration 750: Loss = -12396.712275957163
Iteration 760: Loss = -12396.711921217964
Iteration 770: Loss = -12396.71154301419
Iteration 780: Loss = -12396.711185134085
Iteration 790: Loss = -12396.71086239646
Iteration 800: Loss = -12396.710533205536
Iteration 810: Loss = -12396.710190646194
Iteration 820: Loss = -12396.709864067394
Iteration 830: Loss = -12396.709528811509
Iteration 840: Loss = -12396.709201111791
Iteration 850: Loss = -12396.70892490532
Iteration 860: Loss = -12396.708577123694
Iteration 870: Loss = -12396.708307290079
Iteration 880: Loss = -12396.708002485291
Iteration 890: Loss = -12396.7076911187
Iteration 900: Loss = -12396.70746340917
Iteration 910: Loss = -12396.70714455275
Iteration 920: Loss = -12396.706909563467
Iteration 930: Loss = -12396.706597100598
Iteration 940: Loss = -12396.706333311573
Iteration 950: Loss = -12396.706102003818
Iteration 960: Loss = -12396.705837521356
Iteration 970: Loss = -12396.705570914151
Iteration 980: Loss = -12396.705344880578
Iteration 990: Loss = -12396.705060540055
Iteration 1000: Loss = -12396.7048345804
Iteration 1010: Loss = -12396.704635882665
Iteration 1020: Loss = -12396.704380147245
Iteration 1030: Loss = -12396.704139706117
Iteration 1040: Loss = -12396.703898016562
Iteration 1050: Loss = -12396.703702108069
Iteration 1060: Loss = -12396.703509205114
Iteration 1070: Loss = -12396.70326379282
Iteration 1080: Loss = -12396.703055400803
Iteration 1090: Loss = -12396.702830217526
Iteration 1100: Loss = -12396.702637568806
Iteration 1110: Loss = -12396.702440153565
Iteration 1120: Loss = -12396.702227250282
Iteration 1130: Loss = -12396.7020242291
Iteration 1140: Loss = -12396.701843214727
Iteration 1150: Loss = -12396.701669507236
Iteration 1160: Loss = -12396.701470528993
Iteration 1170: Loss = -12396.70126987097
Iteration 1180: Loss = -12396.701108417692
Iteration 1190: Loss = -12396.700913827204
Iteration 1200: Loss = -12396.70071005053
Iteration 1210: Loss = -12396.700560708847
Iteration 1220: Loss = -12396.700368558137
Iteration 1230: Loss = -12396.700192265125
Iteration 1240: Loss = -12396.700063065047
Iteration 1250: Loss = -12396.699842181728
Iteration 1260: Loss = -12396.69970555419
Iteration 1270: Loss = -12396.69956124218
Iteration 1280: Loss = -12396.699356143175
Iteration 1290: Loss = -12396.699241005406
Iteration 1300: Loss = -12396.69908158802
Iteration 1310: Loss = -12396.69891105563
Iteration 1320: Loss = -12396.69876287362
Iteration 1330: Loss = -12396.698599242074
Iteration 1340: Loss = -12396.698445312804
Iteration 1350: Loss = -12396.698293838068
Iteration 1360: Loss = -12396.698156624543
Iteration 1370: Loss = -12396.698059625056
Iteration 1380: Loss = -12396.697883595383
Iteration 1390: Loss = -12396.697728796633
Iteration 1400: Loss = -12396.697629768838
Iteration 1410: Loss = -12396.69748477293
Iteration 1420: Loss = -12396.697339637472
Iteration 1430: Loss = -12396.697208161066
Iteration 1440: Loss = -12396.697071991204
Iteration 1450: Loss = -12396.69691167034
Iteration 1460: Loss = -12396.696863960358
Iteration 1470: Loss = -12396.696734468695
Iteration 1480: Loss = -12396.69657664812
Iteration 1490: Loss = -12396.69647685211
Iteration 1500: Loss = -12396.696322468986
Iteration 1510: Loss = -12396.696221436974
Iteration 1520: Loss = -12396.696090839698
Iteration 1530: Loss = -12396.69602883459
Iteration 1540: Loss = -12396.695902218158
Iteration 1550: Loss = -12396.695761059666
Iteration 1560: Loss = -12396.695661467567
Iteration 1570: Loss = -12396.695526525331
Iteration 1580: Loss = -12396.695418125533
Iteration 1590: Loss = -12396.695357076165
Iteration 1600: Loss = -12396.695226730613
Iteration 1610: Loss = -12396.69510980101
Iteration 1620: Loss = -12396.695023285585
Iteration 1630: Loss = -12396.694906590725
Iteration 1640: Loss = -12396.69481257117
Iteration 1650: Loss = -12396.694694122745
Iteration 1660: Loss = -12396.694597755639
Iteration 1670: Loss = -12396.694481422206
Iteration 1680: Loss = -12396.694395263698
Iteration 1690: Loss = -12396.694296348962
Iteration 1700: Loss = -12396.694242537182
Iteration 1710: Loss = -12396.694122586116
Iteration 1720: Loss = -12396.694006626076
Iteration 1730: Loss = -12396.693931778329
Iteration 1740: Loss = -12396.693873681703
Iteration 1750: Loss = -12396.69377992576
Iteration 1760: Loss = -12396.69365158921
Iteration 1770: Loss = -12396.69358047582
Iteration 1780: Loss = -12396.693486195487
Iteration 1790: Loss = -12396.693386147126
Iteration 1800: Loss = -12396.693353177816
Iteration 1810: Loss = -12396.693259079944
Iteration 1820: Loss = -12396.693163569238
Iteration 1830: Loss = -12396.693068606028
Iteration 1840: Loss = -12396.6929810599
Iteration 1850: Loss = -12396.69291302577
Iteration 1860: Loss = -12396.692821593433
Iteration 1870: Loss = -12396.692736680952
Iteration 1880: Loss = -12396.692704180625
Iteration 1890: Loss = -12396.692605149996
Iteration 1900: Loss = -12396.69252121045
Iteration 1910: Loss = -12396.692442265843
Iteration 1920: Loss = -12396.692350768852
Iteration 1930: Loss = -12396.692288821525
Iteration 1940: Loss = -12396.692207492153
Iteration 1950: Loss = -12396.692120529471
Iteration 1960: Loss = -12396.692094689377
Iteration 1970: Loss = -12396.691975395803
Iteration 1980: Loss = -12396.691945674216
Iteration 1990: Loss = -12396.691861410678
Iteration 2000: Loss = -12396.691783995651
Iteration 2010: Loss = -12396.691759263278
Iteration 2020: Loss = -12396.69163881187
Iteration 2030: Loss = -12396.691594938744
Iteration 2040: Loss = -12396.691539092517
Iteration 2050: Loss = -12396.691461653994
Iteration 2060: Loss = -12396.69136504588
Iteration 2070: Loss = -12396.691339587307
Iteration 2080: Loss = -12396.691297477586
Iteration 2090: Loss = -12396.691196179543
Iteration 2100: Loss = -12396.691127475548
Iteration 2110: Loss = -12396.69110632356
Iteration 2120: Loss = -12396.691030260787
Iteration 2130: Loss = -12396.690906463768
Iteration 2140: Loss = -12396.690858504691
Iteration 2150: Loss = -12396.690848535165
Iteration 2160: Loss = -12396.690809766256
Iteration 2170: Loss = -12396.690800940893
Iteration 2180: Loss = -12396.690696990481
Iteration 2190: Loss = -12396.690622539638
Iteration 2200: Loss = -12396.690558959252
Iteration 2210: Loss = -12396.690512181312
Iteration 2220: Loss = -12396.690441065593
Iteration 2230: Loss = -12396.690419893475
Iteration 2240: Loss = -12396.690381059405
Iteration 2250: Loss = -12396.690298926063
Iteration 2260: Loss = -12396.690226279763
Iteration 2270: Loss = -12396.6902135126
Iteration 2280: Loss = -12396.69013900307
Iteration 2290: Loss = -12396.690095843158
Iteration 2300: Loss = -12396.690050850444
Iteration 2310: Loss = -12396.689986751742
Iteration 2320: Loss = -12396.68992559367
Iteration 2330: Loss = -12396.689892954932
Iteration 2340: Loss = -12396.689805462436
Iteration 2350: Loss = -12396.68983015267
1
Iteration 2360: Loss = -12396.689735043385
Iteration 2370: Loss = -12396.689675934987
Iteration 2380: Loss = -12396.689655773192
Iteration 2390: Loss = -12396.689618048884
Iteration 2400: Loss = -12396.689563267004
Iteration 2410: Loss = -12396.689505305543
Iteration 2420: Loss = -12396.689476958141
Iteration 2430: Loss = -12396.68944562328
Iteration 2440: Loss = -12396.689368963418
Iteration 2450: Loss = -12396.689320630085
Iteration 2460: Loss = -12396.689306888817
Iteration 2470: Loss = -12396.689273662188
Iteration 2480: Loss = -12396.689187245996
Iteration 2490: Loss = -12396.68919356531
1
Iteration 2500: Loss = -12396.689138684453
Iteration 2510: Loss = -12396.689106044176
Iteration 2520: Loss = -12396.689040106647
Iteration 2530: Loss = -12396.689019099618
Iteration 2540: Loss = -12396.688959801833
Iteration 2550: Loss = -12396.688942769339
Iteration 2560: Loss = -12396.688857451398
Iteration 2570: Loss = -12396.68880790798
Iteration 2580: Loss = -12396.688801051787
Iteration 2590: Loss = -12396.688764650926
Iteration 2600: Loss = -12396.68873414843
Iteration 2610: Loss = -12396.688716613407
Iteration 2620: Loss = -12396.688636517034
Iteration 2630: Loss = -12396.688611771026
Iteration 2640: Loss = -12396.688612451433
1
Iteration 2650: Loss = -12396.688547176102
Iteration 2660: Loss = -12396.688504371783
Iteration 2670: Loss = -12396.688510720167
1
Iteration 2680: Loss = -12396.68846815476
Iteration 2690: Loss = -12396.68846256843
Iteration 2700: Loss = -12396.688365156951
Iteration 2710: Loss = -12396.688351089897
Iteration 2720: Loss = -12396.68833862022
Iteration 2730: Loss = -12396.688257762555
Iteration 2740: Loss = -12396.688258366703
1
Iteration 2750: Loss = -12396.688218835141
Iteration 2760: Loss = -12396.688216532633
Iteration 2770: Loss = -12396.688149781761
Iteration 2780: Loss = -12396.688121283092
Iteration 2790: Loss = -12396.688106479583
Iteration 2800: Loss = -12396.68807734154
Iteration 2810: Loss = -12396.688001261633
Iteration 2820: Loss = -12396.68797132392
Iteration 2830: Loss = -12396.687975244762
1
Iteration 2840: Loss = -12396.687957886734
Iteration 2850: Loss = -12396.68793007718
Iteration 2860: Loss = -12396.687860703341
Iteration 2870: Loss = -12396.687840093799
Iteration 2880: Loss = -12396.687831968731
Iteration 2890: Loss = -12396.6877559939
Iteration 2900: Loss = -12396.687755974519
Iteration 2910: Loss = -12396.687747553196
Iteration 2920: Loss = -12396.68770103246
Iteration 2930: Loss = -12396.687741900361
1
Iteration 2940: Loss = -12396.687639280573
Iteration 2950: Loss = -12396.68763483057
Iteration 2960: Loss = -12396.68757009359
Iteration 2970: Loss = -12396.687556486437
Iteration 2980: Loss = -12396.68755104699
Iteration 2990: Loss = -12396.687508256702
Iteration 3000: Loss = -12396.68746470524
Iteration 3010: Loss = -12396.687466051966
1
Iteration 3020: Loss = -12396.687413639711
Iteration 3030: Loss = -12396.68743661977
1
Iteration 3040: Loss = -12396.6873450741
Iteration 3050: Loss = -12396.687366103128
1
Iteration 3060: Loss = -12396.687312046388
Iteration 3070: Loss = -12396.687282751695
Iteration 3080: Loss = -12396.68728314182
1
Iteration 3090: Loss = -12396.687281760025
Iteration 3100: Loss = -12396.687234414308
Iteration 3110: Loss = -12396.687224893525
Iteration 3120: Loss = -12396.687176568248
Iteration 3130: Loss = -12396.687180288955
1
Iteration 3140: Loss = -12396.687169420355
Iteration 3150: Loss = -12396.687120177263
Iteration 3160: Loss = -12396.687077059407
Iteration 3170: Loss = -12396.687089853696
1
Iteration 3180: Loss = -12396.68706633262
Iteration 3190: Loss = -12396.68704374746
Iteration 3200: Loss = -12396.687020639287
Iteration 3210: Loss = -12396.686980520237
Iteration 3220: Loss = -12396.686983472524
1
Iteration 3230: Loss = -12396.686940649675
Iteration 3240: Loss = -12396.686943250204
1
Iteration 3250: Loss = -12396.686912326377
Iteration 3260: Loss = -12396.686904233844
Iteration 3270: Loss = -12396.686890686511
Iteration 3280: Loss = -12396.686829157112
Iteration 3290: Loss = -12396.686819519597
Iteration 3300: Loss = -12396.68680313606
Iteration 3310: Loss = -12396.68679026272
Iteration 3320: Loss = -12396.686757262703
Iteration 3330: Loss = -12396.686764886661
1
Iteration 3340: Loss = -12396.68673947156
Iteration 3350: Loss = -12396.686690575445
Iteration 3360: Loss = -12396.686678991482
Iteration 3370: Loss = -12396.686650188518
Iteration 3380: Loss = -12396.686612379293
Iteration 3390: Loss = -12396.68664989329
1
Iteration 3400: Loss = -12396.686622702842
2
Iteration 3410: Loss = -12396.686626164847
3
Stopping early at iteration 3409 due to no improvement.
pi: tensor([[0.1381, 0.8619],
        [0.1075, 0.8925]], dtype=torch.float64)
alpha: tensor([0.1111, 0.8889])
beta: tensor([[[0.1879, 0.1861],
         [0.1148, 0.1998]],

        [[0.8946, 0.2082],
         [0.1500, 0.0989]],

        [[0.2349, 0.2093],
         [0.7006, 0.1510]],

        [[0.4352, 0.1830],
         [0.9611, 0.7664]],

        [[0.3546, 0.1808],
         [0.9059, 0.1182]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18571.150136370834
Iteration 100: Loss = -12397.923616468723
Iteration 200: Loss = -12397.176371674816
Iteration 300: Loss = -12396.972448021785
Iteration 400: Loss = -12396.853598235042
Iteration 500: Loss = -12396.762983312321
Iteration 600: Loss = -12396.689920522127
Iteration 700: Loss = -12396.628759617379
Iteration 800: Loss = -12396.578378457838
Iteration 900: Loss = -12396.538448228766
Iteration 1000: Loss = -12396.50848678918
Iteration 1100: Loss = -12396.48711140884
Iteration 1200: Loss = -12396.471036515397
Iteration 1300: Loss = -12396.45662397395
Iteration 1400: Loss = -12396.440880555321
Iteration 1500: Loss = -12396.420695905455
Iteration 1600: Loss = -12396.392515448282
Iteration 1700: Loss = -12396.35410788986
Iteration 1800: Loss = -12396.311933166528
Iteration 1900: Loss = -12396.280732911282
Iteration 2000: Loss = -12396.262027290097
Iteration 2100: Loss = -12396.250044824968
Iteration 2200: Loss = -12396.241480592134
Iteration 2300: Loss = -12396.234674926389
Iteration 2400: Loss = -12396.228518268561
Iteration 2500: Loss = -12396.222530067475
Iteration 2600: Loss = -12396.216208481148
Iteration 2700: Loss = -12396.20929276396
Iteration 2800: Loss = -12396.208598522851
Iteration 2900: Loss = -12396.193069147417
Iteration 3000: Loss = -12396.185515266689
Iteration 3100: Loss = -12396.175467732617
Iteration 3200: Loss = -12396.1663990544
Iteration 3300: Loss = -12396.157937965854
Iteration 3400: Loss = -12396.150860101736
Iteration 3500: Loss = -12396.145199235703
Iteration 3600: Loss = -12396.140088251428
Iteration 3700: Loss = -12396.136575021768
Iteration 3800: Loss = -12396.133763606487
Iteration 3900: Loss = -12396.132059403866
Iteration 4000: Loss = -12396.1318448996
Iteration 4100: Loss = -12396.134520874788
1
Iteration 4200: Loss = -12396.13015013926
Iteration 4300: Loss = -12396.13043937237
1
Iteration 4400: Loss = -12396.129212964544
Iteration 4500: Loss = -12396.127967392038
Iteration 4600: Loss = -12396.14062062011
1
Iteration 4700: Loss = -12396.126949930474
Iteration 4800: Loss = -12396.126400541929
Iteration 4900: Loss = -12396.121869478957
Iteration 5000: Loss = -12396.123350689018
1
Iteration 5100: Loss = -12396.121195741234
Iteration 5200: Loss = -12396.127996252044
1
Iteration 5300: Loss = -12396.12123210578
2
Iteration 5400: Loss = -12396.12088049876
Iteration 5500: Loss = -12396.1192559416
Iteration 5600: Loss = -12396.118552476786
Iteration 5700: Loss = -12396.119932891661
1
Iteration 5800: Loss = -12396.14458342204
2
Iteration 5900: Loss = -12396.167815016755
3
Iteration 6000: Loss = -12396.117134652417
Iteration 6100: Loss = -12396.128330100055
1
Iteration 6200: Loss = -12396.116723195475
Iteration 6300: Loss = -12396.167447649119
1
Iteration 6400: Loss = -12396.118259844896
2
Iteration 6500: Loss = -12396.124226630373
3
Iteration 6600: Loss = -12396.117271858577
4
Iteration 6700: Loss = -12396.123739818258
5
Iteration 6800: Loss = -12396.127367116991
6
Iteration 6900: Loss = -12396.120842075934
7
Iteration 7000: Loss = -12396.117047739881
8
Iteration 7100: Loss = -12396.13773886728
9
Iteration 7200: Loss = -12396.117853459895
10
Stopping early at iteration 7200 due to no improvement.
tensor([[ 1.0552, -2.6445],
        [ 1.1175, -2.6225],
        [ 1.0821, -2.6804],
        [ 0.3746, -3.3585],
        [ 0.5653, -3.1215],
        [ 0.9082, -2.8359],
        [ 0.9287, -2.7274],
        [-0.4323, -4.1829],
        [ 1.1255, -2.7407],
        [ 0.7774, -2.9169],
        [ 0.7204, -3.0723],
        [-0.4574, -4.1579],
        [ 0.3202, -3.3777],
        [ 0.0204, -3.7439],
        [ 1.2194, -2.6059],
        [ 0.6859, -3.0099],
        [ 0.8473, -2.8553],
        [ 0.9881, -2.7412],
        [ 0.7329, -3.0344],
        [ 0.7104, -2.9911],
        [ 0.8660, -2.7176],
        [ 0.9865, -2.7566],
        [ 0.1709, -3.5750],
        [ 1.1743, -2.6068],
        [ 0.3388, -3.3691],
        [-0.2628, -4.0203],
        [ 1.1748, -2.5612],
        [ 0.2865, -3.5740],
        [ 1.2074, -2.6338],
        [ 1.1127, -2.5511],
        [ 1.0717, -2.6912],
        [ 0.8945, -2.7964],
        [ 0.1872, -3.5630],
        [ 1.1614, -2.6116],
        [ 0.8386, -2.8716],
        [-0.4611, -4.1542],
        [-0.4280, -4.1872],
        [ 1.0462, -2.6794],
        [ 0.6920, -2.9079],
        [ 1.1173, -2.5546],
        [ 1.0593, -2.6588],
        [ 0.4811, -3.2231],
        [ 1.0119, -2.6072],
        [ 0.9061, -2.7309],
        [ 1.0234, -2.8095],
        [ 0.7817, -3.0097],
        [ 1.1539, -2.5918],
        [ 0.8477, -2.9147],
        [ 1.1753, -2.5629],
        [ 0.9999, -2.7808],
        [ 0.9177, -2.7232],
        [ 1.1408, -2.6520],
        [ 0.6862, -3.1309],
        [ 1.0293, -2.7012],
        [ 0.7037, -2.9857],
        [ 1.0104, -2.7052],
        [ 1.1199, -2.5357],
        [ 1.1752, -2.6294],
        [ 1.1078, -2.5325],
        [ 1.0635, -2.7487],
        [ 0.9909, -2.8388],
        [ 1.0069, -2.5962],
        [ 1.1647, -2.6030],
        [ 1.1317, -2.6454],
        [ 1.1378, -2.6267],
        [ 0.0429, -3.6364],
        [ 1.0387, -2.7137],
        [ 1.1553, -2.5468],
        [ 1.1106, -2.5524],
        [ 1.0004, -2.7380],
        [ 0.8626, -2.9505],
        [ 0.6949, -2.9009],
        [ 0.0763, -3.6572],
        [ 0.7653, -2.9598],
        [ 0.9996, -2.7810],
        [ 1.0756, -2.5642],
        [-0.4296, -4.1857],
        [ 1.1546, -2.5933],
        [ 1.1551, -2.6026],
        [ 0.6439, -3.0897],
        [ 1.1803, -2.5666],
        [ 0.0598, -3.6157],
        [ 1.0634, -2.6272],
        [ 0.9384, -2.7540],
        [ 0.8837, -2.8530],
        [ 0.8756, -2.8717],
        [ 0.4194, -3.2266],
        [ 1.1769, -2.5759],
        [ 1.1457, -2.5795],
        [ 1.1696, -2.5898],
        [ 1.0258, -2.6595],
        [ 0.6997, -3.0253],
        [ 0.9961, -2.8515],
        [ 0.7953, -2.9040],
        [ 0.4282, -3.1937],
        [ 0.8062, -2.8879],
        [ 0.8668, -2.8411],
        [ 0.3594, -3.2987],
        [ 1.0464, -2.6178],
        [ 1.1491, -2.6362]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.1595, 0.8405],
        [0.3878, 0.6122]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9765, 0.0235], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1963, 0.1987],
         [0.1148, 0.2035]],

        [[0.8946, 0.2075],
         [0.1500, 0.0989]],

        [[0.2349, 0.2071],
         [0.7006, 0.1510]],

        [[0.4352, 0.1937],
         [0.9611, 0.7664]],

        [[0.3546, 0.1935],
         [0.9059, 0.1182]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0017326397458452079
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -23807.982155648187
Iteration 10: Loss = -12397.490994604812
Iteration 20: Loss = -12397.025137178924
Iteration 30: Loss = -12396.856105491042
Iteration 40: Loss = -12396.805696121019
Iteration 50: Loss = -12396.787585727214
Iteration 60: Loss = -12396.777728919966
Iteration 70: Loss = -12396.771114620562
Iteration 80: Loss = -12396.76630755793
Iteration 90: Loss = -12396.762592503479
Iteration 100: Loss = -12396.759602675866
Iteration 110: Loss = -12396.75710192486
Iteration 120: Loss = -12396.754805104967
Iteration 130: Loss = -12396.752775172621
Iteration 140: Loss = -12396.750938731677
Iteration 150: Loss = -12396.749190319055
Iteration 160: Loss = -12396.747509900648
Iteration 170: Loss = -12396.745972985716
Iteration 180: Loss = -12396.744506955414
Iteration 190: Loss = -12396.743104087242
Iteration 200: Loss = -12396.741811299049
Iteration 210: Loss = -12396.740529044966
Iteration 220: Loss = -12396.73927656389
Iteration 230: Loss = -12396.738132435898
Iteration 240: Loss = -12396.73703198228
Iteration 250: Loss = -12396.735914327559
Iteration 260: Loss = -12396.734926290259
Iteration 270: Loss = -12396.73393429399
Iteration 280: Loss = -12396.73296538961
Iteration 290: Loss = -12396.732039923516
Iteration 300: Loss = -12396.731161786664
Iteration 310: Loss = -12396.730291557044
Iteration 320: Loss = -12396.729464132655
Iteration 330: Loss = -12396.728683755775
Iteration 340: Loss = -12396.72787344338
Iteration 350: Loss = -12396.7271699869
Iteration 360: Loss = -12396.726409838795
Iteration 370: Loss = -12396.72568866795
Iteration 380: Loss = -12396.724981905076
Iteration 390: Loss = -12396.724305253576
Iteration 400: Loss = -12396.723722941291
Iteration 410: Loss = -12396.72306481076
Iteration 420: Loss = -12396.722421634584
Iteration 430: Loss = -12396.721833448737
Iteration 440: Loss = -12396.721229989273
Iteration 450: Loss = -12396.72067006488
Iteration 460: Loss = -12396.720171345944
Iteration 470: Loss = -12396.719591748222
Iteration 480: Loss = -12396.719044809684
Iteration 490: Loss = -12396.71854358303
Iteration 500: Loss = -12396.718051145843
Iteration 510: Loss = -12396.717560285886
Iteration 520: Loss = -12396.717068877831
Iteration 530: Loss = -12396.716597468076
Iteration 540: Loss = -12396.71619987561
Iteration 550: Loss = -12396.715718892297
Iteration 560: Loss = -12396.71523759145
Iteration 570: Loss = -12396.7148556445
Iteration 580: Loss = -12396.714428240122
Iteration 590: Loss = -12396.713991576135
Iteration 600: Loss = -12396.713610583436
Iteration 610: Loss = -12396.713175574945
Iteration 620: Loss = -12396.712797306589
Iteration 630: Loss = -12396.71245014406
Iteration 640: Loss = -12396.712089552218
Iteration 650: Loss = -12396.711727308653
Iteration 660: Loss = -12396.71134391085
Iteration 670: Loss = -12396.711027708503
Iteration 680: Loss = -12396.71068071872
Iteration 690: Loss = -12396.710312316274
Iteration 700: Loss = -12396.70997820027
Iteration 710: Loss = -12396.709662732655
Iteration 720: Loss = -12396.709367301077
Iteration 730: Loss = -12396.70903087668
Iteration 740: Loss = -12396.708755184754
Iteration 750: Loss = -12396.708449624915
Iteration 760: Loss = -12396.708145958577
Iteration 770: Loss = -12396.707845792407
Iteration 780: Loss = -12396.707528643943
Iteration 790: Loss = -12396.707306946915
Iteration 800: Loss = -12396.706982537002
Iteration 810: Loss = -12396.706739826086
Iteration 820: Loss = -12396.706435384795
Iteration 830: Loss = -12396.706231324784
Iteration 840: Loss = -12396.705965808085
Iteration 850: Loss = -12396.705681359495
Iteration 860: Loss = -12396.705444352154
Iteration 870: Loss = -12396.705197020125
Iteration 880: Loss = -12396.704967140178
Iteration 890: Loss = -12396.704740805126
Iteration 900: Loss = -12396.704480559854
Iteration 910: Loss = -12396.704282980963
Iteration 920: Loss = -12396.70402802847
Iteration 930: Loss = -12396.703823377668
Iteration 940: Loss = -12396.703572774684
Iteration 950: Loss = -12396.703344339609
Iteration 960: Loss = -12396.703125034139
Iteration 970: Loss = -12396.702951625419
Iteration 980: Loss = -12396.702739967977
Iteration 990: Loss = -12396.702521586205
Iteration 1000: Loss = -12396.70229200586
Iteration 1010: Loss = -12396.70208765746
Iteration 1020: Loss = -12396.701913425875
Iteration 1030: Loss = -12396.701694255133
Iteration 1040: Loss = -12396.701529541633
Iteration 1050: Loss = -12396.701408621586
Iteration 1060: Loss = -12396.701146789786
Iteration 1070: Loss = -12396.701005599873
Iteration 1080: Loss = -12396.700814235757
Iteration 1090: Loss = -12396.700630743353
Iteration 1100: Loss = -12396.7004892471
Iteration 1110: Loss = -12396.700282322005
Iteration 1120: Loss = -12396.700112287404
Iteration 1130: Loss = -12396.699940235587
Iteration 1140: Loss = -12396.699787326352
Iteration 1150: Loss = -12396.699645275223
Iteration 1160: Loss = -12396.699477988195
Iteration 1170: Loss = -12396.699312602499
Iteration 1180: Loss = -12396.69918356254
Iteration 1190: Loss = -12396.699011215727
Iteration 1200: Loss = -12396.698824575611
Iteration 1210: Loss = -12396.698649890603
Iteration 1220: Loss = -12396.698536304388
Iteration 1230: Loss = -12396.698396915077
Iteration 1240: Loss = -12396.698249952562
Iteration 1250: Loss = -12396.698114805336
Iteration 1260: Loss = -12396.697974482258
Iteration 1270: Loss = -12396.697821547878
Iteration 1280: Loss = -12396.69765157364
Iteration 1290: Loss = -12396.69754911348
Iteration 1300: Loss = -12396.697426449758
Iteration 1310: Loss = -12396.697291289129
Iteration 1320: Loss = -12396.697136400673
Iteration 1330: Loss = -12396.69701328078
Iteration 1340: Loss = -12396.696881276668
Iteration 1350: Loss = -12396.69676606359
Iteration 1360: Loss = -12396.696647615388
Iteration 1370: Loss = -12396.696549086235
Iteration 1380: Loss = -12396.696390929545
Iteration 1390: Loss = -12396.696236768053
Iteration 1400: Loss = -12396.69617267985
Iteration 1410: Loss = -12396.69601397101
Iteration 1420: Loss = -12396.695941063677
Iteration 1430: Loss = -12396.695798454346
Iteration 1440: Loss = -12396.69573472092
Iteration 1450: Loss = -12396.695541380492
Iteration 1460: Loss = -12396.695470520313
Iteration 1470: Loss = -12396.695403954289
Iteration 1480: Loss = -12396.69525214621
Iteration 1490: Loss = -12396.695149927336
Iteration 1500: Loss = -12396.69507614099
Iteration 1510: Loss = -12396.69490532531
Iteration 1520: Loss = -12396.694827251093
Iteration 1530: Loss = -12396.694739787687
Iteration 1540: Loss = -12396.694623294059
Iteration 1550: Loss = -12396.694557404586
Iteration 1560: Loss = -12396.694475878845
Iteration 1570: Loss = -12396.694349662679
Iteration 1580: Loss = -12396.69428708316
Iteration 1590: Loss = -12396.694165824347
Iteration 1600: Loss = -12396.694103147724
Iteration 1610: Loss = -12396.693984757567
Iteration 1620: Loss = -12396.693888054504
Iteration 1630: Loss = -12396.693805913266
Iteration 1640: Loss = -12396.693676182478
Iteration 1650: Loss = -12396.6936268159
Iteration 1660: Loss = -12396.6935200164
Iteration 1670: Loss = -12396.693454640112
Iteration 1680: Loss = -12396.69334947453
Iteration 1690: Loss = -12396.693250339758
Iteration 1700: Loss = -12396.693179990903
Iteration 1710: Loss = -12396.693115214903
Iteration 1720: Loss = -12396.692993383107
Iteration 1730: Loss = -12396.692975009984
Iteration 1740: Loss = -12396.692847806491
Iteration 1750: Loss = -12396.692786110456
Iteration 1760: Loss = -12396.692710628446
Iteration 1770: Loss = -12396.692671117018
Iteration 1780: Loss = -12396.692557645072
Iteration 1790: Loss = -12396.692465562757
Iteration 1800: Loss = -12396.692397833574
Iteration 1810: Loss = -12396.692324362044
Iteration 1820: Loss = -12396.692269296296
Iteration 1830: Loss = -12396.692154583587
Iteration 1840: Loss = -12396.692081072402
Iteration 1850: Loss = -12396.692041928576
Iteration 1860: Loss = -12396.691985606849
Iteration 1870: Loss = -12396.69188052182
Iteration 1880: Loss = -12396.69181598377
Iteration 1890: Loss = -12396.69172952336
Iteration 1900: Loss = -12396.691709774985
Iteration 1910: Loss = -12396.691595086775
Iteration 1920: Loss = -12396.691554552546
Iteration 1930: Loss = -12396.691472263872
Iteration 1940: Loss = -12396.691456077755
Iteration 1950: Loss = -12396.691358774715
Iteration 1960: Loss = -12396.691274492221
Iteration 1970: Loss = -12396.691225358081
Iteration 1980: Loss = -12396.691177677245
Iteration 1990: Loss = -12396.691120534013
Iteration 2000: Loss = -12396.691075596087
Iteration 2010: Loss = -12396.691012836343
Iteration 2020: Loss = -12396.69089189396
Iteration 2030: Loss = -12396.6908890493
Iteration 2040: Loss = -12396.690806339311
Iteration 2050: Loss = -12396.690752932702
Iteration 2060: Loss = -12396.690703139777
Iteration 2070: Loss = -12396.690664309275
Iteration 2080: Loss = -12396.690624479963
Iteration 2090: Loss = -12396.690525282123
Iteration 2100: Loss = -12396.690465825925
Iteration 2110: Loss = -12396.690434604438
Iteration 2120: Loss = -12396.690400962792
Iteration 2130: Loss = -12396.690351978239
Iteration 2140: Loss = -12396.690276839401
Iteration 2150: Loss = -12396.690228693691
Iteration 2160: Loss = -12396.69017731956
Iteration 2170: Loss = -12396.69013491181
Iteration 2180: Loss = -12396.690055319146
Iteration 2190: Loss = -12396.690002078689
Iteration 2200: Loss = -12396.689950083219
Iteration 2210: Loss = -12396.689882791392
Iteration 2220: Loss = -12396.689886420401
1
Iteration 2230: Loss = -12396.68981316068
Iteration 2240: Loss = -12396.68978960042
Iteration 2250: Loss = -12396.689733300002
Iteration 2260: Loss = -12396.689658840627
Iteration 2270: Loss = -12396.689653439948
Iteration 2280: Loss = -12396.689606326832
Iteration 2290: Loss = -12396.689550479814
Iteration 2300: Loss = -12396.689485946366
Iteration 2310: Loss = -12396.689424232198
Iteration 2320: Loss = -12396.689389252078
Iteration 2330: Loss = -12396.689352879868
Iteration 2340: Loss = -12396.689339868675
Iteration 2350: Loss = -12396.689253959708
Iteration 2360: Loss = -12396.689222569823
Iteration 2370: Loss = -12396.689175357189
Iteration 2380: Loss = -12396.689142976626
Iteration 2390: Loss = -12396.689085964386
Iteration 2400: Loss = -12396.689072388419
Iteration 2410: Loss = -12396.688986297582
Iteration 2420: Loss = -12396.688952745968
Iteration 2430: Loss = -12396.68898175642
1
Iteration 2440: Loss = -12396.688919934946
Iteration 2450: Loss = -12396.688852949641
Iteration 2460: Loss = -12396.688808389583
Iteration 2470: Loss = -12396.688763828617
Iteration 2480: Loss = -12396.688751205917
Iteration 2490: Loss = -12396.68873347351
Iteration 2500: Loss = -12396.68867869085
Iteration 2510: Loss = -12396.688613215096
Iteration 2520: Loss = -12396.688583531733
Iteration 2530: Loss = -12396.68857009442
Iteration 2540: Loss = -12396.688464870265
Iteration 2550: Loss = -12396.688528950363
1
Iteration 2560: Loss = -12396.688456771783
Iteration 2570: Loss = -12396.688434074505
Iteration 2580: Loss = -12396.688371062295
Iteration 2590: Loss = -12396.688319709954
Iteration 2600: Loss = -12396.688309871417
Iteration 2610: Loss = -12396.6882775599
Iteration 2620: Loss = -12396.688260921272
Iteration 2630: Loss = -12396.688207267183
Iteration 2640: Loss = -12396.688212286694
1
Iteration 2650: Loss = -12396.688180143927
Iteration 2660: Loss = -12396.688116273646
Iteration 2670: Loss = -12396.68809235907
Iteration 2680: Loss = -12396.688074970432
Iteration 2690: Loss = -12396.688028108158
Iteration 2700: Loss = -12396.688009002653
Iteration 2710: Loss = -12396.6879962357
Iteration 2720: Loss = -12396.687959093264
Iteration 2730: Loss = -12396.687902296591
Iteration 2740: Loss = -12396.687886322012
Iteration 2750: Loss = -12396.687865496751
Iteration 2760: Loss = -12396.687839611952
Iteration 2770: Loss = -12396.687823909702
Iteration 2780: Loss = -12396.687765302826
Iteration 2790: Loss = -12396.68772916486
Iteration 2800: Loss = -12396.6877094343
Iteration 2810: Loss = -12396.687667241087
Iteration 2820: Loss = -12396.687636113586
Iteration 2830: Loss = -12396.687625964923
Iteration 2840: Loss = -12396.687615447554
Iteration 2850: Loss = -12396.687551312654
Iteration 2860: Loss = -12396.687537102602
Iteration 2870: Loss = -12396.687503161113
Iteration 2880: Loss = -12396.687490118864
Iteration 2890: Loss = -12396.687452156135
Iteration 2900: Loss = -12396.687463775013
1
Iteration 2910: Loss = -12396.687449579802
Iteration 2920: Loss = -12396.687398338388
Iteration 2930: Loss = -12396.687386709915
Iteration 2940: Loss = -12396.687369397909
Iteration 2950: Loss = -12396.687313272298
Iteration 2960: Loss = -12396.687294905862
Iteration 2970: Loss = -12396.687280790027
Iteration 2980: Loss = -12396.687270731984
Iteration 2990: Loss = -12396.687208644295
Iteration 3000: Loss = -12396.687201172874
Iteration 3010: Loss = -12396.687206684815
1
Iteration 3020: Loss = -12396.687149824973
Iteration 3030: Loss = -12396.687132167342
Iteration 3040: Loss = -12396.687118812311
Iteration 3050: Loss = -12396.687112976488
Iteration 3060: Loss = -12396.687086854767
Iteration 3070: Loss = -12396.68703403322
Iteration 3080: Loss = -12396.687018305156
Iteration 3090: Loss = -12396.687013458666
Iteration 3100: Loss = -12396.687016514135
1
Iteration 3110: Loss = -12396.686937533324
Iteration 3120: Loss = -12396.686941503689
1
Iteration 3130: Loss = -12396.686916191717
Iteration 3140: Loss = -12396.68692724463
1
Iteration 3150: Loss = -12396.68690903141
Iteration 3160: Loss = -12396.686881885207
Iteration 3170: Loss = -12396.68682842863
Iteration 3180: Loss = -12396.68684091241
1
Iteration 3190: Loss = -12396.686801601365
Iteration 3200: Loss = -12396.686779372829
Iteration 3210: Loss = -12396.686756202369
Iteration 3220: Loss = -12396.686703340316
Iteration 3230: Loss = -12396.686718267125
1
Iteration 3240: Loss = -12396.68672644147
2
Iteration 3250: Loss = -12396.686652283131
Iteration 3260: Loss = -12396.686664195862
1
Iteration 3270: Loss = -12396.686626886707
Iteration 3280: Loss = -12396.686638747404
1
Iteration 3290: Loss = -12396.686604052564
Iteration 3300: Loss = -12396.686618803322
1
Iteration 3310: Loss = -12396.686577469722
Iteration 3320: Loss = -12396.686558285079
Iteration 3330: Loss = -12396.686555317981
Iteration 3340: Loss = -12396.686523494158
Iteration 3350: Loss = -12396.686520422798
Iteration 3360: Loss = -12396.686513031978
Iteration 3370: Loss = -12396.686476404098
Iteration 3380: Loss = -12396.686454736864
Iteration 3390: Loss = -12396.68640298813
Iteration 3400: Loss = -12396.686443859733
1
Iteration 3410: Loss = -12396.686404840462
2
Iteration 3420: Loss = -12396.68638755216
Iteration 3430: Loss = -12396.686382758968
Iteration 3440: Loss = -12396.68640108071
1
Iteration 3450: Loss = -12396.686343385805
Iteration 3460: Loss = -12396.686333507792
Iteration 3470: Loss = -12396.686324473845
Iteration 3480: Loss = -12396.686287932742
Iteration 3490: Loss = -12396.686281863811
Iteration 3500: Loss = -12396.686228780187
Iteration 3510: Loss = -12396.6862801869
1
Iteration 3520: Loss = -12396.68625217785
2
Iteration 3530: Loss = -12396.68623564671
3
Stopping early at iteration 3529 due to no improvement.
pi: tensor([[0.1385, 0.8615],
        [0.1079, 0.8921]], dtype=torch.float64)
alpha: tensor([0.1116, 0.8884])
beta: tensor([[[0.1879, 0.1862],
         [0.7395, 0.1998]],

        [[0.6544, 0.2082],
         [0.4254, 0.6003]],

        [[0.1171, 0.2093],
         [0.6228, 0.5675]],

        [[0.3426, 0.1831],
         [0.6443, 0.6639]],

        [[0.2612, 0.1809],
         [0.7508, 0.6476]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23807.499136744245
Iteration 100: Loss = -12422.169277865603
Iteration 200: Loss = -12402.839876430447
Iteration 300: Loss = -12398.529910038991
Iteration 400: Loss = -12397.724768998656
Iteration 500: Loss = -12397.431092075416
Iteration 600: Loss = -12397.289748094636
Iteration 700: Loss = -12397.187168126782
Iteration 800: Loss = -12397.103921687716
Iteration 900: Loss = -12397.027342345213
Iteration 1000: Loss = -12396.960610838605
Iteration 1100: Loss = -12396.890844140056
Iteration 1200: Loss = -12396.830190119688
Iteration 1300: Loss = -12396.77624211879
Iteration 1400: Loss = -12396.708566324753
Iteration 1500: Loss = -12396.612301547004
Iteration 1600: Loss = -12396.562105664916
Iteration 1700: Loss = -12396.518948786395
Iteration 1800: Loss = -12396.474572854238
Iteration 1900: Loss = -12396.446503242512
Iteration 2000: Loss = -12396.422709798106
Iteration 2100: Loss = -12396.399771779486
Iteration 2200: Loss = -12396.381564026246
Iteration 2300: Loss = -12396.358910745294
Iteration 2400: Loss = -12396.321079143885
Iteration 2500: Loss = -12396.307756308339
Iteration 2600: Loss = -12396.29620878845
Iteration 2700: Loss = -12396.286192682617
Iteration 2800: Loss = -12396.277271614455
Iteration 2900: Loss = -12396.269135632592
Iteration 3000: Loss = -12396.26150697352
Iteration 3100: Loss = -12396.254353048032
Iteration 3200: Loss = -12396.247762384035
Iteration 3300: Loss = -12396.241599550614
Iteration 3400: Loss = -12396.235617906254
Iteration 3500: Loss = -12396.229530454517
Iteration 3600: Loss = -12396.223201561395
Iteration 3700: Loss = -12396.21650567528
Iteration 3800: Loss = -12396.20936363125
Iteration 3900: Loss = -12396.201914984864
Iteration 4000: Loss = -12396.194043645206
Iteration 4100: Loss = -12396.186017107615
Iteration 4200: Loss = -12396.178050988327
Iteration 4300: Loss = -12396.19745127569
1
Iteration 4400: Loss = -12396.163246869202
Iteration 4500: Loss = -12396.157679798598
Iteration 4600: Loss = -12396.151270606695
Iteration 4700: Loss = -12396.171069263506
1
Iteration 4800: Loss = -12396.142743804834
Iteration 4900: Loss = -12396.139640108011
Iteration 5000: Loss = -12396.13740752835
Iteration 5100: Loss = -12396.135226247665
Iteration 5200: Loss = -12396.134052662124
Iteration 5300: Loss = -12396.132134578207
Iteration 5400: Loss = -12396.132457746842
1
Iteration 5500: Loss = -12396.12971576853
Iteration 5600: Loss = -12396.128688213512
Iteration 5700: Loss = -12396.1274991109
Iteration 5800: Loss = -12396.126713354239
Iteration 5900: Loss = -12396.137550056997
1
Iteration 6000: Loss = -12396.124414452794
Iteration 6100: Loss = -12396.128201156327
1
Iteration 6200: Loss = -12396.122475732243
Iteration 6300: Loss = -12396.121939597364
Iteration 6400: Loss = -12396.121287834378
Iteration 6500: Loss = -12396.121722577149
1
Iteration 6600: Loss = -12396.119708012413
Iteration 6700: Loss = -12396.11900700172
Iteration 6800: Loss = -12396.118759885543
Iteration 6900: Loss = -12396.139376916773
1
Iteration 7000: Loss = -12396.133832240654
2
Iteration 7100: Loss = -12396.11751734033
Iteration 7200: Loss = -12396.11738096887
Iteration 7300: Loss = -12396.12236367015
1
Iteration 7400: Loss = -12396.118518414842
2
Iteration 7500: Loss = -12396.156677749932
3
Iteration 7600: Loss = -12396.118898059538
4
Iteration 7700: Loss = -12396.116407268826
Iteration 7800: Loss = -12396.117962662533
1
Iteration 7900: Loss = -12396.119650114393
2
Iteration 8000: Loss = -12396.126818082716
3
Iteration 8100: Loss = -12396.120130914827
4
Iteration 8200: Loss = -12396.117471895532
5
Iteration 8300: Loss = -12396.139643261646
6
Iteration 8400: Loss = -12396.269173916
7
Iteration 8500: Loss = -12396.116297282972
Iteration 8600: Loss = -12396.115649713143
Iteration 8700: Loss = -12396.30175802601
1
Iteration 8800: Loss = -12396.128357759495
2
Iteration 8900: Loss = -12396.174130173653
3
Iteration 9000: Loss = -12396.136816786258
4
Iteration 9100: Loss = -12396.11743873131
5
Iteration 9200: Loss = -12396.116050805009
6
Iteration 9300: Loss = -12396.115284524683
Iteration 9400: Loss = -12396.154730334216
1
Iteration 9500: Loss = -12396.121886124543
2
Iteration 9600: Loss = -12396.11588217775
3
Iteration 9700: Loss = -12396.12031139666
4
Iteration 9800: Loss = -12396.115068542878
Iteration 9900: Loss = -12396.118226618857
1
Iteration 10000: Loss = -12396.229304282928
2
Iteration 10100: Loss = -12396.114990109738
Iteration 10200: Loss = -12396.127028080908
1
Iteration 10300: Loss = -12396.138876763298
2
Iteration 10400: Loss = -12396.115217497889
3
Iteration 10500: Loss = -12396.11763344281
4
Iteration 10600: Loss = -12396.123032170563
5
Iteration 10700: Loss = -12396.11895564691
6
Iteration 10800: Loss = -12396.114848694771
Iteration 10900: Loss = -12396.147587575893
1
Iteration 11000: Loss = -12396.115868691553
2
Iteration 11100: Loss = -12396.115769784368
3
Iteration 11200: Loss = -12396.118316740281
4
Iteration 11300: Loss = -12396.119937705058
5
Iteration 11400: Loss = -12396.11493238196
6
Iteration 11500: Loss = -12396.11733127269
7
Iteration 11600: Loss = -12396.194296779275
8
Iteration 11700: Loss = -12396.11745469124
9
Iteration 11800: Loss = -12396.449934886943
10
Stopping early at iteration 11800 due to no improvement.
tensor([[ 1.5394, -3.8323],
        [ 0.3962, -5.0115],
        [ 1.6603, -3.7671],
        [ 1.7304, -3.6708],
        [ 1.4812, -3.8785],
        [ 1.4848, -3.9259],
        [ 1.9750, -3.3614],
        [ 1.9916, -3.4249],
        [ 1.3933, -4.1251],
        [ 1.9715, -3.3959],
        [ 1.9979, -3.4548],
        [ 1.9740, -3.3989],
        [ 1.9129, -3.4574],
        [ 2.0198, -3.4077],
        [ 1.8077, -3.6722],
        [ 1.8926, -3.4754],
        [ 1.9653, -3.4076],
        [ 1.9564, -3.4420],
        [ 1.3977, -4.0334],
        [ 1.9874, -3.3881],
        [ 0.5171, -4.7564],
        [ 1.8097, -3.6010],
        [ 2.0029, -3.4089],
        [ 1.8777, -3.5644],
        [ 1.8926, -3.4882],
        [ 1.7767, -3.6456],
        [ 0.8925, -4.5116],
        [ 2.0115, -3.4946],
        [ 0.5495, -4.9453],
        [ 0.3599, -4.9751],
        [ 2.0009, -3.4261],
        [ 1.9116, -3.4531],
        [ 1.9685, -3.4477],
        [ 1.8483, -3.5883],
        [ 1.9815, -3.4014],
        [ 1.9453, -3.4226],
        [ 1.9161, -3.5091],
        [ 1.7931, -3.6024],
        [ 1.4505, -3.8393],
        [ 1.5227, -3.8239],
        [ 1.4623, -3.9254],
        [ 1.9614, -3.4168],
        [ 1.9509, -3.3579],
        [ 1.1537, -4.1593],
        [ 2.0007, -3.4844],
        [ 1.0771, -4.3739],
        [ 2.0111, -3.4025],
        [ 2.0147, -3.4116],
        [ 2.0079, -3.3976],
        [ 0.7558, -4.6873],
        [ 0.8488, -4.4755],
        [ 1.1558, -4.3031],
        [ 1.8670, -3.6059],
        [ 1.5758, -3.8204],
        [ 1.9826, -3.3825],
        [ 1.9663, -3.4191],
        [ 1.9462, -3.3881],
        [ 1.9159, -3.5473],
        [ 1.0712, -4.2519],
        [ 1.1038, -4.3712],
        [ 2.0492, -3.4356],
        [ 1.5892, -3.6990],
        [ 1.8417, -3.5900],
        [ 1.0413, -4.4075],
        [ 1.9518, -3.4755],
        [ 1.6576, -3.6927],
        [ 1.4937, -3.9244],
        [ 1.9474, -3.4283],
        [ 1.4740, -3.8678],
        [ 2.0103, -3.3965],
        [ 1.9173, -3.5504],
        [ 1.9247, -3.3654],
        [ 1.4297, -3.9727],
        [ 1.6941, -3.7005],
        [ 1.9576, -3.4837],
        [ 1.8062, -3.5161],
        [ 1.6953, -3.7255],
        [ 2.0143, -3.4009],
        [ 1.9311, -3.4922],
        [ 1.3201, -4.0827],
        [ 0.5009, -4.9161],
        [ 1.9838, -3.3702],
        [ 1.9583, -3.4080],
        [ 1.6211, -3.7426],
        [ 1.9969, -3.4074],
        [ 1.7172, -3.6962],
        [ 1.9478, -3.3802],
        [ 0.9265, -4.4941],
        [ 0.7606, -4.6296],
        [ 1.9325, -3.4924],
        [ 1.8072, -3.5545],
        [ 1.3796, -4.0213],
        [ 1.9725, -3.5264],
        [ 1.9784, -3.3943],
        [ 1.0042, -4.2985],
        [ 1.8507, -3.5172],
        [ 1.6408, -3.7367],
        [ 1.7549, -3.5841],
        [ 1.9373, -3.4052],
        [ 2.0244, -3.4217]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.1660, 0.8340],
        [0.3954, 0.6046]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9955, 0.0045], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1983, 0.1984],
         [0.7395, 0.2016]],

        [[0.6544, 0.2093],
         [0.4254, 0.6003]],

        [[0.1171, 0.2091],
         [0.6228, 0.5675]],

        [[0.3426, 0.1957],
         [0.6443, 0.6639]],

        [[0.2612, 0.1951],
         [0.7508, 0.6476]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0017326397458452079
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -22947.17413942325
Iteration 10: Loss = -12397.738049695738
Iteration 20: Loss = -12397.738049783706
1
Iteration 30: Loss = -12397.738050250256
2
Iteration 40: Loss = -12397.738051800614
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[1.6591e-26, 1.0000e+00],
        [9.1145e-11, 1.0000e+00]], dtype=torch.float64)
alpha: tensor([8.9948e-11, 1.0000e+00])
beta: tensor([[[0.1832, 0.1509],
         [0.1693, 0.1984]],

        [[0.2607, 0.2441],
         [0.3387, 0.9055]],

        [[0.3435, 0.2535],
         [0.6009, 0.7861]],

        [[0.5230, 0.2901],
         [0.5051, 0.2814]],

        [[0.7736, 0.1146],
         [0.4966, 0.2469]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22946.999575766487
Iteration 100: Loss = -12407.89828174919
Iteration 200: Loss = -12400.101299224254
Iteration 300: Loss = -12398.780351271
Iteration 400: Loss = -12398.178713627138
Iteration 500: Loss = -12397.898424737383
Iteration 600: Loss = -12397.739983921032
Iteration 700: Loss = -12397.63466844122
Iteration 800: Loss = -12397.560134032454
Iteration 900: Loss = -12397.507376373382
Iteration 1000: Loss = -12397.465672872791
Iteration 1100: Loss = -12397.429396787142
Iteration 1200: Loss = -12397.386857645723
Iteration 1300: Loss = -12397.35485791287
Iteration 1400: Loss = -12397.329084240031
Iteration 1500: Loss = -12397.305169587611
Iteration 1600: Loss = -12397.282426136384
Iteration 1700: Loss = -12397.26021186182
Iteration 1800: Loss = -12397.238064678397
Iteration 1900: Loss = -12397.21555047032
Iteration 2000: Loss = -12397.1924720911
Iteration 2100: Loss = -12397.168250414896
Iteration 2200: Loss = -12397.141900720562
Iteration 2300: Loss = -12397.112466421979
Iteration 2400: Loss = -12397.078418396304
Iteration 2500: Loss = -12397.038249625375
Iteration 2600: Loss = -12396.990630569378
Iteration 2700: Loss = -12396.93566211855
Iteration 2800: Loss = -12396.873640803624
Iteration 2900: Loss = -12396.807492153246
Iteration 3000: Loss = -12396.743350236155
Iteration 3100: Loss = -12396.730846815357
Iteration 3200: Loss = -12396.653642714675
Iteration 3300: Loss = -12396.62320652531
Iteration 3400: Loss = -12396.600577378253
Iteration 3500: Loss = -12396.584719541495
Iteration 3600: Loss = -12396.571031582374
Iteration 3700: Loss = -12396.559475961549
Iteration 3800: Loss = -12396.603437039186
1
Iteration 3900: Loss = -12396.535403055976
Iteration 4000: Loss = -12396.52103251893
Iteration 4100: Loss = -12396.50315085141
Iteration 4200: Loss = -12396.479151391342
Iteration 4300: Loss = -12396.445933035817
Iteration 4400: Loss = -12396.401354072887
Iteration 4500: Loss = -12396.353422031074
Iteration 4600: Loss = -12396.316795403178
Iteration 4700: Loss = -12396.290424692017
Iteration 4800: Loss = -12396.27388033132
Iteration 4900: Loss = -12396.26189172596
Iteration 5000: Loss = -12396.253846812377
Iteration 5100: Loss = -12396.24419294749
Iteration 5200: Loss = -12396.237100514983
Iteration 5300: Loss = -12396.23023490352
Iteration 5400: Loss = -12396.22332272568
Iteration 5500: Loss = -12396.215495875033
Iteration 5600: Loss = -12396.207239965122
Iteration 5700: Loss = -12396.216675697084
1
Iteration 5800: Loss = -12396.189817590777
Iteration 5900: Loss = -12396.18094033694
Iteration 6000: Loss = -12396.199651949175
1
Iteration 6100: Loss = -12396.172930388864
Iteration 6200: Loss = -12396.161170989546
Iteration 6300: Loss = -12396.152349854012
Iteration 6400: Loss = -12396.149739714658
Iteration 6500: Loss = -12396.148744310647
Iteration 6600: Loss = -12396.14117889467
Iteration 6700: Loss = -12396.142192470468
1
Iteration 6800: Loss = -12396.155101095157
2
Iteration 6900: Loss = -12396.146189889629
3
Iteration 7000: Loss = -12396.133760465997
Iteration 7100: Loss = -12396.132525791269
Iteration 7200: Loss = -12396.140847911716
1
Iteration 7300: Loss = -12396.130165300907
Iteration 7400: Loss = -12396.13071845294
1
Iteration 7500: Loss = -12396.127682201039
Iteration 7600: Loss = -12396.128871119821
1
Iteration 7700: Loss = -12396.125136878414
Iteration 7800: Loss = -12396.124080454212
Iteration 7900: Loss = -12396.122826124469
Iteration 8000: Loss = -12396.12531065457
1
Iteration 8100: Loss = -12396.122205533371
Iteration 8200: Loss = -12396.121418072398
Iteration 8300: Loss = -12396.122718981022
1
Iteration 8400: Loss = -12396.11896311268
Iteration 8500: Loss = -12396.11870681264
Iteration 8600: Loss = -12396.119348528939
1
Iteration 8700: Loss = -12396.120934991519
2
Iteration 8800: Loss = -12396.141980631512
3
Iteration 8900: Loss = -12396.11808011228
Iteration 9000: Loss = -12396.117644340271
Iteration 9100: Loss = -12396.133554102518
1
Iteration 9200: Loss = -12396.117653322484
2
Iteration 9300: Loss = -12396.136570038327
3
Iteration 9400: Loss = -12396.127257461321
4
Iteration 9500: Loss = -12396.312214440348
5
Iteration 9600: Loss = -12396.116859348207
Iteration 9700: Loss = -12396.118808559271
1
Iteration 9800: Loss = -12396.117040156347
2
Iteration 9900: Loss = -12396.115861480792
Iteration 10000: Loss = -12396.118452669945
1
Iteration 10100: Loss = -12396.11701234047
2
Iteration 10200: Loss = -12396.161649780846
3
Iteration 10300: Loss = -12396.11667582345
4
Iteration 10400: Loss = -12396.116953507071
5
Iteration 10500: Loss = -12396.157630150492
6
Iteration 10600: Loss = -12396.133870119924
7
Iteration 10700: Loss = -12396.116845228704
8
Iteration 10800: Loss = -12396.123923313025
9
Iteration 10900: Loss = -12396.132521467378
10
Stopping early at iteration 10900 due to no improvement.
tensor([[ 0.5654, -3.6318],
        [ 1.1536, -3.0818],
        [ 0.9633, -3.2943],
        [ 1.1154, -3.1130],
        [ 1.3740, -2.8109],
        [ 1.4162, -2.8219],
        [ 1.3389, -2.8177],
        [ 1.3540, -2.8906],
        [ 1.1022, -3.2520],
        [ 1.3076, -2.8845],
        [ 0.2083, -4.0771],
        [ 1.4057, -2.7920],
        [ 0.8362, -3.3590],
        [ 1.3095, -2.9475],
        [ 1.4124, -2.9013],
        [ 1.3971, -2.7965],
        [ 1.1284, -3.0712],
        [ 1.3582, -2.8663],
        [ 0.2241, -4.0367],
        [ 1.1536, -3.0461],
        [ 1.3297, -2.7594],
        [ 1.3328, -2.9043],
        [ 1.1929, -3.0473],
        [ 0.1054, -4.1700],
        [ 1.0139, -3.1912],
        [ 0.9084, -3.3424],
        [ 1.2671, -2.9651],
        [ 1.4795, -2.8664],
        [ 1.3010, -3.0290],
        [ 0.9361, -3.2276],
        [ 1.4333, -2.8230],
        [ 1.0712, -3.1176],
        [ 1.2276, -3.0165],
        [ 1.2140, -3.0536],
        [ 1.1030, -3.1034],
        [ 1.1146, -3.0768],
        [ 1.3604, -2.8924],
        [ 1.3849, -2.8366],
        [ 1.2983, -2.8056],
        [ 1.1060, -3.0650],
        [ 1.2533, -2.9605],
        [ 1.4058, -2.7961],
        [ 1.3126, -2.8092],
        [ 1.3739, -2.7645],
        [ 1.0738, -3.2493],
        [ 1.1486, -3.1331],
        [ 1.2856, -2.9559],
        [ 0.4791, -3.7780],
        [ 0.7616, -3.4725],
        [ 1.3734, -2.8996],
        [ 1.3581, -2.7849],
        [ 1.0907, -3.1952],
        [ 1.2018, -3.1061],
        [ 1.4009, -2.8248],
        [ 1.2082, -2.9809],
        [ 1.4099, -2.8015],
        [ 1.3735, -2.7831],
        [ 1.3607, -2.9357],
        [ 1.3683, -2.7746],
        [ 1.2144, -3.0880],
        [ 0.5434, -3.7765],
        [ 1.2995, -2.8061],
        [ 1.1189, -3.1426],
        [ 1.4415, -2.8280],
        [ 1.1810, -3.0762],
        [ 1.3608, -2.8172],
        [ 1.3581, -2.8879],
        [ 1.3467, -2.8533],
        [ 1.3099, -2.8534],
        [ 1.2836, -2.9495],
        [ 0.6170, -3.6876],
        [ 1.1411, -2.9601],
        [ 1.0787, -3.1497],
        [ 0.6346, -3.5848],
        [ 1.1739, -3.0992],
        [ 1.2563, -2.8871],
        [ 1.4163, -2.8324],
        [ 1.2200, -3.0222],
        [ 1.1018, -3.1495],
        [ 1.4011, -2.8279],
        [ 1.1064, -3.1357],
        [ 1.0426, -3.1320],
        [ 1.3288, -2.8603],
        [ 1.3930, -2.7970],
        [-0.0172, -4.2483],
        [ 1.4043, -2.8368],
        [ 0.8330, -3.3132],
        [ 0.5821, -3.6654],
        [ 1.0430, -3.1779],
        [ 1.4007, -2.8531],
        [ 0.7893, -3.3953],
        [ 1.0211, -3.2001],
        [ 1.1932, -3.1420],
        [ 1.1996, -2.9976],
        [ 1.0676, -3.0550],
        [ 1.2850, -2.9074],
        [ 0.6901, -3.5145],
        [ 1.3824, -2.7758],
        [ 1.2225, -2.9421],
        [ 1.3362, -2.9404]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.1659, 0.8341],
        [0.3937, 0.6063]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9855, 0.0145], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1968, 0.1986],
         [0.1693, 0.2031]],

        [[0.2607, 0.2076],
         [0.3387, 0.9055]],

        [[0.3435, 0.2074],
         [0.6009, 0.7861]],

        [[0.5230, 0.1943],
         [0.5051, 0.2814]],

        [[0.7736, 0.1937],
         [0.4966, 0.2469]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0017326397458452079
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -25144.2705559228
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.5741,    nan]],

        [[0.8242,    nan],
         [0.9262, 0.1945]],

        [[0.9644,    nan],
         [0.1814, 0.8398]],

        [[0.8565,    nan],
         [0.4022, 0.3075]],

        [[0.4185,    nan],
         [0.5735, 0.6137]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25145.307151027228
Iteration 100: Loss = -12402.801267591607
Iteration 200: Loss = -12400.624466532494
Iteration 300: Loss = -12399.620591386894
Iteration 400: Loss = -12399.005841946853
Iteration 500: Loss = -12398.610922175674
Iteration 600: Loss = -12398.344689430598
Iteration 700: Loss = -12398.155745169628
Iteration 800: Loss = -12398.01832507933
Iteration 900: Loss = -12397.910121104926
Iteration 1000: Loss = -12397.827179642261
Iteration 1100: Loss = -12398.249798301274
1
Iteration 1200: Loss = -12397.70716157926
Iteration 1300: Loss = -12397.662747588058
Iteration 1400: Loss = -12397.625626644653
Iteration 1500: Loss = -12397.59498142478
Iteration 1600: Loss = -12397.56730028048
Iteration 1700: Loss = -12397.544051315892
Iteration 1800: Loss = -12397.523694612988
Iteration 1900: Loss = -12397.505787213177
Iteration 2000: Loss = -12397.489850310418
Iteration 2100: Loss = -12397.47551888107
Iteration 2200: Loss = -12397.462607372789
Iteration 2300: Loss = -12397.450581940066
Iteration 2400: Loss = -12397.439521067989
Iteration 2500: Loss = -12397.429054630884
Iteration 2600: Loss = -12397.424820892902
Iteration 2700: Loss = -12397.409344097628
Iteration 2800: Loss = -12397.399720376308
Iteration 2900: Loss = -12397.390261177297
Iteration 3000: Loss = -12397.380690262255
Iteration 3100: Loss = -12397.3703824217
Iteration 3200: Loss = -12397.359553106971
Iteration 3300: Loss = -12397.351705128038
Iteration 3400: Loss = -12397.334729014698
Iteration 3500: Loss = -12397.32037801009
Iteration 3600: Loss = -12397.304654223322
Iteration 3700: Loss = -12397.298738955482
Iteration 3800: Loss = -12397.269899125793
Iteration 3900: Loss = -12397.25157996846
Iteration 4000: Loss = -12397.23276253597
Iteration 4100: Loss = -12397.212971571049
Iteration 4200: Loss = -12397.19203404785
Iteration 4300: Loss = -12397.169456174259
Iteration 4400: Loss = -12397.152161293583
Iteration 4500: Loss = -12397.117160328522
Iteration 4600: Loss = -12397.086323810452
Iteration 4700: Loss = -12397.051771592167
Iteration 4800: Loss = -12397.013200269654
Iteration 4900: Loss = -12396.971134068961
Iteration 5000: Loss = -12396.92657056433
Iteration 5100: Loss = -12396.88764499619
Iteration 5200: Loss = -12396.837845568461
Iteration 5300: Loss = -12396.797477601514
Iteration 5400: Loss = -12396.760395844307
Iteration 5500: Loss = -12396.726529013622
Iteration 5600: Loss = -12396.694152786471
Iteration 5700: Loss = -12396.66562509046
Iteration 5800: Loss = -12396.662368787986
Iteration 5900: Loss = -12396.621546814982
Iteration 6000: Loss = -12396.604245189665
Iteration 6100: Loss = -12396.588205656797
Iteration 6200: Loss = -12396.572788534522
Iteration 6300: Loss = -12396.557417020296
Iteration 6400: Loss = -12396.541500410474
Iteration 6500: Loss = -12396.524350557898
Iteration 6600: Loss = -12396.505555860474
Iteration 6700: Loss = -12396.484445539121
Iteration 6800: Loss = -12396.460370796603
Iteration 6900: Loss = -12396.443824868706
Iteration 7000: Loss = -12396.390578802191
Iteration 7100: Loss = -12396.311939783252
Iteration 7200: Loss = -12395.925520401546
Iteration 7300: Loss = -12395.239059988815
Iteration 7400: Loss = -12395.028323110939
Iteration 7500: Loss = -12394.943015027628
Iteration 7600: Loss = -12394.868594369824
Iteration 7700: Loss = -12303.450093652724
Iteration 7800: Loss = -11986.213111922578
Iteration 7900: Loss = -11986.046434615551
Iteration 8000: Loss = -11985.966399189592
Iteration 8100: Loss = -11985.707783386782
Iteration 8200: Loss = -11985.709655403818
1
Iteration 8300: Loss = -11985.683046921664
Iteration 8400: Loss = -11982.34207555021
Iteration 8500: Loss = -11982.339964640083
Iteration 8600: Loss = -11982.334264462532
Iteration 8700: Loss = -11982.33224127618
Iteration 8800: Loss = -11982.330537592316
Iteration 8900: Loss = -11982.328312466692
Iteration 9000: Loss = -11982.334442455694
1
Iteration 9100: Loss = -11982.326622351484
Iteration 9200: Loss = -11982.326342811526
Iteration 9300: Loss = -11982.336723149572
1
Iteration 9400: Loss = -11982.324388305451
Iteration 9500: Loss = -11982.337729770947
1
Iteration 9600: Loss = -11982.322704181162
Iteration 9700: Loss = -11982.32523789773
1
Iteration 9800: Loss = -11982.322399782408
Iteration 9900: Loss = -11982.319500796368
Iteration 10000: Loss = -11982.321862294848
1
Iteration 10100: Loss = -11982.318026157107
Iteration 10200: Loss = -11982.326895945855
1
Iteration 10300: Loss = -11982.344051588349
2
Iteration 10400: Loss = -11982.320377363996
3
Iteration 10500: Loss = -11982.32046744858
4
Iteration 10600: Loss = -11982.348878566527
5
Iteration 10700: Loss = -11982.342446003244
6
Iteration 10800: Loss = -11982.298927502583
Iteration 10900: Loss = -11981.680622027769
Iteration 11000: Loss = -11981.559252059293
Iteration 11100: Loss = -11978.810559543432
Iteration 11200: Loss = -11969.444715236206
Iteration 11300: Loss = -11969.427556835115
Iteration 11400: Loss = -11965.95370408196
Iteration 11500: Loss = -11965.956883338202
1
Iteration 11600: Loss = -11966.033816175051
2
Iteration 11700: Loss = -11965.919187313531
Iteration 11800: Loss = -11965.913681598118
Iteration 11900: Loss = -11965.917042613939
1
Iteration 12000: Loss = -11965.92150730919
2
Iteration 12100: Loss = -11965.935394350268
3
Iteration 12200: Loss = -11965.91342802805
Iteration 12300: Loss = -11965.917113119423
1
Iteration 12400: Loss = -11965.918861817292
2
Iteration 12500: Loss = -11965.912952602815
Iteration 12600: Loss = -11965.914095715982
1
Iteration 12700: Loss = -11965.916959914115
2
Iteration 12800: Loss = -11965.944542471709
3
Iteration 12900: Loss = -11965.9143655941
4
Iteration 13000: Loss = -11965.915056262416
5
Iteration 13100: Loss = -11965.911904574761
Iteration 13200: Loss = -11965.91730067816
1
Iteration 13300: Loss = -11965.914417046833
2
Iteration 13400: Loss = -11965.919324410674
3
Iteration 13500: Loss = -11965.912409068824
4
Iteration 13600: Loss = -11965.912090006215
5
Iteration 13700: Loss = -11965.91026344863
Iteration 13800: Loss = -11965.911624212004
1
Iteration 13900: Loss = -11965.911526142789
2
Iteration 14000: Loss = -11965.942408918314
3
Iteration 14100: Loss = -11965.911384207879
4
Iteration 14200: Loss = -11965.910266597662
5
Iteration 14300: Loss = -11965.937074553478
6
Iteration 14400: Loss = -11966.000108311835
7
Iteration 14500: Loss = -11965.992038221817
8
Iteration 14600: Loss = -11965.948366486256
9
Iteration 14700: Loss = -11965.903081049048
Iteration 14800: Loss = -11965.903333830083
1
Iteration 14900: Loss = -11965.903596712978
2
Iteration 15000: Loss = -11965.913418873879
3
Iteration 15100: Loss = -11965.906265220867
4
Iteration 15200: Loss = -11965.910538367945
5
Iteration 15300: Loss = -11965.905421637619
6
Iteration 15400: Loss = -11965.875255355484
Iteration 15500: Loss = -11965.875622439156
1
Iteration 15600: Loss = -11965.882923544892
2
Iteration 15700: Loss = -11965.91146073482
3
Iteration 15800: Loss = -11965.876156257385
4
Iteration 15900: Loss = -11965.874711510742
Iteration 16000: Loss = -11965.875971651565
1
Iteration 16100: Loss = -11965.875036653188
2
Iteration 16200: Loss = -11962.610994618939
Iteration 16300: Loss = -11962.60569996558
Iteration 16400: Loss = -11962.605296120479
Iteration 16500: Loss = -11962.61584389073
1
Iteration 16600: Loss = -11962.603680922579
Iteration 16700: Loss = -11962.612889914519
1
Iteration 16800: Loss = -11962.604127390863
2
Iteration 16900: Loss = -11962.6034560396
Iteration 17000: Loss = -11962.604336363589
1
Iteration 17100: Loss = -11962.610362060797
2
Iteration 17200: Loss = -11962.625258218595
3
Iteration 17300: Loss = -11962.607554085458
4
Iteration 17400: Loss = -11962.603437637597
Iteration 17500: Loss = -11962.603681350207
1
Iteration 17600: Loss = -11962.622350451342
2
Iteration 17700: Loss = -11962.662755027013
3
Iteration 17800: Loss = -11962.617175926902
4
Iteration 17900: Loss = -11962.604428244416
5
Iteration 18000: Loss = -11962.60312629716
Iteration 18100: Loss = -11962.632923145828
1
Iteration 18200: Loss = -11962.609939461798
2
Iteration 18300: Loss = -11962.610885191305
3
Iteration 18400: Loss = -11962.600495578921
Iteration 18500: Loss = -11962.63481049029
1
Iteration 18600: Loss = -11962.599845154826
Iteration 18700: Loss = -11962.64523362507
1
Iteration 18800: Loss = -11962.601053043594
2
Iteration 18900: Loss = -11962.602242808613
3
Iteration 19000: Loss = -11962.603663285752
4
Iteration 19100: Loss = -11962.599852234674
5
Iteration 19200: Loss = -11962.623060794383
6
Iteration 19300: Loss = -11962.607559753256
7
Iteration 19400: Loss = -11962.603686148886
8
Iteration 19500: Loss = -11962.603200512418
9
Iteration 19600: Loss = -11962.60149235819
10
Stopping early at iteration 19600 due to no improvement.
tensor([[  2.2461,  -5.0348],
        [  8.2319, -11.7284],
        [ -8.8778,   5.6215],
        [ -7.9569,   6.5583],
        [ -8.7902,   7.4013],
        [  0.9903,  -2.6173],
        [  7.2123,  -9.5458],
        [  8.7068, -10.7330],
        [ -2.6382,   1.2370],
        [ -6.1642,   3.6088],
        [  6.8883, -10.4289],
        [ -3.3636,   1.7663],
        [  5.5250,  -8.2211],
        [ -8.0285,   6.6221],
        [  8.6536, -10.0822],
        [ -6.9350,   5.2026],
        [  9.2686, -10.9180],
        [ -6.3585,   4.6827],
        [  7.0017, -11.5242],
        [-10.2644,   7.7041],
        [-10.9845,   6.3693],
        [  7.6616,  -9.0485],
        [  8.6423, -10.0297],
        [ -5.3312,   3.2845],
        [  3.3540,  -4.7624],
        [  8.9653, -10.4844],
        [ -4.1900,   1.0415],
        [  7.8234,  -9.3244],
        [  7.8507,  -9.3292],
        [  8.3625,  -9.9283],
        [  8.2621,  -9.6934],
        [ -6.4593,   4.5142],
        [ -3.7967,   1.9205],
        [  3.2614,  -5.0731],
        [ -4.5848,   2.8725],
        [ -7.6326,   5.7970],
        [ -2.1824,   0.5053],
        [ -6.8168,   5.2622],
        [  7.3627, -10.1756],
        [ -9.8031,   7.8527],
        [ -5.2659,   3.5406],
        [  8.5601, -10.2179],
        [ -9.4612,   8.0402],
        [ -5.6681,   4.0521],
        [ -4.3928,   3.0048],
        [  7.3628, -10.0605],
        [ -5.4199,   3.1622],
        [ -5.5972,   4.1909],
        [ -7.3697,   5.9609],
        [  8.4163, -10.2980],
        [ -8.1874,   5.4345],
        [ -5.2421,   3.4143],
        [  8.4949, -10.8171],
        [  2.4508,  -5.8946],
        [ -6.8188,   5.4106],
        [  7.9605,  -9.5907],
        [  8.4139, -10.6919],
        [ -3.9818,   2.2238],
        [ -9.1551,   7.7650],
        [  8.3670, -10.4981],
        [  3.3990,  -4.9317],
        [  3.2086,  -5.3992],
        [  8.4497, -10.4966],
        [  8.9329, -10.4702],
        [  7.9825,  -9.7975],
        [ -5.1133,   1.8404],
        [  7.6196,  -9.1454],
        [  8.2696,  -9.6563],
        [  7.8702,  -9.6058],
        [ -5.7800,   3.9988],
        [  4.6178,  -6.0905],
        [ -8.2872,   6.8522],
        [  8.4982, -10.5706],
        [  8.2211,  -9.6091],
        [  7.1174,  -8.9744],
        [ -8.4774,   6.6654],
        [ -6.1091,   4.6822],
        [ -5.5958,   4.1910],
        [ -4.9365,   3.5169],
        [ -2.9880,   1.5959],
        [-10.1747,   7.2022],
        [  3.6263,  -8.2415],
        [-11.8745,   7.6151],
        [ -7.3488,   3.5571],
        [ -0.1967,  -2.6145],
        [  8.3872, -10.5419],
        [ -9.1980,   7.4143],
        [ -4.9679,   3.4048],
        [  7.6850,  -9.0972],
        [ -3.2740,   1.8012],
        [ -5.0334,   3.5770],
        [  7.8165,  -9.2471],
        [  8.3159, -10.0242],
        [  8.4929, -10.7583],
        [  9.0968, -10.8362],
        [ -8.0432,   6.2288],
        [ -5.6885,   4.1132],
        [  8.3081,  -9.7113],
        [  5.4200,  -6.8770],
        [ -7.4580,   5.8284]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7891, 0.2109],
        [0.1785, 0.8215]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5003, 0.4997], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2936, 0.1140],
         [0.5741, 0.2950]],

        [[0.8242, 0.1127],
         [0.9262, 0.1945]],

        [[0.9644, 0.1094],
         [0.1814, 0.8398]],

        [[0.8565, 0.1025],
         [0.4022, 0.3075]],

        [[0.4185, 0.0991],
         [0.5735, 0.6137]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448427857772554
time is 1
tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 95
Adjusted Rand Index: 0.8080702804390127
time is 2
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 4
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.906116425292723
Average Adjusted Rand Index: 0.906743082439674
11938.822284900833
new:  [-0.0017326397458452079, -0.0017326397458452079, -0.0017326397458452079, 0.906116425292723] [0.0, 0.0, 0.0, 0.906743082439674] [12396.117853459895, 12396.449934886943, 12396.132521467378, 11962.60149235819]
prior:  [0.0, 0.0, 0.0, 0.0] [0.0, 0.0, 0.0, 0.0] [12396.686626164847, 12396.68623564671, 12397.738051800614, nan]
-----------------------------------------------------------------------------------------
This iteration is 12
True Objective function: Loss = -11983.204727971859
Iteration 0: Loss = -24703.872945315325
Iteration 10: Loss = -12466.628667058374
Iteration 20: Loss = -12466.627474390281
Iteration 30: Loss = -12466.615217361685
Iteration 40: Loss = -12466.484675544787
Iteration 50: Loss = -12465.73673319245
Iteration 60: Loss = -12464.669336902682
Iteration 70: Loss = -12464.051868753346
Iteration 80: Loss = -12463.736752956856
Iteration 90: Loss = -12463.557320752443
Iteration 100: Loss = -12463.443376189562
Iteration 110: Loss = -12463.365278982279
Iteration 120: Loss = -12463.308408266661
Iteration 130: Loss = -12463.26490960523
Iteration 140: Loss = -12463.230143123947
Iteration 150: Loss = -12463.20123950612
Iteration 160: Loss = -12463.176441044478
Iteration 170: Loss = -12463.154628751487
Iteration 180: Loss = -12463.135020309373
Iteration 190: Loss = -12463.117133744283
Iteration 200: Loss = -12463.100608760127
Iteration 210: Loss = -12463.085130579158
Iteration 220: Loss = -12463.070567587212
Iteration 230: Loss = -12463.056704202614
Iteration 240: Loss = -12463.043596699374
Iteration 250: Loss = -12463.030989952167
Iteration 260: Loss = -12463.018902208501
Iteration 270: Loss = -12463.007319983177
Iteration 280: Loss = -12462.996245175636
Iteration 290: Loss = -12462.985519803724
Iteration 300: Loss = -12462.975183501405
Iteration 310: Loss = -12462.965238049743
Iteration 320: Loss = -12462.955700092318
Iteration 330: Loss = -12462.9464728092
Iteration 340: Loss = -12462.937644372441
Iteration 350: Loss = -12462.929140749931
Iteration 360: Loss = -12462.92091264624
Iteration 370: Loss = -12462.913021692166
Iteration 380: Loss = -12462.905410887422
Iteration 390: Loss = -12462.89811527336
Iteration 400: Loss = -12462.89113515341
Iteration 410: Loss = -12462.884404446082
Iteration 420: Loss = -12462.877988837725
Iteration 430: Loss = -12462.87178443061
Iteration 440: Loss = -12462.865957019245
Iteration 450: Loss = -12462.86030796581
Iteration 460: Loss = -12462.85488205199
Iteration 470: Loss = -12462.849744529814
Iteration 480: Loss = -12462.844869477256
Iteration 490: Loss = -12462.840156944738
Iteration 500: Loss = -12462.835724718203
Iteration 510: Loss = -12462.831455601592
Iteration 520: Loss = -12462.827462142728
Iteration 530: Loss = -12462.823554635272
Iteration 540: Loss = -12462.819907175726
Iteration 550: Loss = -12462.816475182774
Iteration 560: Loss = -12462.813187335645
Iteration 570: Loss = -12462.810089481201
Iteration 580: Loss = -12462.807104348556
Iteration 590: Loss = -12462.804260684907
Iteration 600: Loss = -12462.801610980088
Iteration 610: Loss = -12462.79906202896
Iteration 620: Loss = -12462.796648076375
Iteration 630: Loss = -12462.794423243666
Iteration 640: Loss = -12462.792245986298
Iteration 650: Loss = -12462.790209146193
Iteration 660: Loss = -12462.788276330271
Iteration 670: Loss = -12462.786385219415
Iteration 680: Loss = -12462.784645706904
Iteration 690: Loss = -12462.783012235686
Iteration 700: Loss = -12462.781498949356
Iteration 710: Loss = -12462.779929211452
Iteration 720: Loss = -12462.778579997037
Iteration 730: Loss = -12462.777206829198
Iteration 740: Loss = -12462.775992598334
Iteration 750: Loss = -12462.774714057312
Iteration 760: Loss = -12462.773577722186
Iteration 770: Loss = -12462.772500172776
Iteration 780: Loss = -12462.771512701556
Iteration 790: Loss = -12462.770513258753
Iteration 800: Loss = -12462.769528717412
Iteration 810: Loss = -12462.768692517247
Iteration 820: Loss = -12462.767839020873
Iteration 830: Loss = -12462.767005569376
Iteration 840: Loss = -12462.766252067084
Iteration 850: Loss = -12462.765485817346
Iteration 860: Loss = -12462.764827436398
Iteration 870: Loss = -12462.764203812227
Iteration 880: Loss = -12462.763537578927
Iteration 890: Loss = -12462.762920109204
Iteration 900: Loss = -12462.762330528763
Iteration 910: Loss = -12462.761833787403
Iteration 920: Loss = -12462.7612628071
Iteration 930: Loss = -12462.76079526049
Iteration 940: Loss = -12462.76028612494
Iteration 950: Loss = -12462.759851081435
Iteration 960: Loss = -12462.759386924856
Iteration 970: Loss = -12462.758939990117
Iteration 980: Loss = -12462.758569792104
Iteration 990: Loss = -12462.758204648879
Iteration 1000: Loss = -12462.757837620467
Iteration 1010: Loss = -12462.757496842607
Iteration 1020: Loss = -12462.757138907575
Iteration 1030: Loss = -12462.756858295672
Iteration 1040: Loss = -12462.756479614247
Iteration 1050: Loss = -12462.756217988148
Iteration 1060: Loss = -12462.755916162832
Iteration 1070: Loss = -12462.755613370278
Iteration 1080: Loss = -12462.75538900033
Iteration 1090: Loss = -12462.7550820276
Iteration 1100: Loss = -12462.754841955064
Iteration 1110: Loss = -12462.754610967148
Iteration 1120: Loss = -12462.754397616261
Iteration 1130: Loss = -12462.754159140015
Iteration 1140: Loss = -12462.753942153155
Iteration 1150: Loss = -12462.753760026104
Iteration 1160: Loss = -12462.753566475842
Iteration 1170: Loss = -12462.753409978308
Iteration 1180: Loss = -12462.753229398535
Iteration 1190: Loss = -12462.753062091238
Iteration 1200: Loss = -12462.75285311829
Iteration 1210: Loss = -12462.752722070765
Iteration 1220: Loss = -12462.75254370576
Iteration 1230: Loss = -12462.752373559406
Iteration 1240: Loss = -12462.752210200759
Iteration 1250: Loss = -12462.75209681802
Iteration 1260: Loss = -12462.751954915893
Iteration 1270: Loss = -12462.75182383838
Iteration 1280: Loss = -12462.751715050194
Iteration 1290: Loss = -12462.751596438398
Iteration 1300: Loss = -12462.751447259394
Iteration 1310: Loss = -12462.751301077295
Iteration 1320: Loss = -12462.751243792329
Iteration 1330: Loss = -12462.751148002704
Iteration 1340: Loss = -12462.750993808071
Iteration 1350: Loss = -12462.750901523723
Iteration 1360: Loss = -12462.750801506392
Iteration 1370: Loss = -12462.750743071832
Iteration 1380: Loss = -12462.750582612616
Iteration 1390: Loss = -12462.750538927206
Iteration 1400: Loss = -12462.750431107554
Iteration 1410: Loss = -12462.750296587741
Iteration 1420: Loss = -12462.750300543734
1
Iteration 1430: Loss = -12462.750195839157
Iteration 1440: Loss = -12462.750106932499
Iteration 1450: Loss = -12462.750048616506
Iteration 1460: Loss = -12462.749975484245
Iteration 1470: Loss = -12462.74987235697
Iteration 1480: Loss = -12462.749772159163
Iteration 1490: Loss = -12462.749760148106
Iteration 1500: Loss = -12462.749650614414
Iteration 1510: Loss = -12462.749610639508
Iteration 1520: Loss = -12462.749525354795
Iteration 1530: Loss = -12462.749468753627
Iteration 1540: Loss = -12462.74945450407
Iteration 1550: Loss = -12462.749356658007
Iteration 1560: Loss = -12462.749320926308
Iteration 1570: Loss = -12462.74932227062
1
Iteration 1580: Loss = -12462.749181611032
Iteration 1590: Loss = -12462.749191616529
1
Iteration 1600: Loss = -12462.749152794953
Iteration 1610: Loss = -12462.749062576802
Iteration 1620: Loss = -12462.74898094522
Iteration 1630: Loss = -12462.748966893045
Iteration 1640: Loss = -12462.748913868407
Iteration 1650: Loss = -12462.74887786126
Iteration 1660: Loss = -12462.748838970334
Iteration 1670: Loss = -12462.748801218031
Iteration 1680: Loss = -12462.74874886702
Iteration 1690: Loss = -12462.748683129359
Iteration 1700: Loss = -12462.748642331775
Iteration 1710: Loss = -12462.748638842957
Iteration 1720: Loss = -12462.748600763716
Iteration 1730: Loss = -12462.748508068593
Iteration 1740: Loss = -12462.748554830408
1
Iteration 1750: Loss = -12462.748466394429
Iteration 1760: Loss = -12462.748433285946
Iteration 1770: Loss = -12462.748401130675
Iteration 1780: Loss = -12462.74839633683
Iteration 1790: Loss = -12462.748334947863
Iteration 1800: Loss = -12462.748307996379
Iteration 1810: Loss = -12462.748249410923
Iteration 1820: Loss = -12462.74829933353
1
Iteration 1830: Loss = -12462.748244360326
Iteration 1840: Loss = -12462.74818010641
Iteration 1850: Loss = -12462.748157819176
Iteration 1860: Loss = -12462.748158377186
1
Iteration 1870: Loss = -12462.748169834638
2
Iteration 1880: Loss = -12462.748081903614
Iteration 1890: Loss = -12462.74806123748
Iteration 1900: Loss = -12462.748033426995
Iteration 1910: Loss = -12462.747996495244
Iteration 1920: Loss = -12462.748024341952
1
Iteration 1930: Loss = -12462.747967514806
Iteration 1940: Loss = -12462.747910864018
Iteration 1950: Loss = -12462.747943647579
1
Iteration 1960: Loss = -12462.74788474903
Iteration 1970: Loss = -12462.747846757393
Iteration 1980: Loss = -12462.747846181468
Iteration 1990: Loss = -12462.747779024307
Iteration 2000: Loss = -12462.747803512435
1
Iteration 2010: Loss = -12462.747816385569
2
Iteration 2020: Loss = -12462.747748717706
Iteration 2030: Loss = -12462.747764732685
1
Iteration 2040: Loss = -12462.747721713742
Iteration 2050: Loss = -12462.747695330354
Iteration 2060: Loss = -12462.747664596684
Iteration 2070: Loss = -12462.747661084739
Iteration 2080: Loss = -12462.747666349487
1
Iteration 2090: Loss = -12462.747683493362
2
Iteration 2100: Loss = -12462.74764415142
Iteration 2110: Loss = -12462.747635665333
Iteration 2120: Loss = -12462.747545935576
Iteration 2130: Loss = -12462.747557940487
1
Iteration 2140: Loss = -12462.747568267065
2
Iteration 2150: Loss = -12462.74755637481
3
Stopping early at iteration 2149 due to no improvement.
pi: tensor([[3.5525e-02, 9.6447e-01],
        [1.0000e+00, 3.6901e-08]], dtype=torch.float64)
alpha: tensor([0.5090, 0.4910])
beta: tensor([[[0.2004, 0.1972],
         [0.9633, 0.2003]],

        [[0.1269, 0.1834],
         [0.3472, 0.9647]],

        [[0.7612, 0.2134],
         [0.7110, 0.3000]],

        [[0.9907, 0.2040],
         [0.2804, 0.8465]],

        [[0.8889, 0.2038],
         [0.2720, 0.4879]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24568.339730612
Iteration 100: Loss = -12468.795068680045
Iteration 200: Loss = -12467.464060684057
Iteration 300: Loss = -12466.980570204536
Iteration 400: Loss = -12466.713455095633
Iteration 500: Loss = -12466.503718862632
Iteration 600: Loss = -12466.308593213422
Iteration 700: Loss = -12466.14268274831
Iteration 800: Loss = -12466.02783256946
Iteration 900: Loss = -12465.937512243576
Iteration 1000: Loss = -12465.797192307706
Iteration 1100: Loss = -12465.41374427276
Iteration 1200: Loss = -12465.049036070943
Iteration 1300: Loss = -12464.960924656338
Iteration 1400: Loss = -12464.921875217593
Iteration 1500: Loss = -12464.89970350721
Iteration 1600: Loss = -12464.882674131326
Iteration 1700: Loss = -12464.8671499699
Iteration 1800: Loss = -12464.851611785598
Iteration 1900: Loss = -12464.835069931329
Iteration 2000: Loss = -12464.816351873484
Iteration 2100: Loss = -12464.793673342625
Iteration 2200: Loss = -12464.76469361836
Iteration 2300: Loss = -12464.728670180888
Iteration 2400: Loss = -12464.689602214432
Iteration 2500: Loss = -12464.652796629796
Iteration 2600: Loss = -12464.620779556903
Iteration 2700: Loss = -12464.59382148774
Iteration 2800: Loss = -12464.571048325472
Iteration 2900: Loss = -12464.552091282723
Iteration 3000: Loss = -12464.536063578704
Iteration 3100: Loss = -12464.522602102777
Iteration 3200: Loss = -12464.511153688649
Iteration 3300: Loss = -12464.501396802296
Iteration 3400: Loss = -12464.493329543355
Iteration 3500: Loss = -12464.485905553285
Iteration 3600: Loss = -12464.479772931532
Iteration 3700: Loss = -12464.47450432828
Iteration 3800: Loss = -12464.469788375172
Iteration 3900: Loss = -12464.465825844753
Iteration 4000: Loss = -12464.462293797764
Iteration 4100: Loss = -12464.45927126616
Iteration 4200: Loss = -12464.45650106961
Iteration 4300: Loss = -12464.454195157292
Iteration 4400: Loss = -12464.451965847993
Iteration 4500: Loss = -12464.450144560595
Iteration 4600: Loss = -12464.448444425227
Iteration 4700: Loss = -12464.452391815814
1
Iteration 4800: Loss = -12464.454643641862
2
Iteration 4900: Loss = -12464.446447445853
Iteration 5000: Loss = -12464.445241856944
Iteration 5100: Loss = -12464.483219661302
1
Iteration 5200: Loss = -12464.44125948309
Iteration 5300: Loss = -12464.440424989698
Iteration 5400: Loss = -12464.439656484406
Iteration 5500: Loss = -12464.446193581942
1
Iteration 5600: Loss = -12464.438329304317
Iteration 5700: Loss = -12464.530247199993
1
Iteration 5800: Loss = -12464.43720191496
Iteration 5900: Loss = -12464.436676206096
Iteration 6000: Loss = -12464.499076503935
1
Iteration 6100: Loss = -12464.435774375388
Iteration 6200: Loss = -12464.435358713326
Iteration 6300: Loss = -12464.435180257582
Iteration 6400: Loss = -12464.434636312146
Iteration 6500: Loss = -12464.434331334032
Iteration 6600: Loss = -12464.434042025521
Iteration 6700: Loss = -12464.440174710902
1
Iteration 6800: Loss = -12464.433503381224
Iteration 6900: Loss = -12464.433275953244
Iteration 7000: Loss = -12464.435450968107
1
Iteration 7100: Loss = -12464.4327672401
Iteration 7200: Loss = -12464.432558783848
Iteration 7300: Loss = -12464.433438745453
1
Iteration 7400: Loss = -12464.43221515939
Iteration 7500: Loss = -12464.43205171115
Iteration 7600: Loss = -12464.431862369283
Iteration 7700: Loss = -12464.431830964193
Iteration 7800: Loss = -12464.431589761687
Iteration 7900: Loss = -12464.431453969795
Iteration 8000: Loss = -12464.460516076471
1
Iteration 8100: Loss = -12464.43124433157
Iteration 8200: Loss = -12464.431083265832
Iteration 8300: Loss = -12464.430995771543
Iteration 8400: Loss = -12464.43093841916
Iteration 8500: Loss = -12464.430790201975
Iteration 8600: Loss = -12464.430729999283
Iteration 8700: Loss = -12464.438047434716
1
Iteration 8800: Loss = -12464.430525884229
Iteration 8900: Loss = -12464.430448280613
Iteration 9000: Loss = -12464.630273128614
1
Iteration 9100: Loss = -12464.430318268065
Iteration 9200: Loss = -12464.430270844836
Iteration 9300: Loss = -12464.430472996679
1
Iteration 9400: Loss = -12464.43015938393
Iteration 9500: Loss = -12464.432028428017
1
Iteration 9600: Loss = -12464.471566431133
2
Iteration 9700: Loss = -12464.43001777482
Iteration 9800: Loss = -12464.43007176485
1
Iteration 9900: Loss = -12464.437556536845
2
Iteration 10000: Loss = -12464.42978642217
Iteration 10100: Loss = -12464.436724964482
1
Iteration 10200: Loss = -12464.429838429025
2
Iteration 10300: Loss = -12464.429845282733
3
Iteration 10400: Loss = -12464.567027551777
4
Iteration 10500: Loss = -12464.429654480447
Iteration 10600: Loss = -12464.461841221857
1
Iteration 10700: Loss = -12464.429564518461
Iteration 10800: Loss = -12464.453333313711
1
Iteration 10900: Loss = -12464.433513186677
2
Iteration 11000: Loss = -12464.42952244837
Iteration 11100: Loss = -12464.43949786889
1
Iteration 11200: Loss = -12464.429479465565
Iteration 11300: Loss = -12464.75768678351
1
Iteration 11400: Loss = -12464.42944803149
Iteration 11500: Loss = -12464.429412325822
Iteration 11600: Loss = -12464.430952720144
1
Iteration 11700: Loss = -12464.429346578014
Iteration 11800: Loss = -12464.429595895122
1
Iteration 11900: Loss = -12464.429438531273
2
Iteration 12000: Loss = -12464.432197355358
3
Iteration 12100: Loss = -12464.429388162696
4
Iteration 12200: Loss = -12464.42928990557
Iteration 12300: Loss = -12464.42938848189
1
Iteration 12400: Loss = -12464.440524084494
2
Iteration 12500: Loss = -12464.429857899333
3
Iteration 12600: Loss = -12464.445721088026
4
Iteration 12700: Loss = -12464.429227467364
Iteration 12800: Loss = -12464.4295174288
1
Iteration 12900: Loss = -12464.430509692513
2
Iteration 13000: Loss = -12464.432978477658
3
Iteration 13100: Loss = -12464.63265837583
4
Iteration 13200: Loss = -12464.42921550957
Iteration 13300: Loss = -12464.429358504321
1
Iteration 13400: Loss = -12464.430209527109
2
Iteration 13500: Loss = -12464.437053813617
3
Iteration 13600: Loss = -12464.429115483976
Iteration 13700: Loss = -12464.42949399181
1
Iteration 13800: Loss = -12464.466696757572
2
Iteration 13900: Loss = -12464.429157649201
3
Iteration 14000: Loss = -12464.433401819124
4
Iteration 14100: Loss = -12464.42931433135
5
Iteration 14200: Loss = -12464.429120220231
6
Iteration 14300: Loss = -12464.430174620687
7
Iteration 14400: Loss = -12464.429090146155
Iteration 14500: Loss = -12464.483313065743
1
Iteration 14600: Loss = -12464.429106517764
2
Iteration 14700: Loss = -12464.48004891195
3
Iteration 14800: Loss = -12464.429041776846
Iteration 14900: Loss = -12464.429058733536
1
Iteration 15000: Loss = -12464.429288628035
2
Iteration 15100: Loss = -12464.429092051409
3
Iteration 15200: Loss = -12464.430265528117
4
Iteration 15300: Loss = -12464.429237963399
5
Iteration 15400: Loss = -12464.44522756887
6
Iteration 15500: Loss = -12464.429061603349
7
Iteration 15600: Loss = -12464.430544733348
8
Iteration 15700: Loss = -12464.435645172409
9
Iteration 15800: Loss = -12464.429511138836
10
Stopping early at iteration 15800 due to no improvement.
tensor([[ 0.3666, -4.9818],
        [ 0.0116, -4.6269],
        [ 0.4724, -5.0876],
        [-0.0374, -4.5778],
        [ 0.5527, -5.1679],
        [ 0.3778, -4.9930],
        [ 0.4742, -5.0894],
        [ 0.1618, -4.7770],
        [ 0.1890, -4.8042],
        [ 0.2083, -4.8236],
        [ 0.2957, -4.9109],
        [ 0.2422, -4.8574],
        [ 0.0349, -4.6501],
        [ 0.0152, -4.6304],
        [ 0.2038, -4.8190],
        [ 0.2295, -4.8447],
        [ 0.4978, -5.1130],
        [ 0.4686, -5.0838],
        [ 0.1491, -4.7643],
        [ 0.3708, -4.9860],
        [ 0.1969, -4.8121],
        [ 0.3435, -4.9587],
        [ 0.0970, -4.7122],
        [ 0.1568, -4.7720],
        [ 0.1593, -4.7745],
        [ 0.4183, -5.0335],
        [ 0.0151, -4.6303],
        [ 0.2655, -4.8807],
        [ 0.3181, -4.9333],
        [ 0.4300, -5.0452],
        [ 0.2804, -4.8956],
        [-0.2045, -4.4107],
        [ 0.1485, -4.7638],
        [ 0.2945, -4.9097],
        [ 0.1882, -4.8034],
        [ 0.3335, -4.9487],
        [ 0.1923, -4.8075],
        [ 0.3447, -4.9599],
        [ 0.4565, -5.0717],
        [ 0.1979, -4.8132],
        [ 0.5527, -5.1679],
        [ 0.3194, -4.9347],
        [ 0.2752, -4.8904],
        [ 0.0117, -4.6269],
        [ 0.1844, -4.7996],
        [ 0.3333, -4.9485],
        [ 0.2414, -4.8566],
        [ 0.5941, -5.2093],
        [ 0.4698, -5.0850],
        [ 0.4590, -5.0742],
        [ 0.2512, -4.8664],
        [ 0.4116, -5.0268],
        [ 0.3780, -4.9932],
        [ 0.2895, -4.9047],
        [ 0.2851, -4.9003],
        [ 0.1926, -4.8078],
        [ 0.4824, -5.0977],
        [ 0.1968, -4.8121],
        [ 0.0156, -4.6308],
        [ 0.1224, -4.7376],
        [ 0.1172, -4.7324],
        [ 0.2468, -4.8621],
        [ 0.4671, -5.0824],
        [ 0.1586, -4.7738],
        [ 0.2886, -4.9038],
        [ 0.2952, -4.9104],
        [ 0.1670, -4.7822],
        [ 0.6858, -5.3010],
        [ 0.3664, -4.9816],
        [ 0.0688, -4.6840],
        [ 0.4174, -5.0326],
        [ 0.3721, -4.9873],
        [ 0.0234, -4.6387],
        [ 0.2267, -4.8419],
        [ 0.2830, -4.8982],
        [ 0.1432, -4.7584],
        [ 0.4563, -5.0715],
        [ 0.1968, -4.8120],
        [ 0.5275, -5.1427],
        [ 0.1462, -4.7615],
        [ 0.3615, -4.9767],
        [ 0.4541, -5.0693],
        [ 0.3348, -4.9500],
        [ 0.3691, -4.9843],
        [ 0.2818, -4.8971],
        [ 0.1953, -4.8105],
        [ 0.0081, -4.6233],
        [ 0.3609, -4.9761],
        [ 0.2817, -4.8969],
        [ 0.4514, -5.0666],
        [ 0.3915, -5.0067],
        [ 0.1395, -4.7547],
        [ 0.3697, -4.9849],
        [ 0.3754, -4.9906],
        [ 0.2806, -4.8958],
        [ 0.1904, -4.8057],
        [ 0.1487, -4.7639],
        [ 0.3471, -4.9624],
        [ 0.3474, -4.9626],
        [ 0.3255, -4.9407]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.5865e-01, 4.1351e-02],
        [9.9998e-01, 1.9398e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9941, 0.0059], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2027, 0.1881],
         [0.9633, 0.2735]],

        [[0.1269, 0.1663],
         [0.3472, 0.9647]],

        [[0.7612, 0.2540],
         [0.7110, 0.3000]],

        [[0.9907, 0.2349],
         [0.2804, 0.8465]],

        [[0.8889, 0.1141],
         [0.2720, 0.4879]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 60
Adjusted Rand Index: 0.016025857647765086
Global Adjusted Rand Index: 0.001347999694545769
Average Adjusted Rand Index: 0.003205171529553017
Iteration 0: Loss = -18483.87753423986
Iteration 10: Loss = -12464.578884777102
Iteration 20: Loss = -12464.003199457871
Iteration 30: Loss = -12463.710158297605
Iteration 40: Loss = -12463.540560098052
Iteration 50: Loss = -12463.431158188141
Iteration 60: Loss = -12463.354990452723
Iteration 70: Loss = -12463.298424034216
Iteration 80: Loss = -12463.253941438488
Iteration 90: Loss = -12463.217124034367
Iteration 100: Loss = -12463.18553290073
Iteration 110: Loss = -12463.157314994407
Iteration 120: Loss = -12463.131531971963
Iteration 130: Loss = -12463.107565629189
Iteration 140: Loss = -12463.085009247261
Iteration 150: Loss = -12463.063504637073
Iteration 160: Loss = -12463.043145154325
Iteration 170: Loss = -12463.023618266656
Iteration 180: Loss = -12463.004973931489
Iteration 190: Loss = -12462.987258153895
Iteration 200: Loss = -12462.970342248425
Iteration 210: Loss = -12462.954324287666
Iteration 220: Loss = -12462.939214119615
Iteration 230: Loss = -12462.925025024995
Iteration 240: Loss = -12462.91158478807
Iteration 250: Loss = -12462.899116523791
Iteration 260: Loss = -12462.887407969907
Iteration 270: Loss = -12462.876554781396
Iteration 280: Loss = -12462.866432602672
Iteration 290: Loss = -12462.857127382827
Iteration 300: Loss = -12462.84847505582
Iteration 310: Loss = -12462.840469328845
Iteration 320: Loss = -12462.833102747909
Iteration 330: Loss = -12462.826284274332
Iteration 340: Loss = -12462.820027970289
Iteration 350: Loss = -12462.814251938544
Iteration 360: Loss = -12462.808978489866
Iteration 370: Loss = -12462.804053705924
Iteration 380: Loss = -12462.799557784783
Iteration 390: Loss = -12462.795480095541
Iteration 400: Loss = -12462.791630823498
Iteration 410: Loss = -12462.788159703798
Iteration 420: Loss = -12462.784935074258
Iteration 430: Loss = -12462.781967360013
Iteration 440: Loss = -12462.779237572631
Iteration 450: Loss = -12462.776733725314
Iteration 460: Loss = -12462.7744611362
Iteration 470: Loss = -12462.772290307306
Iteration 480: Loss = -12462.77033710938
Iteration 490: Loss = -12462.768521157219
Iteration 500: Loss = -12462.76687696323
Iteration 510: Loss = -12462.765313126727
Iteration 520: Loss = -12462.763936965515
Iteration 530: Loss = -12462.762594535665
Iteration 540: Loss = -12462.761393513303
Iteration 550: Loss = -12462.76033186246
Iteration 560: Loss = -12462.75925945191
Iteration 570: Loss = -12462.758294789297
Iteration 580: Loss = -12462.757418209321
Iteration 590: Loss = -12462.756601714289
Iteration 600: Loss = -12462.755816525432
Iteration 610: Loss = -12462.755148227
Iteration 620: Loss = -12462.754482374794
Iteration 630: Loss = -12462.753927917318
Iteration 640: Loss = -12462.753359337328
Iteration 650: Loss = -12462.752804702764
Iteration 660: Loss = -12462.752350359315
Iteration 670: Loss = -12462.751934901533
Iteration 680: Loss = -12462.751512007582
Iteration 690: Loss = -12462.751118582695
Iteration 700: Loss = -12462.75074212601
Iteration 710: Loss = -12462.750433422018
Iteration 720: Loss = -12462.75016132541
Iteration 730: Loss = -12462.749880414878
Iteration 740: Loss = -12462.74956246268
Iteration 750: Loss = -12462.749411373383
Iteration 760: Loss = -12462.749152784507
Iteration 770: Loss = -12462.748935862419
Iteration 780: Loss = -12462.748764143978
Iteration 790: Loss = -12462.74861346274
Iteration 800: Loss = -12462.748422063023
Iteration 810: Loss = -12462.748277001567
Iteration 820: Loss = -12462.74815173936
Iteration 830: Loss = -12462.748046393506
Iteration 840: Loss = -12462.7478872479
Iteration 850: Loss = -12462.747795573316
Iteration 860: Loss = -12462.747668075614
Iteration 870: Loss = -12462.747590924711
Iteration 880: Loss = -12462.747436579199
Iteration 890: Loss = -12462.747378948163
Iteration 900: Loss = -12462.7473708899
Iteration 910: Loss = -12462.747214451372
Iteration 920: Loss = -12462.7471676014
Iteration 930: Loss = -12462.747076825142
Iteration 940: Loss = -12462.747083274926
1
Iteration 950: Loss = -12462.747009965751
Iteration 960: Loss = -12462.746948312291
Iteration 970: Loss = -12462.74690658386
Iteration 980: Loss = -12462.746867795287
Iteration 990: Loss = -12462.74682722844
Iteration 1000: Loss = -12462.746780787247
Iteration 1010: Loss = -12462.746746560168
Iteration 1020: Loss = -12462.746721367641
Iteration 1030: Loss = -12462.746717278247
Iteration 1040: Loss = -12462.746661007584
Iteration 1050: Loss = -12462.746617106259
Iteration 1060: Loss = -12462.746615020109
Iteration 1070: Loss = -12462.746588827898
Iteration 1080: Loss = -12462.746555317482
Iteration 1090: Loss = -12462.746534653888
Iteration 1100: Loss = -12462.746530442346
Iteration 1110: Loss = -12462.74649842953
Iteration 1120: Loss = -12462.74646191884
Iteration 1130: Loss = -12462.746495554142
1
Iteration 1140: Loss = -12462.746516551311
2
Iteration 1150: Loss = -12462.74644100424
Iteration 1160: Loss = -12462.746439311666
Iteration 1170: Loss = -12462.746413744015
Iteration 1180: Loss = -12462.746451370473
1
Iteration 1190: Loss = -12462.746413831923
2
Iteration 1200: Loss = -12462.746409929674
Iteration 1210: Loss = -12462.74642859534
1
Iteration 1220: Loss = -12462.746407728313
Iteration 1230: Loss = -12462.74641885412
1
Iteration 1240: Loss = -12462.746378906322
Iteration 1250: Loss = -12462.746415865107
1
Iteration 1260: Loss = -12462.746369862092
Iteration 1270: Loss = -12462.746377412657
1
Iteration 1280: Loss = -12462.746346994922
Iteration 1290: Loss = -12462.746384761645
1
Iteration 1300: Loss = -12462.74634546544
Iteration 1310: Loss = -12462.746347745102
1
Iteration 1320: Loss = -12462.74634458014
Iteration 1330: Loss = -12462.74634476984
1
Iteration 1340: Loss = -12462.746350068357
2
Iteration 1350: Loss = -12462.746382443629
3
Stopping early at iteration 1349 due to no improvement.
pi: tensor([[0.3673, 0.6327],
        [0.6409, 0.3591]], dtype=torch.float64)
alpha: tensor([0.5032, 0.4968])
beta: tensor([[[0.2003, 0.1972],
         [0.9661, 0.2004]],

        [[0.9658, 0.1834],
         [0.4415, 0.6356]],

        [[0.9977, 0.2134],
         [0.1145, 0.2655]],

        [[0.8670, 0.2040],
         [0.5930, 0.3329]],

        [[0.3917, 0.2038],
         [0.7871, 0.7102]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18483.95455592913
Iteration 100: Loss = -12499.559872850814
Iteration 200: Loss = -12474.486801228008
Iteration 300: Loss = -12467.090662314453
Iteration 400: Loss = -12466.152820286145
Iteration 500: Loss = -12465.822998768535
Iteration 600: Loss = -12465.585469703135
Iteration 700: Loss = -12465.433769943158
Iteration 800: Loss = -12465.323842414104
Iteration 900: Loss = -12465.232574775122
Iteration 1000: Loss = -12465.15523305003
Iteration 1100: Loss = -12465.079806513144
Iteration 1200: Loss = -12465.00631278772
Iteration 1300: Loss = -12464.932347623591
Iteration 1400: Loss = -12464.847052622048
Iteration 1500: Loss = -12464.729395242914
Iteration 1600: Loss = -12464.581059961572
Iteration 1700: Loss = -12464.339409354243
Iteration 1800: Loss = -12463.913001776396
Iteration 1900: Loss = -12463.41261997297
Iteration 2000: Loss = -12462.992424947168
Iteration 2100: Loss = -12462.890894580176
Iteration 2200: Loss = -12462.833223213358
Iteration 2300: Loss = -12462.81752280744
Iteration 2400: Loss = -12462.759102201027
Iteration 2500: Loss = -12462.731535738465
Iteration 2600: Loss = -12462.70750575902
Iteration 2700: Loss = -12462.687076022094
Iteration 2800: Loss = -12462.648372949605
Iteration 2900: Loss = -12462.596323416365
Iteration 3000: Loss = -12462.498104140634
Iteration 3100: Loss = -12462.300874217322
Iteration 3200: Loss = -12462.010511373983
Iteration 3300: Loss = -12461.797258800807
Iteration 3400: Loss = -12461.686338963847
Iteration 3500: Loss = -12461.598345814755
Iteration 3600: Loss = -12461.510309590316
Iteration 3700: Loss = -12461.483041275229
Iteration 3800: Loss = -12461.400347359202
Iteration 3900: Loss = -12461.359303607469
Iteration 4000: Loss = -12461.318142018468
Iteration 4100: Loss = -12461.260369953736
Iteration 4200: Loss = -12461.158840208944
Iteration 4300: Loss = -12461.076280377172
Iteration 4400: Loss = -12460.933600395747
Iteration 4500: Loss = -12460.877622764903
Iteration 4600: Loss = -12460.832928405442
Iteration 4700: Loss = -12460.794755583162
Iteration 4800: Loss = -12460.656400338707
Iteration 4900: Loss = -12460.454457434027
Iteration 5000: Loss = -12460.396217309719
Iteration 5100: Loss = -12460.365827374124
Iteration 5200: Loss = -12460.346780793943
Iteration 5300: Loss = -12460.333883435165
Iteration 5400: Loss = -12460.324545406496
Iteration 5500: Loss = -12460.317404819083
Iteration 5600: Loss = -12460.311787142458
Iteration 5700: Loss = -12460.308192307799
Iteration 5800: Loss = -12460.303627822155
Iteration 5900: Loss = -12460.300552074008
Iteration 6000: Loss = -12460.52347061894
1
Iteration 6100: Loss = -12460.29566783849
Iteration 6200: Loss = -12460.29369480979
Iteration 6300: Loss = -12460.29303520933
Iteration 6400: Loss = -12460.29050636738
Iteration 6500: Loss = -12460.289162986423
Iteration 6600: Loss = -12460.28791658884
Iteration 6700: Loss = -12460.286925831757
Iteration 6800: Loss = -12460.285879680274
Iteration 6900: Loss = -12460.284983110996
Iteration 7000: Loss = -12460.32809076831
1
Iteration 7100: Loss = -12460.283418689109
Iteration 7200: Loss = -12460.29094521569
1
Iteration 7300: Loss = -12460.28659674529
2
Iteration 7400: Loss = -12460.281524578788
Iteration 7500: Loss = -12460.281033325604
Iteration 7600: Loss = -12460.280781954258
Iteration 7700: Loss = -12460.323512509704
1
Iteration 7800: Loss = -12460.355354941374
2
Iteration 7900: Loss = -12460.327372471029
3
Iteration 8000: Loss = -12460.278891365635
Iteration 8100: Loss = -12460.278711612606
Iteration 8200: Loss = -12460.284115077378
1
Iteration 8300: Loss = -12460.28750016178
2
Iteration 8400: Loss = -12460.291141307422
3
Iteration 8500: Loss = -12460.277370581025
Iteration 8600: Loss = -12460.277227551049
Iteration 8700: Loss = -12460.309164015149
1
Iteration 8800: Loss = -12460.280543712459
2
Iteration 8900: Loss = -12460.276538250462
Iteration 9000: Loss = -12460.277501587769
1
Iteration 9100: Loss = -12460.277093322675
2
Iteration 9200: Loss = -12460.27627993855
Iteration 9300: Loss = -12460.275744339267
Iteration 9400: Loss = -12460.312258309208
1
Iteration 9500: Loss = -12460.275406809438
Iteration 9600: Loss = -12460.276603145123
1
Iteration 9700: Loss = -12460.280527699204
2
Iteration 9800: Loss = -12460.278206160732
3
Iteration 9900: Loss = -12460.27587655378
4
Iteration 10000: Loss = -12460.274849414967
Iteration 10100: Loss = -12460.275057378733
1
Iteration 10200: Loss = -12460.275382638798
2
Iteration 10300: Loss = -12460.326984548796
3
Iteration 10400: Loss = -12460.373989512102
4
Iteration 10500: Loss = -12460.275047750783
5
Iteration 10600: Loss = -12460.289166809507
6
Iteration 10700: Loss = -12460.276116877945
7
Iteration 10800: Loss = -12460.275120832284
8
Iteration 10900: Loss = -12460.287013333691
9
Iteration 11000: Loss = -12460.275357444798
10
Stopping early at iteration 11000 due to no improvement.
tensor([[ 1.9036, -3.3560],
        [ 0.1595, -1.6262],
        [ 3.2690, -4.9949],
        [-1.6283,  0.0703],
        [ 3.8301, -5.5766],
        [ 2.7406, -4.9017],
        [ 3.0623, -4.4495],
        [ 0.8186, -4.7384],
        [ 2.1674, -4.3031],
        [ 1.4334, -2.8216],
        [ 4.1031, -5.4896],
        [-0.4050, -1.6313],
        [ 0.4917, -1.9211],
        [-2.3273, -1.8937],
        [ 0.4999, -2.0748],
        [ 2.1029, -3.4931],
        [ 3.1805, -4.5803],
        [ 2.2578, -6.1512],
        [ 1.0398, -4.0592],
        [ 2.6461, -4.0365],
        [ 2.1715, -3.6845],
        [ 2.1260, -3.7285],
        [ 1.0087, -2.4602],
        [ 2.3517, -3.7930],
        [ 2.1231, -3.5773],
        [ 4.3213, -5.9424],
        [ 1.4977, -3.7642],
        [ 3.4181, -5.1638],
        [ 1.5995, -6.2147],
        [ 3.2478, -4.8254],
        [ 2.3256, -5.2759],
        [-2.0236,  0.5968],
        [ 1.5969, -3.2352],
        [ 1.8833, -5.1806],
        [ 2.8306, -4.2274],
        [ 1.6835, -3.7745],
        [ 1.5614, -4.6060],
        [ 3.2130, -5.1971],
        [ 1.4010, -2.8487],
        [ 1.4305, -2.8311],
        [ 3.2037, -5.7765],
        [ 3.4783, -4.9821],
        [ 2.8373, -4.6111],
        [-0.5413, -1.5074],
        [ 2.1999, -3.9395],
        [ 2.7209, -7.3361],
        [ 0.6971, -3.1329],
        [ 2.6895, -5.7982],
        [ 2.6266, -4.0177],
        [ 3.5854, -6.1678],
        [ 1.7545, -3.3942],
        [ 0.7541, -2.4395],
        [ 1.2656, -3.4656],
        [ 1.3534, -3.2595],
        [ 2.6025, -4.6548],
        [ 3.0428, -4.4820],
        [ 5.3785, -7.3459],
        [ 1.5699, -3.2527],
        [ 1.3057, -3.1961],
        [ 1.2863, -2.6741],
        [ 0.7293, -2.5358],
        [-0.7713, -3.8439],
        [ 3.6631, -5.0648],
        [ 3.1439, -4.5547],
        [ 0.3479, -4.9631],
        [ 3.5661, -5.0278],
        [ 0.8311, -2.4892],
        [ 5.6588, -7.4337],
        [ 2.5524, -6.4551],
        [ 1.2170, -2.6247],
        [ 3.0863, -5.5097],
        [ 1.5176, -5.9729],
        [ 0.6246, -2.1198],
        [ 1.9380, -3.3273],
        [ 1.3942, -2.9029],
        [ 2.8579, -4.4902],
        [ 3.0367, -4.9256],
        [ 2.1247, -3.6053],
        [ 4.2768, -6.9732],
        [ 0.4178, -1.9743],
        [ 1.9243, -4.1229],
        [ 1.4421, -5.3748],
        [ 2.4594, -4.1786],
        [ 1.9968, -3.3963],
        [ 2.0035, -3.4507],
        [ 0.2047, -1.6289],
        [-0.6954, -0.7274],
        [ 2.6199, -4.0148],
        [ 2.4170, -4.7547],
        [ 1.6059, -4.4300],
        [ 2.9223, -4.3276],
        [ 0.0885, -1.4758],
        [ 3.5361, -5.0120],
        [ 1.8403, -3.2641],
        [ 4.7231, -6.3150],
        [ 1.6438, -3.7699],
        [ 1.6595, -4.8935],
        [ 2.8685, -4.6478],
        [ 2.9004, -4.3220],
        [ 3.8347, -5.2267]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.2023e-05, 9.9999e-01],
        [9.9999e-01, 5.6981e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9540, 0.0460], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2074, 0.1403],
         [0.9661, 0.1924]],

        [[0.9658, 0.2080],
         [0.4415, 0.6356]],

        [[0.9977, 0.2092],
         [0.1145, 0.2655]],

        [[0.8670, 0.2493],
         [0.5930, 0.3329]],

        [[0.3917, 0.2611],
         [0.7871, 0.7102]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
time is 1
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
time is 2
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
time is 3
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.010427427162149738
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: -0.014100938522947548
Global Adjusted Rand Index: -0.0015534621705078576
Average Adjusted Rand Index: -0.004308721449583262
Iteration 0: Loss = -18484.761373499943
Iteration 10: Loss = -12465.002158760732
Iteration 20: Loss = -12464.216956806813
Iteration 30: Loss = -12463.821664914438
Iteration 40: Loss = -12463.603057113958
Iteration 50: Loss = -12463.46411250528
Iteration 60: Loss = -12463.364781789018
Iteration 70: Loss = -12463.28687122435
Iteration 80: Loss = -12463.221031592711
Iteration 90: Loss = -12463.163004955502
Iteration 100: Loss = -12463.110824457788
Iteration 110: Loss = -12463.063770911873
Iteration 120: Loss = -12463.021762666762
Iteration 130: Loss = -12462.984695758218
Iteration 140: Loss = -12462.952438356762
Iteration 150: Loss = -12462.924658856631
Iteration 160: Loss = -12462.900953909842
Iteration 170: Loss = -12462.880819757676
Iteration 180: Loss = -12462.863722002789
Iteration 190: Loss = -12462.849334700437
Iteration 200: Loss = -12462.837075929196
Iteration 210: Loss = -12462.826661645237
Iteration 220: Loss = -12462.817724513387
Iteration 230: Loss = -12462.810122881428
Iteration 240: Loss = -12462.803449036555
Iteration 250: Loss = -12462.79778214728
Iteration 260: Loss = -12462.79273573007
Iteration 270: Loss = -12462.788349890025
Iteration 280: Loss = -12462.784488045456
Iteration 290: Loss = -12462.781063916274
Iteration 300: Loss = -12462.778063400963
Iteration 310: Loss = -12462.775331661871
Iteration 320: Loss = -12462.772921827484
Iteration 330: Loss = -12462.77076086604
Iteration 340: Loss = -12462.7687645912
Iteration 350: Loss = -12462.766985325035
Iteration 360: Loss = -12462.765338842406
Iteration 370: Loss = -12462.76390529949
Iteration 380: Loss = -12462.762585429884
Iteration 390: Loss = -12462.761324811134
Iteration 400: Loss = -12462.760285872098
Iteration 410: Loss = -12462.759239064484
Iteration 420: Loss = -12462.75828017368
Iteration 430: Loss = -12462.757475184528
Iteration 440: Loss = -12462.756621278733
Iteration 450: Loss = -12462.755973038129
Iteration 460: Loss = -12462.755288044857
Iteration 470: Loss = -12462.75466385172
Iteration 480: Loss = -12462.754082087256
Iteration 490: Loss = -12462.75352748987
Iteration 500: Loss = -12462.753028729365
Iteration 510: Loss = -12462.752597584364
Iteration 520: Loss = -12462.752161907194
Iteration 530: Loss = -12462.751774699615
Iteration 540: Loss = -12462.75146083621
Iteration 550: Loss = -12462.751106178534
Iteration 560: Loss = -12462.750764131157
Iteration 570: Loss = -12462.750497859606
Iteration 580: Loss = -12462.750213126812
Iteration 590: Loss = -12462.749950078494
Iteration 600: Loss = -12462.749705376638
Iteration 610: Loss = -12462.749473135556
Iteration 620: Loss = -12462.749299770905
Iteration 630: Loss = -12462.749082622056
Iteration 640: Loss = -12462.748969550259
Iteration 650: Loss = -12462.748714521007
Iteration 660: Loss = -12462.748658244112
Iteration 670: Loss = -12462.748446013367
Iteration 680: Loss = -12462.748326287443
Iteration 690: Loss = -12462.748174139655
Iteration 700: Loss = -12462.748080780497
Iteration 710: Loss = -12462.748004275352
Iteration 720: Loss = -12462.747877300528
Iteration 730: Loss = -12462.74771938731
Iteration 740: Loss = -12462.74769417023
Iteration 750: Loss = -12462.747569913523
Iteration 760: Loss = -12462.747469173608
Iteration 770: Loss = -12462.747421997268
Iteration 780: Loss = -12462.747346341448
Iteration 790: Loss = -12462.747290614168
Iteration 800: Loss = -12462.747217849688
Iteration 810: Loss = -12462.747231548015
1
Iteration 820: Loss = -12462.747114320333
Iteration 830: Loss = -12462.747061166629
Iteration 840: Loss = -12462.746997890108
Iteration 850: Loss = -12462.746968164283
Iteration 860: Loss = -12462.746938817894
Iteration 870: Loss = -12462.746872380703
Iteration 880: Loss = -12462.74683244316
Iteration 890: Loss = -12462.74685461506
1
Iteration 900: Loss = -12462.746802287258
Iteration 910: Loss = -12462.746779350213
Iteration 920: Loss = -12462.74673056565
Iteration 930: Loss = -12462.74671804007
Iteration 940: Loss = -12462.746716128237
Iteration 950: Loss = -12462.746644422568
Iteration 960: Loss = -12462.746647513257
1
Iteration 970: Loss = -12462.746617986997
Iteration 980: Loss = -12462.746621416649
1
Iteration 990: Loss = -12462.746576653499
Iteration 1000: Loss = -12462.746571364927
Iteration 1010: Loss = -12462.746521360254
Iteration 1020: Loss = -12462.746506000165
Iteration 1030: Loss = -12462.746550748541
1
Iteration 1040: Loss = -12462.746486878123
Iteration 1050: Loss = -12462.74650890336
1
Iteration 1060: Loss = -12462.74649386192
2
Iteration 1070: Loss = -12462.746474589241
Iteration 1080: Loss = -12462.746446574733
Iteration 1090: Loss = -12462.74647659147
1
Iteration 1100: Loss = -12462.746468792251
2
Iteration 1110: Loss = -12462.746439928376
Iteration 1120: Loss = -12462.746426644493
Iteration 1130: Loss = -12462.746418297895
Iteration 1140: Loss = -12462.746400031752
Iteration 1150: Loss = -12462.746442152227
1
Iteration 1160: Loss = -12462.746391017004
Iteration 1170: Loss = -12462.746389231963
Iteration 1180: Loss = -12462.746342244738
Iteration 1190: Loss = -12462.746353399712
1
Iteration 1200: Loss = -12462.746358991562
2
Iteration 1210: Loss = -12462.746357372918
3
Stopping early at iteration 1209 due to no improvement.
pi: tensor([[0.4992, 0.5008],
        [0.4849, 0.5151]], dtype=torch.float64)
alpha: tensor([0.4919, 0.5081])
beta: tensor([[[0.2006, 0.1972],
         [0.4378, 0.2001]],

        [[0.2268, 0.1834],
         [0.4325, 0.2351]],

        [[0.3137, 0.2134],
         [0.5616, 0.1553]],

        [[0.8892, 0.2040],
         [0.1942, 0.0680]],

        [[0.1012, 0.2038],
         [0.9541, 0.0947]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -18484.617881030303
Iteration 100: Loss = -12492.62100453377
Iteration 200: Loss = -12474.098164244204
Iteration 300: Loss = -12467.289943570186
Iteration 400: Loss = -12466.32290887246
Iteration 500: Loss = -12465.85049853681
Iteration 600: Loss = -12465.58720171979
Iteration 700: Loss = -12465.377545487665
Iteration 800: Loss = -12465.200670041
Iteration 900: Loss = -12465.049416340351
Iteration 1000: Loss = -12464.882897875836
Iteration 1100: Loss = -12464.703721531252
Iteration 1200: Loss = -12464.531020786828
Iteration 1300: Loss = -12464.36651488186
Iteration 1400: Loss = -12464.211773084604
Iteration 1500: Loss = -12464.047946803925
Iteration 1600: Loss = -12463.901781684757
Iteration 1700: Loss = -12463.736199401628
Iteration 1800: Loss = -12463.597475170316
Iteration 1900: Loss = -12463.47444498676
Iteration 2000: Loss = -12463.352485232825
Iteration 2100: Loss = -12463.231854611735
Iteration 2200: Loss = -12463.129709676685
Iteration 2300: Loss = -12463.047557909404
Iteration 2400: Loss = -12462.982053705313
Iteration 2500: Loss = -12462.909912829453
Iteration 2600: Loss = -12462.840549688246
Iteration 2700: Loss = -12462.798746838442
Iteration 2800: Loss = -12462.767866582913
Iteration 2900: Loss = -12462.742975354955
Iteration 3000: Loss = -12462.720348698684
Iteration 3100: Loss = -12462.697585800486
Iteration 3200: Loss = -12462.669782217126
Iteration 3300: Loss = -12462.63082920648
Iteration 3400: Loss = -12462.562992477215
Iteration 3500: Loss = -12462.423804718122
Iteration 3600: Loss = -12462.16031877562
Iteration 3700: Loss = -12461.887945010678
Iteration 3800: Loss = -12461.75040977055
Iteration 3900: Loss = -12461.666223243743
Iteration 4000: Loss = -12461.559656799112
Iteration 4100: Loss = -12461.474050581175
Iteration 4200: Loss = -12461.487481843746
1
Iteration 4300: Loss = -12461.37517703133
Iteration 4400: Loss = -12461.330047055002
Iteration 4500: Loss = -12461.278591390275
Iteration 4600: Loss = -12461.183571301897
Iteration 4700: Loss = -12461.036538259334
Iteration 4800: Loss = -12460.93030235642
Iteration 4900: Loss = -12460.865774113743
Iteration 5000: Loss = -12460.821198107717
Iteration 5100: Loss = -12460.774088102287
Iteration 5200: Loss = -12460.501797252882
Iteration 5300: Loss = -12460.40296801798
Iteration 5400: Loss = -12460.374169508044
Iteration 5500: Loss = -12460.34325034947
Iteration 5600: Loss = -12460.329602429936
Iteration 5700: Loss = -12460.320211709148
Iteration 5800: Loss = -12460.313233946375
Iteration 5900: Loss = -12460.307851917572
Iteration 6000: Loss = -12460.303599328226
Iteration 6100: Loss = -12460.300213720397
Iteration 6200: Loss = -12460.297344141965
Iteration 6300: Loss = -12460.294949495848
Iteration 6400: Loss = -12460.293231867461
Iteration 6500: Loss = -12460.291147803795
Iteration 6600: Loss = -12460.289583482665
Iteration 6700: Loss = -12460.288263564786
Iteration 6800: Loss = -12460.287071456307
Iteration 6900: Loss = -12460.286023786355
Iteration 7000: Loss = -12460.286505583194
1
Iteration 7100: Loss = -12460.286086759857
2
Iteration 7200: Loss = -12460.312376194632
3
Iteration 7300: Loss = -12460.303923801686
4
Iteration 7400: Loss = -12460.282758633868
Iteration 7500: Loss = -12460.469194273961
1
Iteration 7600: Loss = -12460.280873796244
Iteration 7700: Loss = -12460.289255323016
1
Iteration 7800: Loss = -12460.280480273263
Iteration 7900: Loss = -12460.370477790737
1
Iteration 8000: Loss = -12460.279092443738
Iteration 8100: Loss = -12460.318103463975
1
Iteration 8200: Loss = -12460.278375485897
Iteration 8300: Loss = -12460.278079524327
Iteration 8400: Loss = -12460.278200861041
1
Iteration 8500: Loss = -12460.278063923819
Iteration 8600: Loss = -12460.277224812566
Iteration 8700: Loss = -12460.277008866247
Iteration 8800: Loss = -12460.27674816247
Iteration 8900: Loss = -12460.2765800066
Iteration 9000: Loss = -12460.276325274726
Iteration 9100: Loss = -12460.277787618852
1
Iteration 9200: Loss = -12460.27593684765
Iteration 9300: Loss = -12460.27576483276
Iteration 9400: Loss = -12460.275891233885
1
Iteration 9500: Loss = -12460.275479628202
Iteration 9600: Loss = -12460.275316216226
Iteration 9700: Loss = -12460.275334721404
1
Iteration 9800: Loss = -12460.275090251673
Iteration 9900: Loss = -12460.509462957163
1
Iteration 10000: Loss = -12460.274848062876
Iteration 10100: Loss = -12460.274829007765
Iteration 10200: Loss = -12460.284538537067
1
Iteration 10300: Loss = -12460.274525048646
Iteration 10400: Loss = -12460.285043178916
1
Iteration 10500: Loss = -12460.36560101372
2
Iteration 10600: Loss = -12460.326920720248
3
Iteration 10700: Loss = -12460.27484544335
4
Iteration 10800: Loss = -12460.278448687568
5
Iteration 10900: Loss = -12460.27880971854
6
Iteration 11000: Loss = -12460.274625881451
7
Iteration 11100: Loss = -12460.279753870642
8
Iteration 11200: Loss = -12460.275354676432
9
Iteration 11300: Loss = -12460.273834247086
Iteration 11400: Loss = -12460.2743333943
1
Iteration 11500: Loss = -12460.33153346446
2
Iteration 11600: Loss = -12460.299275629359
3
Iteration 11700: Loss = -12460.273828920705
Iteration 11800: Loss = -12460.281195738911
1
Iteration 11900: Loss = -12460.280157407988
2
Iteration 12000: Loss = -12460.29864290643
3
Iteration 12100: Loss = -12460.273545956139
Iteration 12200: Loss = -12460.273481045479
Iteration 12300: Loss = -12460.281877984127
1
Iteration 12400: Loss = -12460.273316892719
Iteration 12500: Loss = -12460.274589698778
1
Iteration 12600: Loss = -12460.275146818802
2
Iteration 12700: Loss = -12460.306523139463
3
Iteration 12800: Loss = -12460.27437369334
4
Iteration 12900: Loss = -12460.27703324095
5
Iteration 13000: Loss = -12460.277852172083
6
Iteration 13100: Loss = -12460.277504154996
7
Iteration 13200: Loss = -12460.275266063532
8
Iteration 13300: Loss = -12460.384459344285
9
Iteration 13400: Loss = -12460.273152118349
Iteration 13500: Loss = -12460.27320886307
1
Iteration 13600: Loss = -12460.275516766327
2
Iteration 13700: Loss = -12460.294997342591
3
Iteration 13800: Loss = -12460.284947913264
4
Iteration 13900: Loss = -12460.28492917527
5
Iteration 14000: Loss = -12460.278372712264
6
Iteration 14100: Loss = -12460.288917514006
7
Iteration 14200: Loss = -12460.276205916818
8
Iteration 14300: Loss = -12460.299947211683
9
Iteration 14400: Loss = -12460.278140294517
10
Stopping early at iteration 14400 due to no improvement.
tensor([[-4.0190,  1.2327],
        [-2.5355, -0.7581],
        [-4.8302,  3.4230],
        [-0.0242, -1.7284],
        [-5.4073,  3.9668],
        [-4.5929,  3.0414],
        [-4.7109,  2.8001],
        [-3.8285,  1.7276],
        [-4.2815,  2.1899],
        [-2.8423,  1.4150],
        [-5.6717,  3.9027],
        [-1.4513, -0.2305],
        [-3.5148, -1.1004],
        [-0.8014, -1.2417],
        [-1.9811,  0.5948],
        [-5.1060,  0.4907],
        [-6.1841,  1.5689],
        [-4.8941,  3.5078],
        [-3.3876,  1.7075],
        [-4.0405,  2.6448],
        [-5.2373,  0.6220],
        [-3.6722,  2.1736],
        [-2.8490,  0.6152],
        [-4.0053,  2.1412],
        [-4.0057,  1.6929],
        [-5.9279,  4.3162],
        [-3.5424,  1.7186],
        [-6.0618,  2.5076],
        [-4.6646,  3.1458],
        [-4.7255,  3.3362],
        [-5.1760,  2.4127],
        [-0.5989, -3.2191],
        [-3.2233,  1.6135],
        [-4.3599,  2.7022],
        [-4.2734,  2.7809],
        [-3.6763,  1.7792],
        [-4.0046,  2.1657],
        [-4.9389,  3.4626],
        [-3.8122,  0.4352],
        [-2.9984,  1.2674],
        [-6.0891,  2.8800],
        [-4.9329,  3.5192],
        [-4.4182,  3.0287],
        [-1.4332, -0.4718],
        [-3.7816,  2.3617],
        [-6.0189,  4.0290],
        [-2.6545,  1.1737],
        [-5.7766,  2.6954],
        [-4.5247,  2.1132],
        [-5.9182,  3.8400],
        [-3.3013,  1.8410],
        [-2.3240,  0.8653],
        [-3.0575,  1.6696],
        [-3.2984,  1.3100],
        [-4.3344,  2.9229],
        [-4.4635,  3.0540],
        [-7.3038,  5.5355],
        [-3.1067,  1.7113],
        [-3.2785,  1.2272],
        [-2.6950,  1.2653],
        [-2.3454,  0.9213],
        [-2.2295,  0.8399],
        [-5.1134,  3.5926],
        [-4.6393,  3.0516],
        [-3.4546,  1.8509],
        [-5.0813,  3.5007],
        [-3.2797,  0.0419],
        [-7.4074,  5.7277],
        [-5.2739,  3.7200],
        [-3.0224,  0.8180],
        [-5.0215,  3.5589],
        [-4.6208,  2.8626],
        [-2.4157,  0.3266],
        [-3.3368,  1.9232],
        [-2.9211,  1.3747],
        [-4.4301,  2.9153],
        [-4.6907,  3.2596],
        [-4.1526,  1.5775],
        [-6.2959,  4.8941],
        [-1.9839,  0.4072],
        [-3.8470,  2.1944],
        [-5.2171,  1.5912],
        [-4.2743,  2.3610],
        [-5.0016,  0.3864],
        [-3.5827,  1.8666],
        [-1.6718,  0.1572],
        [-1.4406, -1.4161],
        [-4.0130,  2.6202],
        [-4.3704,  2.8008],
        [-4.1398,  1.8870],
        [-4.3236,  2.9252],
        [-1.7333, -0.1758],
        [-5.1797,  3.3616],
        [-3.5426,  1.5587],
        [-6.2031,  4.8138],
        [-3.4514,  1.9594],
        [-4.2707,  2.2863],
        [-4.4709,  3.0430],
        [-4.3247,  2.8953],
        [-5.3548,  3.7018]], dtype=torch.float64, requires_grad=True)
pi: tensor([[6.9734e-07, 1.0000e+00],
        [1.0000e+00, 1.8706e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0458, 0.9542], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1927, 0.1401],
         [0.4378, 0.2075]],

        [[0.2268, 0.2077],
         [0.4325, 0.2351]],

        [[0.3137, 0.2091],
         [0.5616, 0.1553]],

        [[0.8892, 0.2499],
         [0.1942, 0.0680]],

        [[0.1012, 0.2608],
         [0.9541, 0.0947]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 1
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
time is 2
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 7.13818506546007e-05
time is 3
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.010427427162149738
time is 4
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: -0.014100938522947548
Global Adjusted Rand Index: -0.0015534621705078576
Average Adjusted Rand Index: -0.004308721449583262
Iteration 0: Loss = -19931.052546580897
Iteration 10: Loss = -12466.628762039747
Iteration 20: Loss = -12466.628453948779
Iteration 30: Loss = -12466.625659988305
Iteration 40: Loss = -12466.59357573108
Iteration 50: Loss = -12466.296531253483
Iteration 60: Loss = -12465.27772929055
Iteration 70: Loss = -12464.373177876698
Iteration 80: Loss = -12463.903216266062
Iteration 90: Loss = -12463.654846755231
Iteration 100: Loss = -12463.506625919734
Iteration 110: Loss = -12463.409189773665
Iteration 120: Loss = -12463.340777604373
Iteration 130: Loss = -12463.289941148156
Iteration 140: Loss = -12463.25032152143
Iteration 150: Loss = -12463.218090147273
Iteration 160: Loss = -12463.19097958157
Iteration 170: Loss = -12463.16751843593
Iteration 180: Loss = -12463.146623620029
Iteration 190: Loss = -12463.127752255132
Iteration 200: Loss = -12463.110424531691
Iteration 210: Loss = -12463.094342452236
Iteration 220: Loss = -12463.079246340769
Iteration 230: Loss = -12463.064987350075
Iteration 240: Loss = -12463.051386814292
Iteration 250: Loss = -12463.038482988008
Iteration 260: Loss = -12463.026124670703
Iteration 270: Loss = -12463.014270357686
Iteration 280: Loss = -12463.00290275096
Iteration 290: Loss = -12462.991902444433
Iteration 300: Loss = -12462.981354085261
Iteration 310: Loss = -12462.97122322843
Iteration 320: Loss = -12462.961478956197
Iteration 330: Loss = -12462.952036511071
Iteration 340: Loss = -12462.942980733204
Iteration 350: Loss = -12462.934225378325
Iteration 360: Loss = -12462.925799326273
Iteration 370: Loss = -12462.917750284714
Iteration 380: Loss = -12462.909900776262
Iteration 390: Loss = -12462.90249739504
Iteration 400: Loss = -12462.895270351131
Iteration 410: Loss = -12462.888427890324
Iteration 420: Loss = -12462.881837352254
Iteration 430: Loss = -12462.875468029819
Iteration 440: Loss = -12462.869425092022
Iteration 450: Loss = -12462.863712249671
Iteration 460: Loss = -12462.858135735982
Iteration 470: Loss = -12462.852880192584
Iteration 480: Loss = -12462.847796427624
Iteration 490: Loss = -12462.842965004524
Iteration 500: Loss = -12462.838387764064
Iteration 510: Loss = -12462.834000604524
Iteration 520: Loss = -12462.829848494168
Iteration 530: Loss = -12462.82589283326
Iteration 540: Loss = -12462.822126919493
Iteration 550: Loss = -12462.818573507804
Iteration 560: Loss = -12462.815172241344
Iteration 570: Loss = -12462.81193950994
Iteration 580: Loss = -12462.808883870877
Iteration 590: Loss = -12462.805995038623
Iteration 600: Loss = -12462.803170820287
Iteration 610: Loss = -12462.80057687515
Iteration 620: Loss = -12462.7981062127
Iteration 630: Loss = -12462.795737097156
Iteration 640: Loss = -12462.79353884155
Iteration 650: Loss = -12462.791420870764
Iteration 660: Loss = -12462.789400757174
Iteration 670: Loss = -12462.787495918445
Iteration 680: Loss = -12462.78568820971
Iteration 690: Loss = -12462.783992192466
Iteration 700: Loss = -12462.782427479522
Iteration 710: Loss = -12462.780834166479
Iteration 720: Loss = -12462.77938916038
Iteration 730: Loss = -12462.778001587016
Iteration 740: Loss = -12462.77669032994
Iteration 750: Loss = -12462.77543696329
Iteration 760: Loss = -12462.774264204683
Iteration 770: Loss = -12462.773180242993
Iteration 780: Loss = -12462.772075493578
Iteration 790: Loss = -12462.77108724085
Iteration 800: Loss = -12462.770075992543
Iteration 810: Loss = -12462.769197486658
Iteration 820: Loss = -12462.768329252061
Iteration 830: Loss = -12462.767474288477
Iteration 840: Loss = -12462.766686107234
Iteration 850: Loss = -12462.76594254224
Iteration 860: Loss = -12462.765251342322
Iteration 870: Loss = -12462.764519435515
Iteration 880: Loss = -12462.763908743103
Iteration 890: Loss = -12462.763273665692
Iteration 900: Loss = -12462.762668885804
Iteration 910: Loss = -12462.762109741683
Iteration 920: Loss = -12462.76159829276
Iteration 930: Loss = -12462.761069860051
Iteration 940: Loss = -12462.760574464346
Iteration 950: Loss = -12462.760109234914
Iteration 960: Loss = -12462.759667697588
Iteration 970: Loss = -12462.75925835348
Iteration 980: Loss = -12462.758820882444
Iteration 990: Loss = -12462.758414634674
Iteration 1000: Loss = -12462.758061198569
Iteration 1010: Loss = -12462.757695072753
Iteration 1020: Loss = -12462.757334326072
Iteration 1030: Loss = -12462.756990667554
Iteration 1040: Loss = -12462.756717972248
Iteration 1050: Loss = -12462.756373062264
Iteration 1060: Loss = -12462.755982278186
Iteration 1070: Loss = -12462.755808203214
Iteration 1080: Loss = -12462.755538421385
Iteration 1090: Loss = -12462.75526003094
Iteration 1100: Loss = -12462.755005143943
Iteration 1110: Loss = -12462.75476999435
Iteration 1120: Loss = -12462.754539108957
Iteration 1130: Loss = -12462.754282468966
Iteration 1140: Loss = -12462.754053925084
Iteration 1150: Loss = -12462.753901992457
Iteration 1160: Loss = -12462.753688762701
Iteration 1170: Loss = -12462.753556317435
Iteration 1180: Loss = -12462.753306126688
Iteration 1190: Loss = -12462.75311557622
Iteration 1200: Loss = -12462.752965363012
Iteration 1210: Loss = -12462.752800172426
Iteration 1220: Loss = -12462.752627289405
Iteration 1230: Loss = -12462.75250237409
Iteration 1240: Loss = -12462.752317501496
Iteration 1250: Loss = -12462.752182474009
Iteration 1260: Loss = -12462.751999433365
Iteration 1270: Loss = -12462.751912613076
Iteration 1280: Loss = -12462.751749594529
Iteration 1290: Loss = -12462.751680083382
Iteration 1300: Loss = -12462.751509821876
Iteration 1310: Loss = -12462.751406751668
Iteration 1320: Loss = -12462.751318165121
Iteration 1330: Loss = -12462.751174488776
Iteration 1340: Loss = -12462.75104300028
Iteration 1350: Loss = -12462.750976047315
Iteration 1360: Loss = -12462.750845831915
Iteration 1370: Loss = -12462.750766611498
Iteration 1380: Loss = -12462.750659770792
Iteration 1390: Loss = -12462.75058008799
Iteration 1400: Loss = -12462.750449614707
Iteration 1410: Loss = -12462.750413253463
Iteration 1420: Loss = -12462.750283069852
Iteration 1430: Loss = -12462.750197314046
Iteration 1440: Loss = -12462.750148561396
Iteration 1450: Loss = -12462.750059163323
Iteration 1460: Loss = -12462.749988616088
Iteration 1470: Loss = -12462.749879876817
Iteration 1480: Loss = -12462.749831068502
Iteration 1490: Loss = -12462.74978905035
Iteration 1500: Loss = -12462.749710258056
Iteration 1510: Loss = -12462.74968392028
Iteration 1520: Loss = -12462.749610615834
Iteration 1530: Loss = -12462.749505398966
Iteration 1540: Loss = -12462.749467864296
Iteration 1550: Loss = -12462.749428609592
Iteration 1560: Loss = -12462.749363751345
Iteration 1570: Loss = -12462.74929892813
Iteration 1580: Loss = -12462.749193574213
Iteration 1590: Loss = -12462.749200136028
1
Iteration 1600: Loss = -12462.74916705726
Iteration 1610: Loss = -12462.749057596542
Iteration 1620: Loss = -12462.749026969113
Iteration 1630: Loss = -12462.749010922513
Iteration 1640: Loss = -12462.748943558307
Iteration 1650: Loss = -12462.748919563068
Iteration 1660: Loss = -12462.748883183353
Iteration 1670: Loss = -12462.748781687644
Iteration 1680: Loss = -12462.748809980085
1
Iteration 1690: Loss = -12462.74871283689
Iteration 1700: Loss = -12462.748682262976
Iteration 1710: Loss = -12462.748701313705
1
Iteration 1720: Loss = -12462.748596073629
Iteration 1730: Loss = -12462.748606126877
1
Iteration 1740: Loss = -12462.748501690075
Iteration 1750: Loss = -12462.748466401437
Iteration 1760: Loss = -12462.74844506551
Iteration 1770: Loss = -12462.748426989521
Iteration 1780: Loss = -12462.748399772507
Iteration 1790: Loss = -12462.748373085324
Iteration 1800: Loss = -12462.748334310665
Iteration 1810: Loss = -12462.748334509492
1
Iteration 1820: Loss = -12462.748277931596
Iteration 1830: Loss = -12462.748276879214
Iteration 1840: Loss = -12462.748223770475
Iteration 1850: Loss = -12462.748181971316
Iteration 1860: Loss = -12462.748144709058
Iteration 1870: Loss = -12462.748160367475
1
Iteration 1880: Loss = -12462.748074883755
Iteration 1890: Loss = -12462.748093923341
1
Iteration 1900: Loss = -12462.748051378661
Iteration 1910: Loss = -12462.748016051912
Iteration 1920: Loss = -12462.747977928417
Iteration 1930: Loss = -12462.747960957182
Iteration 1940: Loss = -12462.747940384785
Iteration 1950: Loss = -12462.747878270375
Iteration 1960: Loss = -12462.747914630561
1
Iteration 1970: Loss = -12462.747889346432
2
Iteration 1980: Loss = -12462.747875362475
Iteration 1990: Loss = -12462.747905723629
1
Iteration 2000: Loss = -12462.747842221255
Iteration 2010: Loss = -12462.747778460336
Iteration 2020: Loss = -12462.747822353604
1
Iteration 2030: Loss = -12462.747747189502
Iteration 2040: Loss = -12462.747746222347
Iteration 2050: Loss = -12462.747714761532
Iteration 2060: Loss = -12462.747705391197
Iteration 2070: Loss = -12462.747666460113
Iteration 2080: Loss = -12462.747667621641
1
Iteration 2090: Loss = -12462.747664752631
Iteration 2100: Loss = -12462.74763799179
Iteration 2110: Loss = -12462.747603000505
Iteration 2120: Loss = -12462.747623169154
1
Iteration 2130: Loss = -12462.747580441835
Iteration 2140: Loss = -12462.747561270739
Iteration 2150: Loss = -12462.747543729674
Iteration 2160: Loss = -12462.747522019332
Iteration 2170: Loss = -12462.747490782262
Iteration 2180: Loss = -12462.747514956573
1
Iteration 2190: Loss = -12462.74750489968
2
Iteration 2200: Loss = -12462.747473589363
Iteration 2210: Loss = -12462.747458674752
Iteration 2220: Loss = -12462.747439417866
Iteration 2230: Loss = -12462.747442962605
1
Iteration 2240: Loss = -12462.747427773289
Iteration 2250: Loss = -12462.747395888582
Iteration 2260: Loss = -12462.74741623774
1
Iteration 2270: Loss = -12462.74737542914
Iteration 2280: Loss = -12462.747372345108
Iteration 2290: Loss = -12462.747364362529
Iteration 2300: Loss = -12462.747397279807
1
Iteration 2310: Loss = -12462.747322952811
Iteration 2320: Loss = -12462.747312250815
Iteration 2330: Loss = -12462.747285136345
Iteration 2340: Loss = -12462.747281898193
Iteration 2350: Loss = -12462.747314570699
1
Iteration 2360: Loss = -12462.74723566764
Iteration 2370: Loss = -12462.747288065384
1
Iteration 2380: Loss = -12462.747246011973
2
Iteration 2390: Loss = -12462.747266151577
3
Stopping early at iteration 2389 due to no improvement.
pi: tensor([[1.2194e-04, 9.9988e-01],
        [9.6901e-01, 3.0994e-02]], dtype=torch.float64)
alpha: tensor([0.4922, 0.5078])
beta: tensor([[[0.2003, 0.1972],
         [0.6584, 0.2004]],

        [[0.4264, 0.1834],
         [0.0090, 0.8496]],

        [[0.8709, 0.2134],
         [0.3082, 0.2977]],

        [[0.0762, 0.2040],
         [0.4983, 0.4507]],

        [[0.9869, 0.2038],
         [0.1544, 0.8043]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19930.7385371096
Iteration 100: Loss = -12467.444378656899
Iteration 200: Loss = -12465.601055797928
Iteration 300: Loss = -12464.60015631475
Iteration 400: Loss = -12464.06244493134
Iteration 500: Loss = -12463.789155442018
Iteration 600: Loss = -12463.589301213458
Iteration 700: Loss = -12463.420385334544
Iteration 800: Loss = -12463.267021289372
Iteration 900: Loss = -12463.143046892394
Iteration 1000: Loss = -12463.034590595993
Iteration 1100: Loss = -12462.943920916596
Iteration 1200: Loss = -12462.86960175979
Iteration 1300: Loss = -12462.810499488398
Iteration 1400: Loss = -12462.762949677444
Iteration 1500: Loss = -12462.724319710931
Iteration 1600: Loss = -12462.691537733679
Iteration 1700: Loss = -12462.662323845863
Iteration 1800: Loss = -12462.634598894201
Iteration 1900: Loss = -12462.605879235589
Iteration 2000: Loss = -12462.57301137454
Iteration 2100: Loss = -12462.531307390907
Iteration 2200: Loss = -12462.472588917384
Iteration 2300: Loss = -12462.381483635854
Iteration 2400: Loss = -12462.233152442217
Iteration 2500: Loss = -12462.015739567105
Iteration 2600: Loss = -12461.785723088808
Iteration 2700: Loss = -12461.615424706313
Iteration 2800: Loss = -12461.506338733196
Iteration 2900: Loss = -12461.437202264406
Iteration 3000: Loss = -12461.391742034124
Iteration 3100: Loss = -12461.360042668131
Iteration 3200: Loss = -12461.336645535472
Iteration 3300: Loss = -12461.31862487054
Iteration 3400: Loss = -12461.304075029017
Iteration 3500: Loss = -12461.29172852803
Iteration 3600: Loss = -12461.280922545066
Iteration 3700: Loss = -12461.270096420425
Iteration 3800: Loss = -12461.225966364664
Iteration 3900: Loss = -12199.235622397864
Iteration 4000: Loss = -12103.737542565224
Iteration 4100: Loss = -12029.032292790587
Iteration 4200: Loss = -12006.579442060796
Iteration 4300: Loss = -11993.521477990222
Iteration 4400: Loss = -11993.313850280352
Iteration 4500: Loss = -11980.880334524407
Iteration 4600: Loss = -11972.632398997592
Iteration 4700: Loss = -11972.620674781765
Iteration 4800: Loss = -11972.612843477023
Iteration 4900: Loss = -11972.605344537176
Iteration 5000: Loss = -11972.59122712307
Iteration 5100: Loss = -11972.5699645266
Iteration 5200: Loss = -11972.564469690158
Iteration 5300: Loss = -11972.562048915675
Iteration 5400: Loss = -11972.56168991936
Iteration 5500: Loss = -11972.558416613023
Iteration 5600: Loss = -11972.557020271495
Iteration 5700: Loss = -11972.556792808096
Iteration 5800: Loss = -11972.554712047058
Iteration 5900: Loss = -11972.557756815148
1
Iteration 6000: Loss = -11972.552565552804
Iteration 6100: Loss = -11972.558403819772
1
Iteration 6200: Loss = -11972.554899911267
2
Iteration 6300: Loss = -11972.550860403346
Iteration 6400: Loss = -11972.55063715197
Iteration 6500: Loss = -11972.54138989431
Iteration 6600: Loss = -11972.539034283698
Iteration 6700: Loss = -11972.538896449241
Iteration 6800: Loss = -11972.537225732432
Iteration 6900: Loss = -11972.536818891824
Iteration 7000: Loss = -11972.535227228896
Iteration 7100: Loss = -11972.534691098852
Iteration 7200: Loss = -11972.538944380463
1
Iteration 7300: Loss = -11972.534301407166
Iteration 7400: Loss = -11972.533674248507
Iteration 7500: Loss = -11972.535364467209
1
Iteration 7600: Loss = -11972.536967457767
2
Iteration 7700: Loss = -11972.542791838929
3
Iteration 7800: Loss = -11972.533380301906
Iteration 7900: Loss = -11972.61422735757
1
Iteration 8000: Loss = -11972.530918253717
Iteration 8100: Loss = -11972.53668726513
1
Iteration 8200: Loss = -11972.530519811275
Iteration 8300: Loss = -11972.53010854378
Iteration 8400: Loss = -11972.734710539728
1
Iteration 8500: Loss = -11972.528546561733
Iteration 8600: Loss = -11972.548050734029
1
Iteration 8700: Loss = -11972.537503849528
2
Iteration 8800: Loss = -11972.5176367229
Iteration 8900: Loss = -11972.52071283837
1
Iteration 9000: Loss = -11972.518341498935
2
Iteration 9100: Loss = -11972.517919795504
3
Iteration 9200: Loss = -11972.517501569666
Iteration 9300: Loss = -11972.517162403556
Iteration 9400: Loss = -11972.517061451508
Iteration 9500: Loss = -11972.519856290426
1
Iteration 9600: Loss = -11972.526133285428
2
Iteration 9700: Loss = -11972.51664042447
Iteration 9800: Loss = -11972.517435763724
1
Iteration 9900: Loss = -11972.52877487898
2
Iteration 10000: Loss = -11972.516492791894
Iteration 10100: Loss = -11972.51709686478
1
Iteration 10200: Loss = -11972.518045416818
2
Iteration 10300: Loss = -11972.517313626064
3
Iteration 10400: Loss = -11972.516430613661
Iteration 10500: Loss = -11972.516399590757
Iteration 10600: Loss = -11972.521597543164
1
Iteration 10700: Loss = -11972.526838892056
2
Iteration 10800: Loss = -11972.529447000354
3
Iteration 10900: Loss = -11972.517890380941
4
Iteration 11000: Loss = -11972.517236372207
5
Iteration 11100: Loss = -11972.529591378729
6
Iteration 11200: Loss = -11972.5184846341
7
Iteration 11300: Loss = -11972.5217765336
8
Iteration 11400: Loss = -11972.517603444654
9
Iteration 11500: Loss = -11972.518001402264
10
Stopping early at iteration 11500 due to no improvement.
tensor([[  6.5746,  -9.6927],
        [ -7.2454,   2.9217],
        [  4.3465,  -5.7726],
        [ -7.5727,   5.9476],
        [ -6.4780,   4.5924],
        [-10.1796,   6.8213],
        [  5.5448,  -7.1933],
        [ -5.7612,   4.3503],
        [ -5.0103,   3.6229],
        [  5.5380,  -6.9690],
        [ -7.5869,   6.2004],
        [ -8.2612,   6.6094],
        [  4.1062,  -5.4926],
        [  5.2372,  -6.6347],
        [  6.3867,  -8.0831],
        [ -6.3938,   4.8912],
        [  6.9229,  -8.3192],
        [ -8.6551,   5.9700],
        [  3.4793,  -4.8832],
        [ -7.3577,   5.9530],
        [  5.7139,  -8.3832],
        [  5.5118,  -7.5063],
        [ -4.3894,   3.0031],
        [  2.7488,  -5.7426],
        [ -4.4594,   2.8704],
        [  4.9685,  -9.5837],
        [ -7.0763,   5.2155],
        [ -9.4761,   7.7913],
        [ -4.6902,   3.0304],
        [  6.3970,  -7.9316],
        [  6.2982,  -8.1426],
        [ -4.7526,   3.3464],
        [  0.6697,  -2.5310],
        [  6.4379,  -7.9132],
        [  2.4871,  -4.6840],
        [  4.5134,  -5.9198],
        [  5.1881,  -7.5094],
        [  6.5983,  -9.4527],
        [ -3.0282,   1.4959],
        [  4.8354,  -6.2280],
        [  5.5366,  -7.9593],
        [ -7.9820,   6.5375],
        [  6.0515,  -7.4537],
        [ -6.3714,   4.9204],
        [ -7.4884,   4.9808],
        [ -6.7822,   4.5014],
        [ -7.2986,   5.9100],
        [  5.5573,  -8.7518],
        [  5.6751,  -7.0663],
        [  5.7047,  -7.8350],
        [ -3.2977,   1.4644],
        [ -7.5203,   6.1023],
        [ -6.0834,   4.6321],
        [  5.8504,  -7.2386],
        [  1.5906,  -2.9917],
        [  5.3644,  -7.8783],
        [  5.8522,  -7.2915],
        [ -1.0792,  -1.0704],
        [ -5.4245,   3.3768],
        [ -8.4650,   6.8543],
        [ -2.4567,   1.0575],
        [ -9.7885,   6.0958],
        [ -8.6539,   6.5792],
        [  4.3750,  -6.6594],
        [ -7.6512,   6.1888],
        [ -7.5445,   5.9045],
        [ -6.6810,   5.2573],
        [  6.6275,  -8.4069],
        [  5.7466,  -7.8185],
        [ -6.9728,   5.3083],
        [  5.2045,  -7.2854],
        [  2.8533,  -7.0719],
        [ -9.4876,   6.4170],
        [  4.9686,  -6.7528],
        [  6.6640,  -8.4890],
        [ -6.8849,   5.4658],
        [  4.2059,  -5.7173],
        [  6.7212,  -8.1494],
        [  4.6222,  -8.1104],
        [  6.0051,  -7.3917],
        [  6.1848, -10.8000],
        [ -6.5972,   5.1134],
        [ -7.3084,   5.8861],
        [ -9.5906,   6.0069],
        [  4.5577,  -8.4781],
        [  3.6330,  -6.2074],
        [ -1.5691,   0.0297],
        [ -7.2219,   5.6509],
        [  5.8198,  -7.2362],
        [  5.6104,  -7.3604],
        [ -7.4828,   5.5828],
        [ -7.8362,   6.1644],
        [  5.3649,  -6.7646],
        [  5.6642,  -8.6826],
        [  6.0326,  -7.5953],
        [  5.1529,  -6.9704],
        [ -5.7836,   4.2223],
        [  5.6969,  -7.8533],
        [  5.8525,  -7.4122],
        [ -6.9794,   5.5396]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7646, 0.2354],
        [0.2803, 0.7197]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5366, 0.4634], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3063, 0.1031],
         [0.6584, 0.2951]],

        [[0.4264, 0.0900],
         [0.0090, 0.8496]],

        [[0.8709, 0.1101],
         [0.3082, 0.2977]],

        [[0.0762, 0.1050],
         [0.4983, 0.4507]],

        [[0.9869, 0.1092],
         [0.1544, 0.8043]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 1
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 4
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.9681920683449948
Average Adjusted Rand Index: 0.9681600487729215
Iteration 0: Loss = -21186.236260974296
Iteration 10: Loss = -12463.874802952827
Iteration 20: Loss = -12462.977885519156
Iteration 30: Loss = -12462.851191658016
Iteration 40: Loss = -12462.830350494456
Iteration 50: Loss = -12462.82083283101
Iteration 60: Loss = -12462.812928109992
Iteration 70: Loss = -12462.806063014168
Iteration 80: Loss = -12462.800050218873
Iteration 90: Loss = -12462.794769709397
Iteration 100: Loss = -12462.790226704401
Iteration 110: Loss = -12462.786079509928
Iteration 120: Loss = -12462.782490408374
Iteration 130: Loss = -12462.779330422345
Iteration 140: Loss = -12462.776521769616
Iteration 150: Loss = -12462.77397690248
Iteration 160: Loss = -12462.771721178213
Iteration 170: Loss = -12462.769641484292
Iteration 180: Loss = -12462.767778886684
Iteration 190: Loss = -12462.766080250301
Iteration 200: Loss = -12462.764585587203
Iteration 210: Loss = -12462.763165057466
Iteration 220: Loss = -12462.761920597042
Iteration 230: Loss = -12462.760771408803
Iteration 240: Loss = -12462.759701203517
Iteration 250: Loss = -12462.758719651823
Iteration 260: Loss = -12462.757845091564
Iteration 270: Loss = -12462.757008679408
Iteration 280: Loss = -12462.756263813302
Iteration 290: Loss = -12462.755583461143
Iteration 300: Loss = -12462.754933500815
Iteration 310: Loss = -12462.754341636688
Iteration 320: Loss = -12462.753785333623
Iteration 330: Loss = -12462.753288689244
Iteration 340: Loss = -12462.752769349425
Iteration 350: Loss = -12462.752370751772
Iteration 360: Loss = -12462.75199148682
Iteration 370: Loss = -12462.751583958749
Iteration 380: Loss = -12462.751205712786
Iteration 390: Loss = -12462.75088805255
Iteration 400: Loss = -12462.75063446063
Iteration 410: Loss = -12462.750293423092
Iteration 420: Loss = -12462.750052100131
Iteration 430: Loss = -12462.749814352648
Iteration 440: Loss = -12462.74962866463
Iteration 450: Loss = -12462.749360548692
Iteration 460: Loss = -12462.749160812717
Iteration 470: Loss = -12462.748993436328
Iteration 480: Loss = -12462.748789553601
Iteration 490: Loss = -12462.748648345036
Iteration 500: Loss = -12462.748554658021
Iteration 510: Loss = -12462.74843301058
Iteration 520: Loss = -12462.748216737786
Iteration 530: Loss = -12462.748142192346
Iteration 540: Loss = -12462.748040133954
Iteration 550: Loss = -12462.74784195599
Iteration 560: Loss = -12462.747771487175
Iteration 570: Loss = -12462.747690495424
Iteration 580: Loss = -12462.747596773563
Iteration 590: Loss = -12462.747549215677
Iteration 600: Loss = -12462.747451069241
Iteration 610: Loss = -12462.747360438827
Iteration 620: Loss = -12462.74732489137
Iteration 630: Loss = -12462.74722191474
Iteration 640: Loss = -12462.747158360831
Iteration 650: Loss = -12462.747144728846
Iteration 660: Loss = -12462.747035646886
Iteration 670: Loss = -12462.747037466708
1
Iteration 680: Loss = -12462.746957052548
Iteration 690: Loss = -12462.746913761084
Iteration 700: Loss = -12462.746929673127
1
Iteration 710: Loss = -12462.746886389836
Iteration 720: Loss = -12462.746791825426
Iteration 730: Loss = -12462.746807000496
1
Iteration 740: Loss = -12462.746752042456
Iteration 750: Loss = -12462.74673850632
Iteration 760: Loss = -12462.746729030529
Iteration 770: Loss = -12462.746695980466
Iteration 780: Loss = -12462.7467025889
1
Iteration 790: Loss = -12462.746583720682
Iteration 800: Loss = -12462.74661535564
1
Iteration 810: Loss = -12462.746614677917
2
Iteration 820: Loss = -12462.746604311584
3
Stopping early at iteration 819 due to no improvement.
pi: tensor([[0.4932, 0.5068],
        [0.4744, 0.5256]], dtype=torch.float64)
alpha: tensor([0.4834, 0.5166])
beta: tensor([[[0.2009, 0.1972],
         [0.0814, 0.1999]],

        [[0.9167, 0.1834],
         [0.1042, 0.5104]],

        [[0.9021, 0.2134],
         [0.0840, 0.7436]],

        [[0.9569, 0.2040],
         [0.6782, 0.8102]],

        [[0.2891, 0.2038],
         [0.8063, 0.7932]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21185.52050459678
Iteration 100: Loss = -12476.604690349968
Iteration 200: Loss = -12469.08388865265
Iteration 300: Loss = -12466.416080279523
Iteration 400: Loss = -12465.10244751287
Iteration 500: Loss = -12464.346619744056
Iteration 600: Loss = -12463.857171664937
Iteration 700: Loss = -12463.494807392788
Iteration 800: Loss = -12463.15519815837
Iteration 900: Loss = -12462.709969865118
Iteration 1000: Loss = -12462.211745356746
Iteration 1100: Loss = -12461.92819731318
Iteration 1200: Loss = -12461.780119212402
Iteration 1300: Loss = -12461.683898229174
Iteration 1400: Loss = -12461.612856479653
Iteration 1500: Loss = -12461.556668334568
Iteration 1600: Loss = -12461.5104733281
Iteration 1700: Loss = -12461.470996296293
Iteration 1800: Loss = -12461.43318275923
Iteration 1900: Loss = -12228.992182522723
Iteration 2000: Loss = -12211.440409146995
Iteration 2100: Loss = -12187.793181171863
Iteration 2200: Loss = -12167.048464642805
Iteration 2300: Loss = -12166.843229402768
Iteration 2400: Loss = -12164.925230897608
Iteration 2500: Loss = -12146.663425730994
Iteration 2600: Loss = -12146.549304075985
Iteration 2700: Loss = -12146.536776625919
Iteration 2800: Loss = -12146.5267001124
Iteration 2900: Loss = -12138.230155570976
Iteration 3000: Loss = -12138.217871050607
Iteration 3100: Loss = -12138.209793184404
Iteration 3200: Loss = -12138.198005798164
Iteration 3300: Loss = -12138.196044331518
Iteration 3400: Loss = -12138.182975919999
Iteration 3500: Loss = -12138.173394481866
Iteration 3600: Loss = -12138.169192012703
Iteration 3700: Loss = -12138.166584083247
Iteration 3800: Loss = -12138.160647495084
Iteration 3900: Loss = -12138.15783599831
Iteration 4000: Loss = -12138.155292048083
Iteration 4100: Loss = -12138.152730496797
Iteration 4200: Loss = -12138.124523770886
Iteration 4300: Loss = -12138.122603364793
Iteration 4400: Loss = -12138.119705910032
Iteration 4500: Loss = -12138.109795517912
Iteration 4600: Loss = -12138.09430471802
Iteration 4700: Loss = -12138.09979465353
1
Iteration 4800: Loss = -12138.092478733077
Iteration 4900: Loss = -12138.091689034065
Iteration 5000: Loss = -12138.110129536717
1
Iteration 5100: Loss = -12138.09016971242
Iteration 5200: Loss = -12138.095226007756
1
Iteration 5300: Loss = -12138.088866624646
Iteration 5400: Loss = -12138.088916708688
1
Iteration 5500: Loss = -12138.10112433774
2
Iteration 5600: Loss = -12138.08690564314
Iteration 5700: Loss = -12138.086277407338
Iteration 5800: Loss = -12138.084636295662
Iteration 5900: Loss = -12134.90395492382
Iteration 6000: Loss = -12134.97452449867
1
Iteration 6100: Loss = -12134.901182426831
Iteration 6200: Loss = -12134.901026016223
Iteration 6300: Loss = -12134.899888150157
Iteration 6400: Loss = -12134.89917184501
Iteration 6500: Loss = -12134.990500523863
1
Iteration 6600: Loss = -12134.897115338876
Iteration 6700: Loss = -12134.898984414163
1
Iteration 6800: Loss = -12134.89422033332
Iteration 6900: Loss = -12134.954827228985
1
Iteration 7000: Loss = -12134.888708671446
Iteration 7100: Loss = -12134.882666903286
Iteration 7200: Loss = -12134.770574190756
Iteration 7300: Loss = -12131.861236729039
Iteration 7400: Loss = -12128.546137487197
Iteration 7500: Loss = -12125.337460915116
Iteration 7600: Loss = -12117.109498804532
Iteration 7700: Loss = -12078.541360764337
Iteration 7800: Loss = -12029.116302955315
Iteration 7900: Loss = -11972.62063746559
Iteration 8000: Loss = -11972.554791429948
Iteration 8100: Loss = -11972.545770804489
Iteration 8200: Loss = -11972.554878663861
1
Iteration 8300: Loss = -11972.601356432064
2
Iteration 8400: Loss = -11972.533219502811
Iteration 8500: Loss = -11972.528565498918
Iteration 8600: Loss = -11972.527907403593
Iteration 8700: Loss = -11972.530432615767
1
Iteration 8800: Loss = -11972.52680739789
Iteration 8900: Loss = -11972.52668916295
Iteration 9000: Loss = -11972.525565226433
Iteration 9100: Loss = -11972.525091009742
Iteration 9200: Loss = -11972.529470907433
1
Iteration 9300: Loss = -11972.62462679902
2
Iteration 9400: Loss = -11972.515888325117
Iteration 9500: Loss = -11972.51578848715
Iteration 9600: Loss = -11972.517083275743
1
Iteration 9700: Loss = -11972.607864132298
2
Iteration 9800: Loss = -11972.518776941259
3
Iteration 9900: Loss = -11972.531276628328
4
Iteration 10000: Loss = -11972.520816579019
5
Iteration 10100: Loss = -11972.514953515441
Iteration 10200: Loss = -11972.51672412321
1
Iteration 10300: Loss = -11972.516478819827
2
Iteration 10400: Loss = -11972.613716249562
3
Iteration 10500: Loss = -11972.514962364447
4
Iteration 10600: Loss = -11972.514718447095
Iteration 10700: Loss = -11972.514796499898
1
Iteration 10800: Loss = -11972.520089258569
2
Iteration 10900: Loss = -11972.533367788006
3
Iteration 11000: Loss = -11972.519100562975
4
Iteration 11100: Loss = -11972.515180341868
5
Iteration 11200: Loss = -11972.529904587465
6
Iteration 11300: Loss = -11972.515919894284
7
Iteration 11400: Loss = -11972.514541909117
Iteration 11500: Loss = -11972.515008725899
1
Iteration 11600: Loss = -11972.518192752925
2
Iteration 11700: Loss = -11972.514552226045
3
Iteration 11800: Loss = -11972.51451333666
Iteration 11900: Loss = -11972.51813022603
1
Iteration 12000: Loss = -11972.53812984748
2
Iteration 12100: Loss = -11972.519670563272
3
Iteration 12200: Loss = -11972.515310364777
4
Iteration 12300: Loss = -11972.554738114864
5
Iteration 12400: Loss = -11972.515146933214
6
Iteration 12500: Loss = -11972.515633402565
7
Iteration 12600: Loss = -11972.518900116665
8
Iteration 12700: Loss = -11972.514135768808
Iteration 12800: Loss = -11972.52995125348
1
Iteration 12900: Loss = -11972.55621099087
2
Iteration 13000: Loss = -11972.514441581641
3
Iteration 13100: Loss = -11972.536600715923
4
Iteration 13200: Loss = -11972.52676817788
5
Iteration 13300: Loss = -11972.515089987648
6
Iteration 13400: Loss = -11972.52420169063
7
Iteration 13500: Loss = -11972.514633572491
8
Iteration 13600: Loss = -11972.516171686737
9
Iteration 13700: Loss = -11972.516754301907
10
Stopping early at iteration 13700 due to no improvement.
tensor([[ 3.5679, -6.8187],
        [-5.9858,  4.1763],
        [ 3.5473, -6.5388],
        [-7.9066,  5.5482],
        [-6.2259,  4.8382],
        [-8.3904,  5.5051],
        [ 5.5730, -7.5901],
        [-7.3421,  2.7269],
        [-5.5804,  3.0488],
        [ 5.4706, -6.8569],
        [-8.3914,  6.2091],
        [-9.2491,  6.5965],
        [ 4.0446, -5.5533],
        [ 5.6249, -7.0433],
        [ 5.5904, -8.2268],
        [-6.6857,  4.5557],
        [ 5.4183, -9.2930],
        [-8.5363,  5.1338],
        [ 3.4664, -4.8942],
        [-7.9368,  5.6788],
        [ 5.0900, -7.3759],
        [ 4.8762, -7.4566],
        [-4.3956,  2.9956],
        [ 2.4503, -6.0408],
        [-4.3640,  2.9668],
        [ 6.3632, -7.7589],
        [-6.9663,  5.2855],
        [-8.1248,  6.2951],
        [-4.5534,  3.1616],
        [ 6.5119, -8.7827],
        [ 5.3664, -8.5075],
        [-4.7513,  3.3441],
        [ 0.8057, -2.3954],
        [ 5.6900, -7.1513],
        [ 1.2781, -5.8933],
        [ 4.7458, -6.1431],
        [ 5.7764, -7.1627],
        [ 6.6741, -8.4177],
        [-3.0625,  1.4601],
        [ 4.6019, -6.4005],
        [ 5.8618, -7.5436],
        [-8.5423,  6.9435],
        [ 2.4223, -4.7702],
        [-6.3484,  4.8819],
        [-6.8860,  5.4805],
        [-6.4219,  4.7899],
        [-7.6138,  6.1553],
        [ 6.5244, -8.0163],
        [ 5.8090, -8.5284],
        [ 5.9962, -7.9046],
        [-3.4972,  1.2629],
        [-7.2408,  5.7922],
        [-7.1237,  3.5902],
        [ 6.1167, -7.9382],
        [ 1.5162, -3.0643],
        [ 5.6666, -7.4870],
        [ 5.8447, -7.3874],
        [-1.1337, -1.1278],
        [-5.1079,  3.6799],
        [-7.5617,  5.8525],
        [-2.4767,  1.0351],
        [-8.2233,  6.2959],
        [-8.0940,  6.5260],
        [ 4.5415, -6.4446],
        [-7.5934,  6.1913],
        [-7.5944,  6.2077],
        [-7.0520,  5.0851],
        [ 6.6282, -8.3217],
        [ 6.3313, -7.8707],
        [-6.7791,  5.3626],
        [ 5.0354, -6.6133],
        [ 4.2527, -5.6707],
        [-7.6987,  6.0768],
        [ 4.9364, -6.5930],
        [ 2.6227, -5.0483],
        [-7.5296,  5.0297],
        [ 4.2671, -5.6556],
        [ 6.5102, -8.1215],
        [ 5.4779, -6.8818],
        [ 5.3698, -7.0704],
        [ 5.9489, -8.2112],
        [-6.8826,  4.6390],
        [-7.9154,  6.2045],
        [-8.0040,  6.4358],
        [ 6.3055, -7.7245],
        [ 3.9952, -5.8447],
        [-1.9373, -0.3400],
        [-7.3010,  5.9103],
        [ 4.5691, -9.1843],
        [ 5.8137, -7.2014],
        [-7.0702,  5.4917],
        [-7.3636,  5.7695],
        [ 5.1204, -7.3917],
        [ 5.9443, -8.1970],
        [ 5.8999, -9.0580],
        [ 6.4352, -7.9270],
        [-5.8135,  4.1841],
        [ 5.5761, -7.6491],
        [ 5.7901, -7.2346],
        [-7.9479,  4.9920]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7666, 0.2334],
        [0.2800, 0.7200]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5367, 0.4633], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3065, 0.1029],
         [0.0814, 0.2949]],

        [[0.9167, 0.0901],
         [0.1042, 0.5104]],

        [[0.9021, 0.1098],
         [0.0840, 0.7436]],

        [[0.9569, 0.1050],
         [0.6782, 0.8102]],

        [[0.2891, 0.1089],
         [0.8063, 0.7932]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 1
tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 4
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
Global Adjusted Rand Index: 0.9681920683449948
Average Adjusted Rand Index: 0.9681600487729215
11983.204727971859
new:  [-0.0015534621705078576, -0.0015534621705078576, 0.9681920683449948, 0.9681920683449948] [-0.004308721449583262, -0.004308721449583262, 0.9681600487729215, 0.9681600487729215] [12460.275357444798, 12460.278140294517, 11972.518001402264, 11972.516754301907]
prior:  [0.0, 0.0, 0.0, 0.0] [0.0, 0.0, 0.0, 0.0] [12462.746382443629, 12462.746357372918, 12462.747266151577, 12462.746604311584]
-----------------------------------------------------------------------------------------
This iteration is 13
True Objective function: Loss = -11843.78863025153
Iteration 0: Loss = -24396.73826246495
Iteration 10: Loss = -12335.197568157915
Iteration 20: Loss = -12335.197568157946
1
Iteration 30: Loss = -12335.197568157995
2
Iteration 40: Loss = -12335.197568158075
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[1.0000e+00, 7.4640e-15],
        [1.0000e+00, 8.8739e-19]], dtype=torch.float64)
alpha: tensor([1.0000e+00, 7.3793e-15])
beta: tensor([[[0.1966, 0.1911],
         [0.9742, 0.2790]],

        [[0.1804, 0.2445],
         [0.1707, 0.4067]],

        [[0.6718, 0.2629],
         [0.7691, 0.5421]],

        [[0.7649, 0.2686],
         [0.2177, 0.0038]],

        [[0.3835, 0.2416],
         [0.3352, 0.9916]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24089.84607114363
Iteration 100: Loss = -12339.225923067617
Iteration 200: Loss = -12336.429494202197
Iteration 300: Loss = -12335.735031529224
Iteration 400: Loss = -12335.451269558253
Iteration 500: Loss = -12335.289266858366
Iteration 600: Loss = -12335.171347089079
Iteration 700: Loss = -12335.071416889408
Iteration 800: Loss = -12334.992084298468
Iteration 900: Loss = -12334.938342599778
Iteration 1000: Loss = -12334.91342107796
Iteration 1100: Loss = -12334.904554429546
Iteration 1200: Loss = -12334.900303777697
Iteration 1300: Loss = -12334.896968231733
Iteration 1400: Loss = -12334.894044646899
Iteration 1500: Loss = -12334.891265444705
Iteration 1600: Loss = -12334.888525732193
Iteration 1700: Loss = -12334.885673985425
Iteration 1800: Loss = -12334.8826015001
Iteration 1900: Loss = -12334.879193666846
Iteration 2000: Loss = -12334.874945521804
Iteration 2100: Loss = -12334.869051440193
Iteration 2200: Loss = -12334.85885644992
Iteration 2300: Loss = -12334.833415947101
Iteration 2400: Loss = -12334.688903162301
Iteration 2500: Loss = -12333.386574294575
Iteration 2600: Loss = -12333.212483473579
Iteration 2700: Loss = -12333.130606851484
Iteration 2800: Loss = -12333.072945207803
Iteration 2900: Loss = -12333.026497391633
Iteration 3000: Loss = -12332.986489684316
Iteration 3100: Loss = -12332.955651413386
Iteration 3200: Loss = -12332.934176961215
Iteration 3300: Loss = -12332.917566403003
Iteration 3400: Loss = -12332.904005705634
Iteration 3500: Loss = -12332.892633063453
Iteration 3600: Loss = -12332.890893377518
Iteration 3700: Loss = -12332.874759602008
Iteration 3800: Loss = -12332.867586195178
Iteration 3900: Loss = -12332.86145350052
Iteration 4000: Loss = -12332.856017981847
Iteration 4100: Loss = -12332.851116838654
Iteration 4200: Loss = -12332.846841267756
Iteration 4300: Loss = -12332.863777600685
1
Iteration 4400: Loss = -12332.839668415287
Iteration 4500: Loss = -12332.836620763814
Iteration 4600: Loss = -12332.83391349961
Iteration 4700: Loss = -12332.832170443009
Iteration 4800: Loss = -12332.829185590574
Iteration 4900: Loss = -12332.827125604825
Iteration 5000: Loss = -12332.829543544434
1
Iteration 5100: Loss = -12332.823571170304
Iteration 5200: Loss = -12332.822041827307
Iteration 5300: Loss = -12332.820813987208
Iteration 5400: Loss = -12332.81936452666
Iteration 5500: Loss = -12332.818041036295
Iteration 5600: Loss = -12332.816887678147
Iteration 5700: Loss = -12332.816012667172
Iteration 5800: Loss = -12332.81484998914
Iteration 5900: Loss = -12332.813959322963
Iteration 6000: Loss = -12332.813105458405
Iteration 6100: Loss = -12332.818862668944
1
Iteration 6200: Loss = -12332.811546344534
Iteration 6300: Loss = -12332.81084317522
Iteration 6400: Loss = -12332.811940728414
1
Iteration 6500: Loss = -12332.809596925716
Iteration 6600: Loss = -12332.808990995316
Iteration 6700: Loss = -12332.808422709806
Iteration 6800: Loss = -12332.811260581431
1
Iteration 6900: Loss = -12332.807400900232
Iteration 7000: Loss = -12332.806897716604
Iteration 7100: Loss = -12332.806437156563
Iteration 7200: Loss = -12332.806071643623
Iteration 7300: Loss = -12332.805585380464
Iteration 7400: Loss = -12332.80515153919
Iteration 7500: Loss = -12332.881131985963
1
Iteration 7600: Loss = -12332.80435241596
Iteration 7700: Loss = -12332.803986331694
Iteration 7800: Loss = -12332.803634562802
Iteration 7900: Loss = -12332.803596517222
Iteration 8000: Loss = -12332.802837825298
Iteration 8100: Loss = -12332.802500360549
Iteration 8200: Loss = -12332.80209849378
Iteration 8300: Loss = -12332.801989851743
Iteration 8400: Loss = -12332.801414612522
Iteration 8500: Loss = -12332.80099000905
Iteration 8600: Loss = -12333.158344745381
1
Iteration 8700: Loss = -12332.800275588825
Iteration 8800: Loss = -12332.799800443107
Iteration 8900: Loss = -12332.799399054487
Iteration 9000: Loss = -12332.79919562659
Iteration 9100: Loss = -12332.798499928165
Iteration 9200: Loss = -12332.797996783842
Iteration 9300: Loss = -12332.797997701435
1
Iteration 9400: Loss = -12332.796917209802
Iteration 9500: Loss = -12332.893284871574
1
Iteration 9600: Loss = -12332.795637687606
Iteration 9700: Loss = -12332.802162873764
1
Iteration 9800: Loss = -12332.7941614166
Iteration 9900: Loss = -12332.793283156792
Iteration 10000: Loss = -12332.798092668461
1
Iteration 10100: Loss = -12332.791208328148
Iteration 10200: Loss = -12332.789973995765
Iteration 10300: Loss = -12332.79470043608
1
Iteration 10400: Loss = -12332.787045433228
Iteration 10500: Loss = -12333.011556019572
1
Iteration 10600: Loss = -12332.783347730481
Iteration 10700: Loss = -12332.781191619117
Iteration 10800: Loss = -12332.790228371932
1
Iteration 10900: Loss = -12332.776213452595
Iteration 11000: Loss = -12332.773412518329
Iteration 11100: Loss = -12332.770779799077
Iteration 11200: Loss = -12332.767499733336
Iteration 11300: Loss = -12332.764491883274
Iteration 11400: Loss = -12332.763537692592
Iteration 11500: Loss = -12332.758669844532
Iteration 11600: Loss = -12332.755932964314
Iteration 11700: Loss = -12332.753747729119
Iteration 11800: Loss = -12332.751213393456
Iteration 11900: Loss = -12332.749144045833
Iteration 12000: Loss = -12332.74770235946
Iteration 12100: Loss = -12332.745889785803
Iteration 12200: Loss = -12332.744542080822
Iteration 12300: Loss = -12332.747674036975
1
Iteration 12400: Loss = -12332.742461754196
Iteration 12500: Loss = -12332.741628733058
Iteration 12600: Loss = -12333.075190937516
1
Iteration 12700: Loss = -12332.740335055347
Iteration 12800: Loss = -12332.739742078189
Iteration 12900: Loss = -12332.739158527811
Iteration 13000: Loss = -12332.739781928865
1
Iteration 13100: Loss = -12332.738168196556
Iteration 13200: Loss = -12332.737894916807
Iteration 13300: Loss = -12332.754951879155
1
Iteration 13400: Loss = -12332.737407382852
Iteration 13500: Loss = -12332.737276324473
Iteration 13600: Loss = -12332.76876684842
1
Iteration 13700: Loss = -12332.736988988965
Iteration 13800: Loss = -12332.736888112779
Iteration 13900: Loss = -12332.817682601584
1
Iteration 14000: Loss = -12332.736721876148
Iteration 14100: Loss = -12332.736617592129
Iteration 14200: Loss = -12332.736630039086
1
Iteration 14300: Loss = -12332.736628607128
2
Iteration 14400: Loss = -12332.736502857895
Iteration 14500: Loss = -12332.756146997865
1
Iteration 14600: Loss = -12332.747292338749
2
Iteration 14700: Loss = -12332.736491551788
Iteration 14800: Loss = -12332.736468754094
Iteration 14900: Loss = -12332.751496860987
1
Iteration 15000: Loss = -12332.736332762994
Iteration 15100: Loss = -12332.777458585728
1
Iteration 15200: Loss = -12332.73631215957
Iteration 15300: Loss = -12332.739033443599
1
Iteration 15400: Loss = -12332.736290401828
Iteration 15500: Loss = -12332.736834725572
1
Iteration 15600: Loss = -12332.736263401563
Iteration 15700: Loss = -12332.737424293116
1
Iteration 15800: Loss = -12332.736250525115
Iteration 15900: Loss = -12332.791111858345
1
Iteration 16000: Loss = -12332.736289209946
2
Iteration 16100: Loss = -12332.738690009988
3
Iteration 16200: Loss = -12332.736271696625
4
Iteration 16300: Loss = -12332.737704367435
5
Iteration 16400: Loss = -12332.736258036428
6
Iteration 16500: Loss = -12332.736244353533
Iteration 16600: Loss = -12332.73701550848
1
Iteration 16700: Loss = -12332.736207779926
Iteration 16800: Loss = -12332.736228574433
1
Iteration 16900: Loss = -12332.736383780735
2
Iteration 17000: Loss = -12332.736215404164
3
Iteration 17100: Loss = -12332.736232006138
4
Iteration 17200: Loss = -12332.74785903323
5
Iteration 17300: Loss = -12332.736674133115
6
Iteration 17400: Loss = -12332.736195828938
Iteration 17500: Loss = -12332.83433160367
1
Iteration 17600: Loss = -12332.73624725475
2
Iteration 17700: Loss = -12332.736247054752
3
Iteration 17800: Loss = -12332.736759129375
4
Iteration 17900: Loss = -12332.73620054563
5
Iteration 18000: Loss = -12332.756470091457
6
Iteration 18100: Loss = -12332.736208460945
7
Iteration 18200: Loss = -12332.737040622898
8
Iteration 18300: Loss = -12332.736471810034
9
Iteration 18400: Loss = -12332.753104606763
10
Stopping early at iteration 18400 due to no improvement.
tensor([[  5.6005, -10.2157],
        [  1.9870,  -6.6022],
        [  5.2931,  -9.9083],
        [  3.9083,  -8.5235],
        [ -0.1645,  -4.4507],
        [  1.3495,  -5.9648],
        [  5.5180, -10.1332],
        [  3.4326,  -8.0478],
        [  3.8980,  -8.5132],
        [  5.6680, -10.2833],
        [  2.5405,  -7.1557],
        [  4.8087,  -9.4239],
        [  5.2929,  -9.9081],
        [  4.9502,  -9.5654],
        [  4.3162,  -8.9314],
        [  5.5758, -10.1911],
        [  5.9534, -10.5686],
        [  4.9913,  -9.6065],
        [  5.6018, -10.2170],
        [  2.1027,  -6.7179],
        [  5.1059,  -9.7211],
        [  4.5418,  -9.1570],
        [  2.5224,  -7.1376],
        [  6.1287, -10.7439],
        [  3.7954,  -8.4106],
        [  5.4360, -10.0512],
        [  5.1182,  -9.7334],
        [  1.0581,  -5.6733],
        [  6.0717, -10.6869],
        [  1.5775,  -6.1927],
        [  3.2258,  -7.8410],
        [  6.6014, -11.2166],
        [  6.1780, -10.7932],
        [  0.7780,  -5.3932],
        [  1.8827,  -6.4979],
        [  5.0215,  -9.6367],
        [  3.0688,  -7.6841],
        [  5.3911, -10.0064],
        [  1.3512,  -5.9664],
        [  5.4921, -10.1073],
        [  5.5470, -10.1622],
        [  4.9973,  -9.6125],
        [  4.9440,  -9.5592],
        [  4.3973,  -9.0125],
        [  5.2898,  -9.9050],
        [  5.4506, -10.0659],
        [  3.8890,  -8.5042],
        [  1.8577,  -6.4729],
        [  5.1139,  -9.7292],
        [ -3.9396,  -0.6756],
        [  2.3042,  -6.9194],
        [  2.3488,  -6.9640],
        [  3.3339,  -7.9491],
        [  3.3074,  -7.9226],
        [  5.6436, -10.2588],
        [  2.4604,  -7.0757],
        [  5.2373,  -9.8525],
        [  5.5630, -10.1782],
        [  5.0111,  -9.6263],
        [  4.9858,  -9.6010],
        [  2.9964,  -7.6116],
        [  5.5184, -10.1336],
        [  0.6813,  -5.2966],
        [  4.4642,  -9.0794],
        [  6.5714, -11.1866],
        [  5.6044, -10.2196],
        [  5.4137, -10.0289],
        [  6.6616, -11.2768],
        [  5.0301,  -9.6453],
        [  4.5040,  -9.1192],
        [  5.4846, -10.0998],
        [  4.4445,  -9.0597],
        [  4.6025,  -9.2178],
        [  1.0416,  -5.6568],
        [  3.3620,  -7.9772],
        [  5.7923, -10.4075],
        [  2.8321,  -7.4473],
        [  6.7586, -11.3738],
        [  5.3078,  -9.9231],
        [  4.2378,  -8.8530],
        [  1.0025,  -5.6177],
        [  0.2087,  -4.8239],
        [  6.3220, -10.9372],
        [  5.0619,  -9.6771],
        [  5.3440,  -9.9592],
        [  4.4265,  -9.0417],
        [ -1.6250,  -2.9902],
        [  5.1821,  -9.7973],
        [  5.5620, -10.1772],
        [  5.9176, -10.5328],
        [  3.2810,  -7.8962],
        [  2.9713,  -7.5865],
        [  5.1982,  -9.8134],
        [  5.5715, -10.1867],
        [  4.0801,  -8.6953],
        [  4.2683,  -8.8835],
        [  5.6582, -10.2734],
        [  1.6670,  -6.2822],
        [  5.2188,  -9.8341],
        [  5.1230,  -9.7382]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 3.9675e-09],
        [8.8748e-06, 9.9999e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9881, 0.0119], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1985, 0.1724],
         [0.9742, 0.0054]],

        [[0.1804, 0.2732],
         [0.1707, 0.4067]],

        [[0.6718, 0.1844],
         [0.7691, 0.5421]],

        [[0.7649, 0.1434],
         [0.2177, 0.0038]],

        [[0.3835, 0.1012],
         [0.3352, 0.9916]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: -0.005513710431743218
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.007614171548597778
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
Global Adjusted Rand Index: -0.0005767300838704906
Average Adjusted Rand Index: 3.936258277526016e-06
Iteration 0: Loss = -25051.69700482489
Iteration 10: Loss = -12335.197568971424
Iteration 20: Loss = -12335.197569465356
1
Iteration 30: Loss = -12335.197570286466
2
Iteration 40: Loss = -12335.197571630863
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[1.0000e+00, 1.2638e-10],
        [1.0000e+00, 6.7272e-13]], dtype=torch.float64)
alpha: tensor([1.0000e+00, 1.2494e-10])
beta: tensor([[[0.1966, 0.1911],
         [0.7908, 0.2790]],

        [[0.5156, 0.2445],
         [0.9164, 0.3849]],

        [[0.0606, 0.2629],
         [0.1197, 0.4435]],

        [[0.3947, 0.2686],
         [0.9535, 0.5780]],

        [[0.7754, 0.2415],
         [0.8246, 0.6982]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25051.741375825484
Iteration 100: Loss = -12378.928415137623
Iteration 200: Loss = -12356.738155278983
Iteration 300: Loss = -12339.517274213107
Iteration 400: Loss = -12336.800780854448
Iteration 500: Loss = -12336.122950945413
Iteration 600: Loss = -12335.714659612662
Iteration 700: Loss = -12335.361144027296
Iteration 800: Loss = -12335.066135230212
Iteration 900: Loss = -12334.935952439559
Iteration 1000: Loss = -12334.84886479561
Iteration 1100: Loss = -12334.77519640459
Iteration 1200: Loss = -12334.716118832934
Iteration 1300: Loss = -12334.65442781346
Iteration 1400: Loss = -12334.596036785793
Iteration 1500: Loss = -12334.543478802816
Iteration 1600: Loss = -12334.494825350892
Iteration 1700: Loss = -12334.448196805582
Iteration 1800: Loss = -12334.404221485598
Iteration 1900: Loss = -12334.363532332409
Iteration 2000: Loss = -12334.326486043316
Iteration 2100: Loss = -12334.293168203714
Iteration 2200: Loss = -12334.263369039143
Iteration 2300: Loss = -12334.234132891046
Iteration 2400: Loss = -12334.20551004386
Iteration 2500: Loss = -12334.360810788306
1
Iteration 2600: Loss = -12334.152029585166
Iteration 2700: Loss = -12334.127414224662
Iteration 2800: Loss = -12334.102863710003
Iteration 2900: Loss = -12334.078564379854
Iteration 3000: Loss = -12334.053533760023
Iteration 3100: Loss = -12334.02929585781
Iteration 3200: Loss = -12334.009129406606
Iteration 3300: Loss = -12333.982989231707
Iteration 3400: Loss = -12333.961172141662
Iteration 3500: Loss = -12334.045092337214
1
Iteration 3600: Loss = -12333.923540183183
Iteration 3700: Loss = -12333.906580563862
Iteration 3800: Loss = -12333.890262175113
Iteration 3900: Loss = -12333.873572667106
Iteration 4000: Loss = -12333.85612929063
Iteration 4100: Loss = -12333.83868861466
Iteration 4200: Loss = -12333.82864472813
Iteration 4300: Loss = -12333.80551290038
Iteration 4400: Loss = -12333.786451871194
Iteration 4500: Loss = -12333.766867554887
Iteration 4600: Loss = -12333.748601149764
Iteration 4700: Loss = -12333.731310491463
Iteration 4800: Loss = -12333.715443627643
Iteration 4900: Loss = -12333.702757108978
Iteration 5000: Loss = -12333.721333840633
1
Iteration 5100: Loss = -12333.69111606126
Iteration 5200: Loss = -12333.68862912479
Iteration 5300: Loss = -12333.70560546824
1
Iteration 5400: Loss = -12333.682928029482
Iteration 5500: Loss = -12333.678652167091
Iteration 5600: Loss = -12333.724679966512
1
Iteration 5700: Loss = -12333.661005171793
Iteration 5800: Loss = -12333.642578795756
Iteration 5900: Loss = -12333.616928171998
Iteration 6000: Loss = -12333.624181285933
1
Iteration 6100: Loss = -12333.562531600835
Iteration 6200: Loss = -12333.508258652411
Iteration 6300: Loss = -12333.451872071502
Iteration 6400: Loss = -12333.381480820748
Iteration 6500: Loss = -12333.296101285408
Iteration 6600: Loss = -12333.245838378392
Iteration 6700: Loss = -12333.22568417801
Iteration 6800: Loss = -12333.21702538203
Iteration 6900: Loss = -12333.212584760216
Iteration 7000: Loss = -12333.21277780536
1
Iteration 7100: Loss = -12333.213408217647
2
Iteration 7200: Loss = -12333.204865288219
Iteration 7300: Loss = -12333.202482258768
Iteration 7400: Loss = -12333.199926645599
Iteration 7500: Loss = -12333.174383674517
Iteration 7600: Loss = -12333.097969577972
Iteration 7700: Loss = -12333.161998946018
1
Iteration 7800: Loss = -12327.744490392295
Iteration 7900: Loss = -12327.643322990818
Iteration 8000: Loss = -12327.612690753418
Iteration 8100: Loss = -12327.594144903147
Iteration 8200: Loss = -12327.578197985014
Iteration 8300: Loss = -12327.565093060333
Iteration 8400: Loss = -12327.556861559722
Iteration 8500: Loss = -12327.549418227885
Iteration 8600: Loss = -12327.541401942695
Iteration 8700: Loss = -12327.541762704805
1
Iteration 8800: Loss = -12327.549331988153
2
Iteration 8900: Loss = -12327.534722523682
Iteration 9000: Loss = -12327.530631829188
Iteration 9100: Loss = -12327.530712327052
1
Iteration 9200: Loss = -12327.52821643942
Iteration 9300: Loss = -12327.710420935367
1
Iteration 9400: Loss = -12327.531469366402
2
Iteration 9500: Loss = -12327.538041811895
3
Iteration 9600: Loss = -12327.530838534374
4
Iteration 9700: Loss = -12327.5485044664
5
Iteration 9800: Loss = -12327.528198646076
Iteration 9900: Loss = -12327.536082160725
1
Iteration 10000: Loss = -12327.542586449941
2
Iteration 10100: Loss = -12327.575226808096
3
Iteration 10200: Loss = -12327.52388067802
Iteration 10300: Loss = -12327.524914744637
1
Iteration 10400: Loss = -12327.526856519975
2
Iteration 10500: Loss = -12327.525475341696
3
Iteration 10600: Loss = -12327.52312504268
Iteration 10700: Loss = -12327.522724781664
Iteration 10800: Loss = -12327.525923436624
1
Iteration 10900: Loss = -12327.522705836865
Iteration 11000: Loss = -12327.522263524603
Iteration 11100: Loss = -12327.526737275179
1
Iteration 11200: Loss = -12327.522041376365
Iteration 11300: Loss = -12327.522831568986
1
Iteration 11400: Loss = -12327.521847927756
Iteration 11500: Loss = -12327.521761106405
Iteration 11600: Loss = -12327.5251272823
1
Iteration 11700: Loss = -12327.52157704415
Iteration 11800: Loss = -12327.521685788599
1
Iteration 11900: Loss = -12327.522230743616
2
Iteration 12000: Loss = -12327.531980782524
3
Iteration 12100: Loss = -12327.562120842318
4
Iteration 12200: Loss = -12327.530345015159
5
Iteration 12300: Loss = -12327.524563565155
6
Iteration 12400: Loss = -12327.535051428977
7
Iteration 12500: Loss = -12327.527438617737
8
Iteration 12600: Loss = -12327.521167177216
Iteration 12700: Loss = -12327.52789857037
1
Iteration 12800: Loss = -12327.546600608335
2
Iteration 12900: Loss = -12327.521704964258
3
Iteration 13000: Loss = -12327.528616847272
4
Iteration 13100: Loss = -12327.520620813126
Iteration 13200: Loss = -12327.519762376809
Iteration 13300: Loss = -12327.519686083568
Iteration 13400: Loss = -12327.52691146147
1
Iteration 13500: Loss = -12327.502152600473
Iteration 13600: Loss = -12327.502632468208
1
Iteration 13700: Loss = -12327.51015469228
2
Iteration 13800: Loss = -12327.503463163639
3
Iteration 13900: Loss = -12327.503342770442
4
Iteration 14000: Loss = -12327.502252927947
5
Iteration 14100: Loss = -12327.502222711502
6
Iteration 14200: Loss = -12327.502264813058
7
Iteration 14300: Loss = -12327.502604452968
8
Iteration 14400: Loss = -12327.515129348145
9
Iteration 14500: Loss = -12327.505736163008
10
Stopping early at iteration 14500 due to no improvement.
tensor([[-2.0981,  0.3899],
        [-5.3347,  1.5143],
        [-0.8622, -1.2490],
        [-2.5009,  0.6300],
        [-5.8134,  4.3342],
        [-7.8610,  5.9489],
        [-7.8272,  5.9747],
        [-6.3289,  4.8996],
        [-7.6953,  6.0460],
        [-2.9629,  1.3513],
        [-5.4084,  2.5065],
        [-0.0275, -1.7195],
        [-5.6948,  2.0544],
        [-5.8599,  3.9198],
        [-4.9124,  0.6076],
        [-3.0329,  1.6337],
        [-8.4992,  5.6996],
        [-2.7740,  1.2865],
        [-4.4510,  2.9426],
        [-4.9028,  2.8621],
        [-2.5829,  0.9912],
        [-3.0763,  1.4159],
        [-3.9050,  2.4829],
        [-3.7748,  2.2534],
        [-3.6252,  1.9670],
        [-4.3027,  2.4891],
        [-7.8886,  5.9757],
        [-4.0242,  2.0538],
        [-5.5517,  3.6458],
        [-2.5565,  0.3167],
        [-4.4036,  1.4143],
        [-1.8058,  0.4034],
        [-5.9089,  3.9866],
        [-3.8461,  2.3318],
        [-7.4308,  6.0028],
        [-2.9182,  0.7607],
        [-0.1679, -1.3920],
        [-0.2146, -1.4747],
        [-2.0080, -0.8231],
        [-4.0793,  1.6712],
        [-1.5475,  0.1195],
        [-1.8039,  0.1350],
        [-3.9130,  1.9406],
        [-2.6800,  1.0707],
        [-2.7766, -0.3482],
        [-2.5204,  1.1030],
        [-9.1821,  4.5669],
        [-6.4839,  1.8686],
        [-0.2248, -1.6115],
        [-6.4761,  3.8593],
        [-3.7885, -0.8267],
        [-2.6871,  0.1005],
        [-3.6708,  1.1567],
        [-6.6694,  5.2341],
        [-3.8204,  2.3485],
        [-4.6569,  3.1250],
        [-4.8237,  2.5679],
        [-4.2206,  1.4218],
        [-4.2325,  2.1358],
        [-7.9577,  6.1554],
        [-8.6835,  5.1066],
        [-4.7548,  2.6358],
        [-7.4910,  5.9827],
        [-0.1661, -1.2647],
        [-5.3154,  2.5996],
        [-0.4907, -1.5794],
        [-3.0198,  1.6108],
        [-3.6631,  2.0996],
        [-2.7905,  1.4023],
        [-3.8901,  2.4587],
        [-2.7741,  0.7718],
        [-2.6317,  1.1808],
        [-3.3172,  1.7666],
        [-4.1345,  2.3354],
        [-8.4438,  5.1956],
        [-6.3821,  4.0509],
        [-6.1578,  3.9911],
        [-4.6234,  2.9770],
        [-3.2485,  1.8390],
        [-3.2976,  1.8161],
        [-1.7168,  0.2688],
        [-5.1097,  3.3078],
        [-2.5884,  0.6711],
        [-4.8990,  3.0457],
        [-3.8280,  1.8530],
        [-4.3465,  0.8193],
        [-2.8412,  0.0731],
        [-5.2558,  3.8381],
        [-2.4924,  0.6669],
        [-4.1942,  2.2806],
        [-5.3822,  3.9525],
        [-0.2065, -1.2658],
        [-7.8315,  5.9054],
        [-3.5235,  1.0973],
        [-2.9945,  1.0003],
        [-3.2517,  1.4261],
        [-7.5810,  6.1379],
        [-7.2688,  5.8319],
        [-0.8617, -0.6890],
        [-1.8443,  0.4389]], dtype=torch.float64, requires_grad=True)
pi: tensor([[7.5146e-06, 9.9999e-01],
        [9.8090e-01, 1.9099e-02]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0818, 0.9182], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2077, 0.1944],
         [0.7908, 0.2018]],

        [[0.5156, 0.1307],
         [0.9164, 0.3849]],

        [[0.0606, 0.1552],
         [0.1197, 0.4435]],

        [[0.3947, 0.2330],
         [0.9535, 0.5780]],

        [[0.7754, 0.1541],
         [0.8246, 0.6982]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0023115331287292215
time is 1
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 67
Adjusted Rand Index: 0.0971476681975918
time is 2
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.012100586122140291
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: -0.010655595301203209
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.016398586040285303
Global Adjusted Rand Index: -0.0013931300752109495
Average Adjusted Rand Index: 0.023460555637508686
Iteration 0: Loss = -22841.917607921874
Iteration 10: Loss = -12335.197623719321
Iteration 20: Loss = -12335.197640350216
1
Iteration 30: Loss = -12335.197664197258
2
Iteration 40: Loss = -12335.197709214719
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[3.2582e-10, 1.0000e+00],
        [9.9631e-09, 1.0000e+00]], dtype=torch.float64)
alpha: tensor([9.8494e-09, 1.0000e+00])
beta: tensor([[[0.2784, 0.1911],
         [0.5574, 0.1966]],

        [[0.7994, 0.2445],
         [0.1067, 0.2304]],

        [[0.4077, 0.2629],
         [0.1886, 0.5541]],

        [[0.3677, 0.2686],
         [0.1626, 0.7913]],

        [[0.1434, 0.2382],
         [0.4212, 0.1611]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22841.479045656237
Iteration 100: Loss = -12337.442030651404
Iteration 200: Loss = -12335.836907028637
Iteration 300: Loss = -12335.361800425353
Iteration 400: Loss = -12335.124151377648
Iteration 500: Loss = -12334.976661782963
Iteration 600: Loss = -12334.872003794679
Iteration 700: Loss = -12334.79112243303
Iteration 800: Loss = -12334.722994874654
Iteration 900: Loss = -12334.662528237164
Iteration 1000: Loss = -12334.60907072464
Iteration 1100: Loss = -12334.559814211394
Iteration 1200: Loss = -12334.512431510353
Iteration 1300: Loss = -12334.466020892683
Iteration 1400: Loss = -12334.420062820383
Iteration 1500: Loss = -12334.374482034948
Iteration 1600: Loss = -12334.328963724553
Iteration 1700: Loss = -12334.28337114583
Iteration 1800: Loss = -12334.237786437476
Iteration 1900: Loss = -12334.192514478369
Iteration 2000: Loss = -12334.148171317918
Iteration 2100: Loss = -12334.105316699377
Iteration 2200: Loss = -12334.064674425621
Iteration 2300: Loss = -12334.026921422806
Iteration 2400: Loss = -12333.992399204935
Iteration 2500: Loss = -12333.961238362814
Iteration 2600: Loss = -12333.933122801523
Iteration 2700: Loss = -12333.907651526375
Iteration 2800: Loss = -12333.883980458471
Iteration 2900: Loss = -12333.86155632538
Iteration 3000: Loss = -12333.839739322926
Iteration 3100: Loss = -12333.818389960868
Iteration 3200: Loss = -12333.797220108358
Iteration 3300: Loss = -12333.77678929744
Iteration 3400: Loss = -12333.772077331489
Iteration 3500: Loss = -12333.741698082853
Iteration 3600: Loss = -12333.726082752426
Iteration 3700: Loss = -12333.714531355647
Iteration 3800: Loss = -12333.707105699843
Iteration 3900: Loss = -12333.719839290748
1
Iteration 4000: Loss = -12333.701902984805
Iteration 4100: Loss = -12333.70370258841
1
Iteration 4200: Loss = -12333.700322916711
Iteration 4300: Loss = -12333.69979990453
Iteration 4400: Loss = -12333.703307313222
1
Iteration 4500: Loss = -12333.699211753225
Iteration 4600: Loss = -12333.700857595175
1
Iteration 4700: Loss = -12333.701982612522
2
Iteration 4800: Loss = -12333.69870277695
Iteration 4900: Loss = -12333.698480953877
Iteration 5000: Loss = -12333.706019169398
1
Iteration 5100: Loss = -12333.698148941257
Iteration 5200: Loss = -12333.699412452328
1
Iteration 5300: Loss = -12333.697793642006
Iteration 5400: Loss = -12333.697723496512
Iteration 5500: Loss = -12333.697333319904
Iteration 5600: Loss = -12333.700049891575
1
Iteration 5700: Loss = -12333.69673270064
Iteration 5800: Loss = -12333.69633261843
Iteration 5900: Loss = -12333.695963394995
Iteration 6000: Loss = -12333.695383737579
Iteration 6100: Loss = -12333.697254930965
1
Iteration 6200: Loss = -12333.693581525564
Iteration 6300: Loss = -12333.692214145227
Iteration 6400: Loss = -12333.690156993434
Iteration 6500: Loss = -12333.686623907397
Iteration 6600: Loss = -12333.69378343763
1
Iteration 6700: Loss = -12333.653691409552
Iteration 6800: Loss = -12333.516585097037
Iteration 6900: Loss = -12333.433804785309
Iteration 7000: Loss = -12332.9115348099
Iteration 7100: Loss = -12193.003575040968
Iteration 7200: Loss = -11859.08631287098
Iteration 7300: Loss = -11858.981499730404
Iteration 7400: Loss = -11858.98531261683
1
Iteration 7500: Loss = -11858.967250656016
Iteration 7600: Loss = -11858.959203558077
Iteration 7700: Loss = -11858.949948294183
Iteration 7800: Loss = -11858.946137813653
Iteration 7900: Loss = -11858.943854734793
Iteration 8000: Loss = -11858.943105108936
Iteration 8100: Loss = -11858.949440154343
1
Iteration 8200: Loss = -11858.947361526967
2
Iteration 8300: Loss = -11858.939123417587
Iteration 8400: Loss = -11858.947425284648
1
Iteration 8500: Loss = -11858.940140441566
2
Iteration 8600: Loss = -11858.938362154666
Iteration 8700: Loss = -11858.9526212151
1
Iteration 8800: Loss = -11858.926064609701
Iteration 8900: Loss = -11858.939737153696
1
Iteration 9000: Loss = -11858.944380931787
2
Iteration 9100: Loss = -11858.978503854016
3
Iteration 9200: Loss = -11858.920342843861
Iteration 9300: Loss = -11858.93472414338
1
Iteration 9400: Loss = -11858.92846798059
2
Iteration 9500: Loss = -11858.917436846516
Iteration 9600: Loss = -11858.922148993424
1
Iteration 9700: Loss = -11858.923874702368
2
Iteration 9800: Loss = -11858.928558305799
3
Iteration 9900: Loss = -11858.919598593824
4
Iteration 10000: Loss = -11858.918383476454
5
Iteration 10100: Loss = -11858.91707224272
Iteration 10200: Loss = -11858.917522345786
1
Iteration 10300: Loss = -11859.057990653804
2
Iteration 10400: Loss = -11858.916683725405
Iteration 10500: Loss = -11858.917058183004
1
Iteration 10600: Loss = -11858.91772788092
2
Iteration 10700: Loss = -11858.934059845538
3
Iteration 10800: Loss = -11858.932181747763
4
Iteration 10900: Loss = -11858.937391634663
5
Iteration 11000: Loss = -11858.921403474493
6
Iteration 11100: Loss = -11858.91658339971
Iteration 11200: Loss = -11858.916545638755
Iteration 11300: Loss = -11858.917242948948
1
Iteration 11400: Loss = -11858.917718793222
2
Iteration 11500: Loss = -11858.916225155852
Iteration 11600: Loss = -11858.916158543781
Iteration 11700: Loss = -11858.92225303725
1
Iteration 11800: Loss = -11858.916098457072
Iteration 11900: Loss = -11858.919723083087
1
Iteration 12000: Loss = -11858.930328459339
2
Iteration 12100: Loss = -11858.92896153144
3
Iteration 12200: Loss = -11858.943116310413
4
Iteration 12300: Loss = -11858.954073643561
5
Iteration 12400: Loss = -11858.91759260515
6
Iteration 12500: Loss = -11858.91426043706
Iteration 12600: Loss = -11858.919418387079
1
Iteration 12700: Loss = -11858.93123009581
2
Iteration 12800: Loss = -11858.914585319906
3
Iteration 12900: Loss = -11858.912622382559
Iteration 13000: Loss = -11858.91424334248
1
Iteration 13100: Loss = -11858.943877070265
2
Iteration 13200: Loss = -11858.988194663834
3
Iteration 13300: Loss = -11858.996943436234
4
Iteration 13400: Loss = -11858.924786597585
5
Iteration 13500: Loss = -11851.750206255698
Iteration 13600: Loss = -11851.74350761103
Iteration 13700: Loss = -11839.554668687157
Iteration 13800: Loss = -11839.531610263126
Iteration 13900: Loss = -11839.53088508854
Iteration 14000: Loss = -11839.527235102745
Iteration 14100: Loss = -11839.52763376264
1
Iteration 14200: Loss = -11839.530825652155
2
Iteration 14300: Loss = -11839.545854354168
3
Iteration 14400: Loss = -11839.52819566946
4
Iteration 14500: Loss = -11839.527945684236
5
Iteration 14600: Loss = -11839.501135832574
Iteration 14700: Loss = -11839.504988100802
1
Iteration 14800: Loss = -11836.48604719441
Iteration 14900: Loss = -11836.477775924044
Iteration 15000: Loss = -11836.513749742731
1
Iteration 15100: Loss = -11836.490225135274
2
Iteration 15200: Loss = -11836.477134772194
Iteration 15300: Loss = -11836.485885619599
1
Iteration 15400: Loss = -11836.483314898589
2
Iteration 15500: Loss = -11836.477568837478
3
Iteration 15600: Loss = -11836.478646399422
4
Iteration 15700: Loss = -11836.48435653537
5
Iteration 15800: Loss = -11836.498963607883
6
Iteration 15900: Loss = -11836.476982935432
Iteration 16000: Loss = -11836.491286113995
1
Iteration 16100: Loss = -11836.482940768774
2
Iteration 16200: Loss = -11836.476509473694
Iteration 16300: Loss = -11836.477133222006
1
Iteration 16400: Loss = -11836.480797106911
2
Iteration 16500: Loss = -11836.49929320523
3
Iteration 16600: Loss = -11836.487769794945
4
Iteration 16700: Loss = -11836.477381539598
5
Iteration 16800: Loss = -11836.478151317417
6
Iteration 16900: Loss = -11836.511497481031
7
Iteration 17000: Loss = -11836.47995284241
8
Iteration 17100: Loss = -11836.477105753887
9
Iteration 17200: Loss = -11836.476577263224
10
Stopping early at iteration 17200 due to no improvement.
tensor([[  6.8414,  -9.3999],
        [-10.1050,   7.6842],
        [  5.1461,  -6.8321],
        [  4.6634,  -6.0609],
        [  1.5589,  -2.9591],
        [ -7.0241,   4.3679],
        [  7.0322,  -9.2707],
        [ -7.0747,   5.6853],
        [ -9.2255,   7.8196],
        [  4.4115,  -6.2351],
        [ -8.3871,   5.8333],
        [  1.2793,  -3.8540],
        [  1.9641,  -3.9079],
        [ -7.7398,   6.2931],
        [  7.4017,  -8.9714],
        [ -9.1359,   4.9889],
        [  3.6903,  -6.2798],
        [ -6.7256,   5.3385],
        [ -9.0157,   7.5002],
        [ -8.9768,   7.1320],
        [ -5.1056,   3.4311],
        [  7.2682,  -9.6873],
        [ -3.9239,   0.4761],
        [  3.9603,  -6.0416],
        [ -7.7963,   6.1772],
        [ -8.4122,   6.8183],
        [  6.1191,  -7.8808],
        [ -8.5369,   7.1201],
        [  4.9786,  -7.8784],
        [ -9.8015,   7.9880],
        [ -9.5348,   7.8788],
        [  7.4763,  -9.1546],
        [  3.8286,  -6.1552],
        [  5.5319,  -8.5942],
        [ -2.2011,   0.7765],
        [ -9.1665,   7.3749],
        [ -9.0453,   6.6781],
        [ -9.0468,   7.6517],
        [ -9.6654,   8.2736],
        [ -7.7896,   6.3816],
        [ -9.0591,   7.3950],
        [ -3.9805,   2.5349],
        [  6.4667,  -8.2411],
        [  3.8134,  -6.8083],
        [ -7.2111,   5.4816],
        [  5.2784,  -6.6660],
        [  2.9288,  -4.9085],
        [ -4.9163,   3.4844],
        [  5.3988,  -6.8199],
        [  6.6170,  -8.0134],
        [  5.1104,  -6.7078],
        [  2.7808,  -4.4314],
        [  6.7075,  -8.0982],
        [-10.2345,   7.4894],
        [  7.0772,  -9.0521],
        [ -5.9069,   4.0084],
        [  2.2866,  -5.0251],
        [ -9.2081,   6.8817],
        [  6.8141,  -9.3666],
        [ -7.7787,   6.2445],
        [ -5.7301,   4.2073],
        [  7.1801, -10.8343],
        [ -7.3404,   5.4148],
        [ -5.4680,   3.9370],
        [  7.1064,  -8.5200],
        [  6.2544,  -8.4135],
        [ -8.9070,   7.3608],
        [  5.1254,  -9.7406],
        [  3.1971,  -4.6408],
        [  3.2611,  -4.7695],
        [ -9.3623,   7.7136],
        [  6.5603,  -7.9613],
        [ -8.9186,   7.2977],
        [ -9.8860,   7.8198],
        [ -2.9181,   1.4121],
        [ -9.7801,   7.2469],
        [  7.0652,  -9.1282],
        [  6.8020,  -8.1967],
        [  6.7066,  -8.0992],
        [ -8.1433,   6.4562],
        [ -9.3240,   7.8668],
        [  7.7341,  -9.9762],
        [  7.5592,  -9.5206],
        [ -8.8092,   7.3700],
        [ -6.3567,   4.9589],
        [ -4.9698,   3.5430],
        [  6.8052,  -8.4300],
        [ -9.5292,   8.1355],
        [ -9.4813,   7.1320],
        [  7.4654,  -9.0766],
        [ -9.7283,   8.3403],
        [  7.7801,  -9.3060],
        [ -6.3894,   4.9344],
        [  7.0964,  -8.5928],
        [  5.2817,  -6.7519],
        [ -9.3850,   7.8004],
        [ -9.4477,   7.4092],
        [ -9.9259,   7.3830],
        [  6.7312,  -8.3395],
        [  7.5605,  -9.1419]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7246, 0.2754],
        [0.2662, 0.7338]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4811, 0.5189], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2950, 0.0955],
         [0.5574, 0.2996]],

        [[0.7994, 0.0912],
         [0.1067, 0.2304]],

        [[0.4077, 0.0972],
         [0.1886, 0.5541]],

        [[0.3677, 0.1159],
         [0.1626, 0.7913]],

        [[0.1434, 0.0971],
         [0.4212, 0.1611]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.9919999860917265
Average Adjusted Rand Index: 0.9919992163297293
Iteration 0: Loss = -17402.75252455552
Iteration 10: Loss = -12334.10971580143
Iteration 20: Loss = -12334.029694856139
Iteration 30: Loss = -12333.998389888498
Iteration 40: Loss = -12333.984008853451
Iteration 50: Loss = -12333.97632742817
Iteration 60: Loss = -12333.971602546706
Iteration 70: Loss = -12333.968094581765
Iteration 80: Loss = -12333.965276514711
Iteration 90: Loss = -12333.962713284678
Iteration 100: Loss = -12333.96027345403
Iteration 110: Loss = -12333.957871278048
Iteration 120: Loss = -12333.955476027792
Iteration 130: Loss = -12333.953003053908
Iteration 140: Loss = -12333.95052402975
Iteration 150: Loss = -12333.948027723749
Iteration 160: Loss = -12333.945427108345
Iteration 170: Loss = -12333.942785628991
Iteration 180: Loss = -12333.940230639051
Iteration 190: Loss = -12333.937539131317
Iteration 200: Loss = -12333.934908630514
Iteration 210: Loss = -12333.932307239978
Iteration 220: Loss = -12333.929735663063
Iteration 230: Loss = -12333.927262823985
Iteration 240: Loss = -12333.924821865981
Iteration 250: Loss = -12333.9225284749
Iteration 260: Loss = -12333.920303472352
Iteration 270: Loss = -12333.918228600494
Iteration 280: Loss = -12333.916254877142
Iteration 290: Loss = -12333.914444860182
Iteration 300: Loss = -12333.912718619851
Iteration 310: Loss = -12333.911116413994
Iteration 320: Loss = -12333.90965884552
Iteration 330: Loss = -12333.908259424903
Iteration 340: Loss = -12333.906984699903
Iteration 350: Loss = -12333.905744408914
Iteration 360: Loss = -12333.904667079787
Iteration 370: Loss = -12333.903571975747
Iteration 380: Loss = -12333.90260344823
Iteration 390: Loss = -12333.901684865323
Iteration 400: Loss = -12333.900805353847
Iteration 410: Loss = -12333.899981968643
Iteration 420: Loss = -12333.899223778008
Iteration 430: Loss = -12333.898459133661
Iteration 440: Loss = -12333.897751852057
Iteration 450: Loss = -12333.89711582939
Iteration 460: Loss = -12333.89645467718
Iteration 470: Loss = -12333.895868463405
Iteration 480: Loss = -12333.895303463341
Iteration 490: Loss = -12333.894788634903
Iteration 500: Loss = -12333.89422632699
Iteration 510: Loss = -12333.893748722661
Iteration 520: Loss = -12333.89327702298
Iteration 530: Loss = -12333.892763722455
Iteration 540: Loss = -12333.892313269229
Iteration 550: Loss = -12333.891931774557
Iteration 560: Loss = -12333.891527621128
Iteration 570: Loss = -12333.891108616794
Iteration 580: Loss = -12333.890717525672
Iteration 590: Loss = -12333.890339429263
Iteration 600: Loss = -12333.889965564802
Iteration 610: Loss = -12333.889574831419
Iteration 620: Loss = -12333.889186806484
Iteration 630: Loss = -12333.888814161517
Iteration 640: Loss = -12333.888461327668
Iteration 650: Loss = -12333.888085370405
Iteration 660: Loss = -12333.887652067542
Iteration 670: Loss = -12333.887293637941
Iteration 680: Loss = -12333.886926137813
Iteration 690: Loss = -12333.886513257643
Iteration 700: Loss = -12333.886052181702
Iteration 710: Loss = -12333.885619429857
Iteration 720: Loss = -12333.885179996283
Iteration 730: Loss = -12333.884683201706
Iteration 740: Loss = -12333.88415065286
Iteration 750: Loss = -12333.883649003456
Iteration 760: Loss = -12333.883017649741
Iteration 770: Loss = -12333.882422512252
Iteration 780: Loss = -12333.881763940411
Iteration 790: Loss = -12333.880990930942
Iteration 800: Loss = -12333.880224368362
Iteration 810: Loss = -12333.879346986547
Iteration 820: Loss = -12333.878351223198
Iteration 830: Loss = -12333.877260534484
Iteration 840: Loss = -12333.875996443128
Iteration 850: Loss = -12333.874639584965
Iteration 860: Loss = -12333.872981757966
Iteration 870: Loss = -12333.871132125962
Iteration 880: Loss = -12333.86898006908
Iteration 890: Loss = -12333.866380818445
Iteration 900: Loss = -12333.86332168072
Iteration 910: Loss = -12333.859692476202
Iteration 920: Loss = -12333.855351797642
Iteration 930: Loss = -12333.850160837857
Iteration 940: Loss = -12333.8440224664
Iteration 950: Loss = -12333.836878199287
Iteration 960: Loss = -12333.828747232685
Iteration 970: Loss = -12333.819934397794
Iteration 980: Loss = -12333.810702565774
Iteration 990: Loss = -12333.801586297353
Iteration 1000: Loss = -12333.793099871089
Iteration 1010: Loss = -12333.785646833672
Iteration 1020: Loss = -12333.779478062104
Iteration 1030: Loss = -12333.774567753844
Iteration 1040: Loss = -12333.77090374436
Iteration 1050: Loss = -12333.768306179734
Iteration 1060: Loss = -12333.766526260584
Iteration 1070: Loss = -12333.765333091013
Iteration 1080: Loss = -12333.764606748646
Iteration 1090: Loss = -12333.764195580556
Iteration 1100: Loss = -12333.7639504462
Iteration 1110: Loss = -12333.763837858305
Iteration 1120: Loss = -12333.763786809095
Iteration 1130: Loss = -12333.763781814247
Iteration 1140: Loss = -12333.763805372257
1
Iteration 1150: Loss = -12333.763831510141
2
Iteration 1160: Loss = -12333.763870835299
3
Stopping early at iteration 1159 due to no improvement.
pi: tensor([[0.0951, 0.9049],
        [0.0577, 0.9423]], dtype=torch.float64)
alpha: tensor([0.0599, 0.9401])
beta: tensor([[[0.1607, 0.1853],
         [0.7101, 0.1994]],

        [[0.0790, 0.1472],
         [0.9095, 0.7595]],

        [[0.9068, 0.1768],
         [0.1493, 0.4988]],

        [[0.4532, 0.2156],
         [0.1661, 0.5127]],

        [[0.8877, 0.1539],
         [0.1611, 0.6652]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17402.446211681265
Iteration 100: Loss = -12336.59751159261
Iteration 200: Loss = -12335.20412044733
Iteration 300: Loss = -12334.882447934984
Iteration 400: Loss = -12334.695524727853
Iteration 500: Loss = -12334.558253177926
Iteration 600: Loss = -12334.413796718345
Iteration 700: Loss = -12334.243524737978
Iteration 800: Loss = -12334.084347549238
Iteration 900: Loss = -12333.975215448529
Iteration 1000: Loss = -12333.93488460024
Iteration 1100: Loss = -12333.909173706308
Iteration 1200: Loss = -12333.845765217528
Iteration 1300: Loss = -12333.826068388156
Iteration 1400: Loss = -12333.757923568335
Iteration 1500: Loss = -12333.732504510937
Iteration 1600: Loss = -12333.703540708202
Iteration 1700: Loss = -12333.667684654109
Iteration 1800: Loss = -12333.614805967783
Iteration 1900: Loss = -12333.552637805793
Iteration 2000: Loss = -12333.50205671277
Iteration 2100: Loss = -12333.464835309635
Iteration 2200: Loss = -12333.436971263263
Iteration 2300: Loss = -12333.386280903853
Iteration 2400: Loss = -12333.305256966989
Iteration 2500: Loss = -12333.260892984315
Iteration 2600: Loss = -12333.23912668189
Iteration 2700: Loss = -12333.22751794291
Iteration 2800: Loss = -12333.221001790078
Iteration 2900: Loss = -12333.216936776143
Iteration 3000: Loss = -12333.214205107299
Iteration 3100: Loss = -12333.212099451002
Iteration 3200: Loss = -12333.210430196263
Iteration 3300: Loss = -12333.208865609591
Iteration 3400: Loss = -12333.207307689494
Iteration 3500: Loss = -12333.205496277858
Iteration 3600: Loss = -12333.20306880631
Iteration 3700: Loss = -12333.199181093638
Iteration 3800: Loss = -12333.191516860292
Iteration 3900: Loss = -12333.171715960145
Iteration 4000: Loss = -12333.10008126602
Iteration 4100: Loss = -12332.775571041486
Iteration 4200: Loss = -12327.77466549663
Iteration 4300: Loss = -12327.61225591511
Iteration 4400: Loss = -12327.578630253778
Iteration 4500: Loss = -12327.56060380601
Iteration 4600: Loss = -12327.548135181243
Iteration 4700: Loss = -12327.53901038678
Iteration 4800: Loss = -12327.533666151843
Iteration 4900: Loss = -12327.527385361132
Iteration 5000: Loss = -12327.523267107967
Iteration 5100: Loss = -12327.520947929848
Iteration 5200: Loss = -12327.518130210721
Iteration 5300: Loss = -12327.516449728852
Iteration 5400: Loss = -12327.514632644066
Iteration 5500: Loss = -12327.512813397087
Iteration 5600: Loss = -12327.510781954921
Iteration 5700: Loss = -12327.517830330702
1
Iteration 5800: Loss = -12327.530160485281
2
Iteration 5900: Loss = -12327.508613885442
Iteration 6000: Loss = -12327.507944031893
Iteration 6100: Loss = -12327.567667492714
1
Iteration 6200: Loss = -12327.507040349803
Iteration 6300: Loss = -12327.526646903441
1
Iteration 6400: Loss = -12327.506315046652
Iteration 6500: Loss = -12327.524214709274
1
Iteration 6600: Loss = -12327.505646347392
Iteration 6700: Loss = -12327.539007790989
1
Iteration 6800: Loss = -12327.504911333324
Iteration 6900: Loss = -12327.523147271337
1
Iteration 7000: Loss = -12327.529868540989
2
Iteration 7100: Loss = -12327.510357531677
3
Iteration 7200: Loss = -12327.661679511932
4
Iteration 7300: Loss = -12327.50406210155
Iteration 7400: Loss = -12327.505533381276
1
Iteration 7500: Loss = -12327.504791049425
2
Iteration 7600: Loss = -12327.504330868034
3
Iteration 7700: Loss = -12327.502808302454
Iteration 7800: Loss = -12327.502658428672
Iteration 7900: Loss = -12327.502604872914
Iteration 8000: Loss = -12327.503078509066
1
Iteration 8100: Loss = -12327.50242909405
Iteration 8200: Loss = -12327.510049936867
1
Iteration 8300: Loss = -12327.502343489368
Iteration 8400: Loss = -12327.505804748575
1
Iteration 8500: Loss = -12327.504440479483
2
Iteration 8600: Loss = -12327.501965620046
Iteration 8700: Loss = -12327.502080146543
1
Iteration 8800: Loss = -12327.502812795823
2
Iteration 8900: Loss = -12327.566205726325
3
Iteration 9000: Loss = -12327.503872965335
4
Iteration 9100: Loss = -12327.502210827357
5
Iteration 9200: Loss = -12327.502144885128
6
Iteration 9300: Loss = -12327.513124917787
7
Iteration 9400: Loss = -12327.57833251583
8
Iteration 9500: Loss = -12327.501456483276
Iteration 9600: Loss = -12327.501499041802
1
Iteration 9700: Loss = -12327.54145888412
2
Iteration 9800: Loss = -12327.506862932376
3
Iteration 9900: Loss = -12327.508623901123
4
Iteration 10000: Loss = -12327.503995861112
5
Iteration 10100: Loss = -12327.50202865706
6
Iteration 10200: Loss = -12327.50145485386
Iteration 10300: Loss = -12327.503039117417
1
Iteration 10400: Loss = -12327.53789212485
2
Iteration 10500: Loss = -12327.501250835281
Iteration 10600: Loss = -12327.526089201145
1
Iteration 10700: Loss = -12327.522329975298
2
Iteration 10800: Loss = -12327.521115656411
3
Iteration 10900: Loss = -12327.508193451264
4
Iteration 11000: Loss = -12327.502152908506
5
Iteration 11100: Loss = -12327.554890042527
6
Iteration 11200: Loss = -12327.501326030391
7
Iteration 11300: Loss = -12327.50142192504
8
Iteration 11400: Loss = -12327.505346702017
9
Iteration 11500: Loss = -12327.501062637586
Iteration 11600: Loss = -12327.503252141814
1
Iteration 11700: Loss = -12327.540333153713
2
Iteration 11800: Loss = -12327.502317649434
3
Iteration 11900: Loss = -12327.553424432006
4
Iteration 12000: Loss = -12327.503267769129
5
Iteration 12100: Loss = -12327.50040330322
Iteration 12200: Loss = -12327.512194280753
1
Iteration 12300: Loss = -12327.502053558943
2
Iteration 12400: Loss = -12327.502043230807
3
Iteration 12500: Loss = -12327.502185996234
4
Iteration 12600: Loss = -12327.608558219392
5
Iteration 12700: Loss = -12327.50023122852
Iteration 12800: Loss = -12327.50885816046
1
Iteration 12900: Loss = -12327.500209127376
Iteration 13000: Loss = -12327.501272004054
1
Iteration 13100: Loss = -12327.500176982288
Iteration 13200: Loss = -12327.500964739824
1
Iteration 13300: Loss = -12327.526159962566
2
Iteration 13400: Loss = -12327.52139042445
3
Iteration 13500: Loss = -12327.51362854735
4
Iteration 13600: Loss = -12327.532544054407
5
Iteration 13700: Loss = -12327.502279899792
6
Iteration 13800: Loss = -12327.501237990442
7
Iteration 13900: Loss = -12327.500326819216
8
Iteration 14000: Loss = -12327.505438466591
9
Iteration 14100: Loss = -12327.500191051171
10
Stopping early at iteration 14100 due to no improvement.
tensor([[ 0.1642, -2.3171],
        [ 2.4186, -4.4256],
        [-0.9211, -0.5374],
        [ 0.8667, -2.2593],
        [ 6.6493, -8.2167],
        [ 7.0189, -8.4103],
        [ 6.5370, -8.7313],
        [ 4.3566, -5.7526],
        [ 6.7834, -8.3729],
        [ 1.3203, -2.9883],
        [ 3.2574, -4.6513],
        [-2.2215, -0.5282],
        [ 2.6668, -5.0801],
        [ 4.1929, -5.5839],
        [ 1.9616, -3.5519],
        [ 1.5282, -3.1347],
        [ 6.6538, -8.7125],
        [ 0.8618, -3.1965],
        [ 2.8386, -4.5537],
        [ 3.1849, -4.5775],
        [ 1.0542, -2.5147],
        [ 1.5040, -2.9821],
        [ 2.4093, -3.9715],
        [ 2.2974, -3.7291],
        [ 2.0176, -3.5724],
        [ 2.6878, -4.0990],
        [ 4.6431, -6.0452],
        [ 2.2036, -3.8679],
        [ 3.9046, -5.2926],
        [-0.8734, -3.7419],
        [ 2.1436, -3.6677],
        [ 0.1657, -2.0413],
        [ 3.6731, -6.1900],
        [ 2.3537, -3.8194],
        [ 3.6518, -5.0381],
        [ 1.0006, -2.6734],
        [-1.4718, -0.2456],
        [-1.6779, -0.4148],
        [-1.4361, -2.6179],
        [ 2.1478, -3.5975],
        [-0.1555, -1.8249],
        [-0.5740, -2.5093],
        [ 2.2255, -3.6244],
        [ 1.0944, -2.6528],
        [ 0.3952, -2.0263],
        [ 0.6236, -2.9956],
        [ 3.0644, -4.5573],
        [ 3.3796, -4.9692],
        [-2.6486, -1.2602],
        [ 6.8854, -8.5687],
        [ 0.4581, -2.4987],
        [ 0.6998, -2.0861],
        [ 1.5641, -3.2585],
        [ 6.9026, -8.4902],
        [ 1.9145, -4.2484],
        [ 2.8719, -4.9050],
        [ 2.4667, -4.9209],
        [ 1.9000, -3.7385],
        [ 2.3287, -4.0355],
        [ 4.3076, -6.1507],
        [ 2.9150, -5.6833],
        [ 2.9493, -4.4377],
        [ 3.2957, -5.3194],
        [-1.2687, -0.1680],
        [ 3.2508, -4.6604],
        [-1.5982, -0.5078],
        [ 1.0678, -3.5593],
        [ 1.8648, -3.8934],
        [ 1.3809, -2.8078],
        [ 2.4684, -3.8756],
        [ 1.0676, -2.4745],
        [ 1.0328, -2.7727],
        [ 1.8449, -3.2354],
        [ 2.4637, -4.0026],
        [ 4.3362, -5.8164],
        [ 4.2421, -6.2002],
        [ 4.0117, -6.1394],
        [ 1.4912, -6.1065],
        [ 1.5259, -3.5601],
        [ 1.8370, -3.2695],
        [ 0.1142, -1.8658],
        [ 2.8337, -5.5801],
        [-0.1846, -3.4413],
        [ 3.1669, -4.7754],
        [ 2.1378, -3.5435],
        [ 1.8001, -3.3615],
        [ 0.1093, -2.7980],
        [ 3.8452, -5.2326],
        [-0.0514, -3.2060],
        [ 2.5129, -3.9553],
        [ 3.7132, -5.6191],
        [-1.2320, -0.1701],
        [ 6.6821, -8.6336],
        [ 1.5207, -3.0923],
        [ 0.8396, -3.1495],
        [ 1.1959, -3.4767],
        [ 3.1952, -7.8104],
        [ 3.1069, -4.5686],
        [-0.7753, -0.9445],
        [ 0.4298, -1.8504]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.9117e-02, 9.8088e-01],
        [1.0000e+00, 1.8078e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9185, 0.0815], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2017, 0.1943],
         [0.7101, 0.2074]],

        [[0.0790, 0.1307],
         [0.9095, 0.7595]],

        [[0.9068, 0.1550],
         [0.1493, 0.4988]],

        [[0.4532, 0.2335],
         [0.1661, 0.5127]],

        [[0.8877, 0.1543],
         [0.1611, 0.6652]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0023115331287292215
time is 1
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 33
Adjusted Rand Index: 0.0971476681975918
time is 2
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.012100586122140291
time is 3
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: -0.010655595301203209
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.016398586040285303
Global Adjusted Rand Index: -0.0013931300752109495
Average Adjusted Rand Index: 0.023460555637508686
Iteration 0: Loss = -24930.81267213389
Iteration 10: Loss = -12334.310706043787
Iteration 20: Loss = -12334.109314140329
Iteration 30: Loss = -12334.036936125032
Iteration 40: Loss = -12334.007903767135
Iteration 50: Loss = -12333.99574779941
Iteration 60: Loss = -12333.990465796422
Iteration 70: Loss = -12333.98787316813
Iteration 80: Loss = -12333.986073928949
Iteration 90: Loss = -12333.984450402531
Iteration 100: Loss = -12333.982547819574
Iteration 110: Loss = -12333.98018121302
Iteration 120: Loss = -12333.977353095303
Iteration 130: Loss = -12333.973889937799
Iteration 140: Loss = -12333.96978383841
Iteration 150: Loss = -12333.964997449595
Iteration 160: Loss = -12333.95935810698
Iteration 170: Loss = -12333.952940922083
Iteration 180: Loss = -12333.945724286466
Iteration 190: Loss = -12333.937846798048
Iteration 200: Loss = -12333.929562154806
Iteration 210: Loss = -12333.92098935809
Iteration 220: Loss = -12333.912642406163
Iteration 230: Loss = -12333.90493218399
Iteration 240: Loss = -12333.89816989528
Iteration 250: Loss = -12333.892489163316
Iteration 260: Loss = -12333.88791045374
Iteration 270: Loss = -12333.884436098293
Iteration 280: Loss = -12333.881730815534
Iteration 290: Loss = -12333.879637796728
Iteration 300: Loss = -12333.87784178174
Iteration 310: Loss = -12333.876301850192
Iteration 320: Loss = -12333.874811535681
Iteration 330: Loss = -12333.873297885355
Iteration 340: Loss = -12333.871527238425
Iteration 350: Loss = -12333.869593567038
Iteration 360: Loss = -12333.867290684306
Iteration 370: Loss = -12333.864586560645
Iteration 380: Loss = -12333.861354694953
Iteration 390: Loss = -12333.857399111956
Iteration 400: Loss = -12333.852687032691
Iteration 410: Loss = -12333.847074622177
Iteration 420: Loss = -12333.840437836301
Iteration 430: Loss = -12333.832794798824
Iteration 440: Loss = -12333.82429531166
Iteration 450: Loss = -12333.815248607736
Iteration 460: Loss = -12333.806032812388
Iteration 470: Loss = -12333.797151772504
Iteration 480: Loss = -12333.789168348127
Iteration 490: Loss = -12333.782336942682
Iteration 500: Loss = -12333.776798947196
Iteration 510: Loss = -12333.772589970571
Iteration 520: Loss = -12333.769455299544
Iteration 530: Loss = -12333.76731780668
Iteration 540: Loss = -12333.765829445274
Iteration 550: Loss = -12333.764933748193
Iteration 560: Loss = -12333.764364121573
Iteration 570: Loss = -12333.764057083365
Iteration 580: Loss = -12333.76387732796
Iteration 590: Loss = -12333.763801857092
Iteration 600: Loss = -12333.763773838738
Iteration 610: Loss = -12333.763821059492
1
Iteration 620: Loss = -12333.763786950029
2
Iteration 630: Loss = -12333.763878518126
3
Stopping early at iteration 629 due to no improvement.
pi: tensor([[0.9422, 0.0578],
        [0.9048, 0.0952]], dtype=torch.float64)
alpha: tensor([0.9400, 0.0600])
beta: tensor([[[0.1994, 0.1853],
         [0.8779, 0.1607]],

        [[0.8805, 0.1472],
         [0.6798, 0.2273]],

        [[0.5697, 0.1768],
         [0.2530, 0.3338]],

        [[0.7567, 0.2156],
         [0.1869, 0.2275]],

        [[0.9997, 0.1539],
         [0.7446, 0.3651]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24897.519057118694
Iteration 100: Loss = -12410.099689741644
Iteration 200: Loss = -12384.565669396543
Iteration 300: Loss = -12366.127137874528
Iteration 400: Loss = -12350.57833073546
Iteration 500: Loss = -12344.826820573777
Iteration 600: Loss = -12337.509726685186
Iteration 700: Loss = -12336.593600567954
Iteration 800: Loss = -12336.148867999334
Iteration 900: Loss = -12335.87473244679
Iteration 1000: Loss = -12335.68759928131
Iteration 1100: Loss = -12335.551951715019
Iteration 1200: Loss = -12335.449352984522
Iteration 1300: Loss = -12335.369400489286
Iteration 1400: Loss = -12335.305508925312
Iteration 1500: Loss = -12335.253453808224
Iteration 1600: Loss = -12335.210403940013
Iteration 1700: Loss = -12335.174318740757
Iteration 1800: Loss = -12335.143683260398
Iteration 1900: Loss = -12335.117469037321
Iteration 2000: Loss = -12335.094775388552
Iteration 2100: Loss = -12335.07501423951
Iteration 2200: Loss = -12335.057667206493
Iteration 2300: Loss = -12335.042287390823
Iteration 2400: Loss = -12335.028557713164
Iteration 2500: Loss = -12335.01618285605
Iteration 2600: Loss = -12335.004802984095
Iteration 2700: Loss = -12334.994188483159
Iteration 2800: Loss = -12334.983917629774
Iteration 2900: Loss = -12334.974387304452
Iteration 3000: Loss = -12334.966534385212
Iteration 3100: Loss = -12334.95995257601
Iteration 3200: Loss = -12334.95403405523
Iteration 3300: Loss = -12334.94857825949
Iteration 3400: Loss = -12334.94359944976
Iteration 3500: Loss = -12334.938942431107
Iteration 3600: Loss = -12334.934541418012
Iteration 3700: Loss = -12334.930280837967
Iteration 3800: Loss = -12334.926072196999
Iteration 3900: Loss = -12334.921648925943
Iteration 4000: Loss = -12334.916615731594
Iteration 4100: Loss = -12334.910591951008
Iteration 4200: Loss = -12334.904836631184
Iteration 4300: Loss = -12334.900188110723
Iteration 4400: Loss = -12334.896335424683
Iteration 4500: Loss = -12334.893372902574
Iteration 4600: Loss = -12334.891265615044
Iteration 4700: Loss = -12334.889634705945
Iteration 4800: Loss = -12334.88835486682
Iteration 4900: Loss = -12334.887776162146
Iteration 5000: Loss = -12334.886064763177
Iteration 5100: Loss = -12334.884663626828
Iteration 5200: Loss = -12334.882918497187
Iteration 5300: Loss = -12334.881583345757
Iteration 5400: Loss = -12334.880239771766
Iteration 5500: Loss = -12334.8787579225
Iteration 5600: Loss = -12334.87723018291
Iteration 5700: Loss = -12334.874657558565
Iteration 5800: Loss = -12334.872054904401
Iteration 5900: Loss = -12334.869240970358
Iteration 6000: Loss = -12334.865790969694
Iteration 6100: Loss = -12334.861189829544
Iteration 6200: Loss = -12334.854516793082
Iteration 6300: Loss = -12334.842957531939
Iteration 6400: Loss = -12334.81962455598
Iteration 6500: Loss = -12334.787403967539
Iteration 6600: Loss = -12334.759195830247
Iteration 6700: Loss = -12334.730269340844
Iteration 6800: Loss = -12334.692434894167
Iteration 6900: Loss = -12334.65729528055
Iteration 7000: Loss = -12334.62928956613
Iteration 7100: Loss = -12334.588650595899
Iteration 7200: Loss = -12334.54871228647
Iteration 7300: Loss = -12334.516619539669
Iteration 7400: Loss = -12334.490012652912
Iteration 7500: Loss = -12334.45978526341
Iteration 7600: Loss = -12334.438472112692
Iteration 7700: Loss = -12334.40486253805
Iteration 7800: Loss = -12334.337674720577
Iteration 7900: Loss = -12334.304254008997
Iteration 8000: Loss = -12334.282962748664
Iteration 8100: Loss = -12334.264611416645
Iteration 8200: Loss = -12334.254736955616
Iteration 8300: Loss = -12334.23513845429
Iteration 8400: Loss = -12334.211527216346
Iteration 8500: Loss = -12334.188178576243
Iteration 8600: Loss = -12334.162812164588
Iteration 8700: Loss = -12334.137162919533
Iteration 8800: Loss = -12334.126620257532
Iteration 8900: Loss = -12334.105138117142
Iteration 9000: Loss = -12334.083324567522
Iteration 9100: Loss = -12334.236287478456
1
Iteration 9200: Loss = -12334.067517441403
Iteration 9300: Loss = -12334.061162336018
Iteration 9400: Loss = -12334.08264334838
1
Iteration 9500: Loss = -12333.998364003693
Iteration 9600: Loss = -12333.953554844411
Iteration 9700: Loss = -12333.697205365872
Iteration 9800: Loss = -12333.64426292175
Iteration 9900: Loss = -12333.641387621461
Iteration 10000: Loss = -12333.63489723628
Iteration 10100: Loss = -12333.634556932464
Iteration 10200: Loss = -12333.63318595669
Iteration 10300: Loss = -12333.631822445443
Iteration 10400: Loss = -12333.63377397728
1
Iteration 10500: Loss = -12333.631206871203
Iteration 10600: Loss = -12333.638851348342
1
Iteration 10700: Loss = -12333.630876488483
Iteration 10800: Loss = -12333.649657726377
1
Iteration 10900: Loss = -12333.643728449511
2
Iteration 11000: Loss = -12333.679831965628
3
Iteration 11100: Loss = -12333.630415375297
Iteration 11200: Loss = -12333.650214296547
1
Iteration 11300: Loss = -12333.630329863061
Iteration 11400: Loss = -12333.679050868077
1
Iteration 11500: Loss = -12333.630310680328
Iteration 11600: Loss = -12334.289776870288
1
Iteration 11700: Loss = -12333.630266792445
Iteration 11800: Loss = -12333.63015794005
Iteration 11900: Loss = -12333.632299496336
1
Iteration 12000: Loss = -12333.631553888974
2
Iteration 12100: Loss = -12333.726089297958
3
Iteration 12200: Loss = -12333.630112392635
Iteration 12300: Loss = -12333.630329586902
1
Iteration 12400: Loss = -12333.630499441735
2
Iteration 12500: Loss = -12333.630221178042
3
Iteration 12600: Loss = -12333.63124559463
4
Iteration 12700: Loss = -12333.630151573268
5
Iteration 12800: Loss = -12333.629562939677
Iteration 12900: Loss = -12333.632185380477
1
Iteration 13000: Loss = -12333.62909647937
Iteration 13100: Loss = -12333.631914255253
1
Iteration 13200: Loss = -12333.629173875855
2
Iteration 13300: Loss = -12333.701233934471
3
Iteration 13400: Loss = -12333.628962370887
Iteration 13500: Loss = -12333.629844607289
1
Iteration 13600: Loss = -12333.629390273103
2
Iteration 13700: Loss = -12333.629344121771
3
Iteration 13800: Loss = -12333.628930527915
Iteration 13900: Loss = -12333.641065041616
1
Iteration 14000: Loss = -12333.628959547752
2
Iteration 14100: Loss = -12333.702125823667
3
Iteration 14200: Loss = -12333.628958516076
4
Iteration 14300: Loss = -12333.669204291153
5
Iteration 14400: Loss = -12333.629546524578
6
Iteration 14500: Loss = -12333.628937151541
7
Iteration 14600: Loss = -12333.647537226032
8
Iteration 14700: Loss = -12333.62894522021
9
Iteration 14800: Loss = -12333.629527356703
10
Stopping early at iteration 14800 due to no improvement.
tensor([[-2.8854,  1.4544],
        [-2.9768,  1.3412],
        [-3.0036,  1.3873],
        [-3.3279,  1.0273],
        [-2.6622,  1.2674],
        [-2.9456,  0.7555],
        [-2.7645,  1.3466],
        [-2.9243,  1.3103],
        [-2.6500,  1.0819],
        [-3.0180,  1.3780],
        [-2.9126,  1.5018],
        [-3.4128,  1.0911],
        [-3.9416,  0.3528],
        [-2.8748,  1.3985],
        [-3.9859,  0.3507],
        [-3.8472,  0.5650],
        [-2.5043,  0.9605],
        [-3.4775,  0.9986],
        [-2.9942,  1.1580],
        [-3.0748,  1.0628],
        [-3.5136,  0.8973],
        [-3.1175,  1.3750],
        [-2.8829,  1.3751],
        [-3.0045,  1.3063],
        [-2.8406,  1.2864],
        [-2.8647,  1.4606],
        [-2.7354,  1.3377],
        [-2.9483,  1.4886],
        [-2.8558,  1.2993],
        [-2.9423,  1.4957],
        [-3.1110,  1.0878],
        [-2.9116,  1.4736],
        [-2.7774,  1.3909],
        [-2.8928,  1.2936],
        [-2.8788,  1.2060],
        [-2.9882,  1.3477],
        [-2.8311,  1.4435],
        [-3.1420,  1.1943],
        [-2.8739,  1.4643],
        [-2.8905,  1.4268],
        [-2.8432,  1.3641],
        [-3.2777,  1.1606],
        [-2.9844,  1.4487],
        [-2.9163,  1.4184],
        [-2.9557,  1.4463],
        [-3.1643,  1.1410],
        [-2.7687,  1.2144],
        [-2.9895,  1.2400],
        [-3.2399,  1.2868],
        [-2.7772,  1.1028],
        [-3.0042,  1.4329],
        [-2.9159,  1.4771],
        [-3.2174,  1.1403],
        [-2.7051,  1.3062],
        [-3.5440,  0.7503],
        [-2.8166,  1.4015],
        [-3.4368,  0.9563],
        [-3.0089,  1.2580],
        [-3.0067,  1.4036],
        [-3.4432,  0.3018],
        [-2.9365,  1.3535],
        [-2.8171,  1.3517],
        [-2.7318,  1.3093],
        [-2.9267,  1.4163],
        [-2.8136,  1.4233],
        [-3.9199,  0.3999],
        [-3.1126,  1.2450],
        [-2.8757,  1.4761],
        [-2.9560,  1.3483],
        [-3.3248,  1.0488],
        [-2.9084,  1.4439],
        [-3.2557,  1.0975],
        [-3.1650,  1.1706],
        [-2.8220,  1.4349],
        [-4.4263, -0.1889],
        [-3.1556,  0.8764],
        [-3.5357,  0.6982],
        [-4.3921, -0.2231],
        [-3.4939,  0.8714],
        [-3.0302,  1.2430],
        [-2.8859,  1.3679],
        [-2.9474,  1.3405],
        [-2.9137,  1.5111],
        [-3.2900,  0.9373],
        [-3.4045,  0.9511],
        [-2.9449,  1.4821],
        [-2.8799,  1.4723],
        [-3.5054,  0.7249],
        [-3.4624,  0.9199],
        [-4.2940, -0.1590],
        [-2.9290,  1.2473],
        [-2.9273,  1.4282],
        [-2.7103,  1.3201],
        [-3.0825,  1.2441],
        [-3.9209,  0.2010],
        [-2.9087,  1.3653],
        [-2.9665,  1.1442],
        [-2.9418,  1.2799],
        [-3.0486,  1.3506],
        [-3.1205,  1.2836]], dtype=torch.float64, requires_grad=True)
pi: tensor([[8.5337e-01, 1.4663e-01],
        [9.9984e-01, 1.6151e-04]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0142, 0.9858], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1998, 0.2000],
         [0.8779, 0.1967]],

        [[0.8805, 0.2452],
         [0.6798, 0.2273]],

        [[0.5697, 0.1933],
         [0.2530, 0.3338]],

        [[0.7567, 0.2160],
         [0.1869, 0.2275]],

        [[0.9997, 0.1763],
         [0.7446, 0.3651]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0014220233587568716
Average Adjusted Rand Index: 0.0
11843.78863025153
new:  [-0.0013931300752109495, 0.9919999860917265, -0.0013931300752109495, -0.0014220233587568716] [0.023460555637508686, 0.9919992163297293, 0.023460555637508686, 0.0] [12327.505736163008, 11836.476577263224, 12327.500191051171, 12333.629527356703]
prior:  [0.0, 0.0, 0.0, 0.0] [0.0, 0.0, 0.0, 0.0] [12335.197571630863, 12335.197709214719, 12333.763870835299, 12333.763878518126]
-----------------------------------------------------------------------------------------
This iteration is 14
True Objective function: Loss = -11934.672583504685
Iteration 0: Loss = -17838.81743427457
Iteration 10: Loss = -12424.502539211537
Iteration 20: Loss = -12415.860360419512
Iteration 30: Loss = -12416.486476961652
1
Iteration 40: Loss = -12416.697247967364
2
Iteration 50: Loss = -12416.795999898122
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[3.0914e-04, 9.9969e-01],
        [6.7625e-04, 9.9932e-01]], dtype=torch.float64)
alpha: tensor([0.0182, 0.9818])
beta: tensor([[[0.6628, 0.1422],
         [0.3880, 0.2000]],

        [[0.1963, 0.2046],
         [0.3704, 0.7067]],

        [[0.7726, 0.2341],
         [0.8078, 0.5331]],

        [[0.2150, 0.1826],
         [0.9422, 0.7875]],

        [[0.0690, 0.1681],
         [0.9072, 0.7041]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 59
Adjusted Rand Index: 0.02918749315918129
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.003534718566533304
Average Adjusted Rand Index: 0.005837498631836259
tensor([[-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17805.88751050575
Iteration 100: Loss = -12417.481101712296
Iteration 200: Loss = -12415.66728834238
Iteration 300: Loss = -12415.446745897636
Iteration 400: Loss = -12415.357094023355
Iteration 500: Loss = -12415.231743997343
Iteration 600: Loss = -12415.04930298059
Iteration 700: Loss = -12414.960067319615
Iteration 800: Loss = -12414.930857551812
Iteration 900: Loss = -12414.915988108818
Iteration 1000: Loss = -12414.906298457661
Iteration 1100: Loss = -12414.898853835419
Iteration 1200: Loss = -12414.89243862838
Iteration 1300: Loss = -12414.886423253074
Iteration 1400: Loss = -12414.88040877566
Iteration 1500: Loss = -12414.874027692498
Iteration 1600: Loss = -12414.86675874899
Iteration 1700: Loss = -12414.857856392873
Iteration 1800: Loss = -12414.845555364649
Iteration 1900: Loss = -12414.822305386533
Iteration 2000: Loss = -12414.644095574637
Iteration 2100: Loss = -12414.501689363304
Iteration 2200: Loss = -12414.412728419453
Iteration 2300: Loss = -12414.328040740002
Iteration 2400: Loss = -12414.250924890126
Iteration 2500: Loss = -12414.181477198079
Iteration 2600: Loss = -12414.064211268183
Iteration 2700: Loss = -12413.933815852452
Iteration 2800: Loss = -12413.893998287265
Iteration 2900: Loss = -12413.872541960221
Iteration 3000: Loss = -12413.858373178362
Iteration 3100: Loss = -12413.84814069762
Iteration 3200: Loss = -12413.840395690122
Iteration 3300: Loss = -12413.834273145872
Iteration 3400: Loss = -12413.82936610253
Iteration 3500: Loss = -12413.82537242573
Iteration 3600: Loss = -12413.821979707003
Iteration 3700: Loss = -12413.81912171449
Iteration 3800: Loss = -12413.816659352944
Iteration 3900: Loss = -12413.814542613703
Iteration 4000: Loss = -12413.812676977173
Iteration 4100: Loss = -12413.811042151967
Iteration 4200: Loss = -12413.809573860057
Iteration 4300: Loss = -12413.808309120008
Iteration 4400: Loss = -12413.807134273266
Iteration 4500: Loss = -12413.80610379371
Iteration 4600: Loss = -12413.847814570794
1
Iteration 4700: Loss = -12413.804309380028
Iteration 4800: Loss = -12413.803500525844
Iteration 4900: Loss = -12413.802801686126
Iteration 5000: Loss = -12413.802155854177
Iteration 5100: Loss = -12413.801577446486
Iteration 5200: Loss = -12413.801029097685
Iteration 5300: Loss = -12413.866066350109
1
Iteration 5400: Loss = -12413.800017058988
Iteration 5500: Loss = -12413.79958675093
Iteration 5600: Loss = -12413.7991808172
Iteration 5700: Loss = -12413.79933746053
1
Iteration 5800: Loss = -12413.798494788716
Iteration 5900: Loss = -12413.79819555563
Iteration 6000: Loss = -12413.909655546542
1
Iteration 6100: Loss = -12413.797577315625
Iteration 6200: Loss = -12413.797313075778
Iteration 6300: Loss = -12413.79705868521
Iteration 6400: Loss = -12413.796924416793
Iteration 6500: Loss = -12413.796661704113
Iteration 6600: Loss = -12413.79642334296
Iteration 6700: Loss = -12413.797050328185
1
Iteration 6800: Loss = -12413.796043355725
Iteration 6900: Loss = -12413.795911072819
Iteration 7000: Loss = -12413.795732130291
Iteration 7100: Loss = -12413.795595827141
Iteration 7200: Loss = -12413.795976812678
1
Iteration 7300: Loss = -12413.795333449172
Iteration 7400: Loss = -12413.795185009816
Iteration 7500: Loss = -12413.795079114045
Iteration 7600: Loss = -12413.795127258905
1
Iteration 7700: Loss = -12413.794843920581
Iteration 7800: Loss = -12413.794797733546
Iteration 7900: Loss = -12413.794816758385
1
Iteration 8000: Loss = -12413.794613849677
Iteration 8100: Loss = -12413.79450398444
Iteration 8200: Loss = -12413.79442786942
Iteration 8300: Loss = -12413.822072406747
1
Iteration 8400: Loss = -12413.794251755116
Iteration 8500: Loss = -12413.794239223662
Iteration 8600: Loss = -12413.794136870363
Iteration 8700: Loss = -12413.946748714823
1
Iteration 8800: Loss = -12413.79405783312
Iteration 8900: Loss = -12413.793980966566
Iteration 9000: Loss = -12413.793919559437
Iteration 9100: Loss = -12413.793961751793
1
Iteration 9200: Loss = -12413.793911250168
Iteration 9300: Loss = -12413.793777081453
Iteration 9400: Loss = -12413.793774522792
Iteration 9500: Loss = -12413.90180826999
1
Iteration 9600: Loss = -12413.793671140724
Iteration 9700: Loss = -12413.793631186913
Iteration 9800: Loss = -12413.793627105266
Iteration 9900: Loss = -12414.00991788068
1
Iteration 10000: Loss = -12413.79352839214
Iteration 10100: Loss = -12413.793479779211
Iteration 10200: Loss = -12413.793479800113
1
Iteration 10300: Loss = -12413.795650459833
2
Iteration 10400: Loss = -12413.793395797924
Iteration 10500: Loss = -12413.793399761864
1
Iteration 10600: Loss = -12414.229573794677
2
Iteration 10700: Loss = -12413.793404651646
3
Iteration 10800: Loss = -12413.793340338696
Iteration 10900: Loss = -12413.793324619186
Iteration 11000: Loss = -12413.793760180837
1
Iteration 11100: Loss = -12413.79328169219
Iteration 11200: Loss = -12413.793277829427
Iteration 11300: Loss = -12413.793238308874
Iteration 11400: Loss = -12413.793232657124
Iteration 11500: Loss = -12413.793539661498
1
Iteration 11600: Loss = -12413.793216701071
Iteration 11700: Loss = -12413.793199410627
Iteration 11800: Loss = -12413.793164730101
Iteration 11900: Loss = -12414.624558653946
1
Iteration 12000: Loss = -12413.793176931287
2
Iteration 12100: Loss = -12413.793129007394
Iteration 12200: Loss = -12413.793106795827
Iteration 12300: Loss = -12413.793171378626
1
Iteration 12400: Loss = -12413.793196112232
2
Iteration 12500: Loss = -12413.793108043821
3
Iteration 12600: Loss = -12413.793117926116
4
Iteration 12700: Loss = -12413.799498302578
5
Iteration 12800: Loss = -12413.793142600354
6
Iteration 12900: Loss = -12413.793080234287
Iteration 13000: Loss = -12413.793592808259
1
Iteration 13100: Loss = -12413.793091176589
2
Iteration 13200: Loss = -12413.7930676471
Iteration 13300: Loss = -12413.793175044078
1
Iteration 13400: Loss = -12413.793045852337
Iteration 13500: Loss = -12413.82614201997
1
Iteration 13600: Loss = -12413.793071632455
2
Iteration 13700: Loss = -12413.793060994716
3
Iteration 13800: Loss = -12413.815577544909
4
Iteration 13900: Loss = -12413.793067519406
5
Iteration 14000: Loss = -12413.902933834785
6
Iteration 14100: Loss = -12413.793038888993
Iteration 14200: Loss = -12413.79303527716
Iteration 14300: Loss = -12413.79346966086
1
Iteration 14400: Loss = -12413.793021082289
Iteration 14500: Loss = -12413.793089687004
1
Iteration 14600: Loss = -12413.79307341896
2
Iteration 14700: Loss = -12413.793061515224
3
Iteration 14800: Loss = -12413.793028520871
4
Iteration 14900: Loss = -12413.793196851138
5
Iteration 15000: Loss = -12413.793037025673
6
Iteration 15100: Loss = -12413.793030572306
7
Iteration 15200: Loss = -12413.793171121757
8
Iteration 15300: Loss = -12413.79298863763
Iteration 15400: Loss = -12413.793020268018
1
Iteration 15500: Loss = -12413.79525900518
2
Iteration 15600: Loss = -12413.793217750896
3
Iteration 15700: Loss = -12413.793025633122
4
Iteration 15800: Loss = -12413.794184332653
5
Iteration 15900: Loss = -12413.79420554474
6
Iteration 16000: Loss = -12413.79336371377
7
Iteration 16100: Loss = -12413.793245181318
8
Iteration 16200: Loss = -12413.793028624805
9
Iteration 16300: Loss = -12413.794355729287
10
Stopping early at iteration 16300 due to no improvement.
tensor([[ -8.9715,   4.3563],
        [ -6.5681,   1.9528],
        [ -6.0644,   1.4492],
        [ -8.0892,   3.4740],
        [ -7.3683,   2.7531],
        [ -6.1724,   1.5571],
        [ -9.0598,   4.4446],
        [ -8.9018,   4.2865],
        [ -5.8351,   1.2199],
        [-10.1153,   5.5001],
        [ -8.3845,   3.7693],
        [ -8.5734,   3.9582],
        [ -7.0880,   2.4728],
        [ -8.9330,   4.3178],
        [ -5.3408,   0.7256],
        [-10.8389,   6.2237],
        [ -9.7249,   5.1097],
        [-10.4958,   5.8805],
        [ -7.4135,   2.7983],
        [ -9.2633,   4.6481],
        [ -6.1176,   1.5024],
        [ -5.9293,   1.3141],
        [ -9.5421,   4.9269],
        [ -9.6033,   4.9880],
        [ -8.1027,   3.4874],
        [ -7.7810,   3.1658],
        [ -6.5989,   1.9836],
        [ -8.1743,   3.5591],
        [ -8.3972,   3.7819],
        [ -9.6360,   5.0208],
        [ -8.0356,   3.4204],
        [ -8.3844,   3.7692],
        [ -7.5572,   2.9419],
        [ -7.4580,   2.8428],
        [ -8.8488,   4.2336],
        [ -8.8061,   4.1909],
        [ -8.6887,   4.0735],
        [ -9.2055,   4.5903],
        [ -7.6554,   3.0402],
        [ -6.7231,   2.1078],
        [ -4.3603,  -0.2549],
        [ -6.5043,   1.8890],
        [ -8.3907,   3.7754],
        [ -9.9078,   5.2926],
        [ -9.1186,   4.5034],
        [ -8.7317,   4.1164],
        [-10.9392,   6.3240],
        [ -6.2871,   1.6719],
        [ -6.4188,   1.8036],
        [ -9.0991,   4.4838],
        [ -9.2283,   4.6131],
        [-11.0266,   6.4113],
        [ -8.0570,   3.4418],
        [-11.0394,   6.4242],
        [ -8.5017,   3.8865],
        [ -7.8898,   3.2745],
        [-10.4988,   5.8836],
        [ -7.3000,   2.6848],
        [ -9.0077,   4.3924],
        [ -7.1251,   2.5099],
        [ -6.5256,   1.9103],
        [-11.0421,   6.4269],
        [ -7.3832,   2.7680],
        [ -7.7196,   3.1044],
        [ -7.5155,   2.9002],
        [-10.3639,   5.7487],
        [ -8.4832,   3.8679],
        [ -8.6308,   4.0156],
        [ -8.9419,   4.3266],
        [ -5.8476,   1.2323],
        [ -7.1749,   2.5597],
        [-10.1473,   5.5321],
        [ -6.9003,   2.2850],
        [-10.7567,   6.1415],
        [ -6.9666,   2.3514],
        [-11.2595,   6.6443],
        [ -7.3128,   2.6976],
        [ -8.6691,   4.0539],
        [ -5.9174,   1.3022],
        [ -9.4036,   4.7884],
        [ -5.7510,   1.1358],
        [ -8.1785,   3.5633],
        [-10.9015,   6.2863],
        [ -6.9952,   2.3800],
        [ -6.2798,   1.6646],
        [ -7.8080,   3.1927],
        [ -6.0245,   1.4093],
        [-10.2803,   5.6650],
        [ -6.6136,   1.9984],
        [ -3.9041,  -0.7111],
        [ -1.0541,  -3.5611],
        [ -8.8885,   4.2733],
        [ -9.4605,   4.8452],
        [ -6.6787,   2.0635],
        [ -9.2643,   4.6491],
        [ -5.4823,   0.8671],
        [ -6.9494,   2.3342],
        [ -8.4835,   3.8683],
        [ -6.5916,   1.9764],
        [ -9.9536,   5.3383]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 4.6190e-06],
        [5.8293e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0099, 0.9901], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1905, 0.1236],
         [0.3880, 0.2006]],

        [[0.1963, 0.2490],
         [0.3704, 0.7067]],

        [[0.7726, 0.1606],
         [0.8078, 0.5331]],

        [[0.2150, 0.3096],
         [0.9422, 0.7875]],

        [[0.0690, 0.1859],
         [0.9072, 0.7041]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: -0.00491035742834304
Global Adjusted Rand Index: -0.00027902358001543297
Average Adjusted Rand Index: -0.0014286130260745746
Iteration 0: Loss = -26145.89692674504
Iteration 10: Loss = -12415.710025507336
Iteration 20: Loss = -12415.709949303331
Iteration 30: Loss = -12415.70983405247
Iteration 40: Loss = -12415.709846442362
1
Iteration 50: Loss = -12415.709864796412
2
Iteration 60: Loss = -12415.709810527425
Iteration 70: Loss = -12415.70973986236
Iteration 80: Loss = -12415.709701279955
Iteration 90: Loss = -12415.709622935603
Iteration 100: Loss = -12415.709503327966
Iteration 110: Loss = -12415.709407573167
Iteration 120: Loss = -12415.70930853752
Iteration 130: Loss = -12415.709182025597
Iteration 140: Loss = -12415.709061906533
Iteration 150: Loss = -12415.708931958401
Iteration 160: Loss = -12415.708760937123
Iteration 170: Loss = -12415.708561414274
Iteration 180: Loss = -12415.708324410354
Iteration 190: Loss = -12415.708194781591
Iteration 200: Loss = -12415.707970795436
Iteration 210: Loss = -12415.707704513425
Iteration 220: Loss = -12415.707355364573
Iteration 230: Loss = -12415.706992705896
Iteration 240: Loss = -12415.706567289055
Iteration 250: Loss = -12415.706166313306
Iteration 260: Loss = -12415.705637768091
Iteration 270: Loss = -12415.705062090557
Iteration 280: Loss = -12415.704431374512
Iteration 290: Loss = -12415.70377557509
Iteration 300: Loss = -12415.703066637301
Iteration 310: Loss = -12415.702292156506
Iteration 320: Loss = -12415.701507079399
Iteration 330: Loss = -12415.700698913311
Iteration 340: Loss = -12415.699732780664
Iteration 350: Loss = -12415.698799008618
Iteration 360: Loss = -12415.697848206539
Iteration 370: Loss = -12415.696861234677
Iteration 380: Loss = -12415.695908362683
Iteration 390: Loss = -12415.694914145275
Iteration 400: Loss = -12415.694011869484
Iteration 410: Loss = -12415.692985869224
Iteration 420: Loss = -12415.692080067827
Iteration 430: Loss = -12415.691210860647
Iteration 440: Loss = -12415.690331878111
Iteration 450: Loss = -12415.689600374415
Iteration 460: Loss = -12415.688810579653
Iteration 470: Loss = -12415.688145746213
Iteration 480: Loss = -12415.687487835508
Iteration 490: Loss = -12415.686853474695
Iteration 500: Loss = -12415.686283032468
Iteration 510: Loss = -12415.685827908297
Iteration 520: Loss = -12415.685286008831
Iteration 530: Loss = -12415.684855111753
Iteration 540: Loss = -12415.684467733201
Iteration 550: Loss = -12415.684126835962
Iteration 560: Loss = -12415.683817397521
Iteration 570: Loss = -12415.683485408204
Iteration 580: Loss = -12415.683208263472
Iteration 590: Loss = -12415.68292490456
Iteration 600: Loss = -12415.682721967587
Iteration 610: Loss = -12415.682492733931
Iteration 620: Loss = -12415.682294909833
Iteration 630: Loss = -12415.682149999704
Iteration 640: Loss = -12415.68195780494
Iteration 650: Loss = -12415.681783675791
Iteration 660: Loss = -12415.681687010127
Iteration 670: Loss = -12415.681563417995
Iteration 680: Loss = -12415.681448465546
Iteration 690: Loss = -12415.681339804898
Iteration 700: Loss = -12415.68122336277
Iteration 710: Loss = -12415.681153699787
Iteration 720: Loss = -12415.681043924127
Iteration 730: Loss = -12415.680997216185
Iteration 740: Loss = -12415.680886256947
Iteration 750: Loss = -12415.680784464941
Iteration 760: Loss = -12415.680706265537
Iteration 770: Loss = -12415.680706811836
1
Iteration 780: Loss = -12415.680570507146
Iteration 790: Loss = -12415.680556573981
Iteration 800: Loss = -12415.680511395614
Iteration 810: Loss = -12415.680463258825
Iteration 820: Loss = -12415.680394998286
Iteration 830: Loss = -12415.680350858183
Iteration 840: Loss = -12415.680316818169
Iteration 850: Loss = -12415.680258976983
Iteration 860: Loss = -12415.680192382972
Iteration 870: Loss = -12415.68013904268
Iteration 880: Loss = -12415.68007140157
Iteration 890: Loss = -12415.68007354062
1
Iteration 900: Loss = -12415.67997503474
Iteration 910: Loss = -12415.679953271107
Iteration 920: Loss = -12415.679918097101
Iteration 930: Loss = -12415.679839632701
Iteration 940: Loss = -12415.679755237403
Iteration 950: Loss = -12415.67967961484
Iteration 960: Loss = -12415.679644873428
Iteration 970: Loss = -12415.67950135405
Iteration 980: Loss = -12415.679366865
Iteration 990: Loss = -12415.679187440199
Iteration 1000: Loss = -12415.679021220198
Iteration 1010: Loss = -12415.678744871504
Iteration 1020: Loss = -12415.67845241173
Iteration 1030: Loss = -12415.678024513365
Iteration 1040: Loss = -12415.677586169217
Iteration 1050: Loss = -12415.677153941044
Iteration 1060: Loss = -12415.67669389664
Iteration 1070: Loss = -12415.676278508694
Iteration 1080: Loss = -12415.675924240371
Iteration 1090: Loss = -12415.675569410714
Iteration 1100: Loss = -12415.675252894436
Iteration 1110: Loss = -12415.674939911029
Iteration 1120: Loss = -12415.674694702888
Iteration 1130: Loss = -12415.674441623123
Iteration 1140: Loss = -12415.674187689618
Iteration 1150: Loss = -12415.673936294066
Iteration 1160: Loss = -12415.673704338153
Iteration 1170: Loss = -12415.67354341771
Iteration 1180: Loss = -12415.67335921141
Iteration 1190: Loss = -12415.673096257246
Iteration 1200: Loss = -12415.672902089422
Iteration 1210: Loss = -12415.672762445778
Iteration 1220: Loss = -12415.672602280043
Iteration 1230: Loss = -12415.67244073533
Iteration 1240: Loss = -12415.672237339291
Iteration 1250: Loss = -12415.672098937976
Iteration 1260: Loss = -12415.671975513687
Iteration 1270: Loss = -12415.671848539178
Iteration 1280: Loss = -12415.671725493838
Iteration 1290: Loss = -12415.671620445097
Iteration 1300: Loss = -12415.671468410455
Iteration 1310: Loss = -12415.671363736537
Iteration 1320: Loss = -12415.671274738512
Iteration 1330: Loss = -12415.67115029458
Iteration 1340: Loss = -12415.671033667766
Iteration 1350: Loss = -12415.670949485391
Iteration 1360: Loss = -12415.670846270394
Iteration 1370: Loss = -12415.670790220414
Iteration 1380: Loss = -12415.670715141809
Iteration 1390: Loss = -12415.670592134953
Iteration 1400: Loss = -12415.670551676869
Iteration 1410: Loss = -12415.670468808174
Iteration 1420: Loss = -12415.67040908715
Iteration 1430: Loss = -12415.670340812912
Iteration 1440: Loss = -12415.670244683406
Iteration 1450: Loss = -12415.670149833632
Iteration 1460: Loss = -12415.670102422157
Iteration 1470: Loss = -12415.670051781606
Iteration 1480: Loss = -12415.669995277627
Iteration 1490: Loss = -12415.669970375371
Iteration 1500: Loss = -12415.669901361925
Iteration 1510: Loss = -12415.669875162888
Iteration 1520: Loss = -12415.669788164822
Iteration 1530: Loss = -12415.669720448495
Iteration 1540: Loss = -12415.669671073196
Iteration 1550: Loss = -12415.669682566608
1
Iteration 1560: Loss = -12415.66961031062
Iteration 1570: Loss = -12415.66957385601
Iteration 1580: Loss = -12415.66954819321
Iteration 1590: Loss = -12415.669502593835
Iteration 1600: Loss = -12415.66950181958
Iteration 1610: Loss = -12415.669430950102
Iteration 1620: Loss = -12415.66941821462
Iteration 1630: Loss = -12415.669380242658
Iteration 1640: Loss = -12415.669329795184
Iteration 1650: Loss = -12415.669312346468
Iteration 1660: Loss = -12415.669292621253
Iteration 1670: Loss = -12415.669251687601
Iteration 1680: Loss = -12415.669177458596
Iteration 1690: Loss = -12415.66919141077
1
Iteration 1700: Loss = -12415.669154933266
Iteration 1710: Loss = -12415.6691393427
Iteration 1720: Loss = -12415.66912875793
Iteration 1730: Loss = -12415.669050621846
Iteration 1740: Loss = -12415.669077176915
1
Iteration 1750: Loss = -12415.669044802207
Iteration 1760: Loss = -12415.669051738107
1
Iteration 1770: Loss = -12415.66899602938
Iteration 1780: Loss = -12415.669012820079
1
Iteration 1790: Loss = -12415.668984109338
Iteration 1800: Loss = -12415.668920787244
Iteration 1810: Loss = -12415.668937980317
1
Iteration 1820: Loss = -12415.668958456517
2
Iteration 1830: Loss = -12415.668918578785
Iteration 1840: Loss = -12415.66887988008
Iteration 1850: Loss = -12415.66887412067
Iteration 1860: Loss = -12415.668851766308
Iteration 1870: Loss = -12415.6688560029
1
Iteration 1880: Loss = -12415.668846755005
Iteration 1890: Loss = -12415.668808914386
Iteration 1900: Loss = -12415.668819112225
1
Iteration 1910: Loss = -12415.668757970201
Iteration 1920: Loss = -12415.668785058342
1
Iteration 1930: Loss = -12415.66878249386
2
Iteration 1940: Loss = -12415.6687332444
Iteration 1950: Loss = -12415.668764743388
1
Iteration 1960: Loss = -12415.668728234863
Iteration 1970: Loss = -12415.668773308787
1
Iteration 1980: Loss = -12415.668703699586
Iteration 1990: Loss = -12415.668721384009
1
Iteration 2000: Loss = -12415.668712222589
2
Iteration 2010: Loss = -12415.668689778662
Iteration 2020: Loss = -12415.668689026765
Iteration 2030: Loss = -12415.668712090821
1
Iteration 2040: Loss = -12415.668672751222
Iteration 2050: Loss = -12415.668660809295
Iteration 2060: Loss = -12415.6686471704
Iteration 2070: Loss = -12415.668687779818
1
Iteration 2080: Loss = -12415.668641386193
Iteration 2090: Loss = -12415.66862733373
Iteration 2100: Loss = -12415.668602699494
Iteration 2110: Loss = -12415.668643183588
1
Iteration 2120: Loss = -12415.66858510156
Iteration 2130: Loss = -12415.6686019148
1
Iteration 2140: Loss = -12415.668637116038
2
Iteration 2150: Loss = -12415.668640049927
3
Stopping early at iteration 2149 due to no improvement.
pi: tensor([[0.9887, 0.0113],
        [0.9418, 0.0582]], dtype=torch.float64)
alpha: tensor([0.9881, 0.0119])
beta: tensor([[[0.1986, 0.2279],
         [0.1832, 0.2241]],

        [[0.0552, 0.2083],
         [0.9926, 0.4768]],

        [[0.7968, 0.2311],
         [0.9281, 0.0367]],

        [[0.1604, 0.1898],
         [0.5646, 0.6602]],

        [[0.5402, 0.1975],
         [0.3991, 0.6548]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26145.548738328285
Iteration 100: Loss = -12436.414748053969
Iteration 200: Loss = -12417.7268864321
Iteration 300: Loss = -12416.350892888655
Iteration 400: Loss = -12415.904457794199
Iteration 500: Loss = -12415.716669232756
Iteration 600: Loss = -12415.618447264598
Iteration 700: Loss = -12415.56095023264
Iteration 800: Loss = -12415.522165459892
Iteration 900: Loss = -12415.493807168294
Iteration 1000: Loss = -12415.472839116945
Iteration 1100: Loss = -12415.45698063336
Iteration 1200: Loss = -12415.444440856418
Iteration 1300: Loss = -12415.43432541763
Iteration 1400: Loss = -12415.42577396868
Iteration 1500: Loss = -12415.41838527858
Iteration 1600: Loss = -12415.412156645963
Iteration 1700: Loss = -12415.407013035705
Iteration 1800: Loss = -12415.402559982798
Iteration 1900: Loss = -12415.398654287255
Iteration 2000: Loss = -12415.395202428104
Iteration 2100: Loss = -12415.392100011082
Iteration 2200: Loss = -12415.38925878276
Iteration 2300: Loss = -12415.38673985938
Iteration 2400: Loss = -12415.384390573628
Iteration 2500: Loss = -12415.382213828972
Iteration 2600: Loss = -12415.380231562554
Iteration 2700: Loss = -12415.378393157267
Iteration 2800: Loss = -12415.376720749031
Iteration 2900: Loss = -12415.375132541143
Iteration 3000: Loss = -12415.373776532182
Iteration 3100: Loss = -12415.37242025792
Iteration 3200: Loss = -12415.371197630511
Iteration 3300: Loss = -12415.370046506941
Iteration 3400: Loss = -12415.36898728085
Iteration 3500: Loss = -12415.367979368177
Iteration 3600: Loss = -12415.36699378861
Iteration 3700: Loss = -12415.366043735747
Iteration 3800: Loss = -12415.365093410739
Iteration 3900: Loss = -12415.364152165266
Iteration 4000: Loss = -12415.36326162041
Iteration 4100: Loss = -12415.362313070162
Iteration 4200: Loss = -12415.361395812155
Iteration 4300: Loss = -12415.360417471773
Iteration 4400: Loss = -12415.359413051881
Iteration 4500: Loss = -12415.358316560658
Iteration 4600: Loss = -12415.357179970268
Iteration 4700: Loss = -12415.355963238975
Iteration 4800: Loss = -12415.354643350089
Iteration 4900: Loss = -12415.353133349752
Iteration 5000: Loss = -12415.35149441867
Iteration 5100: Loss = -12415.349638751213
Iteration 5200: Loss = -12415.347497336765
Iteration 5300: Loss = -12415.345099204487
Iteration 5400: Loss = -12415.342349266652
Iteration 5500: Loss = -12415.339136861177
Iteration 5600: Loss = -12415.335348692925
Iteration 5700: Loss = -12415.331019390687
Iteration 5800: Loss = -12415.325943595833
Iteration 5900: Loss = -12415.32027278167
Iteration 6000: Loss = -12415.314462653647
Iteration 6100: Loss = -12415.307688418756
Iteration 6200: Loss = -12415.302032109234
Iteration 6300: Loss = -12415.295834747021
Iteration 6400: Loss = -12415.288574538123
Iteration 6500: Loss = -12415.285432142531
Iteration 6600: Loss = -12415.266417125573
Iteration 6700: Loss = -12415.253296364594
Iteration 6800: Loss = -12415.237982884115
Iteration 6900: Loss = -12415.231604112516
Iteration 7000: Loss = -12415.22166993332
Iteration 7100: Loss = -12415.23797051137
1
Iteration 7200: Loss = -12415.23557016653
2
Iteration 7300: Loss = -12415.210500379908
Iteration 7400: Loss = -12415.197646877188
Iteration 7500: Loss = -12415.18343889429
Iteration 7600: Loss = -12415.185356873424
1
Iteration 7700: Loss = -12415.17138737874
Iteration 7800: Loss = -12415.169517857956
Iteration 7900: Loss = -12415.169640065511
1
Iteration 8000: Loss = -12415.17444872219
2
Iteration 8100: Loss = -12415.16756923552
Iteration 8200: Loss = -12415.167470628357
Iteration 8300: Loss = -12415.16678219886
Iteration 8400: Loss = -12415.170657476956
1
Iteration 8500: Loss = -12415.166118320145
Iteration 8600: Loss = -12415.224539204535
1
Iteration 8700: Loss = -12415.165619461548
Iteration 8800: Loss = -12415.165375858824
Iteration 8900: Loss = -12415.16550030893
1
Iteration 9000: Loss = -12415.164952777479
Iteration 9100: Loss = -12415.192835841459
1
Iteration 9200: Loss = -12415.16467696897
Iteration 9300: Loss = -12415.16441557388
Iteration 9400: Loss = -12415.238126420503
1
Iteration 9500: Loss = -12415.164112481843
Iteration 9600: Loss = -12415.16398530459
Iteration 9700: Loss = -12415.164292754196
1
Iteration 9800: Loss = -12415.16375714994
Iteration 9900: Loss = -12415.1651324362
1
Iteration 10000: Loss = -12415.163526610075
Iteration 10100: Loss = -12415.23244773122
1
Iteration 10200: Loss = -12415.163376066583
Iteration 10300: Loss = -12415.164566709425
1
Iteration 10400: Loss = -12415.16320721216
Iteration 10500: Loss = -12415.16314483799
Iteration 10600: Loss = -12415.169348925705
1
Iteration 10700: Loss = -12415.16305471357
Iteration 10800: Loss = -12415.163066882938
1
Iteration 10900: Loss = -12415.163263288056
2
Iteration 11000: Loss = -12415.170820669793
3
Iteration 11100: Loss = -12415.21417121907
4
Iteration 11200: Loss = -12415.16271164084
Iteration 11300: Loss = -12415.1631303652
1
Iteration 11400: Loss = -12415.162644077895
Iteration 11500: Loss = -12415.16268571101
1
Iteration 11600: Loss = -12415.1635603218
2
Iteration 11700: Loss = -12415.162541594653
Iteration 11800: Loss = -12415.173717974101
1
Iteration 11900: Loss = -12415.162534810866
Iteration 12000: Loss = -12415.16320226706
1
Iteration 12100: Loss = -12415.162510343092
Iteration 12200: Loss = -12415.194128988198
1
Iteration 12300: Loss = -12415.162416820862
Iteration 12400: Loss = -12415.162371619186
Iteration 12500: Loss = -12415.162379688081
1
Iteration 12600: Loss = -12415.162319286137
Iteration 12700: Loss = -12415.162520734017
1
Iteration 12800: Loss = -12415.162290171149
Iteration 12900: Loss = -12415.166930831854
1
Iteration 13000: Loss = -12415.162261754913
Iteration 13100: Loss = -12415.164164938291
1
Iteration 13200: Loss = -12415.16230319235
2
Iteration 13300: Loss = -12415.16222065448
Iteration 13400: Loss = -12415.175230765353
1
Iteration 13500: Loss = -12415.162147341924
Iteration 13600: Loss = -12415.163111334026
1
Iteration 13700: Loss = -12415.162234644496
2
Iteration 13800: Loss = -12415.16217650533
3
Iteration 13900: Loss = -12415.163670289794
4
Iteration 14000: Loss = -12415.162180370296
5
Iteration 14100: Loss = -12415.17463056509
6
Iteration 14200: Loss = -12415.162105709573
Iteration 14300: Loss = -12415.162959688465
1
Iteration 14400: Loss = -12415.184617790903
2
Iteration 14500: Loss = -12415.162091195893
Iteration 14600: Loss = -12415.276987553467
1
Iteration 14700: Loss = -12415.162091586448
2
Iteration 14800: Loss = -12415.164247857356
3
Iteration 14900: Loss = -12415.162086483126
Iteration 15000: Loss = -12415.162313500705
1
Iteration 15100: Loss = -12415.173869335758
2
Iteration 15200: Loss = -12415.162038648456
Iteration 15300: Loss = -12415.17177994788
1
Iteration 15400: Loss = -12415.162030311374
Iteration 15500: Loss = -12415.169506949736
1
Iteration 15600: Loss = -12415.162007696646
Iteration 15700: Loss = -12415.162287987181
1
Iteration 15800: Loss = -12415.472149412803
2
Iteration 15900: Loss = -12415.162064172175
3
Iteration 16000: Loss = -12415.168935612257
4
Iteration 16100: Loss = -12415.165373115226
5
Iteration 16200: Loss = -12415.162063710724
6
Iteration 16300: Loss = -12415.187589752419
7
Iteration 16400: Loss = -12415.161991521449
Iteration 16500: Loss = -12415.175246061583
1
Iteration 16600: Loss = -12415.162004487009
2
Iteration 16700: Loss = -12415.162455151662
3
Iteration 16800: Loss = -12415.426866094049
4
Iteration 16900: Loss = -12415.162006121198
5
Iteration 17000: Loss = -12415.165697701217
6
Iteration 17100: Loss = -12415.218780371853
7
Iteration 17200: Loss = -12415.161996084005
8
Iteration 17300: Loss = -12415.162332190785
9
Iteration 17400: Loss = -12415.370128211975
10
Stopping early at iteration 17400 due to no improvement.
tensor([[ 1.0483e-01, -1.7044e+00],
        [-3.9283e-01, -2.1811e+00],
        [-6.3565e-02, -2.1750e+00],
        [-3.5959e-01, -1.8170e+00],
        [-6.7734e-02, -1.4489e+00],
        [-1.6885e+00, -2.9267e+00],
        [-8.3670e-02, -1.7175e+00],
        [-5.4325e-01, -1.9030e+00],
        [-2.7604e-01, -1.7367e+00],
        [-1.3322e-01, -1.6796e+00],
        [ 1.0891e-01, -1.4957e+00],
        [ 8.6756e-02, -1.4740e+00],
        [ 1.7504e-01, -1.8930e+00],
        [ 3.5055e-02, -1.6022e+00],
        [ 3.0235e-01, -1.7258e+00],
        [-5.4092e-01, -1.7979e+00],
        [-7.7332e-01, -1.6729e+00],
        [-6.4859e-02, -1.3350e+00],
        [ 1.3049e-01, -1.5299e+00],
        [-5.4908e-01, -1.9190e+00],
        [ 1.8499e-01, -1.6329e+00],
        [-7.1099e-01, -2.3335e+00],
        [-9.8477e-01, -2.2440e+00],
        [-5.7062e-01, -1.7339e+00],
        [-3.9940e-02, -1.4227e+00],
        [-9.1142e-01, -1.9800e+00],
        [ 2.1648e-01, -1.6390e+00],
        [ 2.9438e-01, -1.7359e+00],
        [ 8.7743e-03, -1.5283e+00],
        [-1.4969e+00, -2.8914e+00],
        [ 9.3038e-02, -1.4835e+00],
        [-4.4567e-01, -1.1368e+00],
        [-7.6944e-01, -2.1843e+00],
        [-1.6619e-01, -1.3594e+00],
        [-8.2192e-01, -2.2505e+00],
        [ 2.4829e-01, -1.6358e+00],
        [ 7.3498e-02, -1.4915e+00],
        [-1.9682e-01, -1.3698e+00],
        [ 1.4657e-01, -1.5545e+00],
        [-1.4191e+00, -3.0023e+00],
        [ 3.3329e-01, -1.7281e+00],
        [-7.2634e-02, -1.7337e+00],
        [-2.7601e-01, -1.2333e+00],
        [-3.5903e-01, -2.1977e+00],
        [ 5.4968e-03, -1.4192e+00],
        [ 1.1368e-02, -1.4302e+00],
        [-4.6469e-01, -1.2045e+00],
        [-5.5139e-01, -2.1090e+00],
        [-3.1216e-01, -1.6365e+00],
        [-8.6785e-01, -1.8970e+00],
        [-1.0723e+00, -2.2533e+00],
        [-3.1422e-01, -1.3974e+00],
        [-1.9577e-01, -1.6973e+00],
        [-1.4341e-01, -1.5438e+00],
        [-2.4063e-03, -1.5167e+00],
        [-5.7937e-01, -1.9309e+00],
        [-3.2621e-01, -1.6913e+00],
        [-5.5462e-02, -1.6679e+00],
        [-1.3680e-01, -1.6661e+00],
        [ 1.4361e-02, -1.5024e+00],
        [-5.5532e-01, -2.4664e+00],
        [-3.0220e-01, -1.1602e+00],
        [-1.4061e-01, -1.4197e+00],
        [-1.9723e-02, -1.6326e+00],
        [ 9.7634e-02, -1.5585e+00],
        [ 8.1669e-03, -1.3979e+00],
        [ 5.4739e-02, -1.4411e+00],
        [-1.4506e-01, -1.4739e+00],
        [-2.5959e-01, -1.3813e+00],
        [-6.4232e-01, -2.5775e+00],
        [ 6.8068e-03, -1.5514e+00],
        [-3.4499e-01, -1.3853e+00],
        [-4.0787e-02, -2.1220e+00],
        [-6.2119e-01, -1.6083e+00],
        [ 4.6706e-02, -1.5787e+00],
        [-3.1132e-01, -1.0984e+00],
        [-6.1716e-01, -1.8294e+00],
        [-4.9926e-01, -2.3549e+00],
        [ 2.4730e-01, -1.8663e+00],
        [-1.5246e-03, -1.4722e+00],
        [ 1.6242e-01, -1.5625e+00],
        [-1.5999e-01, -1.9212e+00],
        [-1.2052e-01, -1.2666e+00],
        [-7.2308e-01, -2.4710e+00],
        [-6.8238e-01, -2.2457e+00],
        [ 8.2612e-02, -1.5732e+00],
        [-7.7088e-02, -2.2436e+00],
        [-2.4130e-01, -1.2886e+00],
        [-1.0589e-01, -1.8497e+00],
        [ 2.0734e-01, -1.7431e+00],
        [ 2.5652e-01, -1.7620e+00],
        [-5.1839e-01, -1.7671e+00],
        [ 4.9658e-02, -1.6307e+00],
        [-6.0010e-01, -2.6554e+00],
        [-2.8487e-02, -1.5375e+00],
        [-7.5449e-02, -1.5038e+00],
        [-7.7770e-01, -2.5339e+00],
        [-1.9752e-01, -1.3981e+00],
        [ 7.7011e-02, -1.5358e+00],
        [-5.0138e-01, -1.5089e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.9999e-01, 1.1625e-05],
        [4.3401e-01, 5.6599e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8147, 0.1853], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2005, 0.2078],
         [0.1832, 0.2183]],

        [[0.0552, 0.2100],
         [0.9926, 0.4768]],

        [[0.7968, 0.2176],
         [0.9281, 0.0367]],

        [[0.1604, 0.1896],
         [0.5646, 0.6602]],

        [[0.5402, 0.2088],
         [0.3991, 0.6548]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -35963.78987820312
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.1890,    nan]],

        [[0.6004,    nan],
         [0.5806, 0.8519]],

        [[0.6022,    nan],
         [0.8950, 0.6711]],

        [[0.1305,    nan],
         [0.4707, 0.4228]],

        [[0.2455,    nan],
         [0.5886, 0.8663]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -35897.2757904077
Iteration 100: Loss = -12423.222083052693
Iteration 200: Loss = -12419.449856234316
Iteration 300: Loss = -12418.065287118066
Iteration 400: Loss = -12417.298156384599
Iteration 500: Loss = -12416.825481876775
Iteration 600: Loss = -12416.512657816887
Iteration 700: Loss = -12416.293965792569
Iteration 800: Loss = -12416.134358478359
Iteration 900: Loss = -12416.014066658718
Iteration 1000: Loss = -12415.920764596865
Iteration 1100: Loss = -12415.846940027375
Iteration 1200: Loss = -12415.787322384713
Iteration 1300: Loss = -12415.738525667724
Iteration 1400: Loss = -12415.698028956436
Iteration 1500: Loss = -12415.664036857306
Iteration 1600: Loss = -12415.635204295962
Iteration 1700: Loss = -12415.610543153358
Iteration 1800: Loss = -12415.589301468211
Iteration 1900: Loss = -12415.570835820608
Iteration 2000: Loss = -12415.554674990935
Iteration 2100: Loss = -12415.540481326618
Iteration 2200: Loss = -12415.52793837855
Iteration 2300: Loss = -12415.51679466071
Iteration 2400: Loss = -12415.50688634884
Iteration 2500: Loss = -12415.497994947598
Iteration 2600: Loss = -12415.49001379684
Iteration 2700: Loss = -12415.482811177215
Iteration 2800: Loss = -12415.476252310455
Iteration 2900: Loss = -12415.470372305253
Iteration 3000: Loss = -12415.464977091868
Iteration 3100: Loss = -12415.46010705771
Iteration 3200: Loss = -12415.45561372675
Iteration 3300: Loss = -12415.451525090335
Iteration 3400: Loss = -12415.447758984266
Iteration 3500: Loss = -12415.444304799476
Iteration 3600: Loss = -12415.441096407021
Iteration 3700: Loss = -12415.438180917572
Iteration 3800: Loss = -12415.435467209569
Iteration 3900: Loss = -12415.432911240945
Iteration 4000: Loss = -12415.430618689104
Iteration 4100: Loss = -12415.428467758302
Iteration 4200: Loss = -12415.426450501145
Iteration 4300: Loss = -12415.424563847624
Iteration 4400: Loss = -12415.422844603607
Iteration 4500: Loss = -12415.421192817586
Iteration 4600: Loss = -12415.41969702191
Iteration 4700: Loss = -12415.418296009182
Iteration 4800: Loss = -12415.416967141038
Iteration 4900: Loss = -12415.41572396956
Iteration 5000: Loss = -12415.414565899277
Iteration 5100: Loss = -12415.413498932527
Iteration 5200: Loss = -12415.412481801326
Iteration 5300: Loss = -12415.411534156348
Iteration 5400: Loss = -12415.410665791298
Iteration 5500: Loss = -12415.409792053224
Iteration 5600: Loss = -12415.409005735439
Iteration 5700: Loss = -12415.40823166087
Iteration 5800: Loss = -12415.40757368017
Iteration 5900: Loss = -12415.406885081404
Iteration 6000: Loss = -12415.406309689282
Iteration 6100: Loss = -12415.405713046091
Iteration 6200: Loss = -12415.405127135034
Iteration 6300: Loss = -12415.404626276531
Iteration 6400: Loss = -12415.404098438092
Iteration 6500: Loss = -12415.40366108797
Iteration 6600: Loss = -12415.403198026388
Iteration 6700: Loss = -12415.402774394192
Iteration 6800: Loss = -12415.402343899104
Iteration 6900: Loss = -12415.401973459104
Iteration 7000: Loss = -12415.40157725352
Iteration 7100: Loss = -12415.40126263349
Iteration 7200: Loss = -12415.400903720312
Iteration 7300: Loss = -12415.40053487795
Iteration 7400: Loss = -12415.400225653144
Iteration 7500: Loss = -12415.399877384485
Iteration 7600: Loss = -12415.399575610107
Iteration 7700: Loss = -12415.399245042498
Iteration 7800: Loss = -12415.39889644659
Iteration 7900: Loss = -12415.39857853155
Iteration 8000: Loss = -12415.398154403327
Iteration 8100: Loss = -12415.397745598655
Iteration 8200: Loss = -12415.397239043044
Iteration 8300: Loss = -12415.396706511636
Iteration 8400: Loss = -12415.396133486136
Iteration 8500: Loss = -12415.395774414625
Iteration 8600: Loss = -12415.395285270251
Iteration 8700: Loss = -12415.394997538131
Iteration 8800: Loss = -12415.464913945725
1
Iteration 8900: Loss = -12415.394424069696
Iteration 9000: Loss = -12415.394196053514
Iteration 9100: Loss = -12415.393917779342
Iteration 9200: Loss = -12415.393797582397
Iteration 9300: Loss = -12415.393468730275
Iteration 9400: Loss = -12415.393295220658
Iteration 9500: Loss = -12415.419440388423
1
Iteration 9600: Loss = -12415.392871658187
Iteration 9700: Loss = -12415.392648567671
Iteration 9800: Loss = -12415.392453221626
Iteration 9900: Loss = -12415.394058238327
1
Iteration 10000: Loss = -12415.39195756503
Iteration 10100: Loss = -12415.391709951691
Iteration 10200: Loss = -12415.58442883005
1
Iteration 10300: Loss = -12415.39116717507
Iteration 10400: Loss = -12415.390842054836
Iteration 10500: Loss = -12415.390523910168
Iteration 10600: Loss = -12415.398107143074
1
Iteration 10700: Loss = -12415.389776440254
Iteration 10800: Loss = -12415.389271570677
Iteration 10900: Loss = -12415.388812924995
Iteration 11000: Loss = -12415.576061563866
1
Iteration 11100: Loss = -12415.387499251441
Iteration 11200: Loss = -12415.386692368755
Iteration 11300: Loss = -12415.385732583067
Iteration 11400: Loss = -12415.38550203739
Iteration 11500: Loss = -12415.384615068679
Iteration 11600: Loss = -12415.382235228408
Iteration 11700: Loss = -12415.380474109836
Iteration 11800: Loss = -12415.38712566771
1
Iteration 11900: Loss = -12415.380057925528
Iteration 12000: Loss = -12415.394193204262
1
Iteration 12100: Loss = -12415.375294084432
Iteration 12200: Loss = -12415.358581887274
Iteration 12300: Loss = -12415.34751103431
Iteration 12400: Loss = -12415.297426068306
Iteration 12500: Loss = -12415.290454195208
Iteration 12600: Loss = -12415.1848312123
Iteration 12700: Loss = -12415.16966854981
Iteration 12800: Loss = -12415.16369361544
Iteration 12900: Loss = -12415.165202780248
1
Iteration 13000: Loss = -12415.184704163466
2
Iteration 13100: Loss = -12415.162813027804
Iteration 13200: Loss = -12415.19743697939
1
Iteration 13300: Loss = -12415.162740610409
Iteration 13400: Loss = -12415.1987598029
1
Iteration 13500: Loss = -12415.16267878015
Iteration 13600: Loss = -12415.16267806598
Iteration 13700: Loss = -12415.162659138185
Iteration 13800: Loss = -12415.163938947799
1
Iteration 13900: Loss = -12415.162565984903
Iteration 14000: Loss = -12415.187567609211
1
Iteration 14100: Loss = -12415.162472256981
Iteration 14200: Loss = -12415.173013394624
1
Iteration 14300: Loss = -12415.162414048602
Iteration 14400: Loss = -12415.171350664648
1
Iteration 14500: Loss = -12415.16234437862
Iteration 14600: Loss = -12415.163360301487
1
Iteration 14700: Loss = -12415.162283066908
Iteration 14800: Loss = -12415.16312330414
1
Iteration 14900: Loss = -12415.16229587403
2
Iteration 15000: Loss = -12415.1623475324
3
Iteration 15100: Loss = -12415.163077162733
4
Iteration 15200: Loss = -12415.162233504563
Iteration 15300: Loss = -12415.179461261388
1
Iteration 15400: Loss = -12415.162227528928
Iteration 15500: Loss = -12415.18492435259
1
Iteration 15600: Loss = -12415.162224520778
Iteration 15700: Loss = -12415.22057208812
1
Iteration 15800: Loss = -12415.162175339794
Iteration 15900: Loss = -12415.16342022187
1
Iteration 16000: Loss = -12415.162217119387
2
Iteration 16100: Loss = -12415.21550950677
3
Iteration 16200: Loss = -12415.162120616877
Iteration 16300: Loss = -12415.332335028183
1
Iteration 16400: Loss = -12415.162120996427
2
Iteration 16500: Loss = -12415.1626747998
3
Iteration 16600: Loss = -12415.163398911556
4
Iteration 16700: Loss = -12415.162156123004
5
Iteration 16800: Loss = -12415.165934973233
6
Iteration 16900: Loss = -12415.166772920455
7
Iteration 17000: Loss = -12415.16212832446
8
Iteration 17100: Loss = -12415.162189607498
9
Iteration 17200: Loss = -12415.183731217905
10
Stopping early at iteration 17200 due to no improvement.
tensor([[-0.2265, -2.0634],
        [ 0.2004, -1.6210],
        [-0.0722, -2.2156],
        [-0.0477, -1.5576],
        [-0.0096, -1.4500],
        [-0.3125, -1.6097],
        [-0.9939, -2.6560],
        [ 0.0147, -1.4011],
        [-0.4078, -1.9181],
        [ 0.0913, -1.4800],
        [-0.1241, -1.7854],
        [-0.0477, -1.6321],
        [-0.5760, -2.6758],
        [-0.2156, -1.8806],
        [ 0.3258, -1.7342],
        [-0.0830, -1.3969],
        [-0.2257, -1.1759],
        [-0.0822, -1.3738],
        [ 0.0314, -1.6583],
        [-1.5923, -3.0229],
        [ 0.0044, -1.8456],
        [-0.0907, -1.7530],
        [-0.4664, -1.7856],
        [-0.3871, -1.6084],
        [-0.5262, -1.9728],
        [-0.3607, -1.4851],
        [ 0.1925, -1.6917],
        [ 0.2730, -1.7889],
        [ 0.1027, -1.4944],
        [-0.1277, -1.5693],
        [-0.0935, -1.7028],
        [-0.3485, -1.0782],
        [ 0.0350, -1.4427],
        [-0.5567, -1.8078],
        [-0.0924, -1.5384],
        [ 0.2349, -1.6783],
        [-0.5906, -2.1855],
        [-0.2933, -1.5161],
        [ 0.1080, -1.6203],
        [ 0.0484, -1.5988],
        [-0.5901, -2.6847],
        [ 0.0788, -1.6173],
        [-1.8033, -2.8119],
        [-0.0153, -1.8823],
        [ 0.0492, -1.4401],
        [ 0.0438, -1.4366],
        [-0.3399, -1.1280],
        [ 0.1053, -1.5106],
        [-0.5626, -1.9445],
        [-0.2146, -1.2917],
        [-0.5068, -1.7461],
        [-0.1362, -1.2711],
        [ 0.0748, -1.4802],
        [-0.4573, -1.8920],
        [-0.0847, -1.6311],
        [-0.2397, -1.6542],
        [ 0.0070, -1.3938],
        [-0.3928, -2.0327],
        [ 0.0518, -1.5318],
        [-1.1488, -2.7188],
        [-0.1414, -2.0827],
        [-0.8475, -1.7547],
        [-0.1269, -1.4623],
        [-0.7232, -2.3667],
        [ 0.1351, -1.5522],
        [-0.5976, -2.0573],
        [ 0.0729, -1.4776],
        [-0.1056, -1.4889],
        [-0.1925, -1.3692],
        [-0.4713, -2.4380],
        [ 0.0769, -1.5123],
        [-0.4746, -1.5699],
        [ 0.3160, -1.7979],
        [-0.1941, -1.2321],
        [ 0.1318, -1.5283],
        [-0.2987, -1.1369],
        [-0.1452, -1.4163],
        [ 0.2348, -1.6494],
        [-0.5742, -2.7214],
        [-0.0511, -1.5839],
        [-0.1813, -1.9365],
        [ 0.1107, -1.6804],
        [-0.2888, -1.4838],
        [-1.1740, -2.9551],
        [-0.5555, -2.1523],
        [-0.4788, -2.1651],
        [ 0.4043, -1.7941],
        [-0.2238, -1.3245],
        [ 0.1387, -1.6376],
        [ 0.2983, -1.6849],
        [ 0.1259, -1.9257],
        [-0.0712, -1.3795],
        [-0.2281, -1.9399],
        [-0.2849, -2.3732],
        [-0.2681, -1.8218],
        [-0.2727, -1.7630],
        [-0.0311, -1.8210],
        [-0.0770, -1.3314],
        [-0.0746, -1.7222],
        [-0.3798, -1.4397]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.9996e-01, 3.8903e-05],
        [4.2689e-01, 5.7311e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8190, 0.1810], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1988, 0.2096],
         [0.1890, 0.2202]],

        [[0.6004, 0.2116],
         [0.5806, 0.8519]],

        [[0.6022, 0.2194],
         [0.8950, 0.6711]],

        [[0.1305, 0.1910],
         [0.4707, 0.4228]],

        [[0.2455, 0.2101],
         [0.5886, 0.8663]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -24920.18403113776
Iteration 10: Loss = -12416.025650656493
Iteration 20: Loss = -12415.643044052773
Iteration 30: Loss = -12415.595648552051
Iteration 40: Loss = -12415.587423810164
Iteration 50: Loss = -12415.585939962995
Iteration 60: Loss = -12415.585597754587
Iteration 70: Loss = -12415.585562872797
Iteration 80: Loss = -12415.585506289966
Iteration 90: Loss = -12415.585536936907
1
Iteration 100: Loss = -12415.585508751938
2
Iteration 110: Loss = -12415.585500551597
Iteration 120: Loss = -12415.585469130943
Iteration 130: Loss = -12415.585505382274
1
Iteration 140: Loss = -12415.585458195574
Iteration 150: Loss = -12415.585458461648
1
Iteration 160: Loss = -12415.585408107974
Iteration 170: Loss = -12415.585443914004
1
Iteration 180: Loss = -12415.585398147166
Iteration 190: Loss = -12415.585417012095
1
Iteration 200: Loss = -12415.585435173925
2
Iteration 210: Loss = -12415.585432768194
3
Stopping early at iteration 209 due to no improvement.
pi: tensor([[0.3002, 0.6998],
        [0.3138, 0.6862]], dtype=torch.float64)
alpha: tensor([0.3096, 0.6904])
beta: tensor([[[0.1988, 0.2013],
         [0.8668, 0.1989]],

        [[0.5643, 0.2010],
         [0.9624, 0.6104]],

        [[0.6347, 0.1980],
         [0.7879, 0.0619]],

        [[0.6157, 0.1958],
         [0.4512, 0.2615]],

        [[0.7853, 0.1981],
         [0.5798, 0.7360]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24920.002213386295
Iteration 100: Loss = -12447.572079390005
Iteration 200: Loss = -12127.721317197573
Iteration 300: Loss = -12115.43717810377
Iteration 400: Loss = -12113.49754374316
Iteration 500: Loss = -12112.465158118323
Iteration 600: Loss = -12111.837884450877
Iteration 700: Loss = -12111.424551604485
Iteration 800: Loss = -12111.136075826504
Iteration 900: Loss = -12110.925984400048
Iteration 1000: Loss = -12110.767684821514
Iteration 1100: Loss = -12110.645344252775
Iteration 1200: Loss = -12110.548789658456
Iteration 1300: Loss = -12110.471134904858
Iteration 1400: Loss = -12110.407755185228
Iteration 1500: Loss = -12110.355213690047
Iteration 1600: Loss = -12110.311289462128
Iteration 1700: Loss = -12110.274133261035
Iteration 1800: Loss = -12110.242447992074
Iteration 1900: Loss = -12110.215276571744
Iteration 2000: Loss = -12110.191664444405
Iteration 2100: Loss = -12110.171140028575
Iteration 2200: Loss = -12110.153213158994
Iteration 2300: Loss = -12110.137406509002
Iteration 2400: Loss = -12110.123494448797
Iteration 2500: Loss = -12110.111136413161
Iteration 2600: Loss = -12110.100165296493
Iteration 2700: Loss = -12110.090350813953
Iteration 2800: Loss = -12110.081559512888
Iteration 2900: Loss = -12110.073676625012
Iteration 3000: Loss = -12110.066526174433
Iteration 3100: Loss = -12110.060123302828
Iteration 3200: Loss = -12110.054297420736
Iteration 3300: Loss = -12110.0489644107
Iteration 3400: Loss = -12110.044298147508
Iteration 3500: Loss = -12110.039719822485
Iteration 3600: Loss = -12110.035738028555
Iteration 3700: Loss = -12110.04629069028
1
Iteration 3800: Loss = -12110.028744756464
Iteration 3900: Loss = -12110.025684176706
Iteration 4000: Loss = -12110.022855425852
Iteration 4100: Loss = -12110.020238846273
Iteration 4200: Loss = -12110.01783182036
Iteration 4300: Loss = -12110.015719958545
Iteration 4400: Loss = -12110.01353059974
Iteration 4500: Loss = -12110.011623055972
Iteration 4600: Loss = -12110.009864197866
Iteration 4700: Loss = -12110.00826009316
Iteration 4800: Loss = -12110.006812451824
Iteration 4900: Loss = -12110.005534341108
Iteration 5000: Loss = -12110.004373177475
Iteration 5100: Loss = -12110.003279986018
Iteration 5200: Loss = -12110.006616204226
1
Iteration 5300: Loss = -12110.00587977805
2
Iteration 5400: Loss = -12110.00063895894
Iteration 5500: Loss = -12109.99992639508
Iteration 5600: Loss = -12109.999264354185
Iteration 5700: Loss = -12109.998643620564
Iteration 5800: Loss = -12109.998100495906
Iteration 5900: Loss = -12109.997733657106
Iteration 6000: Loss = -12109.997115277143
Iteration 6100: Loss = -12109.996970899789
Iteration 6200: Loss = -12109.996562206943
Iteration 6300: Loss = -12109.996195396729
Iteration 6400: Loss = -12109.995750536065
Iteration 6500: Loss = -12109.995263042036
Iteration 6600: Loss = -12109.995798553267
1
Iteration 6700: Loss = -12109.999460972638
2
Iteration 6800: Loss = -12109.99439807095
Iteration 6900: Loss = -12109.994367874497
Iteration 7000: Loss = -12109.993863628028
Iteration 7100: Loss = -12109.993688282762
Iteration 7200: Loss = -12109.99328304402
Iteration 7300: Loss = -12110.00193279626
1
Iteration 7400: Loss = -12109.99281833783
Iteration 7500: Loss = -12109.992862079362
1
Iteration 7600: Loss = -12109.99242714954
Iteration 7700: Loss = -12110.002471243428
1
Iteration 7800: Loss = -12109.992817992386
2
Iteration 7900: Loss = -12110.023114010419
3
Iteration 8000: Loss = -12109.991337458436
Iteration 8100: Loss = -12109.992077217436
1
Iteration 8200: Loss = -12109.996073158496
2
Iteration 8300: Loss = -12109.99018977173
Iteration 8400: Loss = -12109.990093981889
Iteration 8500: Loss = -12109.988913744208
Iteration 8600: Loss = -12109.987706103217
Iteration 8700: Loss = -12110.237111359298
1
Iteration 8800: Loss = -12109.980023426966
Iteration 8900: Loss = -12109.971120533533
Iteration 9000: Loss = -12107.914516488561
Iteration 9100: Loss = -12100.922694420871
Iteration 9200: Loss = -12081.688477300539
Iteration 9300: Loss = -12015.901282584493
Iteration 9400: Loss = -11945.04791931399
Iteration 9500: Loss = -11932.966573102805
Iteration 9600: Loss = -11932.95306479138
Iteration 9700: Loss = -11932.981126771603
1
Iteration 9800: Loss = -11932.94902577896
Iteration 9900: Loss = -11932.951953909549
1
Iteration 10000: Loss = -11932.950013572115
2
Iteration 10100: Loss = -11932.944060752188
Iteration 10200: Loss = -11932.945045964907
1
Iteration 10300: Loss = -11933.014642119093
2
Iteration 10400: Loss = -11932.965469015351
3
Iteration 10500: Loss = -11932.948960477874
4
Iteration 10600: Loss = -11932.951428320379
5
Iteration 10700: Loss = -11932.943436462734
Iteration 10800: Loss = -11932.94442815495
1
Iteration 10900: Loss = -11932.9455916418
2
Iteration 11000: Loss = -11932.963052345936
3
Iteration 11100: Loss = -11932.943953860648
4
Iteration 11200: Loss = -11932.941906895856
Iteration 11300: Loss = -11932.944102644326
1
Iteration 11400: Loss = -11932.947271574243
2
Iteration 11500: Loss = -11932.941795222747
Iteration 11600: Loss = -11932.941935756537
1
Iteration 11700: Loss = -11932.941171404877
Iteration 11800: Loss = -11932.941545896221
1
Iteration 11900: Loss = -11932.952373029892
2
Iteration 12000: Loss = -11932.947553095535
3
Iteration 12100: Loss = -11932.944138992687
4
Iteration 12200: Loss = -11932.942808370568
5
Iteration 12300: Loss = -11932.94598875825
6
Iteration 12400: Loss = -11932.95124323935
7
Iteration 12500: Loss = -11932.943261818926
8
Iteration 12600: Loss = -11932.941054570116
Iteration 12700: Loss = -11926.269760898396
Iteration 12800: Loss = -11926.269796516197
1
Iteration 12900: Loss = -11926.303611937446
2
Iteration 13000: Loss = -11926.2962012607
3
Iteration 13100: Loss = -11926.35387206704
4
Iteration 13200: Loss = -11926.268999248368
Iteration 13300: Loss = -11926.27392092952
1
Iteration 13400: Loss = -11926.27254290291
2
Iteration 13500: Loss = -11926.356052292316
3
Iteration 13600: Loss = -11926.295346576957
4
Iteration 13700: Loss = -11926.271735958899
5
Iteration 13800: Loss = -11926.294454899866
6
Iteration 13900: Loss = -11926.270631430867
7
Iteration 14000: Loss = -11926.268113285681
Iteration 14100: Loss = -11926.294719976024
1
Iteration 14200: Loss = -11926.268079840687
Iteration 14300: Loss = -11926.26807686046
Iteration 14400: Loss = -11926.267846208808
Iteration 14500: Loss = -11926.26975911485
1
Iteration 14600: Loss = -11926.273063382712
2
Iteration 14700: Loss = -11926.270242385213
3
Iteration 14800: Loss = -11926.268753295952
4
Iteration 14900: Loss = -11926.270546049105
5
Iteration 15000: Loss = -11926.322156712547
6
Iteration 15100: Loss = -11926.286982160067
7
Iteration 15200: Loss = -11926.278437287563
8
Iteration 15300: Loss = -11926.290707875645
9
Iteration 15400: Loss = -11926.276575326143
10
Stopping early at iteration 15400 due to no improvement.
tensor([[-7.6163,  6.2101],
        [ 5.0940, -6.5226],
        [ 3.7871, -5.4948],
        [ 6.8514, -8.2613],
        [ 5.1740, -9.3203],
        [ 6.2066, -7.6806],
        [-0.9281, -0.5289],
        [-7.8112,  6.2614],
        [ 5.8665, -7.2547],
        [ 6.9523, -8.7169],
        [-7.5276,  6.1337],
        [-2.8435,  1.3879],
        [-7.8110,  6.4168],
        [-7.4915,  6.0882],
        [ 6.2306, -7.6175],
        [ 4.9038, -6.3787],
        [-6.2977,  3.2354],
        [ 6.2788, -7.8399],
        [-6.2441,  4.7198],
        [ 3.7042, -8.3194],
        [ 5.5682, -7.5945],
        [ 6.0489, -7.6200],
        [ 6.4134, -7.9870],
        [-8.1589,  6.7423],
        [-7.7300,  5.9433],
        [ 5.9113, -7.8604],
        [-5.4497,  3.6690],
        [-6.3811,  4.8327],
        [-8.1167,  6.7029],
        [-7.9986,  3.3833],
        [-8.3377,  6.4286],
        [-3.6001,  1.9474],
        [-8.5582,  6.9460],
        [ 6.2533, -8.1084],
        [ 5.7083, -7.1535],
        [-6.9711,  5.4325],
        [ 5.5664, -7.5449],
        [-8.9836,  6.5376],
        [ 5.5876, -7.0395],
        [-7.8249,  6.3787],
        [ 5.9949, -7.7922],
        [ 6.4997, -8.0019],
        [ 6.4400, -8.0151],
        [-7.0727,  5.1878],
        [ 6.3137, -8.1185],
        [-5.2116,  3.6743],
        [ 6.4977, -8.0192],
        [-7.6465,  5.9138],
        [-4.9620,  3.5681],
        [-7.3099,  4.9585],
        [ 6.6361, -8.0544],
        [ 6.4218, -8.0603],
        [-8.1425,  6.6971],
        [-8.4337,  6.3832],
        [-5.1312,  0.6179],
        [ 5.4126, -9.3201],
        [ 5.5635, -7.1210],
        [ 5.5407, -7.2848],
        [-7.7587,  5.8954],
        [ 2.8315, -4.2534],
        [-6.8799,  5.2771],
        [ 6.3665, -8.2904],
        [ 6.2027, -7.5979],
        [-5.6262,  4.2331],
        [ 5.8868, -8.1280],
        [ 5.9332, -7.4354],
        [ 2.9786, -5.0176],
        [ 5.1717, -8.9365],
        [ 7.0205, -8.8347],
        [ 5.8156, -8.2268],
        [-6.5029,  5.0563],
        [ 7.0785, -8.4869],
        [-5.8115,  4.2610],
        [-8.3875,  7.0012],
        [ 4.1792, -5.9130],
        [ 6.8553, -8.6305],
        [-5.7991,  4.3996],
        [ 6.1977, -7.8749],
        [-6.1808,  4.7865],
        [ 6.1725, -9.0340],
        [ 0.5660, -1.9556],
        [-6.9927,  5.2808],
        [ 6.7531, -8.3500],
        [ 1.7665, -3.5058],
        [-4.3585,  2.5410],
        [-7.7882,  6.2345],
        [-3.7299,  2.1233],
        [-8.5572,  6.6348],
        [ 5.9979, -7.3920],
        [-7.0781,  5.2042],
        [-5.7198,  3.9158],
        [ 6.2259, -8.1033],
        [-8.1079,  6.6244],
        [-4.0338,  2.1936],
        [-6.7237,  2.8470],
        [-8.1923,  5.8128],
        [ 5.2994, -6.6998],
        [-8.3977,  5.5618],
        [ 5.0102, -6.3977],
        [-9.2756,  6.2980]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.2811, 0.7189],
        [0.7866, 0.2134]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5046, 0.4954], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3009, 0.1003],
         [0.8668, 0.2989]],

        [[0.5643, 0.1063],
         [0.9624, 0.6104]],

        [[0.6347, 0.1090],
         [0.7879, 0.0619]],

        [[0.6157, 0.1034],
         [0.4512, 0.2615]],

        [[0.7853, 0.0951],
         [0.5798, 0.7360]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208
time is 1
tensor([0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 3
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.03341076102402177
Average Adjusted Rand Index: 0.9761595611635631
Iteration 0: Loss = -16701.058799793864
Iteration 10: Loss = -12415.701121971171
Iteration 20: Loss = -12415.698881127835
Iteration 30: Loss = -12415.697891542237
Iteration 40: Loss = -12415.696980725683
Iteration 50: Loss = -12415.696134704915
Iteration 60: Loss = -12415.695323295584
Iteration 70: Loss = -12415.694466011942
Iteration 80: Loss = -12415.693574251442
Iteration 90: Loss = -12415.692773420875
Iteration 100: Loss = -12415.69184700728
Iteration 110: Loss = -12415.69089111105
Iteration 120: Loss = -12415.68991749329
Iteration 130: Loss = -12415.688925638378
Iteration 140: Loss = -12415.687852982557
Iteration 150: Loss = -12415.686922518229
Iteration 160: Loss = -12415.686173854903
Iteration 170: Loss = -12415.685567079305
Iteration 180: Loss = -12415.685040639184
Iteration 190: Loss = -12415.684668030412
Iteration 200: Loss = -12415.684312112491
Iteration 210: Loss = -12415.683944324091
Iteration 220: Loss = -12415.683692585646
Iteration 230: Loss = -12415.683409530504
Iteration 240: Loss = -12415.683214462853
Iteration 250: Loss = -12415.682968220528
Iteration 260: Loss = -12415.68279469949
Iteration 270: Loss = -12415.682596566508
Iteration 280: Loss = -12415.682441440516
Iteration 290: Loss = -12415.682278754166
Iteration 300: Loss = -12415.68215390892
Iteration 310: Loss = -12415.682042528282
Iteration 320: Loss = -12415.681901453154
Iteration 330: Loss = -12415.681816393582
Iteration 340: Loss = -12415.681646251007
Iteration 350: Loss = -12415.681602160355
Iteration 360: Loss = -12415.68147009174
Iteration 370: Loss = -12415.681399462175
Iteration 380: Loss = -12415.68132927086
Iteration 390: Loss = -12415.681260364216
Iteration 400: Loss = -12415.681209081735
Iteration 410: Loss = -12415.681130159424
Iteration 420: Loss = -12415.68109881311
Iteration 430: Loss = -12415.681048304168
Iteration 440: Loss = -12415.680976432532
Iteration 450: Loss = -12415.680906748632
Iteration 460: Loss = -12415.680916352392
1
Iteration 470: Loss = -12415.68086376994
Iteration 480: Loss = -12415.680776604157
Iteration 490: Loss = -12415.680798647614
1
Iteration 500: Loss = -12415.680733581357
Iteration 510: Loss = -12415.680673119039
Iteration 520: Loss = -12415.680643243564
Iteration 530: Loss = -12415.680601174305
Iteration 540: Loss = -12415.680585827895
Iteration 550: Loss = -12415.68052471851
Iteration 560: Loss = -12415.680497766905
Iteration 570: Loss = -12415.680509698197
1
Iteration 580: Loss = -12415.680411826776
Iteration 590: Loss = -12415.680489557266
1
Iteration 600: Loss = -12415.680419011753
2
Iteration 610: Loss = -12415.680406981359
Iteration 620: Loss = -12415.680382014109
Iteration 630: Loss = -12415.68030978941
Iteration 640: Loss = -12415.68028234326
Iteration 650: Loss = -12415.68027363286
Iteration 660: Loss = -12415.680223045063
Iteration 670: Loss = -12415.680209259597
Iteration 680: Loss = -12415.680189123868
Iteration 690: Loss = -12415.680139759364
Iteration 700: Loss = -12415.680050906813
Iteration 710: Loss = -12415.679995511444
Iteration 720: Loss = -12415.679990927905
Iteration 730: Loss = -12415.679873857121
Iteration 740: Loss = -12415.679789871207
Iteration 750: Loss = -12415.67972832
Iteration 760: Loss = -12415.679597262673
Iteration 770: Loss = -12415.679419195087
Iteration 780: Loss = -12415.679239529916
Iteration 790: Loss = -12415.679013945082
Iteration 800: Loss = -12415.678740377325
Iteration 810: Loss = -12415.67833769626
Iteration 820: Loss = -12415.677943084858
Iteration 830: Loss = -12415.67754120975
Iteration 840: Loss = -12415.677130750837
Iteration 850: Loss = -12415.676793325052
Iteration 860: Loss = -12415.676436985359
Iteration 870: Loss = -12415.676094748738
Iteration 880: Loss = -12415.675787532151
Iteration 890: Loss = -12415.675499895615
Iteration 900: Loss = -12415.675262754785
Iteration 910: Loss = -12415.674933331167
Iteration 920: Loss = -12415.674662240941
Iteration 930: Loss = -12415.674386011284
Iteration 940: Loss = -12415.674178920186
Iteration 950: Loss = -12415.67389462205
Iteration 960: Loss = -12415.673684077932
Iteration 970: Loss = -12415.673426703435
Iteration 980: Loss = -12415.673234105114
Iteration 990: Loss = -12415.672995460074
Iteration 1000: Loss = -12415.672790390487
Iteration 1010: Loss = -12415.672655567205
Iteration 1020: Loss = -12415.67247350163
Iteration 1030: Loss = -12415.67231152922
Iteration 1040: Loss = -12415.672127187334
Iteration 1050: Loss = -12415.67195206785
Iteration 1060: Loss = -12415.671827770473
Iteration 1070: Loss = -12415.671681273208
Iteration 1080: Loss = -12415.671573946516
Iteration 1090: Loss = -12415.67147587441
Iteration 1100: Loss = -12415.671340546929
Iteration 1110: Loss = -12415.671205123614
Iteration 1120: Loss = -12415.671098059925
Iteration 1130: Loss = -12415.671057886184
Iteration 1140: Loss = -12415.670961743423
Iteration 1150: Loss = -12415.670829208128
Iteration 1160: Loss = -12415.670774879638
Iteration 1170: Loss = -12415.670687424452
Iteration 1180: Loss = -12415.670540203106
Iteration 1190: Loss = -12415.670521047203
Iteration 1200: Loss = -12415.670466003747
Iteration 1210: Loss = -12415.670383330777
Iteration 1220: Loss = -12415.670268790805
Iteration 1230: Loss = -12415.670200482915
Iteration 1240: Loss = -12415.670121866267
Iteration 1250: Loss = -12415.6700658388
Iteration 1260: Loss = -12415.670057760048
Iteration 1270: Loss = -12415.669979801876
Iteration 1280: Loss = -12415.669950720665
Iteration 1290: Loss = -12415.669904797402
Iteration 1300: Loss = -12415.669816542735
Iteration 1310: Loss = -12415.669806131924
Iteration 1320: Loss = -12415.669740857851
Iteration 1330: Loss = -12415.669705284157
Iteration 1340: Loss = -12415.66963780373
Iteration 1350: Loss = -12415.669622752384
Iteration 1360: Loss = -12415.669607646343
Iteration 1370: Loss = -12415.669527917807
Iteration 1380: Loss = -12415.669497167375
Iteration 1390: Loss = -12415.66943300955
Iteration 1400: Loss = -12415.669430205038
Iteration 1410: Loss = -12415.669381115817
Iteration 1420: Loss = -12415.669349491083
Iteration 1430: Loss = -12415.669354791344
1
Iteration 1440: Loss = -12415.669308026356
Iteration 1450: Loss = -12415.669257860514
Iteration 1460: Loss = -12415.669283502348
1
Iteration 1470: Loss = -12415.669238719296
Iteration 1480: Loss = -12415.669193205684
Iteration 1490: Loss = -12415.669183327693
Iteration 1500: Loss = -12415.669141434904
Iteration 1510: Loss = -12415.669118517813
Iteration 1520: Loss = -12415.669126191558
1
Iteration 1530: Loss = -12415.669068908404
Iteration 1540: Loss = -12415.669076966607
1
Iteration 1550: Loss = -12415.669053629432
Iteration 1560: Loss = -12415.669024140532
Iteration 1570: Loss = -12415.669047361514
1
Iteration 1580: Loss = -12415.668956154086
Iteration 1590: Loss = -12415.668966542273
1
Iteration 1600: Loss = -12415.668918967087
Iteration 1610: Loss = -12415.668928250218
1
Iteration 1620: Loss = -12415.668912621319
Iteration 1630: Loss = -12415.668935233209
1
Iteration 1640: Loss = -12415.668868673054
Iteration 1650: Loss = -12415.668880913172
1
Iteration 1660: Loss = -12415.668823355767
Iteration 1670: Loss = -12415.668849809328
1
Iteration 1680: Loss = -12415.668818926128
Iteration 1690: Loss = -12415.668789679403
Iteration 1700: Loss = -12415.66881079788
1
Iteration 1710: Loss = -12415.66876294658
Iteration 1720: Loss = -12415.668765619012
1
Iteration 1730: Loss = -12415.668761917259
Iteration 1740: Loss = -12415.668746299474
Iteration 1750: Loss = -12415.668780629767
1
Iteration 1760: Loss = -12415.66871933066
Iteration 1770: Loss = -12415.6686824459
Iteration 1780: Loss = -12415.668739889526
1
Iteration 1790: Loss = -12415.668683453256
2
Iteration 1800: Loss = -12415.668687668085
3
Stopping early at iteration 1799 due to no improvement.
pi: tensor([[0.9888, 0.0112],
        [0.9421, 0.0579]], dtype=torch.float64)
alpha: tensor([0.9882, 0.0118])
beta: tensor([[[0.1986, 0.2280],
         [0.4004, 0.2242]],

        [[0.5063, 0.2083],
         [0.3629, 0.8571]],

        [[0.0246, 0.2312],
         [0.8014, 0.4194]],

        [[0.1087, 0.1898],
         [0.4628, 0.7404]],

        [[0.6394, 0.1975],
         [0.5927, 0.0149]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -16701.38620726447
Iteration 100: Loss = -12416.286623142709
Iteration 200: Loss = -12415.642197559704
Iteration 300: Loss = -12415.519089096231
Iteration 400: Loss = -12415.459755172953
Iteration 500: Loss = -12415.42184289241
Iteration 600: Loss = -12415.394151049508
Iteration 700: Loss = -12415.375343012774
Iteration 800: Loss = -12415.360205367411
Iteration 900: Loss = -12415.345500082092
Iteration 1000: Loss = -12415.32715609851
Iteration 1100: Loss = -12415.298678528025
Iteration 1200: Loss = -12415.252077695088
Iteration 1300: Loss = -12415.192556473088
Iteration 1400: Loss = -12415.14333284787
Iteration 1500: Loss = -12415.112201913298
Iteration 1600: Loss = -12415.091835160414
Iteration 1700: Loss = -12415.07739053334
Iteration 1800: Loss = -12415.066477070202
Iteration 1900: Loss = -12415.057710261042
Iteration 2000: Loss = -12415.050369620078
Iteration 2100: Loss = -12415.043752398149
Iteration 2200: Loss = -12415.03762622356
Iteration 2300: Loss = -12415.03180130004
Iteration 2400: Loss = -12415.026185408187
Iteration 2500: Loss = -12415.020906601332
Iteration 2600: Loss = -12415.01602527473
Iteration 2700: Loss = -12415.011673810493
Iteration 2800: Loss = -12415.007942066033
Iteration 2900: Loss = -12415.004750424925
Iteration 3000: Loss = -12415.002045365234
Iteration 3100: Loss = -12414.999742951957
Iteration 3200: Loss = -12414.997801287911
Iteration 3300: Loss = -12414.996097091574
Iteration 3400: Loss = -12414.994578202599
Iteration 3500: Loss = -12414.99324238803
Iteration 3600: Loss = -12414.992022880648
Iteration 3700: Loss = -12414.99095585184
Iteration 3800: Loss = -12414.98991942591
Iteration 3900: Loss = -12414.989022547934
Iteration 4000: Loss = -12414.988186133221
Iteration 4100: Loss = -12414.98739639226
Iteration 4200: Loss = -12414.986692103199
Iteration 4300: Loss = -12414.985990721296
Iteration 4400: Loss = -12414.985389153631
Iteration 4500: Loss = -12414.98474581286
Iteration 4600: Loss = -12414.984197009873
Iteration 4700: Loss = -12414.983681229687
Iteration 4800: Loss = -12414.983405614954
Iteration 4900: Loss = -12414.982724661286
Iteration 5000: Loss = -12414.982320910163
Iteration 5100: Loss = -12414.981918105866
Iteration 5200: Loss = -12414.981472942574
Iteration 5300: Loss = -12414.98113719156
Iteration 5400: Loss = -12414.985328395753
1
Iteration 5500: Loss = -12414.980434250083
Iteration 5600: Loss = -12414.987728658172
1
Iteration 5700: Loss = -12414.979806640664
Iteration 5800: Loss = -12414.979520762583
Iteration 5900: Loss = -12415.198045820021
1
Iteration 6000: Loss = -12414.979059318524
Iteration 6100: Loss = -12414.978806582514
Iteration 6200: Loss = -12414.978569595021
Iteration 6300: Loss = -12414.978541337903
Iteration 6400: Loss = -12414.97814895425
Iteration 6500: Loss = -12414.97796803441
Iteration 6600: Loss = -12414.980036322107
1
Iteration 6700: Loss = -12414.9775921283
Iteration 6800: Loss = -12414.977441084819
Iteration 6900: Loss = -12415.360351113566
1
Iteration 7000: Loss = -12414.977187448058
Iteration 7100: Loss = -12414.977047508866
Iteration 7200: Loss = -12414.97690004877
Iteration 7300: Loss = -12414.976732152563
Iteration 7400: Loss = -12414.976686624297
Iteration 7500: Loss = -12414.976531961693
Iteration 7600: Loss = -12414.978682841056
1
Iteration 7700: Loss = -12414.976306021232
Iteration 7800: Loss = -12414.976209400462
Iteration 7900: Loss = -12415.00236355525
1
Iteration 8000: Loss = -12414.976050000352
Iteration 8100: Loss = -12414.975967606373
Iteration 8200: Loss = -12414.97788909848
1
Iteration 8300: Loss = -12414.975814244426
Iteration 8400: Loss = -12414.97577105479
Iteration 8500: Loss = -12414.975663169067
Iteration 8600: Loss = -12414.975717324822
1
Iteration 8700: Loss = -12414.97555692019
Iteration 8800: Loss = -12414.975491592855
Iteration 8900: Loss = -12414.979370619654
1
Iteration 9000: Loss = -12414.97541342304
Iteration 9100: Loss = -12414.975341079682
Iteration 9200: Loss = -12414.990318994256
1
Iteration 9300: Loss = -12414.975270323754
Iteration 9400: Loss = -12414.975218388277
Iteration 9500: Loss = -12414.97620138205
1
Iteration 9600: Loss = -12414.975152751638
Iteration 9700: Loss = -12414.975139515518
Iteration 9800: Loss = -12415.335925757663
1
Iteration 9900: Loss = -12414.975068958782
Iteration 10000: Loss = -12414.975027625864
Iteration 10100: Loss = -12414.974985579543
Iteration 10200: Loss = -12414.976286463096
1
Iteration 10300: Loss = -12414.974965861717
Iteration 10400: Loss = -12414.974939959779
Iteration 10500: Loss = -12414.979618405445
1
Iteration 10600: Loss = -12414.974919922099
Iteration 10700: Loss = -12414.974865709732
Iteration 10800: Loss = -12414.975720364379
1
Iteration 10900: Loss = -12414.974831657142
Iteration 11000: Loss = -12414.974824297407
Iteration 11100: Loss = -12414.974785311755
Iteration 11200: Loss = -12414.974903677772
1
Iteration 11300: Loss = -12414.97479608331
2
Iteration 11400: Loss = -12414.975184511228
3
Iteration 11500: Loss = -12414.974753532064
Iteration 11600: Loss = -12414.974694533303
Iteration 11700: Loss = -12415.030225681592
1
Iteration 11800: Loss = -12414.974737554985
2
Iteration 11900: Loss = -12414.974672278542
Iteration 12000: Loss = -12415.10425022376
1
Iteration 12100: Loss = -12414.974685441615
2
Iteration 12200: Loss = -12414.977004083632
3
Iteration 12300: Loss = -12414.974653173069
Iteration 12400: Loss = -12414.986802677658
1
Iteration 12500: Loss = -12414.974615448129
Iteration 12600: Loss = -12415.070767462134
1
Iteration 12700: Loss = -12414.974620642914
2
Iteration 12800: Loss = -12414.974605711504
Iteration 12900: Loss = -12414.974720831662
1
Iteration 13000: Loss = -12414.974595450562
Iteration 13100: Loss = -12414.996682705148
1
Iteration 13200: Loss = -12414.974595891128
2
Iteration 13300: Loss = -12414.975327927466
3
Iteration 13400: Loss = -12414.974595738273
4
Iteration 13500: Loss = -12414.97555836214
5
Iteration 13600: Loss = -12414.974583581627
Iteration 13700: Loss = -12414.97494659762
1
Iteration 13800: Loss = -12414.974581598197
Iteration 13900: Loss = -12414.974787221745
1
Iteration 14000: Loss = -12414.974580764858
Iteration 14100: Loss = -12414.989495133561
1
Iteration 14200: Loss = -12414.976017372332
2
Iteration 14300: Loss = -12415.094135842364
3
Iteration 14400: Loss = -12414.974891804426
4
Iteration 14500: Loss = -12414.974635739023
5
Iteration 14600: Loss = -12414.985744342699
6
Iteration 14700: Loss = -12414.97554212686
7
Iteration 14800: Loss = -12414.975332213864
8
Iteration 14900: Loss = -12414.987076201069
9
Iteration 15000: Loss = -12414.97458159891
10
Stopping early at iteration 15000 due to no improvement.
tensor([[ 2.2684, -4.1195],
        [ 2.2542, -4.0855],
        [ 2.1725, -4.3739],
        [ 2.2649, -3.9598],
        [ 2.4221, -3.8793],
        [ 2.3938, -3.7858],
        [ 2.4783, -3.8688],
        [ 2.0332, -4.3509],
        [ 2.5253, -3.9360],
        [ 2.1305, -4.1756],
        [ 2.1468, -3.9983],
        [ 2.3663, -4.1766],
        [ 2.5118, -3.9965],
        [ 2.2152, -4.1717],
        [ 2.5048, -3.9028],
        [ 2.3725, -3.8149],
        [ 2.3169, -3.7449],
        [ 2.3559, -3.9887],
        [ 2.3560, -4.1459],
        [ 1.8339, -4.3100],
        [ 2.0580, -4.3229],
        [ 2.4674, -3.8737],
        [ 2.3208, -3.8251],
        [ 1.4550, -4.6101],
        [ 0.7612, -5.3764],
        [ 1.7780, -4.2864],
        [ 2.4480, -4.0171],
        [ 2.2031, -4.1872],
        [ 2.3778, -3.7715],
        [ 2.1120, -4.2346],
        [ 2.4798, -3.8664],
        [ 2.4749, -3.8634],
        [ 1.9278, -4.2975],
        [ 2.3160, -3.8660],
        [ 2.3568, -4.1450],
        [ 2.2717, -4.0761],
        [ 2.4746, -3.8691],
        [ 2.4406, -3.9036],
        [ 2.0079, -4.4183],
        [ 2.1994, -3.9429],
        [ 1.9665, -4.4963],
        [ 2.3234, -4.0986],
        [ 1.0479, -5.2105],
        [ 2.2297, -4.0791],
        [ 1.3877, -4.7084],
        [ 2.4658, -3.8780],
        [ 1.9045, -4.0378],
        [ 2.4546, -3.8496],
        [ 1.6259, -4.7551],
        [ 2.3548, -3.9486],
        [ 1.7375, -4.3673],
        [ 1.5964, -4.4748],
        [ 2.2680, -4.0355],
        [ 2.3923, -3.8326],
        [ 2.1859, -4.1205],
        [ 1.6342, -4.5104],
        [ 1.9148, -4.3491],
        [ 0.9417, -5.5569],
        [ 1.9541, -4.3135],
        [ 1.9822, -4.3230],
        [ 1.6477, -4.7407],
        [ 1.8679, -4.1183],
        [ 1.5905, -4.7500],
        [ 0.9438, -5.4815],
        [ 2.0572, -4.2500],
        [ 2.3558, -3.8308],
        [ 2.3636, -3.8233],
        [ 2.2760, -4.0275],
        [ 1.0453, -5.0599],
        [ 2.4680, -3.8733],
        [ 1.8837, -4.5388],
        [ 2.0438, -4.0185],
        [ 2.4831, -4.0607],
        [ 2.3956, -3.7863],
        [ 2.0589, -4.2424],
        [ 1.5207, -4.3073],
        [ 2.3173, -3.7371],
        [ 1.9842, -4.4427],
        [ 1.7346, -4.7703],
        [ 2.0486, -4.0534],
        [ 2.3581, -4.1434],
        [ 1.8246, -4.5575],
        [ 2.1939, -4.1122],
        [ 1.6844, -4.6586],
        [ 2.4575, -4.0027],
        [ 2.5035, -3.9186],
        [ 2.4331, -4.1933],
        [ 2.3177, -3.8247],
        [ 2.4561, -3.9196],
        [ 2.3370, -4.1229],
        [ 2.4396, -4.1781],
        [ 2.2615, -3.9218],
        [ 2.3898, -3.8768],
        [ 2.5052, -3.9972],
        [ 2.3603, -3.8668],
        [ 1.9382, -4.3618],
        [ 2.3237, -3.9428],
        [ 0.8419, -5.4571],
        [ 1.6436, -4.8188],
        [ 2.2602, -3.8414]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.6396e-01, 3.6039e-02],
        [9.9991e-01, 8.9945e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9981, 0.0019], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2024, 0.2090],
         [0.4004, 0.1586]],

        [[0.5063, 0.2005],
         [0.3629, 0.8571]],

        [[0.0246, 0.1407],
         [0.8014, 0.4194]],

        [[0.1087, 0.1836],
         [0.4628, 0.7404]],

        [[0.6394, 0.1741],
         [0.5927, 0.0149]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
11934.672583504685
new:  [0.0, 0.0, 0.03341076102402177, 0.0] [0.0, 0.0, 0.9761595611635631, 0.0] [12415.370128211975, 12415.183731217905, 11926.276575326143, 12414.97458159891]
prior:  [0.0, 0.0, 0.0, 0.0] [0.0, 0.0, 0.0, 0.0] [12415.668640049927, nan, 12415.585432768194, 12415.668687668085]
-----------------------------------------------------------------------------------------
This iteration is 15
True Objective function: Loss = -11991.168779873846
Iteration 0: Loss = -25914.605085598505
Iteration 10: Loss = -12506.943881599715
Iteration 20: Loss = -12506.043541736528
Iteration 30: Loss = -12506.035841478273
Iteration 40: Loss = -12505.023323174402
Iteration 50: Loss = -12504.988536250487
Iteration 60: Loss = -12504.978034874634
Iteration 70: Loss = -12504.974708146297
Iteration 80: Loss = -12504.973647560684
Iteration 90: Loss = -12504.973297034061
Iteration 100: Loss = -12504.97312817132
Iteration 110: Loss = -12504.973122400572
Iteration 120: Loss = -12504.973066770917
Iteration 130: Loss = -12504.973077728371
1
Iteration 140: Loss = -12504.973082068522
2
Iteration 150: Loss = -12504.973050829536
Iteration 160: Loss = -12504.973064188274
1
Iteration 170: Loss = -12504.973079191212
2
Iteration 180: Loss = -12504.97307466175
3
Stopping early at iteration 179 due to no improvement.
pi: tensor([[0.9658, 0.0342],
        [0.8618, 0.1382]], dtype=torch.float64)
alpha: tensor([0.9620, 0.0380])
beta: tensor([[[0.1968, 0.2148],
         [0.1772, 0.2606]],

        [[0.1805, 0.3010],
         [0.6421, 0.0501]],

        [[0.7838, 0.2122],
         [0.9559, 0.6174]],

        [[0.8945, 0.2718],
         [0.2078, 0.9542]],

        [[0.6452, 0.3040],
         [0.2800, 0.8705]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: -0.0007365134907910085
Average Adjusted Rand Index: -0.002473148582144872
tensor([[-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25812.642863225607
Iteration 100: Loss = -12513.366796731112
Iteration 200: Loss = -12511.790461866953
Iteration 300: Loss = -12511.246121656633
Iteration 400: Loss = -12510.923973297417
Iteration 500: Loss = -12510.744697709766
Iteration 600: Loss = -12510.556762824233
Iteration 700: Loss = -12510.439383706123
Iteration 800: Loss = -12510.344503966371
Iteration 900: Loss = -12510.272379779926
Iteration 1000: Loss = -12510.205086748061
Iteration 1100: Loss = -12510.161279973194
Iteration 1200: Loss = -12510.132286817205
Iteration 1300: Loss = -12510.11420363597
Iteration 1400: Loss = -12510.103568481172
Iteration 1500: Loss = -12510.097384847868
Iteration 1600: Loss = -12510.095173102138
Iteration 1700: Loss = -12510.089139558995
Iteration 1800: Loss = -12510.084928992203
Iteration 1900: Loss = -12510.300085905852
1
Iteration 2000: Loss = -12510.07183394005
Iteration 2100: Loss = -12510.053682681715
Iteration 2200: Loss = -12509.804264541961
Iteration 2300: Loss = -12509.204985459057
Iteration 2400: Loss = -12508.952031487714
Iteration 2500: Loss = -12508.769539321256
Iteration 2600: Loss = -12508.507266221795
Iteration 2700: Loss = -12508.221298934233
Iteration 2800: Loss = -12507.868595841592
Iteration 2900: Loss = -12507.673395002515
Iteration 3000: Loss = -12507.569778922587
Iteration 3100: Loss = -12507.485721682167
Iteration 3200: Loss = -12507.44061720635
Iteration 3300: Loss = -12507.406323923771
Iteration 3400: Loss = -12507.370175823617
Iteration 3500: Loss = -12507.307808657068
Iteration 3600: Loss = -12506.982519855053
Iteration 3700: Loss = -12506.784984757242
Iteration 3800: Loss = -12506.755466128514
Iteration 3900: Loss = -12506.738558598447
Iteration 4000: Loss = -12506.726015387405
Iteration 4100: Loss = -12506.71624508549
Iteration 4200: Loss = -12506.913199320163
1
Iteration 4300: Loss = -12506.70153191884
Iteration 4400: Loss = -12506.69555693293
Iteration 4500: Loss = -12506.690198192648
Iteration 4600: Loss = -12506.687199548132
Iteration 4700: Loss = -12506.681228374515
Iteration 4800: Loss = -12506.677727218315
Iteration 4900: Loss = -12506.675118234025
Iteration 5000: Loss = -12506.672200755247
Iteration 5100: Loss = -12506.6697653287
Iteration 5200: Loss = -12506.66780613541
Iteration 5300: Loss = -12506.673848659224
1
Iteration 5400: Loss = -12506.664192858248
Iteration 5500: Loss = -12506.662650568303
Iteration 5600: Loss = -12507.280061083216
1
Iteration 5700: Loss = -12506.65968167003
Iteration 5800: Loss = -12506.658191656492
Iteration 5900: Loss = -12506.656588612095
Iteration 6000: Loss = -12506.65530105833
Iteration 6100: Loss = -12506.652636051922
Iteration 6200: Loss = -12506.650782033184
Iteration 6300: Loss = -12506.649583150289
Iteration 6400: Loss = -12506.648675721231
Iteration 6500: Loss = -12506.647806261028
Iteration 6600: Loss = -12506.647069113977
Iteration 6700: Loss = -12506.992474768076
1
Iteration 6800: Loss = -12506.645754872934
Iteration 6900: Loss = -12506.645057043088
Iteration 7000: Loss = -12506.644281342426
Iteration 7100: Loss = -12506.661550709918
1
Iteration 7200: Loss = -12506.64141933444
Iteration 7300: Loss = -12506.619229404012
Iteration 7400: Loss = -12506.581276661807
Iteration 7500: Loss = -12506.59917080255
1
Iteration 7600: Loss = -12506.57938628429
Iteration 7700: Loss = -12506.578716286527
Iteration 7800: Loss = -12506.627024470046
1
Iteration 7900: Loss = -12506.577560388378
Iteration 8000: Loss = -12506.577244950493
Iteration 8100: Loss = -12506.576595396138
Iteration 8200: Loss = -12506.576243528689
Iteration 8300: Loss = -12506.575709227815
Iteration 8400: Loss = -12506.575209268118
Iteration 8500: Loss = -12506.57570020181
1
Iteration 8600: Loss = -12506.574125579087
Iteration 8700: Loss = -12506.573696032352
Iteration 8800: Loss = -12506.574599772162
1
Iteration 8900: Loss = -12506.573182089633
Iteration 9000: Loss = -12506.57310569611
Iteration 9100: Loss = -12506.57297972573
Iteration 9200: Loss = -12506.572599408593
Iteration 9300: Loss = -12506.572508694266
Iteration 9400: Loss = -12506.572318591896
Iteration 9500: Loss = -12506.572178493489
Iteration 9600: Loss = -12506.572941324657
1
Iteration 9700: Loss = -12506.571908550535
Iteration 9800: Loss = -12506.57410966143
1
Iteration 9900: Loss = -12506.571545334775
Iteration 10000: Loss = -12506.571650795531
1
Iteration 10100: Loss = -12506.571378228111
Iteration 10200: Loss = -12506.57388764831
1
Iteration 10300: Loss = -12506.570855494934
Iteration 10400: Loss = -12506.570381690368
Iteration 10500: Loss = -12506.577059473404
1
Iteration 10600: Loss = -12506.76300963995
2
Iteration 10700: Loss = -12506.564754268582
Iteration 10800: Loss = -12506.580490760663
1
Iteration 10900: Loss = -12506.564545367937
Iteration 11000: Loss = -12506.568849983942
1
Iteration 11100: Loss = -12506.564393547413
Iteration 11200: Loss = -12506.565060600533
1
Iteration 11300: Loss = -12506.56465135772
2
Iteration 11400: Loss = -12506.56431318488
Iteration 11500: Loss = -12506.635511137638
1
Iteration 11600: Loss = -12506.564040125719
Iteration 11700: Loss = -12506.563336147865
Iteration 11800: Loss = -12506.5632810361
Iteration 11900: Loss = -12506.564519869586
1
Iteration 12000: Loss = -12506.785562724375
2
Iteration 12100: Loss = -12506.563493676638
3
Iteration 12200: Loss = -12506.803963069124
4
Iteration 12300: Loss = -12506.56289275643
Iteration 12400: Loss = -12506.620007327707
1
Iteration 12500: Loss = -12506.563353236019
2
Iteration 12600: Loss = -12506.617233431785
3
Iteration 12700: Loss = -12506.562908791806
4
Iteration 12800: Loss = -12506.578208543175
5
Iteration 12900: Loss = -12506.568723311557
6
Iteration 13000: Loss = -12506.562577046556
Iteration 13100: Loss = -12506.563596236896
1
Iteration 13200: Loss = -12506.566164393142
2
Iteration 13300: Loss = -12506.628841886972
3
Iteration 13400: Loss = -12506.56248418423
Iteration 13500: Loss = -12506.578948823359
1
Iteration 13600: Loss = -12506.562081227245
Iteration 13700: Loss = -12506.566779549012
1
Iteration 13800: Loss = -12506.56255802586
2
Iteration 13900: Loss = -12506.569103328617
3
Iteration 14000: Loss = -12506.561193159614
Iteration 14100: Loss = -12506.560366033693
Iteration 14200: Loss = -12506.561857136543
1
Iteration 14300: Loss = -12506.559808353051
Iteration 14400: Loss = -12506.555099766649
Iteration 14500: Loss = -12506.683994496103
1
Iteration 14600: Loss = -12506.554960081312
Iteration 14700: Loss = -12506.55833665231
1
Iteration 14800: Loss = -12506.555188244525
2
Iteration 14900: Loss = -12506.583537645314
3
Iteration 15000: Loss = -12506.554910453839
Iteration 15100: Loss = -12506.556305311782
1
Iteration 15200: Loss = -12506.561146482303
2
Iteration 15300: Loss = -12506.553652986593
Iteration 15400: Loss = -12506.553827494216
1
Iteration 15500: Loss = -12506.553151367778
Iteration 15600: Loss = -12506.553627656474
1
Iteration 15700: Loss = -12506.591642079024
2
Iteration 15800: Loss = -12506.553127884004
Iteration 15900: Loss = -12506.606321316483
1
Iteration 16000: Loss = -12506.552933542222
Iteration 16100: Loss = -12506.552879325283
Iteration 16200: Loss = -12506.551789540179
Iteration 16300: Loss = -12506.551346603506
Iteration 16400: Loss = -12506.552558212688
1
Iteration 16500: Loss = -12506.55424891092
2
Iteration 16600: Loss = -12506.554380315789
3
Iteration 16700: Loss = -12506.551681695139
4
Iteration 16800: Loss = -12506.552024056664
5
Iteration 16900: Loss = -12506.592705891002
6
Iteration 17000: Loss = -12506.551256675568
Iteration 17100: Loss = -12506.586275588468
1
Iteration 17200: Loss = -12506.54991915469
Iteration 17300: Loss = -12506.550017812546
1
Iteration 17400: Loss = -12506.550065256397
2
Iteration 17500: Loss = -12506.550049928903
3
Iteration 17600: Loss = -12506.574727217556
4
Iteration 17700: Loss = -12506.550040689906
5
Iteration 17800: Loss = -12506.565901865994
6
Iteration 17900: Loss = -12506.550725823146
7
Iteration 18000: Loss = -12506.554275598031
8
Iteration 18100: Loss = -12506.553691666444
9
Iteration 18200: Loss = -12506.55074277462
10
Stopping early at iteration 18200 due to no improvement.
tensor([[-2.3835e-01, -4.3769e+00],
        [ 1.1029e+00, -5.7181e+00],
        [ 2.5257e+00, -7.1410e+00],
        [ 5.5713e+00, -1.0187e+01],
        [ 1.4530e+00, -6.0682e+00],
        [ 2.0011e+00, -6.6163e+00],
        [ 8.6609e-01, -5.4813e+00],
        [ 5.2349e+00, -9.8502e+00],
        [ 2.6570e-02, -4.6418e+00],
        [ 9.8983e-01, -5.6050e+00],
        [ 1.9106e+00, -6.5258e+00],
        [ 5.5351e+00, -1.0150e+01],
        [ 5.3034e+00, -9.9186e+00],
        [ 5.6230e+00, -1.0238e+01],
        [ 5.2234e+00, -9.8386e+00],
        [ 1.6017e+00, -6.2169e+00],
        [ 4.4416e+00, -9.0568e+00],
        [ 4.8134e-01, -5.0966e+00],
        [ 1.0730e+00, -5.6882e+00],
        [ 3.0821e-01, -4.9234e+00],
        [-2.3410e+00, -2.2742e+00],
        [ 3.8914e+00, -8.5067e+00],
        [ 9.8441e-01, -5.5996e+00],
        [ 3.1011e+00, -7.7164e+00],
        [ 1.6407e+00, -6.2559e+00],
        [ 2.1334e+00, -6.7486e+00],
        [ 5.5422e+00, -1.0157e+01],
        [ 5.1694e+00, -9.7846e+00],
        [-5.1049e+00,  4.8972e-01],
        [-1.0612e+00, -3.5540e+00],
        [ 5.3760e+00, -9.9912e+00],
        [ 4.1990e+00, -8.8142e+00],
        [ 1.6016e+00, -6.2168e+00],
        [ 1.7746e+00, -6.3899e+00],
        [ 4.8620e+00, -9.4772e+00],
        [ 9.0686e-01, -5.5221e+00],
        [ 2.2529e+00, -6.8681e+00],
        [ 8.0343e-01, -5.4186e+00],
        [ 1.0081e-01, -4.7160e+00],
        [ 6.7909e-01, -5.2943e+00],
        [ 2.0955e-01, -4.8248e+00],
        [ 1.3718e+00, -5.9870e+00],
        [-2.7835e+00, -1.8317e+00],
        [ 5.3479e+00, -9.9631e+00],
        [ 5.5456e+00, -1.0161e+01],
        [ 5.4526e+00, -1.0068e+01],
        [ 6.4276e-03, -4.6216e+00],
        [ 5.5445e+00, -1.0160e+01],
        [ 5.4980e+00, -1.0113e+01],
        [ 7.9961e-01, -5.4148e+00],
        [ 1.3275e+00, -5.9427e+00],
        [ 1.3949e+00, -6.0102e+00],
        [ 2.0373e+00, -6.6525e+00],
        [ 1.6580e+00, -6.2732e+00],
        [ 3.9084e+00, -8.5236e+00],
        [ 3.3977e+00, -8.0130e+00],
        [ 5.5567e+00, -1.0172e+01],
        [ 4.2810e+00, -8.8963e+00],
        [ 2.2774e+00, -6.8927e+00],
        [ 6.4839e-01, -5.2636e+00],
        [ 1.3286e+00, -5.9439e+00],
        [ 2.6697e+00, -7.2849e+00],
        [ 1.9268e+00, -6.5420e+00],
        [ 5.6134e+00, -1.0229e+01],
        [ 5.1587e+00, -9.7739e+00],
        [ 1.9947e+00, -6.6100e+00],
        [ 1.4558e+00, -6.0711e+00],
        [ 5.2642e+00, -9.8795e+00],
        [ 3.4494e+00, -8.0647e+00],
        [-4.0578e+00, -5.5743e-01],
        [ 1.8620e+00, -6.4772e+00],
        [ 5.5539e+00, -1.0169e+01],
        [ 1.3247e+00, -5.9399e+00],
        [ 5.4392e+00, -1.0054e+01],
        [ 4.0238e+00, -8.6390e+00],
        [ 6.6944e-01, -5.2847e+00],
        [ 5.3473e+00, -9.9625e+00],
        [ 2.2902e+00, -6.9054e+00],
        [ 8.7516e-01, -5.4904e+00],
        [ 2.8539e+00, -7.4691e+00],
        [ 5.4998e+00, -1.0115e+01],
        [ 2.4855e+00, -7.1007e+00],
        [ 7.5932e-01, -5.3745e+00],
        [ 3.7243e+00, -8.3395e+00],
        [ 5.4694e-01, -5.1622e+00],
        [-4.5724e-01, -4.1580e+00],
        [ 1.7716e+00, -6.3868e+00],
        [ 2.8569e+00, -7.4722e+00],
        [ 5.8918e+00, -1.0507e+01],
        [ 5.5518e+00, -1.0167e+01],
        [ 1.0656e+00, -5.6808e+00],
        [ 8.8197e-01, -5.4972e+00],
        [-4.5476e+00, -6.7610e-02],
        [-3.3572e+00, -1.2580e+00],
        [ 3.2280e+00, -7.8432e+00],
        [ 2.6096e+00, -7.2249e+00],
        [ 3.4555e+00, -8.0707e+00],
        [ 5.6250e+00, -1.0240e+01],
        [ 2.1946e+00, -6.8099e+00],
        [ 5.4159e+00, -1.0031e+01]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 2.2842e-08],
        [1.2636e-06, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9471, 0.0529], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2019, 0.2140],
         [0.1772, 0.4452]],

        [[0.1805, 0.2911],
         [0.6421, 0.0501]],

        [[0.7838, 0.1806],
         [0.9559, 0.6174]],

        [[0.8945, 0.1820],
         [0.2078, 0.9542]],

        [[0.6452, 0.2003],
         [0.2800, 0.8705]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.004682108332720131
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.009696969696969697
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.017313838488058987
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.0014658390783536795
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.006464646464646465
Global Adjusted Rand Index: -0.0025353113292992885
Average Adjusted Rand Index: -0.007924680412149793
Iteration 0: Loss = -29204.342456376544
Iteration 10: Loss = -12508.402558906411
Iteration 20: Loss = -12508.281461373768
Iteration 30: Loss = -12508.271180899452
Iteration 40: Loss = -12508.270645723058
Iteration 50: Loss = -12508.27057984523
Iteration 60: Loss = -12508.270441678838
Iteration 70: Loss = -12508.270123717539
Iteration 80: Loss = -12508.269821981634
Iteration 90: Loss = -12508.269483260634
Iteration 100: Loss = -12508.269051157977
Iteration 110: Loss = -12508.268634399863
Iteration 120: Loss = -12508.268183322072
Iteration 130: Loss = -12508.26762335764
Iteration 140: Loss = -12508.267090763396
Iteration 150: Loss = -12508.266524961708
Iteration 160: Loss = -12508.265834314183
Iteration 170: Loss = -12508.265110679353
Iteration 180: Loss = -12508.264377664054
Iteration 190: Loss = -12508.263521475448
Iteration 200: Loss = -12508.262611356684
Iteration 210: Loss = -12508.26161783208
Iteration 220: Loss = -12508.260549891225
Iteration 230: Loss = -12508.259348613545
Iteration 240: Loss = -12508.25813392531
Iteration 250: Loss = -12508.256774267911
Iteration 260: Loss = -12508.255206362079
Iteration 270: Loss = -12508.253564170955
Iteration 280: Loss = -12508.25180829461
Iteration 290: Loss = -12508.249827378619
Iteration 300: Loss = -12508.247711682183
Iteration 310: Loss = -12508.245332248262
Iteration 320: Loss = -12508.242815390118
Iteration 330: Loss = -12508.239978419308
Iteration 340: Loss = -12508.236985457102
Iteration 350: Loss = -12508.233620551331
Iteration 360: Loss = -12508.229884842603
Iteration 370: Loss = -12508.22584814366
Iteration 380: Loss = -12508.221369744619
Iteration 390: Loss = -12508.216479453738
Iteration 400: Loss = -12508.211012221142
Iteration 410: Loss = -12508.205022726299
Iteration 420: Loss = -12508.198306574044
Iteration 430: Loss = -12508.190860544373
Iteration 440: Loss = -12508.182597856472
Iteration 450: Loss = -12508.17338984599
Iteration 460: Loss = -12508.162977812482
Iteration 470: Loss = -12508.151351089851
Iteration 480: Loss = -12508.138162763202
Iteration 490: Loss = -12508.123167804146
Iteration 500: Loss = -12508.106116337081
Iteration 510: Loss = -12508.086446856372
Iteration 520: Loss = -12508.063661532182
Iteration 530: Loss = -12508.037128657377
Iteration 540: Loss = -12508.005769613686
Iteration 550: Loss = -12507.968244655882
Iteration 560: Loss = -12507.92274977124
Iteration 570: Loss = -12507.866463220054
Iteration 580: Loss = -12507.795231568092
Iteration 590: Loss = -12507.702193906758
Iteration 600: Loss = -12507.575830714639
Iteration 610: Loss = -12507.395160881124
Iteration 620: Loss = -12507.120778771417
Iteration 630: Loss = -12506.686988155388
Iteration 640: Loss = -12506.062396990632
Iteration 650: Loss = -12505.44459259832
Iteration 660: Loss = -12505.109024900781
Iteration 670: Loss = -12505.00509728404
Iteration 680: Loss = -12504.979572232083
Iteration 690: Loss = -12504.974045602172
Iteration 700: Loss = -12504.973042926338
Iteration 710: Loss = -12504.972895848116
Iteration 720: Loss = -12504.972966011332
1
Iteration 730: Loss = -12504.972989507802
2
Iteration 740: Loss = -12504.973070434511
3
Stopping early at iteration 739 due to no improvement.
pi: tensor([[0.1381, 0.8619],
        [0.0342, 0.9658]], dtype=torch.float64)
alpha: tensor([0.0381, 0.9619])
beta: tensor([[[0.2607, 0.2148],
         [0.9451, 0.1968]],

        [[0.2020, 0.3010],
         [0.0785, 0.0535]],

        [[0.4167, 0.2122],
         [0.6741, 0.1567]],

        [[0.6924, 0.2718],
         [0.3032, 0.2193]],

        [[0.2169, 0.3040],
         [0.2495, 0.9902]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.006464646464646465
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: -0.0007365134907910085
Average Adjusted Rand Index: -0.002473148582144872
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -29204.11736696083
Iteration 100: Loss = -12651.592327623774
Iteration 200: Loss = -12620.617545174466
Iteration 300: Loss = -12601.437769678947
Iteration 400: Loss = -12586.606057993511
Iteration 500: Loss = -12570.130123374176
Iteration 600: Loss = -12552.423420173836
Iteration 700: Loss = -12535.58732149986
Iteration 800: Loss = -12526.250162091532
Iteration 900: Loss = -12519.848098388818
Iteration 1000: Loss = -12516.433615926548
Iteration 1100: Loss = -12514.80588584614
Iteration 1200: Loss = -12511.203610096434
Iteration 1300: Loss = -12510.889412575232
Iteration 1400: Loss = -12510.711555788283
Iteration 1500: Loss = -12510.593720585943
Iteration 1600: Loss = -12510.508723514717
Iteration 1700: Loss = -12510.443920718168
Iteration 1800: Loss = -12510.393112439831
Iteration 1900: Loss = -12510.352452850811
Iteration 2000: Loss = -12510.318785228279
Iteration 2100: Loss = -12510.290385354292
Iteration 2200: Loss = -12510.26619253491
Iteration 2300: Loss = -12510.245604537566
Iteration 2400: Loss = -12510.228210157797
Iteration 2500: Loss = -12510.213197797673
Iteration 2600: Loss = -12510.199965421754
Iteration 2700: Loss = -12510.187888057628
Iteration 2800: Loss = -12510.176031727317
Iteration 2900: Loss = -12510.162886058308
Iteration 3000: Loss = -12510.147386317163
Iteration 3100: Loss = -12510.11987250277
Iteration 3200: Loss = -12509.996491664371
Iteration 3300: Loss = -12509.501946363358
Iteration 3400: Loss = -12509.384184870638
Iteration 3500: Loss = -12509.359573322372
Iteration 3600: Loss = -12509.235346507247
Iteration 3700: Loss = -12509.190485806464
Iteration 3800: Loss = -12509.166011383842
Iteration 3900: Loss = -12509.154739640522
Iteration 4000: Loss = -12509.14593598762
Iteration 4100: Loss = -12509.142175986533
Iteration 4200: Loss = -12509.322198774684
1
Iteration 4300: Loss = -12509.137474619321
Iteration 4400: Loss = -12509.135233597195
Iteration 4500: Loss = -12509.129654944665
Iteration 4600: Loss = -12509.118836091138
Iteration 4700: Loss = -12509.091019758387
Iteration 4800: Loss = -12509.088471766168
Iteration 4900: Loss = -12509.083983569992
Iteration 5000: Loss = -12509.075257302995
Iteration 5100: Loss = -12509.052426125025
Iteration 5200: Loss = -12509.048458005293
Iteration 5300: Loss = -12509.370098074793
1
Iteration 5400: Loss = -12509.027079765492
Iteration 5500: Loss = -12509.017976453353
Iteration 5600: Loss = -12509.009710190536
Iteration 5700: Loss = -12509.023231040628
1
Iteration 5800: Loss = -12508.998594493069
Iteration 5900: Loss = -12508.995949014792
Iteration 6000: Loss = -12508.993086733817
Iteration 6100: Loss = -12508.994711911082
1
Iteration 6200: Loss = -12508.988632526252
Iteration 6300: Loss = -12508.986578497474
Iteration 6400: Loss = -12508.984245996158
Iteration 6500: Loss = -12508.981980927045
Iteration 6600: Loss = -12508.979382402034
Iteration 6700: Loss = -12508.977061472388
Iteration 6800: Loss = -12508.97536654079
Iteration 6900: Loss = -12508.973910125314
Iteration 7000: Loss = -12508.972724155368
Iteration 7100: Loss = -12508.971550642918
Iteration 7200: Loss = -12508.972420072783
1
Iteration 7300: Loss = -12508.968979857551
Iteration 7400: Loss = -12508.968402756696
Iteration 7500: Loss = -12508.968010994684
Iteration 7600: Loss = -12508.974000720676
1
Iteration 7700: Loss = -12508.96742859926
Iteration 7800: Loss = -12508.967180656806
Iteration 7900: Loss = -12508.966981605681
Iteration 8000: Loss = -12509.10247041382
1
Iteration 8100: Loss = -12508.965226247743
Iteration 8200: Loss = -12508.90491688212
Iteration 8300: Loss = -12508.904669920643
Iteration 8400: Loss = -12508.904466946571
Iteration 8500: Loss = -12508.904307507815
Iteration 8600: Loss = -12508.9041105625
Iteration 8700: Loss = -12509.55021569557
1
Iteration 8800: Loss = -12508.903834201301
Iteration 8900: Loss = -12508.903725983559
Iteration 9000: Loss = -12508.903609181252
Iteration 9100: Loss = -12508.903724365688
1
Iteration 9200: Loss = -12508.903392704693
Iteration 9300: Loss = -12508.903333233444
Iteration 9400: Loss = -12508.907100554055
1
Iteration 9500: Loss = -12508.90317457045
Iteration 9600: Loss = -12508.903103346736
Iteration 9700: Loss = -12508.903007843155
Iteration 9800: Loss = -12508.902950131782
Iteration 9900: Loss = -12508.902892482723
Iteration 10000: Loss = -12508.902825901516
Iteration 10100: Loss = -12508.919476569201
1
Iteration 10200: Loss = -12508.90269202746
Iteration 10300: Loss = -12508.902662980021
Iteration 10400: Loss = -12508.90261495705
Iteration 10500: Loss = -12508.902849291238
1
Iteration 10600: Loss = -12508.902508354551
Iteration 10700: Loss = -12508.902434684072
Iteration 10800: Loss = -12508.902422752495
Iteration 10900: Loss = -12508.902401050836
Iteration 11000: Loss = -12508.902308646271
Iteration 11100: Loss = -12508.902254844565
Iteration 11200: Loss = -12508.902639707028
1
Iteration 11300: Loss = -12508.90218220957
Iteration 11400: Loss = -12508.902141624987
Iteration 11500: Loss = -12508.967905567679
1
Iteration 11600: Loss = -12508.90206769149
Iteration 11700: Loss = -12508.902035474293
Iteration 11800: Loss = -12509.423473798395
1
Iteration 11900: Loss = -12508.901974884835
Iteration 12000: Loss = -12508.901956874868
Iteration 12100: Loss = -12508.928043649847
1
Iteration 12200: Loss = -12508.901862430463
Iteration 12300: Loss = -12508.9018007827
Iteration 12400: Loss = -12509.045527980012
1
Iteration 12500: Loss = -12508.901734177885
Iteration 12600: Loss = -12508.901703587177
Iteration 12700: Loss = -12509.546186284064
1
Iteration 12800: Loss = -12508.901662662312
Iteration 12900: Loss = -12508.901610780347
Iteration 13000: Loss = -12508.901618718523
1
Iteration 13100: Loss = -12508.901587426435
Iteration 13200: Loss = -12508.901564694255
Iteration 13300: Loss = -12508.901560711325
Iteration 13400: Loss = -12508.924224743207
1
Iteration 13500: Loss = -12508.901503457586
Iteration 13600: Loss = -12508.901517168053
1
Iteration 13700: Loss = -12509.247426404367
2
Iteration 13800: Loss = -12508.901484316848
Iteration 13900: Loss = -12508.901478846206
Iteration 14000: Loss = -12508.901403383432
Iteration 14100: Loss = -12508.901408903539
1
Iteration 14200: Loss = -12508.901356980374
Iteration 14300: Loss = -12508.901320391824
Iteration 14400: Loss = -12508.901591675898
1
Iteration 14500: Loss = -12508.901344115184
2
Iteration 14600: Loss = -12508.901308170076
Iteration 14700: Loss = -12508.919781230472
1
Iteration 14800: Loss = -12508.901299029058
Iteration 14900: Loss = -12508.901307452088
1
Iteration 15000: Loss = -12508.90131389054
2
Iteration 15100: Loss = -12508.901368031515
3
Iteration 15200: Loss = -12508.901243054206
Iteration 15300: Loss = -12508.901250176104
1
Iteration 15400: Loss = -12508.911280024098
2
Iteration 15500: Loss = -12508.901254654731
3
Iteration 15600: Loss = -12508.901226186914
Iteration 15700: Loss = -12508.920074818638
1
Iteration 15800: Loss = -12508.901230221214
2
Iteration 15900: Loss = -12509.001996030382
3
Iteration 16000: Loss = -12508.901248554195
4
Iteration 16100: Loss = -12508.901201206583
Iteration 16200: Loss = -12508.912797677873
1
Iteration 16300: Loss = -12508.901162067314
Iteration 16400: Loss = -12508.901198044505
1
Iteration 16500: Loss = -12508.906670295144
2
Iteration 16600: Loss = -12508.901195893215
3
Iteration 16700: Loss = -12508.901157997809
Iteration 16800: Loss = -12509.123065117632
1
Iteration 16900: Loss = -12508.901182915564
2
Iteration 17000: Loss = -12508.919085415786
3
Iteration 17100: Loss = -12508.901168983732
4
Iteration 17200: Loss = -12508.907525458817
5
Iteration 17300: Loss = -12508.901126400377
Iteration 17400: Loss = -12508.901424741909
1
Iteration 17500: Loss = -12508.90043892207
Iteration 17600: Loss = -12508.903705545144
1
Iteration 17700: Loss = -12508.900458712807
2
Iteration 17800: Loss = -12508.912346467874
3
Iteration 17900: Loss = -12508.900413123805
Iteration 18000: Loss = -12508.90392231793
1
Iteration 18100: Loss = -12508.900461602829
2
Iteration 18200: Loss = -12508.900442120366
3
Iteration 18300: Loss = -12508.902803912853
4
Iteration 18400: Loss = -12508.900448286216
5
Iteration 18500: Loss = -12508.900926859333
6
Iteration 18600: Loss = -12508.900435197296
7
Iteration 18700: Loss = -12508.916780355936
8
Iteration 18800: Loss = -12508.900449682254
9
Iteration 18900: Loss = -12508.903734477099
10
Stopping early at iteration 18900 due to no improvement.
tensor([[ -7.3691,   4.8818],
        [ -6.8158,   5.4292],
        [ -7.1338,   5.4501],
        [ -6.9856,   5.5982],
        [ -7.2040,   5.8169],
        [ -7.4187,   5.6097],
        [ -7.2044,   5.5982],
        [ -8.4320,   4.5923],
        [ -7.5796,   5.7644],
        [ -6.9559,   5.5025],
        [ -7.3318,   5.4625],
        [ -7.3508,   5.7749],
        [ -7.8814,   6.2977],
        [ -7.0683,   5.6171],
        [ -7.4111,   5.7199],
        [ -7.3122,   5.3680],
        [ -7.6140,   5.8578],
        [ -8.5444,   3.9292],
        [ -7.8002,   4.2314],
        [ -7.6878,   5.4270],
        [ -7.4376,   5.7961],
        [ -7.7672,   4.8753],
        [ -7.2212,   5.5773],
        [ -7.2274,   5.7989],
        [ -7.4762,   6.0870],
        [ -8.1930,   3.9493],
        [ -7.2286,   5.5677],
        [ -7.6258,   5.9499],
        [ -7.0368,   5.6490],
        [ -8.5927,   3.9775],
        [ -7.5273,   5.6081],
        [ -7.5201,   5.2840],
        [ -7.1980,   5.4879],
        [ -7.2877,   5.8211],
        [ -6.9817,   5.4895],
        [ -7.3908,   5.8386],
        [ -7.3290,   5.7909],
        [ -8.6228,   4.3851],
        [ -7.5180,   5.3941],
        [ -7.1732,   5.4023],
        [ -7.7711,   5.4668],
        [ -7.6720,   5.5667],
        [ -7.2568,   5.8472],
        [ -7.4971,   5.3018],
        [ -6.7725,   5.2113],
        [ -6.7630,   5.3742],
        [ -7.1556,   5.3002],
        [ -7.1598,   4.6837],
        [ -9.0053,   4.3901],
        [-11.0146,   9.2103],
        [ -7.0304,   5.5372],
        [ -7.2931,   5.9064],
        [ -7.1291,   5.5531],
        [ -7.0252,   5.6071],
        [ -8.2208,   4.4496],
        [ -7.4978,   5.1659],
        [ -6.9343,   5.5438],
        [ -6.9047,   5.3466],
        [ -7.2367,   5.7555],
        [ -7.2632,   5.8615],
        [ -7.4635,   5.0982],
        [ -6.8959,   5.4607],
        [ -7.9188,   4.5548],
        [ -7.0319,   5.4386],
        [ -7.6051,   5.6332],
        [-10.7740,   9.2175],
        [ -7.2419,   5.2299],
        [ -7.1957,   5.7065],
        [ -7.4178,   5.9370],
        [ -7.9880,   4.9178],
        [ -8.1310,   4.3347],
        [ -7.1988,   5.6816],
        [ -7.6271,   5.3981],
        [ -7.3744,   5.7555],
        [ -6.8221,   5.4315],
        [ -7.3812,   5.7443],
        [ -7.1075,   5.5779],
        [ -7.1320,   5.5528],
        [ -8.0375,   4.4362],
        [ -6.8162,   5.4213],
        [ -7.7358,   4.9081],
        [ -7.5146,   5.6016],
        [ -7.1234,   5.4446],
        [ -7.4785,   4.8970],
        [ -7.8970,   4.9071],
        [ -7.4934,   5.8502],
        [ -7.6857,   5.0071],
        [ -7.0916,   5.5990],
        [ -7.1546,   5.1489],
        [ -6.8596,   5.0571],
        [ -7.4321,   5.5648],
        [ -7.4750,   6.0832],
        [ -8.2230,   4.4522],
        [ -7.4196,   6.0287],
        [ -8.0765,   4.3982],
        [ -7.2203,   5.6865],
        [ -7.0032,   5.5816],
        [ -7.2570,   5.8694],
        [ -7.3561,   5.8910],
        [ -7.7315,   6.0650]], dtype=torch.float64, requires_grad=True)
pi: tensor([[8.7418e-06, 9.9999e-01],
        [1.4348e-02, 9.8565e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([2.9180e-06, 1.0000e+00], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1627, 0.1866],
         [0.9451, 0.2039]],

        [[0.2020, 0.1210],
         [0.0785, 0.0535]],

        [[0.4167, 0.1921],
         [0.6741, 0.1567]],

        [[0.6924, 0.3323],
         [0.3032, 0.2193]],

        [[0.2169, 0.1707],
         [0.2495, 0.9902]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00039543502922895045
Average Adjusted Rand Index: -0.0004529465619428516
Iteration 0: Loss = -19849.954365083773
Iteration 10: Loss = -12507.070960158855
Iteration 20: Loss = -12506.009363770974
Iteration 30: Loss = -12505.40479176557
Iteration 40: Loss = -12505.094533551368
Iteration 50: Loss = -12505.00136997184
Iteration 60: Loss = -12504.9787752422
Iteration 70: Loss = -12504.973857845032
Iteration 80: Loss = -12504.972997285962
Iteration 90: Loss = -12504.972905203746
Iteration 100: Loss = -12504.973010136284
1
Iteration 110: Loss = -12504.973016763317
2
Iteration 120: Loss = -12504.973061165449
3
Stopping early at iteration 119 due to no improvement.
pi: tensor([[0.9658, 0.0342],
        [0.8619, 0.1381]], dtype=torch.float64)
alpha: tensor([0.9619, 0.0381])
beta: tensor([[[0.1968, 0.2148],
         [0.4469, 0.2607]],

        [[0.6401, 0.3010],
         [0.0319, 0.5920]],

        [[0.0967, 0.2122],
         [0.3935, 0.2530]],

        [[0.8048, 0.2718],
         [0.5898, 0.4233]],

        [[0.9612, 0.3040],
         [0.0960, 0.0556]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: -0.0007365134907910085
Average Adjusted Rand Index: -0.002473148582144872
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19849.491256013695
Iteration 100: Loss = -12509.941972808605
Iteration 200: Loss = -12507.545365750242
Iteration 300: Loss = -12505.5390339677
Iteration 400: Loss = -12503.834180462727
Iteration 500: Loss = -12503.300184684358
Iteration 600: Loss = -12503.042201378063
Iteration 700: Loss = -12502.893499514334
Iteration 800: Loss = -12502.790051665694
Iteration 900: Loss = -12502.733703964353
Iteration 1000: Loss = -12502.705890079436
Iteration 1100: Loss = -12502.689562863714
Iteration 1200: Loss = -12502.677912632184
Iteration 1300: Loss = -12502.667675137573
Iteration 1400: Loss = -12502.65690490903
Iteration 1500: Loss = -12502.644215013017
Iteration 1600: Loss = -12502.63338098497
Iteration 1700: Loss = -12502.62691342425
Iteration 1800: Loss = -12502.6222706471
Iteration 1900: Loss = -12502.6188482319
Iteration 2000: Loss = -12502.616120037159
Iteration 2100: Loss = -12502.614013523853
Iteration 2200: Loss = -12502.612302966667
Iteration 2300: Loss = -12502.610978832809
Iteration 2400: Loss = -12502.609733544265
Iteration 2500: Loss = -12502.60866951412
Iteration 2600: Loss = -12502.607585439642
Iteration 2700: Loss = -12502.606810408632
Iteration 2800: Loss = -12502.606444187608
Iteration 2900: Loss = -12502.606197631068
Iteration 3000: Loss = -12502.605976464483
Iteration 3100: Loss = -12502.605839169017
Iteration 3200: Loss = -12502.60563240545
Iteration 3300: Loss = -12502.605546671712
Iteration 3400: Loss = -12502.605428404635
Iteration 3500: Loss = -12502.605326718036
Iteration 3600: Loss = -12502.605274810614
Iteration 3700: Loss = -12502.605228438051
Iteration 3800: Loss = -12502.605137468721
Iteration 3900: Loss = -12502.60508663472
Iteration 4000: Loss = -12502.605044175109
Iteration 4100: Loss = -12502.605018753437
Iteration 4200: Loss = -12502.604979075586
Iteration 4300: Loss = -12502.604980747872
1
Iteration 4400: Loss = -12502.604947548863
Iteration 4500: Loss = -12502.604920327514
Iteration 4600: Loss = -12502.604893365879
Iteration 4700: Loss = -12502.604862758844
Iteration 4800: Loss = -12502.604890261888
1
Iteration 4900: Loss = -12502.60486641631
2
Iteration 5000: Loss = -12502.604866446867
3
Iteration 5100: Loss = -12502.604833941346
Iteration 5200: Loss = -12502.604867469918
1
Iteration 5300: Loss = -12502.604852737872
2
Iteration 5400: Loss = -12502.60481418997
Iteration 5500: Loss = -12502.60481203572
Iteration 5600: Loss = -12502.604813438962
1
Iteration 5700: Loss = -12502.6048125965
2
Iteration 5800: Loss = -12502.604800273197
Iteration 5900: Loss = -12502.609548601962
1
Iteration 6000: Loss = -12502.605263271174
2
Iteration 6100: Loss = -12502.605419891659
3
Iteration 6200: Loss = -12502.604793447348
Iteration 6300: Loss = -12502.604843017132
1
Iteration 6400: Loss = -12502.60476316879
Iteration 6500: Loss = -12502.605960013814
1
Iteration 6600: Loss = -12502.604814973012
2
Iteration 6700: Loss = -12502.604782307224
3
Iteration 6800: Loss = -12502.74613773051
4
Iteration 6900: Loss = -12502.604772002478
5
Iteration 7000: Loss = -12502.60474891464
Iteration 7100: Loss = -12502.811987105322
1
Iteration 7200: Loss = -12502.604777791707
2
Iteration 7300: Loss = -12502.604762564477
3
Iteration 7400: Loss = -12502.632210740805
4
Iteration 7500: Loss = -12502.60471477409
Iteration 7600: Loss = -12502.60471779733
1
Iteration 7700: Loss = -12502.61744974994
2
Iteration 7800: Loss = -12502.604732578773
3
Iteration 7900: Loss = -12502.604717793949
4
Iteration 8000: Loss = -12502.612424458051
5
Iteration 8100: Loss = -12502.604714607533
Iteration 8200: Loss = -12502.604725201832
1
Iteration 8300: Loss = -12502.626037235588
2
Iteration 8400: Loss = -12502.604717518518
3
Iteration 8500: Loss = -12502.604743442895
4
Iteration 8600: Loss = -12502.604772411823
5
Iteration 8700: Loss = -12502.604736359848
6
Iteration 8800: Loss = -12502.604702515306
Iteration 8900: Loss = -12502.604689624903
Iteration 9000: Loss = -12502.605025940356
1
Iteration 9100: Loss = -12502.604717966446
2
Iteration 9200: Loss = -12502.604741856072
3
Iteration 9300: Loss = -12502.605315487128
4
Iteration 9400: Loss = -12502.604676558625
Iteration 9500: Loss = -12502.604719581974
1
Iteration 9600: Loss = -12502.604882188927
2
Iteration 9700: Loss = -12502.604734620332
3
Iteration 9800: Loss = -12502.604716074084
4
Iteration 9900: Loss = -12502.609450946156
5
Iteration 10000: Loss = -12502.60472064616
6
Iteration 10100: Loss = -12502.604755207718
7
Iteration 10200: Loss = -12502.604706923132
8
Iteration 10300: Loss = -12502.615892696786
9
Iteration 10400: Loss = -12502.604738156037
10
Stopping early at iteration 10400 due to no improvement.
tensor([[ 7.8559e-01, -3.2337e+00],
        [ 1.5367e+00, -3.0621e+00],
        [ 1.3675e+00, -3.7741e+00],
        [ 3.1823e-01, -4.9335e+00],
        [ 1.7420e+00, -3.1295e+00],
        [ 1.8983e+00, -3.2851e+00],
        [ 1.6811e+00, -3.2549e+00],
        [ 1.8363e+00, -3.2290e+00],
        [ 8.6739e-01, -3.0232e+00],
        [ 1.7265e+00, -3.1368e+00],
        [ 1.4710e+00, -3.4065e+00],
        [ 1.9350e+00, -3.3227e+00],
        [ 1.8973e+00, -3.4412e+00],
        [ 1.6805e+00, -3.5685e+00],
        [ 1.8676e+00, -3.2591e+00],
        [ 4.3589e-01, -3.6971e+00],
        [ 1.7574e+00, -3.4351e+00],
        [ 1.0504e+00, -2.6512e+00],
        [ 1.6842e+00, -3.0705e+00],
        [ 1.7223e+00, -3.1781e+00],
        [ 1.0283e+00, -2.6883e+00],
        [ 1.8325e+00, -3.3719e+00],
        [ 1.8158e+00, -3.2922e+00],
        [ 1.8488e+00, -3.2620e+00],
        [ 1.7758e+00, -3.1621e+00],
        [ 1.2552e+00, -3.5814e+00],
        [ 7.4804e-01, -4.5064e+00],
        [ 1.7693e+00, -3.4587e+00],
        [-6.6106e-01, -1.6436e+00],
        [ 1.1880e+00, -2.8933e+00],
        [ 1.8667e+00, -3.2788e+00],
        [ 3.3765e-01, -4.9529e+00],
        [ 1.2205e+00, -2.6473e+00],
        [ 1.6587e+00, -3.3216e+00],
        [ 1.4694e+00, -3.4202e+00],
        [ 1.5965e+00, -3.1531e+00],
        [ 8.3594e-01, -4.1081e+00],
        [ 1.7328e+00, -3.2018e+00],
        [ 1.4703e+00, -3.5006e+00],
        [ 1.7805e+00, -3.3500e+00],
        [ 1.5919e+00, -3.0254e+00],
        [ 1.7811e+00, -3.3248e+00],
        [ 1.1145e+00, -3.7739e+00],
        [ 1.7315e+00, -3.1936e+00],
        [ 1.8623e+00, -3.2957e+00],
        [ 1.7070e+00, -3.2983e+00],
        [ 2.6234e-01, -1.6489e+00],
        [ 1.8488e+00, -3.2564e+00],
        [ 1.4199e+00, -3.8786e+00],
        [ 8.9317e-01, -3.4954e+00],
        [ 1.6753e+00, -3.2432e+00],
        [ 1.5408e+00, -3.3419e+00],
        [ 1.9035e+00, -3.3289e+00],
        [ 1.7498e+00, -3.1361e+00],
        [ 1.7335e+00, -3.4265e+00],
        [ 1.7750e+00, -3.4230e+00],
        [ 1.6687e+00, -3.5548e+00],
        [ 1.8643e+00, -3.3257e+00],
        [ 1.3119e+00, -3.7921e+00],
        [ 1.1775e+00, -3.6341e+00],
        [ 1.6917e+00, -3.2296e+00],
        [ 1.7585e+00, -3.1468e+00],
        [ 1.6445e+00, -3.3672e+00],
        [ 1.6106e+00, -3.5532e+00],
        [ 7.6048e-01, -3.9855e+00],
        [ 1.9123e+00, -3.3019e+00],
        [-2.1913e-03, -4.6130e+00],
        [ 5.4174e-01, -4.6513e+00],
        [ 1.8082e+00, -3.2645e+00],
        [ 1.1138e-01, -1.7496e+00],
        [ 1.3079e+00, -2.8208e+00],
        [ 1.3021e+00, -3.7259e+00],
        [ 2.0317e-01, -4.8184e+00],
        [ 1.9459e+00, -3.3627e+00],
        [ 1.5041e+00, -3.6883e+00],
        [ 1.9194e+00, -3.3447e+00],
        [ 1.4933e+00, -3.3912e+00],
        [ 1.8399e+00, -3.2632e+00],
        [ 1.6654e+00, -3.0984e+00],
        [ 1.7773e+00, -3.3714e+00],
        [ 2.6683e-01, -4.8820e+00],
        [ 1.8449e+00, -3.3440e+00],
        [ 1.3222e+00, -3.0492e+00],
        [ 3.8840e-01, -4.8397e+00],
        [ 9.7346e-01, -2.9093e+00],
        [ 1.1249e+00, -2.9976e+00],
        [ 7.7986e-01, -2.7422e+00],
        [ 1.9040e+00, -3.3023e+00],
        [ 1.8553e+00, -3.2525e+00],
        [ 3.6279e-01, -2.0801e+00],
        [ 1.8652e+00, -3.2764e+00],
        [ 1.6815e+00, -3.2580e+00],
        [-1.3998e+00, -3.2154e+00],
        [ 8.1458e-01, -2.2116e+00],
        [ 1.2740e+00, -3.2245e+00],
        [ 1.1797e+00, -4.1167e+00],
        [ 1.2685e+00, -3.7862e+00],
        [ 1.7126e+00, -3.6239e+00],
        [ 1.8286e+00, -3.5071e+00],
        [ 1.5210e+00, -3.8662e+00]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.9805, 0.0195],
        [0.3712, 0.6288]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9843, 0.0157], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1992, 0.1956],
         [0.4469, 0.3261]],

        [[0.6401, 0.3022],
         [0.0319, 0.5920]],

        [[0.0967, 0.2023],
         [0.3935, 0.2530]],

        [[0.8048, 0.2735],
         [0.5898, 0.4233]],

        [[0.9612, 0.3047],
         [0.0960, 0.0556]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.0005089183229907909
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: 3.35429808991111e-05
Average Adjusted Rand Index: -0.0015563291191436127
Iteration 0: Loss = -20618.118624486913
Iteration 10: Loss = -12507.91179476455
Iteration 20: Loss = -12507.011200274626
Iteration 30: Loss = -12506.492713874595
Iteration 40: Loss = -12505.834895793367
Iteration 50: Loss = -12505.294153526112
Iteration 60: Loss = -12505.05766735617
Iteration 70: Loss = -12504.99210020228
Iteration 80: Loss = -12504.976666165132
Iteration 90: Loss = -12504.973506964187
Iteration 100: Loss = -12504.972967150565
Iteration 110: Loss = -12504.972940725627
Iteration 120: Loss = -12504.973007797287
1
Iteration 130: Loss = -12504.973061337605
2
Iteration 140: Loss = -12504.97305092779
3
Stopping early at iteration 139 due to no improvement.
pi: tensor([[0.1381, 0.8619],
        [0.0342, 0.9658]], dtype=torch.float64)
alpha: tensor([0.0381, 0.9619])
beta: tensor([[[0.2607, 0.2148],
         [0.9595, 0.1968]],

        [[0.2670, 0.3010],
         [0.1500, 0.8976]],

        [[0.4368, 0.2122],
         [0.1431, 0.0132]],

        [[0.1781, 0.2718],
         [0.2827, 0.2680]],

        [[0.4111, 0.3040],
         [0.8170, 0.8764]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.006464646464646465
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: -0.0007365134907910085
Average Adjusted Rand Index: -0.002473148582144872
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20617.731374933555
Iteration 100: Loss = -12526.44826424141
Iteration 200: Loss = -12512.20920458201
Iteration 300: Loss = -12510.518281288694
Iteration 400: Loss = -12509.757862933344
Iteration 500: Loss = -12509.445881696027
Iteration 600: Loss = -12509.323745750391
Iteration 700: Loss = -12509.237020194996
Iteration 800: Loss = -12509.143314327093
Iteration 900: Loss = -12508.972663471877
Iteration 1000: Loss = -12508.704160365038
Iteration 1100: Loss = -12508.639522624408
Iteration 1200: Loss = -12508.591046666388
Iteration 1300: Loss = -12508.557290102111
Iteration 1400: Loss = -12508.509015398686
Iteration 1500: Loss = -12508.471153737153
Iteration 1600: Loss = -12508.453463672427
Iteration 1700: Loss = -12508.36330318603
Iteration 1800: Loss = -12508.335084055416
Iteration 1900: Loss = -12508.324198978244
Iteration 2000: Loss = -12508.311521148351
Iteration 2100: Loss = -12508.297052308953
Iteration 2200: Loss = -12508.284806155481
Iteration 2300: Loss = -12508.269155415874
Iteration 2400: Loss = -12508.246569231742
Iteration 2500: Loss = -12508.21014416553
Iteration 2600: Loss = -12508.154561839576
Iteration 2700: Loss = -12508.092139844506
Iteration 2800: Loss = -12508.041059084037
Iteration 2900: Loss = -12508.004677061339
Iteration 3000: Loss = -12507.980499275362
Iteration 3100: Loss = -12507.963020985637
Iteration 3200: Loss = -12507.948969720384
Iteration 3300: Loss = -12507.93239219746
Iteration 3400: Loss = -12507.909549783657
Iteration 3500: Loss = -12507.90033382669
Iteration 3600: Loss = -12507.892414316828
Iteration 3700: Loss = -12507.885743672052
Iteration 3800: Loss = -12507.879980243493
Iteration 3900: Loss = -12507.874870225005
Iteration 4000: Loss = -12507.870245712955
Iteration 4100: Loss = -12507.865917663794
Iteration 4200: Loss = -12507.861961868519
Iteration 4300: Loss = -12507.858604223165
Iteration 4400: Loss = -12507.856007889806
Iteration 4500: Loss = -12507.853971595261
Iteration 4600: Loss = -12507.852191077896
Iteration 4700: Loss = -12507.850590382142
Iteration 4800: Loss = -12507.849221412494
Iteration 4900: Loss = -12507.84789264197
Iteration 5000: Loss = -12507.846788154215
Iteration 5100: Loss = -12507.845816273266
Iteration 5200: Loss = -12507.844932995
Iteration 5300: Loss = -12507.84408029461
Iteration 5400: Loss = -12507.843367054138
Iteration 5500: Loss = -12507.842698466122
Iteration 5600: Loss = -12507.842053187047
Iteration 5700: Loss = -12507.841469647337
Iteration 5800: Loss = -12507.840902786671
Iteration 5900: Loss = -12507.840376431086
Iteration 6000: Loss = -12507.839880536809
Iteration 6100: Loss = -12507.8393491343
Iteration 6200: Loss = -12507.83902357076
Iteration 6300: Loss = -12507.838288457071
Iteration 6400: Loss = -12507.837839144799
Iteration 6500: Loss = -12507.838635863376
1
Iteration 6600: Loss = -12507.837122354847
Iteration 6700: Loss = -12507.836803875609
Iteration 6800: Loss = -12507.837960579154
1
Iteration 6900: Loss = -12507.836217295646
Iteration 7000: Loss = -12507.83596492411
Iteration 7100: Loss = -12507.866557677764
1
Iteration 7200: Loss = -12507.835208561735
Iteration 7300: Loss = -12507.834975310017
Iteration 7400: Loss = -12507.86842983325
1
Iteration 7500: Loss = -12507.83465264784
Iteration 7600: Loss = -12507.834498467757
Iteration 7700: Loss = -12507.85887324187
1
Iteration 7800: Loss = -12507.83399088407
Iteration 7900: Loss = -12507.833743562192
Iteration 8000: Loss = -12507.841900682875
1
Iteration 8100: Loss = -12507.833503275373
Iteration 8200: Loss = -12507.86450850843
1
Iteration 8300: Loss = -12507.833742531078
2
Iteration 8400: Loss = -12507.834450118762
3
Iteration 8500: Loss = -12507.840781507946
4
Iteration 8600: Loss = -12507.83506896015
5
Iteration 8700: Loss = -12507.880297673026
6
Iteration 8800: Loss = -12507.85925324044
7
Iteration 8900: Loss = -12507.83962702882
8
Iteration 9000: Loss = -12507.83357257409
9
Iteration 9100: Loss = -12507.833313736191
Iteration 9200: Loss = -12507.848531430465
1
Iteration 9300: Loss = -12507.835656313388
2
Iteration 9400: Loss = -12507.891525792475
3
Iteration 9500: Loss = -12507.83795844174
4
Iteration 9600: Loss = -12507.83427206118
5
Iteration 9700: Loss = -12507.872115855236
6
Iteration 9800: Loss = -12507.834629172274
7
Iteration 9900: Loss = -12507.833175946596
Iteration 10000: Loss = -12507.838630741393
1
Iteration 10100: Loss = -12507.83369299454
2
Iteration 10200: Loss = -12507.861570737905
3
Iteration 10300: Loss = -12507.845032443773
4
Iteration 10400: Loss = -12507.83207583557
Iteration 10500: Loss = -12507.832052484602
Iteration 10600: Loss = -12507.868783543421
1
Iteration 10700: Loss = -12507.837733147235
2
Iteration 10800: Loss = -12507.83203130846
Iteration 10900: Loss = -12507.831991853307
Iteration 11000: Loss = -12507.832163402674
1
Iteration 11100: Loss = -12507.836559495243
2
Iteration 11200: Loss = -12507.831853138543
Iteration 11300: Loss = -12507.831829103347
Iteration 11400: Loss = -12507.833656936287
1
Iteration 11500: Loss = -12507.834828923167
2
Iteration 11600: Loss = -12507.834305874583
3
Iteration 11700: Loss = -12507.832179056992
4
Iteration 11800: Loss = -12507.831745295443
Iteration 11900: Loss = -12507.832678948922
1
Iteration 12000: Loss = -12507.833786928722
2
Iteration 12100: Loss = -12507.832316066673
3
Iteration 12200: Loss = -12507.834398369327
4
Iteration 12300: Loss = -12507.90631941952
5
Iteration 12400: Loss = -12507.831972081109
6
Iteration 12500: Loss = -12507.834958061696
7
Iteration 12600: Loss = -12507.845463366251
8
Iteration 12700: Loss = -12507.943761675735
9
Iteration 12800: Loss = -12507.915152809901
10
Stopping early at iteration 12800 due to no improvement.
tensor([[ -7.4502,   4.8967],
        [ -7.6859,   5.5482],
        [ -9.4847,   7.5864],
        [ -9.2351,   7.2072],
        [ -9.4250,   7.4172],
        [ -9.8881,   6.8711],
        [ -9.5420,   7.7471],
        [ -8.8240,   7.4264],
        [ -9.3851,   6.8563],
        [ -9.1267,   7.5938],
        [ -8.9297,   7.5204],
        [-10.1504,   7.5580],
        [ -9.0114,   7.2161],
        [ -9.9479,   5.5905],
        [ -9.1730,   7.6295],
        [ -9.2809,   6.7617],
        [-10.8451,   6.2298],
        [ -8.4508,   7.0180],
        [ -8.9775,   7.2159],
        [ -8.4645,   6.9828],
        [ -9.6372,   7.5384],
        [ -9.0856,   7.5652],
        [ -9.8320,   7.1110],
        [ -9.2880,   7.8467],
        [ -9.8154,   7.8949],
        [ -8.7183,   6.0521],
        [-10.0733,   7.6533],
        [ -8.9221,   7.2497],
        [ -6.4309,   4.4202],
        [ -8.6579,   7.2445],
        [ -9.1372,   7.6655],
        [ -8.9945,   7.5657],
        [ -9.1563,   7.7459],
        [ -9.8330,   7.5320],
        [ -8.6920,   7.2836],
        [ -9.5475,   7.1575],
        [ -8.6794,   7.2862],
        [-10.2414,   7.4856],
        [-10.0597,   7.5572],
        [ -8.9504,   7.5599],
        [ -9.1497,   7.7060],
        [ -9.1121,   7.6661],
        [ -9.1227,   7.5945],
        [-10.1444,   6.9032],
        [ -8.7147,   7.2957],
        [ -8.9144,   7.5107],
        [ -8.9961,   7.2263],
        [ -8.2136,   6.6803],
        [ -8.2336,   6.7900],
        [ -9.0431,   6.8339],
        [ -8.2039,   6.8128],
        [ -8.9041,   7.5076],
        [ -8.9159,   7.3754],
        [ -8.6712,   7.2455],
        [ -9.3385,   7.5109],
        [ -8.6947,   7.2891],
        [ -9.0228,   7.5066],
        [-10.6054,   5.9902],
        [-10.0372,   7.2840],
        [ -9.3170,   7.5724],
        [ -9.8142,   7.6140],
        [ -9.9263,   7.1561],
        [ -8.6193,   7.2328],
        [ -9.7906,   7.6150],
        [-10.1060,   6.8649],
        [ -8.8809,   7.4457],
        [ -9.1298,   7.5308],
        [ -9.0715,   7.4905],
        [ -9.0211,   7.5646],
        [ -8.8480,   6.9823],
        [ -8.5962,   7.0553],
        [ -9.3272,   7.4130],
        [ -8.9359,   7.5398],
        [-10.5261,   6.6935],
        [-11.3862,   7.7255],
        [ -9.4673,   7.6890],
        [ -8.6872,   7.0570],
        [ -8.9914,   6.5886],
        [ -8.9550,   7.5418],
        [ -9.0845,   7.4536],
        [ -8.8997,   7.5092],
        [ -8.6551,   7.2687],
        [ -9.8697,   6.3628],
        [-10.0659,   7.7380],
        [ -8.7991,   7.3875],
        [ -9.1137,   7.2300],
        [ -8.8101,   7.3430],
        [ -9.6382,   7.8183],
        [ -9.2241,   6.7553],
        [ -9.9198,   6.6917],
        [ -8.9949,   7.2873],
        [ -9.1708,   7.7822],
        [ -6.4564,   5.0701],
        [ -8.5996,   7.0228],
        [ -8.6863,   6.9654],
        [ -9.5975,   7.3424],
        [-10.1657,   6.0838],
        [ -8.6916,   7.1808],
        [ -8.7740,   7.3294],
        [ -9.6156,   7.0227]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.2785e-05, 9.9999e-01],
        [9.6790e-01, 3.2096e-02]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([5.7639e-07, 1.0000e+00], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2012, 0.1877],
         [0.9595, 0.2047]],

        [[0.2670, 0.3108],
         [0.1500, 0.8976]],

        [[0.4368, 0.1766],
         [0.1431, 0.0132]],

        [[0.1781, 0.2018],
         [0.2827, 0.2680]],

        [[0.4111, 0.2016],
         [0.8170, 0.8764]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: -0.009009494263563287
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0020148894003650976
Average Adjusted Rand Index: -0.003839251019118786
Iteration 0: Loss = -27091.842606020404
Iteration 10: Loss = -12506.149813621034
Iteration 20: Loss = -12505.128437068768
Iteration 30: Loss = -12505.024192437682
Iteration 40: Loss = -12504.988262396673
Iteration 50: Loss = -12504.977625296135
Iteration 60: Loss = -12504.974537403286
Iteration 70: Loss = -12504.97359680513
Iteration 80: Loss = -12504.973294430123
Iteration 90: Loss = -12504.9731675303
Iteration 100: Loss = -12504.973176930476
1
Iteration 110: Loss = -12504.97305304219
Iteration 120: Loss = -12504.973086164306
1
Iteration 130: Loss = -12504.973056430777
2
Iteration 140: Loss = -12504.97305455169
3
Stopping early at iteration 139 due to no improvement.
pi: tensor([[0.1382, 0.8618],
        [0.0342, 0.9658]], dtype=torch.float64)
alpha: tensor([0.0380, 0.9620])
beta: tensor([[[0.2606, 0.2148],
         [0.4837, 0.1968]],

        [[0.8195, 0.3010],
         [0.6873, 0.6642]],

        [[0.9048, 0.2122],
         [0.9098, 0.9127]],

        [[0.2940, 0.2718],
         [0.7489, 0.3493]],

        [[0.2783, 0.3040],
         [0.1189, 0.7199]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.006464646464646465
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: -0.0007365134907910085
Average Adjusted Rand Index: -0.002473148582144872
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27092.012684017387
Iteration 100: Loss = -12541.409274737502
Iteration 200: Loss = -12519.276874682768
Iteration 300: Loss = -12512.918629238788
Iteration 400: Loss = -12510.225547226964
Iteration 500: Loss = -12508.723472524543
Iteration 600: Loss = -12507.620078433742
Iteration 700: Loss = -12506.053205168406
Iteration 800: Loss = -12505.613852375955
Iteration 900: Loss = -12505.384373862795
Iteration 1000: Loss = -12505.256430429103
Iteration 1100: Loss = -12505.144641787392
Iteration 1200: Loss = -12504.804884183352
Iteration 1300: Loss = -12504.21264352695
Iteration 1400: Loss = -12504.021015172148
Iteration 1500: Loss = -12503.854198232293
Iteration 1600: Loss = -12503.814647242394
Iteration 1700: Loss = -12503.789078672777
Iteration 1800: Loss = -12503.66574708061
Iteration 1900: Loss = -12503.634630824634
Iteration 2000: Loss = -12503.60323584061
Iteration 2100: Loss = -12503.58515150623
Iteration 2200: Loss = -12503.561214887914
Iteration 2300: Loss = -12503.544298928176
Iteration 2400: Loss = -12503.526579697625
Iteration 2500: Loss = -12503.51453567783
Iteration 2600: Loss = -12503.503762572687
Iteration 2700: Loss = -12503.490757956662
Iteration 2800: Loss = -12503.47666981702
Iteration 2900: Loss = -12503.466563660506
Iteration 3000: Loss = -12503.442517815518
Iteration 3100: Loss = -12503.432907139935
Iteration 3200: Loss = -12503.40256299555
Iteration 3300: Loss = -12503.326003689304
Iteration 3400: Loss = -12503.317055541973
Iteration 3500: Loss = -12503.30813015223
Iteration 3600: Loss = -12503.30402428099
Iteration 3700: Loss = -12503.299089323833
Iteration 3800: Loss = -12503.274090519137
Iteration 3900: Loss = -12503.242353787564
Iteration 4000: Loss = -12503.239031220271
Iteration 4100: Loss = -12503.23154185898
Iteration 4200: Loss = -12503.226322080913
Iteration 4300: Loss = -12503.21236737802
Iteration 4400: Loss = -12502.781648610535
Iteration 4500: Loss = -12502.744408275348
Iteration 4600: Loss = -12502.73849342599
Iteration 4700: Loss = -12502.733021568725
Iteration 4800: Loss = -12502.694434546853
Iteration 4900: Loss = -12502.62582061017
Iteration 5000: Loss = -12502.623780179658
Iteration 5100: Loss = -12502.622630748418
Iteration 5200: Loss = -12502.621715986843
Iteration 5300: Loss = -12502.62080988659
Iteration 5400: Loss = -12502.61914881055
Iteration 5500: Loss = -12502.617219114212
Iteration 5600: Loss = -12502.616158408036
Iteration 5700: Loss = -12502.614958823078
Iteration 5800: Loss = -12502.613227854526
Iteration 5900: Loss = -12502.612147860991
Iteration 6000: Loss = -12502.611650747722
Iteration 6100: Loss = -12502.611364611312
Iteration 6200: Loss = -12502.611064923347
Iteration 6300: Loss = -12502.61069805102
Iteration 6400: Loss = -12502.610245718619
Iteration 6500: Loss = -12502.609564234475
Iteration 6600: Loss = -12502.608690470324
Iteration 6700: Loss = -12502.60819952513
Iteration 6800: Loss = -12502.607316300306
Iteration 6900: Loss = -12502.60711206531
Iteration 7000: Loss = -12502.607088403944
Iteration 7100: Loss = -12502.606887850878
Iteration 7200: Loss = -12502.608915247793
1
Iteration 7300: Loss = -12502.606615275932
Iteration 7400: Loss = -12502.6065386351
Iteration 7500: Loss = -12502.606281370177
Iteration 7600: Loss = -12502.606207197841
Iteration 7700: Loss = -12502.606073091098
Iteration 7800: Loss = -12502.612075740553
1
Iteration 7900: Loss = -12502.60586399712
Iteration 8000: Loss = -12502.605742302469
Iteration 8100: Loss = -12502.612671459841
1
Iteration 8200: Loss = -12502.60559202198
Iteration 8300: Loss = -12502.605540844223
Iteration 8400: Loss = -12502.618127486
1
Iteration 8500: Loss = -12502.605443424576
Iteration 8600: Loss = -12502.605364932677
Iteration 8700: Loss = -12502.605301733698
Iteration 8800: Loss = -12502.605264888012
Iteration 8900: Loss = -12502.605171966467
Iteration 9000: Loss = -12502.605134006248
Iteration 9100: Loss = -12502.612279875977
1
Iteration 9200: Loss = -12502.605138108569
2
Iteration 9300: Loss = -12502.605139078065
3
Iteration 9400: Loss = -12502.605104574512
Iteration 9500: Loss = -12502.60531624474
1
Iteration 9600: Loss = -12502.605090568402
Iteration 9700: Loss = -12502.605080354138
Iteration 9800: Loss = -12502.606862322391
1
Iteration 9900: Loss = -12502.605028515738
Iteration 10000: Loss = -12502.60501668114
Iteration 10100: Loss = -12502.619424166545
1
Iteration 10200: Loss = -12502.604955439894
Iteration 10300: Loss = -12502.604921427002
Iteration 10400: Loss = -12502.644808673625
1
Iteration 10500: Loss = -12502.60495403613
2
Iteration 10600: Loss = -12502.604884268938
Iteration 10700: Loss = -12502.64027451363
1
Iteration 10800: Loss = -12502.604947302403
2
Iteration 10900: Loss = -12502.60490430007
3
Iteration 11000: Loss = -12503.08829290984
4
Iteration 11100: Loss = -12502.604853563906
Iteration 11200: Loss = -12502.604872011563
1
Iteration 11300: Loss = -12502.604905839933
2
Iteration 11400: Loss = -12502.605008271932
3
Iteration 11500: Loss = -12502.604853431112
Iteration 11600: Loss = -12502.60484978094
Iteration 11700: Loss = -12502.954969028702
1
Iteration 11800: Loss = -12502.604808471548
Iteration 11900: Loss = -12502.604797943715
Iteration 12000: Loss = -12502.786268858903
1
Iteration 12100: Loss = -12502.604704672278
Iteration 12200: Loss = -12502.604721796692
1
Iteration 12300: Loss = -12502.702747342519
2
Iteration 12400: Loss = -12502.60471966149
3
Iteration 12500: Loss = -12502.604747908395
4
Iteration 12600: Loss = -12502.610203327124
5
Iteration 12700: Loss = -12502.604735488354
6
Iteration 12800: Loss = -12502.604788963683
7
Iteration 12900: Loss = -12502.624036388193
8
Iteration 13000: Loss = -12502.604755078044
9
Iteration 13100: Loss = -12502.60472817002
10
Stopping early at iteration 13100 due to no improvement.
tensor([[-2.8594,  1.1600],
        [-3.0029,  1.5959],
        [-3.3684,  1.7733],
        [-3.3333,  1.9185],
        [-3.4032,  1.4684],
        [-3.3145,  1.8689],
        [-3.1675,  1.7686],
        [-3.2515,  1.8138],
        [-3.1607,  0.7299],
        [-3.7965,  1.0668],
        [-3.5446,  1.3329],
        [-3.4189,  1.8389],
        [-3.4396,  1.8989],
        [-3.8426,  1.4065],
        [-3.3993,  1.7275],
        [-2.7841,  1.3489],
        [-3.3733,  1.8192],
        [-3.2558,  0.4458],
        [-3.1576,  1.5972],
        [-3.8979,  1.0025],
        [-2.6366,  1.0799],
        [-3.3134,  1.8910],
        [-4.2402,  0.8678],
        [-3.6651,  1.4457],
        [-3.6278,  1.3101],
        [-3.1894,  1.6472],
        [-3.3224,  1.9320],
        [-3.7727,  1.4554],
        [-2.7989, -1.8164],
        [-2.7647,  1.3166],
        [-3.3444,  1.8011],
        [-3.4283,  1.8622],
        [-2.9791,  0.8888],
        [-3.4994,  1.4809],
        [-3.1380,  1.7516],
        [-3.5739,  1.1758],
        [-3.3160,  1.6280],
        [-3.1630,  1.7716],
        [-3.8583,  1.1126],
        [-3.2827,  1.8479],
        [-3.0039,  1.6134],
        [-3.7258,  1.3801],
        [-3.5239,  1.3645],
        [-3.2290,  1.6961],
        [-3.3667,  1.7914],
        [-3.5610,  1.4443],
        [-1.6540,  0.2573],
        [-3.4062,  1.6990],
        [-3.5910,  1.7076],
        [-3.1320,  1.2566],
        [-3.5116,  1.4070],
        [-3.1758,  1.7070],
        [-3.3094,  1.9230],
        [-3.5021,  1.3838],
        [-3.6443,  1.5157],
        [-3.3275,  1.8705],
        [-3.4216,  1.8019],
        [-3.3811,  1.8089],
        [-3.2459,  1.8581],
        [-3.1747,  1.6370],
        [-3.2271,  1.6943],
        [-3.8646,  1.0407],
        [-3.2135,  1.7982],
        [-3.8217,  1.3422],
        [-3.3851,  1.3610],
        [-3.4019,  1.8123],
        [-3.0109,  1.6000],
        [-3.6548,  1.5383],
        [-3.2902,  1.7825],
        [-2.2983, -0.4373],
        [-3.0493,  1.0794],
        [-4.1312,  0.8968],
        [-3.4660,  1.5556],
        [-3.4407,  1.8678],
        [-3.4966,  1.6958],
        [-4.3001,  0.9641],
        [-3.1382,  1.7463],
        [-3.4634,  1.6397],
        [-3.1965,  1.5673],
        [-4.0577,  1.0911],
        [-3.3081,  1.8408],
        [-3.3061,  1.8829],
        [-3.1312,  1.2402],
        [-3.9190,  1.3091],
        [-2.6444,  1.2384],
        [-2.9276,  1.1949],
        [-2.7878,  0.7343],
        [-3.8155,  1.3909],
        [-4.3407,  0.7671],
        [-1.9434,  0.4995],
        [-3.7497,  1.3920],
        [-3.1658,  1.7738],
        [-1.9648, -0.1491],
        [-2.2702,  0.7560],
        [-3.1145,  1.3841],
        [-3.6065,  1.6899],
        [-4.5274,  0.5274],
        [-3.3660,  1.9705],
        [-3.3706,  1.9652],
        [-3.7407,  1.6465]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6287, 0.3713],
        [0.0195, 0.9805]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0157, 0.9843], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3261, 0.1956],
         [0.4837, 0.1992]],

        [[0.8195, 0.3022],
         [0.6873, 0.6642]],

        [[0.9048, 0.2023],
         [0.9098, 0.9127]],

        [[0.2940, 0.2735],
         [0.7489, 0.3493]],

        [[0.2783, 0.3047],
         [0.1189, 0.7199]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.0005089183229907909
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: 3.35429808991111e-05
Average Adjusted Rand Index: -0.0015563291191436127
11991.168779873846
new:  [-0.00039543502922895045, 3.35429808991111e-05, -0.0020148894003650976, 3.35429808991111e-05] [-0.0004529465619428516, -0.0015563291191436127, -0.003839251019118786, -0.0015563291191436127] [12508.903734477099, 12502.604738156037, 12507.915152809901, 12502.60472817002]
prior:  [-0.0007365134907910085, -0.0007365134907910085, -0.0007365134907910085, -0.0007365134907910085] [-0.002473148582144872, -0.002473148582144872, -0.002473148582144872, -0.002473148582144872] [12504.973070434511, 12504.973061165449, 12504.97305092779, 12504.97305455169]
-----------------------------------------------------------------------------------------
This iteration is 16
True Objective function: Loss = -11806.594479028518
Iteration 0: Loss = -14927.899790267507
Iteration 10: Loss = -11801.622388506097
Iteration 20: Loss = -11801.693562029286
1
Iteration 30: Loss = -11801.693563400735
2
Iteration 40: Loss = -11801.69356340073
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[0.7249, 0.2751],
        [0.2385, 0.7615]], dtype=torch.float64)
alpha: tensor([0.4567, 0.5433])
beta: tensor([[[0.2931, 0.1059],
         [0.0987, 0.2876]],

        [[0.4437, 0.0929],
         [0.1522, 0.8759]],

        [[0.6676, 0.0986],
         [0.7957, 0.1814]],

        [[0.8139, 0.0964],
         [0.1748, 0.1766]],

        [[0.1685, 0.1013],
         [0.6390, 0.3482]]], dtype=torch.float64)
time is 0
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 1
tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840316589316259
Average Adjusted Rand Index: 0.9839992163297293
tensor([[-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -14828.939599880046
Iteration 100: Loss = -12037.887825049042
Iteration 200: Loss = -11840.816805316384
Iteration 300: Loss = -11800.50667871012
Iteration 400: Loss = -11800.165332756957
Iteration 500: Loss = -11800.052309429348
Iteration 600: Loss = -11799.991176798354
Iteration 700: Loss = -11799.958812311
Iteration 800: Loss = -11799.927584937292
Iteration 900: Loss = -11799.909706181366
Iteration 1000: Loss = -11799.896352435151
Iteration 1100: Loss = -11799.886133510432
Iteration 1200: Loss = -11799.878139120265
Iteration 1300: Loss = -11799.87178169839
Iteration 1400: Loss = -11799.86656939579
Iteration 1500: Loss = -11799.86226826316
Iteration 1600: Loss = -11799.85868880041
Iteration 1700: Loss = -11799.855641232934
Iteration 1800: Loss = -11799.85306387891
Iteration 1900: Loss = -11799.850885616406
Iteration 2000: Loss = -11799.849870865675
Iteration 2100: Loss = -11799.847292814891
Iteration 2200: Loss = -11799.84581816164
Iteration 2300: Loss = -11799.844723083728
Iteration 2400: Loss = -11799.843379279617
Iteration 2500: Loss = -11799.845109594045
1
Iteration 2600: Loss = -11799.841456726697
Iteration 2700: Loss = -11799.840627942098
Iteration 2800: Loss = -11799.840648426634
1
Iteration 2900: Loss = -11799.839232167906
Iteration 3000: Loss = -11799.838632880785
Iteration 3100: Loss = -11799.8381627233
Iteration 3200: Loss = -11799.837618324513
Iteration 3300: Loss = -11799.8371467721
Iteration 3400: Loss = -11799.837422831513
1
Iteration 3500: Loss = -11799.837432230694
2
Iteration 3600: Loss = -11799.836049454592
Iteration 3700: Loss = -11799.836205679025
1
Iteration 3800: Loss = -11799.846685765695
2
Iteration 3900: Loss = -11799.835192092347
Iteration 4000: Loss = -11799.834950732387
Iteration 4100: Loss = -11799.838477557367
1
Iteration 4200: Loss = -11799.834477793975
Iteration 4300: Loss = -11799.855032673695
1
Iteration 4400: Loss = -11799.836988738272
2
Iteration 4500: Loss = -11799.83393768961
Iteration 4600: Loss = -11799.83523590588
1
Iteration 4700: Loss = -11799.844488740653
2
Iteration 4800: Loss = -11799.838250990406
3
Iteration 4900: Loss = -11799.834223761369
4
Iteration 5000: Loss = -11799.834098661899
5
Iteration 5100: Loss = -11799.83329813958
Iteration 5200: Loss = -11799.833368601481
1
Iteration 5300: Loss = -11799.842036900327
2
Iteration 5400: Loss = -11799.83328460153
Iteration 5500: Loss = -11799.835246250173
1
Iteration 5600: Loss = -11799.836558917295
2
Iteration 5700: Loss = -11799.832653488029
Iteration 5800: Loss = -11799.833047001131
1
Iteration 5900: Loss = -11799.832527650002
Iteration 6000: Loss = -11799.832789291646
1
Iteration 6100: Loss = -11799.832434986774
Iteration 6200: Loss = -11799.83421127793
1
Iteration 6300: Loss = -11799.844401826305
2
Iteration 6400: Loss = -11799.833745326529
3
Iteration 6500: Loss = -11799.872312516427
4
Iteration 6600: Loss = -11799.832296451043
Iteration 6700: Loss = -11799.832178142795
Iteration 6800: Loss = -11799.838299920488
1
Iteration 6900: Loss = -11799.832864075011
2
Iteration 7000: Loss = -11799.833035856362
3
Iteration 7100: Loss = -11799.835977894405
4
Iteration 7200: Loss = -11799.857086681786
5
Iteration 7300: Loss = -11799.835507087
6
Iteration 7400: Loss = -11799.83409910512
7
Iteration 7500: Loss = -11799.848454243845
8
Iteration 7600: Loss = -11799.831946526949
Iteration 7700: Loss = -11799.832509710684
1
Iteration 7800: Loss = -11799.838372501006
2
Iteration 7900: Loss = -11799.833441735362
3
Iteration 8000: Loss = -11799.875595402309
4
Iteration 8100: Loss = -11799.833474669049
5
Iteration 8200: Loss = -11799.840886004456
6
Iteration 8300: Loss = -11799.840010520122
7
Iteration 8400: Loss = -11799.852186376773
8
Iteration 8500: Loss = -11799.83178064625
Iteration 8600: Loss = -11799.833265695712
1
Iteration 8700: Loss = -11799.833021757222
2
Iteration 8800: Loss = -11799.83176514094
Iteration 8900: Loss = -11799.832319941941
1
Iteration 9000: Loss = -11799.834608408946
2
Iteration 9100: Loss = -11799.833901685402
3
Iteration 9200: Loss = -11799.899129735351
4
Iteration 9300: Loss = -11799.866867832383
5
Iteration 9400: Loss = -11799.84054515955
6
Iteration 9500: Loss = -11799.831784662903
7
Iteration 9600: Loss = -11799.837664664292
8
Iteration 9700: Loss = -11799.832047501099
9
Iteration 9800: Loss = -11799.83342845724
10
Stopping early at iteration 9800 due to no improvement.
tensor([[  5.0019,  -9.6171],
        [ -8.1222,   3.5070],
        [ -0.0872,  -4.5280],
        [ -9.7445,   5.1293],
        [  4.8234,  -9.4386],
        [ -8.7443,   4.1291],
        [-10.0995,   5.4843],
        [ -6.8661,   2.2509],
        [  3.9548,  -8.5700],
        [  4.8439,  -9.4591],
        [ -9.7448,   5.1296],
        [ -8.1480,   3.5328],
        [  3.5149,  -8.1301],
        [ -8.5617,   3.9465],
        [  2.4219,  -7.0372],
        [  1.6953,  -6.3105],
        [ -9.0055,   4.3903],
        [ -9.7477,   5.1325],
        [  5.0611,  -9.6763],
        [ -9.9220,   5.3068],
        [  4.3512,  -8.9664],
        [  4.6146,  -9.2298],
        [ -6.0833,   1.4681],
        [-10.1770,   5.5618],
        [ -1.4950,  -3.1202],
        [ -9.0578,   4.4426],
        [ -7.0820,   2.4668],
        [  4.0737,  -8.6889],
        [  5.1132,  -9.7284],
        [ -3.2983,  -1.3170],
        [ -7.5032,   2.8880],
        [  2.2015,  -6.8167],
        [  5.3002,  -9.9154],
        [  3.2528,  -7.8680],
        [ -9.2715,   4.6563],
        [ -9.2452,   4.6300],
        [ -7.3370,   2.7217],
        [ -6.7832,   2.1679],
        [ -8.0137,   3.3984],
        [  5.2884,  -9.9036],
        [ -4.2963,  -0.3189],
        [  4.4853,  -9.1006],
        [ -8.6286,   4.0134],
        [  4.8942,  -9.5095],
        [  2.4096,  -7.0248],
        [  0.6578,  -5.2731],
        [  5.6265, -10.2418],
        [  1.5802,  -6.1954],
        [ -6.9933,   2.3781],
        [  3.5567,  -8.1719],
        [  5.2361,  -9.8513],
        [ -9.6016,   4.9864],
        [ -9.1149,   4.4997],
        [  1.7677,  -6.3829],
        [ -7.5302,   2.9150],
        [ -8.1844,   3.5692],
        [ -9.1954,   4.5801],
        [  0.0188,  -4.6340],
        [  3.4242,  -8.0394],
        [  3.5181,  -8.1334],
        [  5.4762, -10.0914],
        [ -5.8371,   1.2219],
        [  2.5959,  -7.2111],
        [ -7.9658,   3.3505],
        [ -4.7463,   0.1310],
        [ -6.2854,   1.6702],
        [ -5.6357,   1.0205],
        [  4.7843,  -9.3996],
        [ -6.4746,   1.8594],
        [  1.7682,  -6.3835],
        [  5.4128, -10.0280],
        [  5.5623, -10.1776],
        [  4.3490,  -8.9642],
        [  2.1430,  -6.7582],
        [  5.7972, -10.4124],
        [  2.9699,  -7.5851],
        [ -9.6866,   5.0714],
        [ -8.4631,   3.8479],
        [  4.9244,  -9.5396],
        [  4.8510,  -9.4662],
        [  4.4261,  -9.0414],
        [ -1.5957,  -3.0195],
        [ -9.6215,   5.0063],
        [  2.8654,  -7.4806],
        [ -7.5407,   2.9255],
        [ -7.1678,   2.5526],
        [ -4.3843,  -0.2310],
        [  1.6417,  -6.2569],
        [  1.1178,  -5.7330],
        [  5.4988, -10.1140],
        [  2.8813,  -7.4965],
        [  4.7510,  -9.3662],
        [  4.7773,  -9.3925],
        [  5.0723,  -9.6876],
        [ -6.8851,   2.2699],
        [  4.6837,  -9.2989],
        [  6.5143, -11.1296],
        [  5.1424,  -9.7576],
        [  6.8250, -11.4402],
        [ -7.4105,   2.7953]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7641, 0.2359],
        [0.2685, 0.7315]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5582, 0.4418], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2929, 0.1058],
         [0.0987, 0.2994]],

        [[0.4437, 0.0932],
         [0.1522, 0.8759]],

        [[0.6676, 0.0986],
         [0.7957, 0.1814]],

        [[0.8139, 0.0966],
         [0.1748, 0.1766]],

        [[0.1685, 0.1009],
         [0.6390, 0.3482]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 1
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840316589316259
Average Adjusted Rand Index: 0.9839992163297293
Iteration 0: Loss = -28574.559490484768
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.2341,    nan]],

        [[0.7052,    nan],
         [0.8162, 0.5074]],

        [[0.8184,    nan],
         [0.7881, 0.8373]],

        [[0.5734,    nan],
         [0.6479, 0.5182]],

        [[0.8393,    nan],
         [0.3663, 0.7590]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -28573.502777686463
Iteration 100: Loss = -12302.785242313297
Iteration 200: Loss = -12298.661322933442
Iteration 300: Loss = -12297.564793974432
Iteration 400: Loss = -12297.030372785262
Iteration 500: Loss = -12296.708975197478
Iteration 600: Loss = -12296.490851577631
Iteration 700: Loss = -12296.337855322927
Iteration 800: Loss = -12296.224270349196
Iteration 900: Loss = -12296.13640914104
Iteration 1000: Loss = -12296.065826943868
Iteration 1100: Loss = -12296.006789047913
Iteration 1200: Loss = -12295.955014251153
Iteration 1300: Loss = -12295.906581272922
Iteration 1400: Loss = -12295.855422722527
Iteration 1500: Loss = -12295.785105532083
Iteration 1600: Loss = -12295.607359282127
Iteration 1700: Loss = -12295.045669689809
Iteration 1800: Loss = -12294.871604842203
Iteration 1900: Loss = -12294.805980743735
Iteration 2000: Loss = -12294.7664951835
Iteration 2100: Loss = -12294.739708600611
Iteration 2200: Loss = -12294.719311648098
Iteration 2300: Loss = -12294.702738151733
Iteration 2400: Loss = -12294.688466172294
Iteration 2500: Loss = -12294.67586953917
Iteration 2600: Loss = -12294.664487173006
Iteration 2700: Loss = -12294.654036953722
Iteration 2800: Loss = -12294.644454894611
Iteration 2900: Loss = -12294.63549085323
Iteration 3000: Loss = -12294.628332924834
Iteration 3100: Loss = -12294.618641003186
Iteration 3200: Loss = -12294.61096258205
Iteration 3300: Loss = -12294.604014474695
Iteration 3400: Loss = -12294.597855627322
Iteration 3500: Loss = -12294.592106650482
Iteration 3600: Loss = -12294.58702643383
Iteration 3700: Loss = -12294.608579010208
1
Iteration 3800: Loss = -12294.57790867239
Iteration 3900: Loss = -12294.573756589942
Iteration 4000: Loss = -12294.569791417434
Iteration 4100: Loss = -12294.565980017049
Iteration 4200: Loss = -12294.562272496747
Iteration 4300: Loss = -12294.558690912387
Iteration 4400: Loss = -12295.254249237785
1
Iteration 4500: Loss = -12294.551415831009
Iteration 4600: Loss = -12294.547794871454
Iteration 4700: Loss = -12294.544168940705
Iteration 4800: Loss = -12294.567201703641
1
Iteration 4900: Loss = -12294.53658013374
Iteration 5000: Loss = -12294.532762576137
Iteration 5100: Loss = -12294.529009618935
Iteration 5200: Loss = -12294.526174210912
Iteration 5300: Loss = -12294.521655739145
Iteration 5400: Loss = -12294.517966577983
Iteration 5500: Loss = -12294.519713225116
1
Iteration 5600: Loss = -12294.510405797786
Iteration 5700: Loss = -12294.506585016728
Iteration 5800: Loss = -12294.502800616583
Iteration 5900: Loss = -12294.512837762251
1
Iteration 6000: Loss = -12294.495162194004
Iteration 6100: Loss = -12294.491499269945
Iteration 6200: Loss = -12294.487942501972
Iteration 6300: Loss = -12294.484521402837
Iteration 6400: Loss = -12294.481306702966
Iteration 6500: Loss = -12294.47840875175
Iteration 6600: Loss = -12294.500656754359
1
Iteration 6700: Loss = -12294.473366165257
Iteration 6800: Loss = -12294.471398700776
Iteration 6900: Loss = -12294.469698087507
Iteration 7000: Loss = -12294.468441232382
Iteration 7100: Loss = -12294.467224278098
Iteration 7200: Loss = -12294.466229259724
Iteration 7300: Loss = -12294.465443415638
Iteration 7400: Loss = -12294.465370521259
Iteration 7500: Loss = -12294.464302459271
Iteration 7600: Loss = -12294.46387274307
Iteration 7700: Loss = -12294.463618517239
Iteration 7800: Loss = -12294.463166014393
Iteration 7900: Loss = -12294.462869337873
Iteration 8000: Loss = -12294.471063882318
1
Iteration 8100: Loss = -12294.46231956989
Iteration 8200: Loss = -12294.462079466854
Iteration 8300: Loss = -12294.473515556085
1
Iteration 8400: Loss = -12294.461620751717
Iteration 8500: Loss = -12294.461368005157
Iteration 8600: Loss = -12294.461165810128
Iteration 8700: Loss = -12294.461093707221
Iteration 8800: Loss = -12294.460735753075
Iteration 8900: Loss = -12294.460576063835
Iteration 9000: Loss = -12294.46063110715
1
Iteration 9100: Loss = -12294.460209105733
Iteration 9200: Loss = -12294.460044572805
Iteration 9300: Loss = -12294.459900515683
Iteration 9400: Loss = -12294.459743022653
Iteration 9500: Loss = -12294.461672824018
1
Iteration 9600: Loss = -12294.45943466049
Iteration 9700: Loss = -12294.459279258206
Iteration 9800: Loss = -12294.462180447515
1
Iteration 9900: Loss = -12294.459090871009
Iteration 10000: Loss = -12294.458922219212
Iteration 10100: Loss = -12294.458829127117
Iteration 10200: Loss = -12294.458856922982
1
Iteration 10300: Loss = -12294.45855291208
Iteration 10400: Loss = -12294.45846872638
Iteration 10500: Loss = -12294.595832919249
1
Iteration 10600: Loss = -12294.458316900818
Iteration 10700: Loss = -12294.458175923506
Iteration 10800: Loss = -12294.458135773591
Iteration 10900: Loss = -12294.460050617461
1
Iteration 11000: Loss = -12294.45797818217
Iteration 11100: Loss = -12294.457865588876
Iteration 11200: Loss = -12294.470691113205
1
Iteration 11300: Loss = -12294.457723288768
Iteration 11400: Loss = -12294.457686333435
Iteration 11500: Loss = -12294.46699924929
1
Iteration 11600: Loss = -12294.457568859789
Iteration 11700: Loss = -12294.457466936596
Iteration 11800: Loss = -12294.587321223207
1
Iteration 11900: Loss = -12294.457387481723
Iteration 12000: Loss = -12294.457336296251
Iteration 12100: Loss = -12294.465111500524
1
Iteration 12200: Loss = -12294.4572217147
Iteration 12300: Loss = -12294.457234168742
1
Iteration 12400: Loss = -12295.023510931469
2
Iteration 12500: Loss = -12294.457347312466
3
Iteration 12600: Loss = -12294.457065952376
Iteration 12700: Loss = -12294.46959465626
1
Iteration 12800: Loss = -12294.457101240303
2
Iteration 12900: Loss = -12294.457055536568
Iteration 13000: Loss = -12294.46011681164
1
Iteration 13100: Loss = -12294.457005105342
Iteration 13200: Loss = -12294.456899066347
Iteration 13300: Loss = -12294.458662467327
1
Iteration 13400: Loss = -12294.45685361802
Iteration 13500: Loss = -12294.457598861021
1
Iteration 13600: Loss = -12294.456871034929
2
Iteration 13700: Loss = -12294.456803345785
Iteration 13800: Loss = -12294.45862881712
1
Iteration 13900: Loss = -12294.456811040069
2
Iteration 14000: Loss = -12294.456734054756
Iteration 14100: Loss = -12294.458131860907
1
Iteration 14200: Loss = -12294.456768900212
2
Iteration 14300: Loss = -12294.469464263862
3
Iteration 14400: Loss = -12294.456676282764
Iteration 14500: Loss = -12294.617119605524
1
Iteration 14600: Loss = -12294.456747104854
2
Iteration 14700: Loss = -12294.456600186611
Iteration 14800: Loss = -12294.457355051378
1
Iteration 14900: Loss = -12294.456604391782
2
Iteration 15000: Loss = -12294.457238180548
3
Iteration 15100: Loss = -12294.45659747571
Iteration 15200: Loss = -12294.457504639495
1
Iteration 15300: Loss = -12294.45665071807
2
Iteration 15400: Loss = -12294.458625670855
3
Iteration 15500: Loss = -12294.456549220005
Iteration 15600: Loss = -12294.458401410995
1
Iteration 15700: Loss = -12294.577890177145
2
Iteration 15800: Loss = -12294.456553966436
3
Iteration 15900: Loss = -12294.473304383982
4
Iteration 16000: Loss = -12294.456690477056
5
Iteration 16100: Loss = -12294.456539540959
Iteration 16200: Loss = -12294.547466932741
1
Iteration 16300: Loss = -12294.45651250765
Iteration 16400: Loss = -12294.459330237656
1
Iteration 16500: Loss = -12294.456524567007
2
Iteration 16600: Loss = -12294.469517353382
3
Iteration 16700: Loss = -12294.456558607904
4
Iteration 16800: Loss = -12294.458154163909
5
Iteration 16900: Loss = -12294.461029933791
6
Iteration 17000: Loss = -12294.45891979952
7
Iteration 17100: Loss = -12294.45734996365
8
Iteration 17200: Loss = -12294.457286013287
9
Iteration 17300: Loss = -12294.476426621613
10
Stopping early at iteration 17300 due to no improvement.
tensor([[-2.1019,  0.1591],
        [-2.6448,  1.2371],
        [-3.1810,  1.7934],
        [-2.3493,  0.7193],
        [-3.0995, -0.2955],
        [-2.2446,  0.3187],
        [-1.9765,  0.5718],
        [-3.7240,  0.7195],
        [-1.7678,  0.2156],
        [-1.1062, -0.4731],
        [-1.8211,  0.4344],
        [-2.3959,  0.9402],
        [-2.5144,  0.8369],
        [-4.3918, -0.2234],
        [-2.9563,  0.9072],
        [-2.2361,  0.2952],
        [-2.3692,  0.9774],
        [-2.7800,  1.3789],
        [-2.6351,  0.7028],
        [-2.4868,  0.6009],
        [-1.9953,  0.2585],
        [-2.8144,  1.0846],
        [-2.8841,  0.1683],
        [-1.5808,  0.1628],
        [-1.8262,  0.4375],
        [-2.7962,  0.5477],
        [-4.1878,  1.3235],
        [-2.5645,  1.1711],
        [-2.9153, -0.3608],
        [-2.7845,  0.0413],
        [-2.8109,  1.3757],
        [-1.9835, -0.2637],
        [-0.6415, -0.8397],
        [-1.8312,  0.4117],
        [-3.5143,  0.6449],
        [-2.1350,  0.7084],
        [-3.1428,  0.4707],
        [-1.6420,  0.0600],
        [-1.8727,  0.4206],
        [-2.7528,  0.3522],
        [-3.8935,  1.1077],
        [-1.7699, -0.0605],
        [-2.3864, -0.3911],
        [-1.8419,  0.4497],
        [-2.5525, -0.2766],
        [-2.3323,  0.7270],
        [-0.8024, -0.9684],
        [-3.5245, -0.4461],
        [-2.1771, -0.9854],
        [-2.9640,  0.9108],
        [-2.0963,  0.1641],
        [-3.0270, -1.0408],
        [-1.7021, -0.2197],
        [-2.3391,  0.7507],
        [-2.0252,  0.5457],
        [-2.1881,  0.6669],
        [-2.1230,  0.6925],
        [-2.3943,  0.9556],
        [-1.5695,  0.1286],
        [-2.8506,  1.0169],
        [-2.1281,  0.6993],
        [-2.1046,  0.7103],
        [-1.4326, -1.0512],
        [-3.1238,  0.7665],
        [-3.0435,  1.6464],
        [-1.8999,  0.1065],
        [-2.0038,  0.2572],
        [-1.9940,  0.5334],
        [-4.0462, -0.4033],
        [-3.8501,  0.3133],
        [-0.7775, -0.6590],
        [-3.8544, -0.7608],
        [-1.8618,  0.3935],
        [-1.1237, -0.4924],
        [-2.5865, -0.3332],
        [-2.3655,  0.6770],
        [-1.9056, -0.1719],
        [-2.0169,  0.5229],
        [-1.1489, -0.5086],
        [-1.6791,  0.2926],
        [-1.3290, -0.1557],
        [-3.2495,  1.1446],
        [-2.2999,  0.8291],
        [-2.2701,  0.8040],
        [-2.7779,  1.3736],
        [-3.3944,  0.7634],
        [-1.8508,  0.4426],
        [-3.8003,  1.4452],
        [-1.9198,  0.3512],
        [-1.8818,  0.4004],
        [-3.0753,  1.0648],
        [-1.9553,  0.5635],
        [-4.2535, -0.3617],
        [-1.5470, -0.3721],
        [-2.9221,  1.5356],
        [-3.1147,  0.7960],
        [-2.6702,  1.2141],
        [-3.4344, -1.1809],
        [-2.4114,  0.4477],
        [-1.7688,  0.2292]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.3168e-04, 9.9987e-01],
        [2.5650e-02, 9.7435e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0923, 0.9077], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2970, 0.2403],
         [0.2341, 0.1943]],

        [[0.7052, 0.2237],
         [0.8162, 0.5074]],

        [[0.8184, 0.2076],
         [0.7881, 0.8373]],

        [[0.5734, 0.1929],
         [0.6479, 0.5182]],

        [[0.8393, 0.2588],
         [0.3663, 0.7590]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.006464646464646465
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0011530395238075323
Average Adjusted Rand Index: -0.001292929292929293
Iteration 0: Loss = -19700.227249572632
Iteration 10: Loss = -12296.087561149052
Iteration 20: Loss = -12295.8900090092
Iteration 30: Loss = -12295.821572600998
Iteration 40: Loss = -12295.76654149485
Iteration 50: Loss = -12295.699913372573
Iteration 60: Loss = -12295.60457304608
Iteration 70: Loss = -12295.47961048759
Iteration 80: Loss = -12295.365341776942
Iteration 90: Loss = -12295.291755956825
Iteration 100: Loss = -12295.238959841616
Iteration 110: Loss = -12295.189567832997
Iteration 120: Loss = -12295.140607843916
Iteration 130: Loss = -12295.095598335107
Iteration 140: Loss = -12295.060572001996
Iteration 150: Loss = -12295.040278787037
Iteration 160: Loss = -12295.035081420694
Iteration 170: Loss = -12295.041156198607
1
Iteration 180: Loss = -12295.053608884775
2
Iteration 190: Loss = -12295.068532012716
3
Stopping early at iteration 189 due to no improvement.
pi: tensor([[0.9697, 0.0303],
        [0.9866, 0.0134]], dtype=torch.float64)
alpha: tensor([0.9694, 0.0306])
beta: tensor([[[0.1935, 0.2628],
         [0.0477, 0.2645]],

        [[0.6887, 0.2136],
         [0.0448, 0.2132]],

        [[0.6779, 0.2030],
         [0.9419, 0.9035]],

        [[0.8352, 0.1871],
         [0.2243, 0.8380]],

        [[0.0145, 0.2518],
         [0.2605, 0.6061]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19700.072322913267
Iteration 100: Loss = -12335.946021740056
Iteration 200: Loss = -12304.588940317653
Iteration 300: Loss = -12297.86103674785
Iteration 400: Loss = -12296.648544615487
Iteration 500: Loss = -12295.976326717855
Iteration 600: Loss = -12295.57508396985
Iteration 700: Loss = -12295.328910509123
Iteration 800: Loss = -12295.162106218584
Iteration 900: Loss = -12295.040114681733
Iteration 1000: Loss = -12294.94687142339
Iteration 1100: Loss = -12294.874984235812
Iteration 1200: Loss = -12294.819525812472
Iteration 1300: Loss = -12294.777295949392
Iteration 1400: Loss = -12294.74599903282
Iteration 1500: Loss = -12294.723499967651
Iteration 1600: Loss = -12294.707371855175
Iteration 1700: Loss = -12294.694852616954
Iteration 1800: Loss = -12294.685258574795
Iteration 1900: Loss = -12294.677572614279
Iteration 2000: Loss = -12294.671291018516
Iteration 2100: Loss = -12294.66581577533
Iteration 2200: Loss = -12294.661149698737
Iteration 2300: Loss = -12295.064944616122
1
Iteration 2400: Loss = -12294.65387747734
Iteration 2500: Loss = -12294.650628675672
Iteration 2600: Loss = -12294.647864711813
Iteration 2700: Loss = -12294.645696341373
Iteration 2800: Loss = -12294.643973755354
Iteration 2900: Loss = -12294.64244295814
Iteration 3000: Loss = -12294.70210867812
1
Iteration 3100: Loss = -12294.639684925847
Iteration 3200: Loss = -12294.638389743426
Iteration 3300: Loss = -12294.637169992553
Iteration 3400: Loss = -12294.635905510435
Iteration 3500: Loss = -12294.634615288636
Iteration 3600: Loss = -12294.6333286609
Iteration 3700: Loss = -12294.676770704727
1
Iteration 3800: Loss = -12294.6284857551
Iteration 3900: Loss = -12294.626838388736
Iteration 4000: Loss = -12294.625139176927
Iteration 4100: Loss = -12294.624297483504
Iteration 4200: Loss = -12294.621657252563
Iteration 4300: Loss = -12294.619732925956
Iteration 4400: Loss = -12294.617888115723
Iteration 4500: Loss = -12294.615452360154
Iteration 4600: Loss = -12294.612844416728
Iteration 4700: Loss = -12294.609993053715
Iteration 4800: Loss = -12294.607447947092
Iteration 4900: Loss = -12294.603227711666
Iteration 5000: Loss = -12294.59919702321
Iteration 5100: Loss = -12294.59946868615
1
Iteration 5200: Loss = -12294.589466970116
Iteration 5300: Loss = -12294.583689715639
Iteration 5400: Loss = -12294.588025496987
1
Iteration 5500: Loss = -12294.562764736826
Iteration 5600: Loss = -12294.554621514862
Iteration 5700: Loss = -12294.546995447898
Iteration 5800: Loss = -12294.539889562704
Iteration 5900: Loss = -12294.533363435425
Iteration 6000: Loss = -12294.527703346641
Iteration 6100: Loss = -12294.523097689813
Iteration 6200: Loss = -12294.51959726368
Iteration 6300: Loss = -12294.517256844827
Iteration 6400: Loss = -12294.515686264143
Iteration 6500: Loss = -12294.51373069989
Iteration 6600: Loss = -12294.488997006465
Iteration 6700: Loss = -12294.486229687811
Iteration 6800: Loss = -12294.485205256924
Iteration 6900: Loss = -12294.484857925694
Iteration 7000: Loss = -12294.484472286698
Iteration 7100: Loss = -12294.518906919013
1
Iteration 7200: Loss = -12294.483630865796
Iteration 7300: Loss = -12294.50555139007
1
Iteration 7400: Loss = -12294.47525957656
Iteration 7500: Loss = -12294.534467432666
1
Iteration 7600: Loss = -12294.475006601268
Iteration 7700: Loss = -12294.475290682412
1
Iteration 7800: Loss = -12294.47485521622
Iteration 7900: Loss = -12294.476149477296
1
Iteration 8000: Loss = -12294.474702348429
Iteration 8100: Loss = -12294.474640782233
Iteration 8200: Loss = -12294.474715961194
1
Iteration 8300: Loss = -12294.474434377542
Iteration 8400: Loss = -12294.470976859906
Iteration 8500: Loss = -12294.467499741331
Iteration 8600: Loss = -12294.46733595168
Iteration 8700: Loss = -12294.467304834103
Iteration 8800: Loss = -12294.467314654463
1
Iteration 8900: Loss = -12294.467125602232
Iteration 9000: Loss = -12294.791180258426
1
Iteration 9100: Loss = -12294.467036689157
Iteration 9200: Loss = -12294.467018138504
Iteration 9300: Loss = -12294.470978211113
1
Iteration 9400: Loss = -12294.574465018753
2
Iteration 9500: Loss = -12294.468699936722
3
Iteration 9600: Loss = -12294.46788995867
4
Iteration 9700: Loss = -12294.467323860385
5
Iteration 9800: Loss = -12294.466811996224
Iteration 9900: Loss = -12294.466791426477
Iteration 10000: Loss = -12294.468058920113
1
Iteration 10100: Loss = -12294.467019879541
2
Iteration 10200: Loss = -12294.466788948424
Iteration 10300: Loss = -12294.589666278904
1
Iteration 10400: Loss = -12294.466651161012
Iteration 10500: Loss = -12294.475731116976
1
Iteration 10600: Loss = -12294.487679692002
2
Iteration 10700: Loss = -12294.46721079497
3
Iteration 10800: Loss = -12294.470189157533
4
Iteration 10900: Loss = -12294.469322138335
5
Iteration 11000: Loss = -12294.466531807244
Iteration 11100: Loss = -12294.46689068635
1
Iteration 11200: Loss = -12294.469124597632
2
Iteration 11300: Loss = -12294.466430551907
Iteration 11400: Loss = -12294.466771771129
1
Iteration 11500: Loss = -12294.46680291336
2
Iteration 11600: Loss = -12294.466771526855
3
Iteration 11700: Loss = -12294.483380935146
4
Iteration 11800: Loss = -12294.466415581486
Iteration 11900: Loss = -12294.46666781848
1
Iteration 12000: Loss = -12294.466479741986
2
Iteration 12100: Loss = -12294.46636788812
Iteration 12200: Loss = -12294.467546601933
1
Iteration 12300: Loss = -12294.467038127406
2
Iteration 12400: Loss = -12294.485637970107
3
Iteration 12500: Loss = -12294.466861770306
4
Iteration 12600: Loss = -12294.472249002487
5
Iteration 12700: Loss = -12294.481156298762
6
Iteration 12800: Loss = -12294.656129300887
7
Iteration 12900: Loss = -12294.456611039612
Iteration 13000: Loss = -12294.567945986453
1
Iteration 13100: Loss = -12294.457083110916
2
Iteration 13200: Loss = -12294.461107458053
3
Iteration 13300: Loss = -12294.45655614818
Iteration 13400: Loss = -12294.457046793166
1
Iteration 13500: Loss = -12294.459902275354
2
Iteration 13600: Loss = -12294.457197014919
3
Iteration 13700: Loss = -12294.461428207953
4
Iteration 13800: Loss = -12294.457140055427
5
Iteration 13900: Loss = -12294.45672053231
6
Iteration 14000: Loss = -12294.538273476377
7
Iteration 14100: Loss = -12294.456541330681
Iteration 14200: Loss = -12294.47756148422
1
Iteration 14300: Loss = -12294.4636526236
2
Iteration 14400: Loss = -12294.471234163362
3
Iteration 14500: Loss = -12294.461614338872
4
Iteration 14600: Loss = -12294.457222286257
5
Iteration 14700: Loss = -12294.460440866667
6
Iteration 14800: Loss = -12294.58324868064
7
Iteration 14900: Loss = -12294.456689220497
8
Iteration 15000: Loss = -12294.45651031874
Iteration 15100: Loss = -12294.530287178894
1
Iteration 15200: Loss = -12294.456416808653
Iteration 15300: Loss = -12294.45892040652
1
Iteration 15400: Loss = -12294.465968361177
2
Iteration 15500: Loss = -12294.459883189405
3
Iteration 15600: Loss = -12294.45930123792
4
Iteration 15700: Loss = -12294.456412579226
Iteration 15800: Loss = -12294.457412689884
1
Iteration 15900: Loss = -12294.541775569181
2
Iteration 16000: Loss = -12294.456993756701
3
Iteration 16100: Loss = -12294.46107874131
4
Iteration 16200: Loss = -12294.456396101094
Iteration 16300: Loss = -12294.465504874735
1
Iteration 16400: Loss = -12294.460673522304
2
Iteration 16500: Loss = -12294.46025486048
3
Iteration 16600: Loss = -12294.57449556561
4
Iteration 16700: Loss = -12294.538734102278
5
Iteration 16800: Loss = -12294.456880797754
6
Iteration 16900: Loss = -12294.45644200467
7
Iteration 17000: Loss = -12294.458422707186
8
Iteration 17100: Loss = -12294.580610602492
9
Iteration 17200: Loss = -12294.457393219667
10
Stopping early at iteration 17200 due to no improvement.
tensor([[-0.1859, -2.4367],
        [ 0.9496, -2.9218],
        [ 1.3788, -3.5825],
        [ 0.7893, -2.2689],
        [ 0.0166, -2.7766],
        [ 0.4407, -2.1115],
        [ 0.5693, -1.9681],
        [ 1.1662, -3.2665],
        [ 0.2802, -1.6931],
        [-1.9941, -2.6211],
        [ 0.4235, -1.8215],
        [ 0.9541, -2.3717],
        [ 0.0418, -3.2996],
        [ 1.0678, -3.0903],
        [ 1.2120, -2.6409],
        [ 0.4260, -2.0947],
        [ 0.2878, -3.0485],
        [ 1.0749, -3.0735],
        [ 0.4280, -2.9002],
        [ 0.8433, -2.2338],
        [ 0.2340, -2.0094],
        [ 1.1122, -2.7763],
        [ 0.8132, -2.2290],
        [ 0.1664, -1.5691],
        [ 0.3926, -1.8608],
        [ 0.8260, -2.5077],
        [ 2.0062, -3.4893],
        [ 1.0366, -2.6901],
        [ 0.2495, -2.2953],
        [ 0.5279, -2.2869],
        [ 1.1941, -2.9820],
        [-0.2750, -1.9856],
        [-0.8978, -0.6955],
        [ 0.0768, -2.1558],
        [ 1.2375, -2.9112],
        [ 0.6748, -2.1588],
        [ 1.0926, -2.5108],
        [ 0.1513, -1.5414],
        [-0.2601, -2.5430],
        [ 0.2411, -2.8540],
        [ 1.4890, -3.5003],
        [ 0.1475, -1.5525],
        [ 0.0593, -1.9262],
        [ 0.2291, -2.0521],
        [ 0.4356, -1.8299],
        [ 0.2357, -2.8134],
        [-1.0955, -0.9256],
        [ 0.0478, -3.0202],
        [-0.1175, -1.3020],
        [ 0.9311, -2.9334],
        [ 0.1820, -2.0684],
        [ 0.2945, -1.6818],
        [ 0.0439, -1.4307],
        [ 0.8440, -2.2352],
        [ 0.0873, -2.4727],
        [ 0.6900, -2.1550],
        [ 0.5783, -2.2266],
        [ 0.7499, -2.5900],
        [ 0.0997, -1.5889],
        [ 0.7191, -3.1379],
        [-0.4579, -3.2756],
        [-0.0268, -2.8309],
        [-0.9018, -1.2777],
        [ 1.2193, -2.6606],
        [ 1.5241, -3.1540],
        [ 0.2811, -1.7157],
        [ 0.2608, -1.9900],
        [ 0.0771, -2.4397],
        [ 1.1086, -2.5243],
        [ 1.3227, -2.8304],
        [-0.7551, -0.8693],
        [ 0.8384, -2.2452],
        [ 0.2814, -1.9635],
        [-0.4293, -1.0544],
        [ 0.1001, -2.1429],
        [ 0.1756, -2.8567],
        [-0.2034, -1.9287],
        [ 0.5706, -1.9585],
        [-0.6651, -1.2993],
        [-0.1915, -2.1533],
        [-0.1565, -1.3222],
        [ 1.4219, -2.9602],
        [ 0.7570, -2.3618],
        [ 0.2425, -2.8216],
        [ 1.0731, -3.0678],
        [ 1.0695, -3.0777],
        [-0.1690, -2.4521],
        [ 1.9008, -3.3301],
        [ 0.4144, -1.8463],
        [ 0.4426, -1.8293],
        [ 1.1209, -3.0086],
        [ 0.5004, -2.0080],
        [ 1.2476, -2.6339],
        [-0.2658, -1.4331],
        [ 1.5159, -2.9310],
        [ 1.1925, -2.7081],
        [ 1.1838, -2.6899],
        [ 0.3930, -1.8501],
        [-0.1457, -2.9952],
        [ 0.2121, -1.7763]], dtype=torch.float64, requires_grad=True)
pi: tensor([[9.7454e-01, 2.5461e-02],
        [9.9996e-01, 4.3042e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9078, 0.0922], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1946, 0.2403],
         [0.0477, 0.2971]],

        [[0.6887, 0.2235],
         [0.0448, 0.2132]],

        [[0.6779, 0.2074],
         [0.9419, 0.9035]],

        [[0.8352, 0.1926],
         [0.2243, 0.8380]],

        [[0.0145, 0.2588],
         [0.2605, 0.6061]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0011530395238075323
Average Adjusted Rand Index: -0.001292929292929293
Iteration 0: Loss = -25531.87183595939
Iteration 10: Loss = -12295.359301640032
Iteration 20: Loss = -12295.226064208002
Iteration 30: Loss = -12295.151875393598
Iteration 40: Loss = -12295.09888353088
Iteration 50: Loss = -12295.06035587518
Iteration 60: Loss = -12295.038690586925
Iteration 70: Loss = -12295.033226933443
Iteration 80: Loss = -12295.039367290772
1
Iteration 90: Loss = -12295.051999995903
2
Iteration 100: Loss = -12295.0671005745
3
Stopping early at iteration 99 due to no improvement.
pi: tensor([[0.9699, 0.0301],
        [0.9772, 0.0228]], dtype=torch.float64)
alpha: tensor([0.9693, 0.0307])
beta: tensor([[[0.1935, 0.2628],
         [0.9989, 0.2644]],

        [[0.9464, 0.2135],
         [0.5556, 0.2019]],

        [[0.9009, 0.2031],
         [0.3855, 0.4550]],

        [[0.2807, 0.1871],
         [0.8997, 0.7641]],

        [[0.4621, 0.2519],
         [0.5055, 0.4353]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25531.53225990536
Iteration 100: Loss = -12282.547129645076
Iteration 200: Loss = -12124.78999672136
Iteration 300: Loss = -12075.007437228176
Iteration 400: Loss = -11993.853533249001
Iteration 500: Loss = -11918.900325067249
Iteration 600: Loss = -11901.432443930662
Iteration 700: Loss = -11890.605788894227
Iteration 800: Loss = -11869.533958287664
Iteration 900: Loss = -11869.469464742708
Iteration 1000: Loss = -11869.350835495596
Iteration 1100: Loss = -11860.484189283732
Iteration 1200: Loss = -11860.461085612096
Iteration 1300: Loss = -11860.40136599702
Iteration 1400: Loss = -11860.388235584185
Iteration 1500: Loss = -11860.378478021436
Iteration 1600: Loss = -11860.371081357622
Iteration 1700: Loss = -11860.365968969087
Iteration 1800: Loss = -11860.36179798708
Iteration 1900: Loss = -11860.35829879612
Iteration 2000: Loss = -11860.355344097716
Iteration 2100: Loss = -11860.352743798985
Iteration 2200: Loss = -11860.35049997569
Iteration 2300: Loss = -11860.348511831304
Iteration 2400: Loss = -11860.346768921434
Iteration 2500: Loss = -11860.345088487698
Iteration 2600: Loss = -11860.34335702472
Iteration 2700: Loss = -11860.302862826471
Iteration 2800: Loss = -11850.542935382982
Iteration 2900: Loss = -11850.541308075935
Iteration 3000: Loss = -11850.537631280225
Iteration 3100: Loss = -11844.27024641948
Iteration 3200: Loss = -11844.269132875017
Iteration 3300: Loss = -11844.268722699822
Iteration 3400: Loss = -11844.267490903518
Iteration 3500: Loss = -11844.266846074503
Iteration 3600: Loss = -11844.26652710892
Iteration 3700: Loss = -11844.265687814033
Iteration 3800: Loss = -11844.265200382244
Iteration 3900: Loss = -11844.270043211676
1
Iteration 4000: Loss = -11844.264367456639
Iteration 4100: Loss = -11844.263937440148
Iteration 4200: Loss = -11844.263669964477
Iteration 4300: Loss = -11844.263385121787
Iteration 4400: Loss = -11844.263012881705
Iteration 4500: Loss = -11844.262719987722
Iteration 4600: Loss = -11844.264364864955
1
Iteration 4700: Loss = -11844.262262755608
Iteration 4800: Loss = -11844.262082840698
Iteration 4900: Loss = -11844.261849400287
Iteration 5000: Loss = -11844.274723199456
1
Iteration 5100: Loss = -11844.261571826284
Iteration 5200: Loss = -11844.261648663814
1
Iteration 5300: Loss = -11844.263140632254
2
Iteration 5400: Loss = -11844.26112069341
Iteration 5500: Loss = -11844.261273215954
1
Iteration 5600: Loss = -11844.260885450354
Iteration 5700: Loss = -11844.260525850163
Iteration 5800: Loss = -11844.260544749815
1
Iteration 5900: Loss = -11844.28676637437
2
Iteration 6000: Loss = -11844.222298664321
Iteration 6100: Loss = -11844.215572863168
Iteration 6200: Loss = -11844.215300452888
Iteration 6300: Loss = -11844.216441114155
1
Iteration 6400: Loss = -11844.226606783588
2
Iteration 6500: Loss = -11844.236157391111
3
Iteration 6600: Loss = -11844.214871604212
Iteration 6700: Loss = -11844.215330112489
1
Iteration 6800: Loss = -11844.215041110276
2
Iteration 6900: Loss = -11844.2178279473
3
Iteration 7000: Loss = -11844.216937669644
4
Iteration 7100: Loss = -11844.215138125479
5
Iteration 7200: Loss = -11844.215030114083
6
Iteration 7300: Loss = -11844.214808900677
Iteration 7400: Loss = -11844.220001381267
1
Iteration 7500: Loss = -11844.217788848016
2
Iteration 7600: Loss = -11844.214403651375
Iteration 7700: Loss = -11844.214107829457
Iteration 7800: Loss = -11844.216247203074
1
Iteration 7900: Loss = -11844.217592831312
2
Iteration 8000: Loss = -11844.230640635096
3
Iteration 8100: Loss = -11844.214498728094
4
Iteration 8200: Loss = -11844.219500835794
5
Iteration 8300: Loss = -11844.216569891418
6
Iteration 8400: Loss = -11844.213668674054
Iteration 8500: Loss = -11844.258208399451
1
Iteration 8600: Loss = -11844.220418096498
2
Iteration 8700: Loss = -11844.213949806748
3
Iteration 8800: Loss = -11844.213383951235
Iteration 8900: Loss = -11844.213727674356
1
Iteration 9000: Loss = -11844.21350590296
2
Iteration 9100: Loss = -11844.215480605428
3
Iteration 9200: Loss = -11844.214245860707
4
Iteration 9300: Loss = -11844.213129013517
Iteration 9400: Loss = -11844.212685211036
Iteration 9500: Loss = -11844.212864937917
1
Iteration 9600: Loss = -11844.21158667339
Iteration 9700: Loss = -11844.213829288501
1
Iteration 9800: Loss = -11844.21253693751
2
Iteration 9900: Loss = -11844.213098018223
3
Iteration 10000: Loss = -11844.263801924875
4
Iteration 10100: Loss = -11844.220663482834
5
Iteration 10200: Loss = -11844.211725929586
6
Iteration 10300: Loss = -11844.214435035477
7
Iteration 10400: Loss = -11844.211777069255
8
Iteration 10500: Loss = -11844.217069189468
9
Iteration 10600: Loss = -11844.21718381925
10
Stopping early at iteration 10600 due to no improvement.
tensor([[ 6.9939, -8.4309],
        [-6.8298,  5.4123],
        [ 1.8658, -3.3228],
        [-8.6838,  7.1183],
        [ 5.9467, -7.5554],
        [-7.8873,  6.2347],
        [-8.6327,  7.1544],
        [-5.2234,  3.5150],
        [ 5.7925, -7.5230],
        [ 7.3338, -8.8410],
        [-9.5325,  6.5368],
        [-6.8842,  5.4297],
        [ 4.5032, -6.4771],
        [-6.6958,  5.0702],
        [ 4.2753, -5.9531],
        [ 2.8224, -5.4312],
        [-7.6223,  6.0325],
        [-8.3537,  6.9302],
        [ 5.3077, -9.7347],
        [-9.6728,  7.3619],
        [ 5.3485, -7.3985],
        [ 6.0069, -7.4755],
        [-4.1594,  2.5634],
        [-8.9213,  7.2114],
        [-0.7002, -1.4731],
        [-7.3902,  5.6011],
        [-7.9717,  5.3796],
        [ 5.2339, -6.9780],
        [ 7.2909, -8.7331],
        [-2.1932,  0.8033],
        [-6.4267,  4.6038],
        [ 2.5002, -5.9088],
        [ 7.3178, -9.1496],
        [ 4.3589, -5.7667],
        [-7.6727,  6.2372],
        [-7.4930,  5.5336],
        [-6.5637,  2.6803],
        [-4.7282,  3.3172],
        [-6.9810,  5.2098],
        [ 6.3077, -7.9303],
        [-4.6275,  0.0123],
        [ 7.0003, -9.3763],
        [-6.6227,  5.1874],
        [ 7.4873, -9.0188],
        [ 4.0111, -6.2224],
        [ 2.5302, -4.0131],
        [ 7.9121, -9.5310],
        [ 2.8156, -4.3611],
        [-5.5296,  3.3155],
        [ 6.8122, -8.2296],
        [ 7.5000, -9.5816],
        [-8.1847,  6.7509],
        [-7.9076,  6.4099],
        [ 3.6782, -5.2715],
        [-6.4130,  4.6321],
        [-6.5473,  4.7611],
        [-8.0813,  6.3538],
        [ 1.9415, -3.3316],
        [ 4.2857, -6.6084],
        [ 4.4867, -6.4727],
        [ 7.3480, -8.9557],
        [-5.7030,  2.3172],
        [ 3.5252, -5.3607],
        [-6.0498,  4.4587],
        [-2.8729,  1.2564],
        [-5.8987,  2.6045],
        [-4.3254,  2.9297],
        [ 6.7071, -8.0992],
        [-5.4151,  3.9109],
        [ 2.4684, -4.8243],
        [ 6.7969, -9.1486],
        [ 5.0999, -7.2970],
        [ 6.3069, -8.1085],
        [ 3.9368, -5.4454],
        [ 7.0327, -8.8922],
        [ 4.5642, -6.2233],
        [-9.2250,  5.9369],
        [-7.1398,  4.5141],
        [ 7.7377, -9.3591],
        [ 5.2576, -8.8274],
        [ 5.8991, -8.1467],
        [-0.9263, -1.6145],
        [-8.3154,  6.8120],
        [ 3.8907, -5.8294],
        [-7.5391,  4.1489],
        [-5.4242,  3.6848],
        [-3.1317,  1.6103],
        [ 1.9903, -5.1999],
        [ 3.1176, -4.5858],
        [ 6.9848, -9.1062],
        [ 2.9189, -6.7785],
        [ 6.4519, -8.0091],
        [ 6.0215, -7.4242],
        [ 6.6079, -8.1071],
        [-5.0771,  3.6845],
        [ 5.9090, -7.5122],
        [ 5.7864, -7.5791],
        [ 6.8581, -8.8648],
        [ 4.3428, -5.7499],
        [-6.1795,  4.7804]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.3971, 0.6029],
        [0.5783, 0.4217]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5540, 0.4460], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2946, 0.1065],
         [0.9989, 0.2969]],

        [[0.9464, 0.0930],
         [0.5556, 0.2019]],

        [[0.9009, 0.0985],
         [0.3855, 0.4550]],

        [[0.2807, 0.0961],
         [0.8997, 0.7641]],

        [[0.4621, 0.1012],
         [0.5055, 0.4353]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 1
tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.03494239431461122
Average Adjusted Rand Index: 0.9839992163297293
Iteration 0: Loss = -24167.263033923206
Iteration 10: Loss = -12296.024222010738
Iteration 20: Loss = -12296.02422201074
1
Iteration 30: Loss = -12296.024222010743
2
Iteration 40: Loss = -12296.02422201075
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[1.7482e-19, 1.0000e+00],
        [5.2165e-16, 1.0000e+00]], dtype=torch.float64)
alpha: tensor([5.9888e-16, 1.0000e+00])
beta: tensor([[[0.3269, 0.2889],
         [0.1119, 0.1954]],

        [[0.3897, 0.1997],
         [0.2024, 0.7671]],

        [[0.9470, 0.1953],
         [0.2903, 0.1387]],

        [[0.0460, 0.1651],
         [0.9922, 0.5757]],

        [[0.6080, 0.2649],
         [0.6442, 0.9425]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24166.384462539507
Iteration 100: Loss = -12355.260960600372
Iteration 200: Loss = -12324.978947743026
Iteration 300: Loss = -12310.33864463202
Iteration 400: Loss = -12301.090300703654
Iteration 500: Loss = -12297.503338192224
Iteration 600: Loss = -12296.703118433783
Iteration 700: Loss = -12296.215961077902
Iteration 800: Loss = -12295.889665772533
Iteration 900: Loss = -12295.655011846797
Iteration 1000: Loss = -12295.478852403849
Iteration 1100: Loss = -12295.342675846789
Iteration 1200: Loss = -12295.234890448668
Iteration 1300: Loss = -12295.14762752034
Iteration 1400: Loss = -12295.075438744578
Iteration 1500: Loss = -12295.014329378213
Iteration 1600: Loss = -12294.96098996476
Iteration 1700: Loss = -12294.91084234611
Iteration 1800: Loss = -12294.855676621935
Iteration 1900: Loss = -12294.812265172968
Iteration 2000: Loss = -12294.7743296641
Iteration 2100: Loss = -12294.751061415502
Iteration 2200: Loss = -12294.73324213568
Iteration 2300: Loss = -12294.71675687937
Iteration 2400: Loss = -12294.70114408049
Iteration 2500: Loss = -12294.687206105358
Iteration 2600: Loss = -12294.676782710489
Iteration 2700: Loss = -12294.669658540806
Iteration 2800: Loss = -12294.703428727074
1
Iteration 2900: Loss = -12294.65897448176
Iteration 3000: Loss = -12294.65438011145
Iteration 3100: Loss = -12294.650161426493
Iteration 3200: Loss = -12294.648137521515
Iteration 3300: Loss = -12294.64311051436
Iteration 3400: Loss = -12294.64023284175
Iteration 3500: Loss = -12294.638024863747
Iteration 3600: Loss = -12294.635735406835
Iteration 3700: Loss = -12294.633641614626
Iteration 3800: Loss = -12294.63176668014
Iteration 3900: Loss = -12294.649262938343
1
Iteration 4000: Loss = -12294.628131526768
Iteration 4100: Loss = -12294.62632090841
Iteration 4200: Loss = -12294.644126251038
1
Iteration 4300: Loss = -12294.62278429304
Iteration 4400: Loss = -12294.620980025333
Iteration 4500: Loss = -12294.619152583839
Iteration 4600: Loss = -12294.61760288366
Iteration 4700: Loss = -12294.615401512032
Iteration 4800: Loss = -12294.613410415377
Iteration 4900: Loss = -12294.774957166146
1
Iteration 5000: Loss = -12294.609177759463
Iteration 5100: Loss = -12294.606957221165
Iteration 5200: Loss = -12294.60462044539
Iteration 5300: Loss = -12294.602576022186
Iteration 5400: Loss = -12294.599562792597
Iteration 5500: Loss = -12294.59685265179
Iteration 5600: Loss = -12294.654515535733
1
Iteration 5700: Loss = -12294.590869607366
Iteration 5800: Loss = -12294.587654751957
Iteration 5900: Loss = -12294.584791296642
Iteration 6000: Loss = -12294.5808997531
Iteration 6100: Loss = -12294.577342486555
Iteration 6200: Loss = -12294.573729959631
Iteration 6300: Loss = -12294.571490402193
Iteration 6400: Loss = -12294.566309181484
Iteration 6500: Loss = -12294.562304887271
Iteration 6600: Loss = -12294.557582434005
Iteration 6700: Loss = -12294.550183116648
Iteration 6800: Loss = -12294.54596871601
Iteration 6900: Loss = -12294.535675271914
Iteration 7000: Loss = -12294.550389249176
1
Iteration 7100: Loss = -12294.513510029245
Iteration 7200: Loss = -12294.507145744825
Iteration 7300: Loss = -12294.52444017858
1
Iteration 7400: Loss = -12294.496245277149
Iteration 7500: Loss = -12294.492566134624
Iteration 7600: Loss = -12294.491085186912
Iteration 7700: Loss = -12294.48672731413
Iteration 7800: Loss = -12294.483778806984
Iteration 7900: Loss = -12294.48210952889
Iteration 8000: Loss = -12294.480824298
Iteration 8100: Loss = -12294.47985448911
Iteration 8200: Loss = -12294.478725532455
Iteration 8300: Loss = -12294.4623167295
Iteration 8400: Loss = -12294.460901885759
Iteration 8500: Loss = -12294.462892151943
1
Iteration 8600: Loss = -12294.459907017563
Iteration 8700: Loss = -12294.461502845836
1
Iteration 8800: Loss = -12294.459431043926
Iteration 8900: Loss = -12294.479159583243
1
Iteration 9000: Loss = -12294.459148302196
Iteration 9100: Loss = -12294.458977672
Iteration 9200: Loss = -12294.496779897916
1
Iteration 9300: Loss = -12294.458711567944
Iteration 9400: Loss = -12294.458614815352
Iteration 9500: Loss = -12294.486544657668
1
Iteration 9600: Loss = -12294.458424873097
Iteration 9700: Loss = -12294.458322809027
Iteration 9800: Loss = -12294.756630750519
1
Iteration 9900: Loss = -12294.458139411916
Iteration 10000: Loss = -12294.458130493307
Iteration 10100: Loss = -12294.520067015026
1
Iteration 10200: Loss = -12294.457962736882
Iteration 10300: Loss = -12294.457896871201
Iteration 10400: Loss = -12294.458302546582
1
Iteration 10500: Loss = -12294.460947645179
2
Iteration 10600: Loss = -12294.457808262867
Iteration 10700: Loss = -12294.457708599512
Iteration 10800: Loss = -12294.483031227404
1
Iteration 10900: Loss = -12294.457518325265
Iteration 11000: Loss = -12294.458535475269
1
Iteration 11100: Loss = -12294.457780114557
2
Iteration 11200: Loss = -12294.457479339822
Iteration 11300: Loss = -12294.459316564113
1
Iteration 11400: Loss = -12294.457263946388
Iteration 11500: Loss = -12294.457257897717
Iteration 11600: Loss = -12294.457386300555
1
Iteration 11700: Loss = -12294.457228790767
Iteration 11800: Loss = -12294.465038116443
1
Iteration 11900: Loss = -12294.457560256327
2
Iteration 12000: Loss = -12294.542182514804
3
Iteration 12100: Loss = -12294.457619144681
4
Iteration 12200: Loss = -12294.755036109598
5
Iteration 12300: Loss = -12294.45697089048
Iteration 12400: Loss = -12294.45859557645
1
Iteration 12500: Loss = -12294.456915125627
Iteration 12600: Loss = -12294.456989881392
1
Iteration 12700: Loss = -12294.45690295181
Iteration 12800: Loss = -12294.456846961906
Iteration 12900: Loss = -12294.459852253349
1
Iteration 13000: Loss = -12294.456856476965
2
Iteration 13100: Loss = -12294.456848763642
3
Iteration 13200: Loss = -12294.456895298645
4
Iteration 13300: Loss = -12294.456750162652
Iteration 13400: Loss = -12294.457541123185
1
Iteration 13500: Loss = -12294.456686731815
Iteration 13600: Loss = -12294.4568103226
1
Iteration 13700: Loss = -12294.456831839801
2
Iteration 13800: Loss = -12294.456638687287
Iteration 13900: Loss = -12294.468507142059
1
Iteration 14000: Loss = -12294.456611077858
Iteration 14100: Loss = -12294.456898887347
1
Iteration 14200: Loss = -12294.45664685982
2
Iteration 14300: Loss = -12294.456624080944
3
Iteration 14400: Loss = -12294.460588053711
4
Iteration 14500: Loss = -12294.456986179566
5
Iteration 14600: Loss = -12294.456679562214
6
Iteration 14700: Loss = -12294.457147962383
7
Iteration 14800: Loss = -12294.4566573773
8
Iteration 14900: Loss = -12294.599122342473
9
Iteration 15000: Loss = -12294.456554582255
Iteration 15100: Loss = -12294.464609292578
1
Iteration 15200: Loss = -12294.456511669527
Iteration 15300: Loss = -12294.464538106151
1
Iteration 15400: Loss = -12294.456476382462
Iteration 15500: Loss = -12294.457552196685
1
Iteration 15600: Loss = -12294.456596820912
2
Iteration 15700: Loss = -12294.463455767363
3
Iteration 15800: Loss = -12294.456482528867
4
Iteration 15900: Loss = -12294.474281075345
5
Iteration 16000: Loss = -12294.45743169548
6
Iteration 16100: Loss = -12294.457055070425
7
Iteration 16200: Loss = -12294.456445268943
Iteration 16300: Loss = -12294.473090011787
1
Iteration 16400: Loss = -12294.456420748533
Iteration 16500: Loss = -12294.457141503914
1
Iteration 16600: Loss = -12294.45654661086
2
Iteration 16700: Loss = -12294.478636694548
3
Iteration 16800: Loss = -12294.458447767105
4
Iteration 16900: Loss = -12294.456772788462
5
Iteration 17000: Loss = -12294.517123071959
6
Iteration 17100: Loss = -12294.45719181547
7
Iteration 17200: Loss = -12294.483740708585
8
Iteration 17300: Loss = -12294.45691963662
9
Iteration 17400: Loss = -12294.467233843743
10
Stopping early at iteration 17400 due to no improvement.
tensor([[-3.0252, -0.7769],
        [-3.0552,  0.8025],
        [-3.1813,  1.7635],
        [-3.5228, -0.4729],
        [-2.1786,  0.6082],
        [-2.7358, -0.1883],
        [-2.3152,  0.2175],
        [-4.5160, -0.0993],
        [-1.7955,  0.1765],
        [-1.0433, -0.4133],
        [-1.9854,  0.2566],
        [-2.3525,  0.9633],
        [-2.4131,  0.9191],
        [-2.7650,  1.3787],
        [-2.6601,  1.1805],
        [-1.9649,  0.5513],
        [-2.8187,  0.5070],
        [-3.3736,  0.7600],
        [-2.3982,  0.9211],
        [-2.4582,  0.6103],
        [-3.1337, -0.8928],
        [-3.1539,  0.7212],
        [-2.8904,  0.1438],
        [-1.5692,  0.1636],
        [-1.8213,  0.4296],
        [-2.5372,  0.7863],
        [-5.0453,  0.4301],
        [-2.5569,  1.1583],
        [-2.0266,  0.5142],
        [-2.1331,  0.6752],
        [-2.8846,  1.2769],
        [-1.6371,  0.0732],
        [-1.5659, -1.7622],
        [-1.8852,  0.3446],
        [-3.0496,  1.0840],
        [-2.2227,  0.6046],
        [-2.6663,  0.9258],
        [-2.8942, -1.2015],
        [-1.8915,  0.3881],
        [-2.4131,  0.6738],
        [-4.7933,  0.1781],
        [-1.5951,  0.1052],
        [-1.7315,  0.2521],
        [-2.0747,  0.2034],
        [-2.1815,  0.0814],
        [-2.2976,  0.7435],
        [-0.6580, -0.8217],
        [-2.4920,  0.5676],
        [-1.2866, -0.1012],
        [-2.6884,  1.1629],
        [-1.8540,  0.3941],
        [-1.9324,  0.0422],
        [-1.4335,  0.0408],
        [-2.7918,  0.2788],
        [-2.4553,  0.1001],
        [-2.1187,  0.7196],
        [-2.6856,  0.1129],
        [-2.5165,  0.8137],
        [-2.3839, -0.6946],
        [-2.6170,  1.2277],
        [-2.8386, -0.0267],
        [-3.0166, -0.2190],
        [-1.0068, -0.6262],
        [-2.6750,  1.1915],
        [-3.0318,  1.6298],
        [-1.9751,  0.0196],
        [-1.8850,  0.3634],
        [-1.9520,  0.5603],
        [-2.5343,  1.0864],
        [-4.0683,  0.0699],
        [-1.1369, -1.0187],
        [-2.2630,  0.8125],
        [-2.5213, -0.2792],
        [-1.8517, -1.2240],
        [-1.8188,  0.4217],
        [-2.2086,  0.8160],
        [-1.8339, -0.1110],
        [-2.5337, -0.0091],
        [-1.5771, -0.9407],
        [-1.6831,  0.2781],
        [-1.3436, -0.1757],
        [-2.9145,  1.4538],
        [-2.5145,  0.5955],
        [-2.2970,  0.7593],
        [-3.0017,  1.1241],
        [-3.0844,  1.0475],
        [-3.4479, -1.1674],
        [-3.3474,  1.8654],
        [-1.9844,  0.2735],
        [-1.8365,  0.4331],
        [-2.7626,  1.3528],
        [-1.9486,  0.5555],
        [-4.0258, -0.1565],
        [-1.2950, -0.1258],
        [-3.1078,  1.3232],
        [-2.9862,  0.9013],
        [-3.4582,  0.4032],
        [-2.4564, -0.2159],
        [-2.9317, -0.0886],
        [-2.7472, -0.7614]], dtype=torch.float64, requires_grad=True)
pi: tensor([[8.6996e-05, 9.9991e-01],
        [2.5930e-02, 9.7407e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0932, 0.9068], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2965, 0.2400],
         [0.1119, 0.1942]],

        [[0.3897, 0.2238],
         [0.2024, 0.7671]],

        [[0.9470, 0.2078],
         [0.2903, 0.1387]],

        [[0.0460, 0.1932],
         [0.9922, 0.5757]],

        [[0.6080, 0.2588],
         [0.6442, 0.9425]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.006464646464646465
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0011530395238075323
Average Adjusted Rand Index: -0.001292929292929293
11806.594479028518
new:  [-0.0011530395238075323, -0.0011530395238075323, 0.03494239431461122, -0.0011530395238075323] [-0.001292929292929293, -0.001292929292929293, 0.9839992163297293, -0.001292929292929293] [12294.476426621613, 12294.457393219667, 11844.21718381925, 12294.467233843743]
prior:  [0.0, 0.0, 0.0, 0.0] [0.0, 0.0, 0.0, 0.0] [nan, 12295.068532012716, 12295.0671005745, 12296.02422201075]
-----------------------------------------------------------------------------------------
This iteration is 17
True Objective function: Loss = -11797.16589123401
Iteration 0: Loss = -16417.141762453248
Iteration 10: Loss = -12294.802105015038
Iteration 20: Loss = -12294.792923720266
Iteration 30: Loss = -12294.785740448506
Iteration 40: Loss = -12294.7793301017
Iteration 50: Loss = -12294.773581415524
Iteration 60: Loss = -12294.768404643042
Iteration 70: Loss = -12294.763687422987
Iteration 80: Loss = -12294.759386697684
Iteration 90: Loss = -12294.755402635425
Iteration 100: Loss = -12294.751850253731
Iteration 110: Loss = -12294.748486055703
Iteration 120: Loss = -12294.745398063078
Iteration 130: Loss = -12294.742544934148
Iteration 140: Loss = -12294.739926376706
Iteration 150: Loss = -12294.737435294624
Iteration 160: Loss = -12294.73514341241
Iteration 170: Loss = -12294.732984972392
Iteration 180: Loss = -12294.730967750416
Iteration 190: Loss = -12294.729037329946
Iteration 200: Loss = -12294.727230629256
Iteration 210: Loss = -12294.725550305579
Iteration 220: Loss = -12294.723965602483
Iteration 230: Loss = -12294.722446870834
Iteration 240: Loss = -12294.72101580582
Iteration 250: Loss = -12294.719608272775
Iteration 260: Loss = -12294.718332833478
Iteration 270: Loss = -12294.717125581235
Iteration 280: Loss = -12294.715960397843
Iteration 290: Loss = -12294.714853115147
Iteration 300: Loss = -12294.713748802591
Iteration 310: Loss = -12294.71276835266
Iteration 320: Loss = -12294.711774846239
Iteration 330: Loss = -12294.7108555953
Iteration 340: Loss = -12294.709923044767
Iteration 350: Loss = -12294.709031095072
Iteration 360: Loss = -12294.708207542362
Iteration 370: Loss = -12294.707410449178
Iteration 380: Loss = -12294.706605314239
Iteration 390: Loss = -12294.705853786854
Iteration 400: Loss = -12294.705141933444
Iteration 410: Loss = -12294.704381456904
Iteration 420: Loss = -12294.703719179195
Iteration 430: Loss = -12294.703058649027
Iteration 440: Loss = -12294.702424772955
Iteration 450: Loss = -12294.701728627564
Iteration 460: Loss = -12294.701096481305
Iteration 470: Loss = -12294.700451177505
Iteration 480: Loss = -12294.699846534613
Iteration 490: Loss = -12294.699180798303
Iteration 500: Loss = -12294.698569470229
Iteration 510: Loss = -12294.697922497378
Iteration 520: Loss = -12294.697274255039
Iteration 530: Loss = -12294.69661685015
Iteration 540: Loss = -12294.695960456582
Iteration 550: Loss = -12294.6952568292
Iteration 560: Loss = -12294.694538012549
Iteration 570: Loss = -12294.69380849345
Iteration 580: Loss = -12294.693067595499
Iteration 590: Loss = -12294.69225496938
Iteration 600: Loss = -12294.69136879841
Iteration 610: Loss = -12294.69050729559
Iteration 620: Loss = -12294.689569420574
Iteration 630: Loss = -12294.688572016437
Iteration 640: Loss = -12294.687456717093
Iteration 650: Loss = -12294.686333734078
Iteration 660: Loss = -12294.685095085195
Iteration 670: Loss = -12294.683783770106
Iteration 680: Loss = -12294.682418432345
Iteration 690: Loss = -12294.680921344336
Iteration 700: Loss = -12294.6792675084
Iteration 710: Loss = -12294.677638570525
Iteration 720: Loss = -12294.675901856974
Iteration 730: Loss = -12294.673982069427
Iteration 740: Loss = -12294.672113085251
Iteration 750: Loss = -12294.670044755028
Iteration 760: Loss = -12294.668002680346
Iteration 770: Loss = -12294.665890742046
Iteration 780: Loss = -12294.663710390045
Iteration 790: Loss = -12294.661630545255
Iteration 800: Loss = -12294.659497770172
Iteration 810: Loss = -12294.657459701803
Iteration 820: Loss = -12294.655476551097
Iteration 830: Loss = -12294.653521765804
Iteration 840: Loss = -12294.651739965046
Iteration 850: Loss = -12294.650125586339
Iteration 860: Loss = -12294.648600239427
Iteration 870: Loss = -12294.647212649064
Iteration 880: Loss = -12294.645996020126
Iteration 890: Loss = -12294.644969753877
Iteration 900: Loss = -12294.644040471681
Iteration 910: Loss = -12294.64329687715
Iteration 920: Loss = -12294.642607786544
Iteration 930: Loss = -12294.642123236652
Iteration 940: Loss = -12294.641663833812
Iteration 950: Loss = -12294.641369440616
Iteration 960: Loss = -12294.641110915783
Iteration 970: Loss = -12294.640972391502
Iteration 980: Loss = -12294.64081426665
Iteration 990: Loss = -12294.640786251819
Iteration 1000: Loss = -12294.640746035073
Iteration 1010: Loss = -12294.640764638109
1
Iteration 1020: Loss = -12294.640772178092
2
Iteration 1030: Loss = -12294.64082784597
3
Stopping early at iteration 1029 due to no improvement.
pi: tensor([[0.1387, 0.8613],
        [0.1264, 0.8736]], dtype=torch.float64)
alpha: tensor([0.1285, 0.8715])
beta: tensor([[[0.2084, 0.2085],
         [0.7210, 0.1936]],

        [[0.6471, 0.1938],
         [0.0994, 0.6662]],

        [[0.8163, 0.2041],
         [0.7644, 0.0126]],

        [[0.4460, 0.1806],
         [0.8952, 0.3613]],

        [[0.8940, 0.2161],
         [0.0735, 0.8916]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -16489.76319052758
Iteration 100: Loss = -12297.533204116158
Iteration 200: Loss = -12296.191878410695
Iteration 300: Loss = -12295.85697915527
Iteration 400: Loss = -12295.718397990517
Iteration 500: Loss = -12295.64464057488
Iteration 600: Loss = -12295.586791937765
Iteration 700: Loss = -12295.513720220664
Iteration 800: Loss = -12295.38294399587
Iteration 900: Loss = -12295.185782839279
Iteration 1000: Loss = -12295.058646717867
Iteration 1100: Loss = -12294.993174800218
Iteration 1200: Loss = -12294.935845179249
Iteration 1300: Loss = -12294.872303561722
Iteration 1400: Loss = -12294.77653969135
Iteration 1500: Loss = -12294.635442150699
Iteration 1600: Loss = -12294.503163540436
Iteration 1700: Loss = -12294.364643771098
Iteration 1800: Loss = -12294.198341770407
Iteration 1900: Loss = -12294.024887603498
Iteration 2000: Loss = -12293.930451974
Iteration 2100: Loss = -12293.897314629088
Iteration 2200: Loss = -12293.882370955052
Iteration 2300: Loss = -12293.874290174974
Iteration 2400: Loss = -12293.8948742176
1
Iteration 2500: Loss = -12293.855551878072
Iteration 2600: Loss = -12293.839754140063
Iteration 2700: Loss = -12293.792459426499
Iteration 2800: Loss = -12293.558739824719
Iteration 2900: Loss = -12293.332823082521
Iteration 3000: Loss = -12293.241250718966
Iteration 3100: Loss = -12293.142138851763
Iteration 3200: Loss = -12293.053527948907
Iteration 3300: Loss = -12292.991418734495
Iteration 3400: Loss = -12292.775163292952
Iteration 3500: Loss = -12292.47286883663
Iteration 3600: Loss = -12292.381912245197
Iteration 3700: Loss = -12292.366646307104
Iteration 3800: Loss = -12292.309607902862
Iteration 3900: Loss = -12292.291673921216
Iteration 4000: Loss = -12292.279526877543
Iteration 4100: Loss = -12292.274982134475
Iteration 4200: Loss = -12292.264842305918
Iteration 4300: Loss = -12292.260254637824
Iteration 4400: Loss = -12292.25665585224
Iteration 4500: Loss = -12292.253826854603
Iteration 4600: Loss = -12292.251520048936
Iteration 4700: Loss = -12292.249628730433
Iteration 4800: Loss = -12292.248822541087
Iteration 4900: Loss = -12292.246727850275
Iteration 5000: Loss = -12292.245585799898
Iteration 5100: Loss = -12292.270696328658
1
Iteration 5200: Loss = -12292.243733580013
Iteration 5300: Loss = -12292.242999924749
Iteration 5400: Loss = -12292.242365364595
Iteration 5500: Loss = -12292.241844259133
Iteration 5600: Loss = -12292.241179846224
Iteration 5700: Loss = -12292.240765187451
Iteration 5800: Loss = -12292.246908132027
1
Iteration 5900: Loss = -12292.239935857391
Iteration 6000: Loss = -12292.23958783816
Iteration 6100: Loss = -12292.247259017038
1
Iteration 6200: Loss = -12292.250757405593
2
Iteration 6300: Loss = -12292.238745053599
Iteration 6400: Loss = -12292.238817530313
1
Iteration 6500: Loss = -12292.243460046247
2
Iteration 6600: Loss = -12292.238059976391
Iteration 6700: Loss = -12292.260830744513
1
Iteration 6800: Loss = -12292.237670554034
Iteration 6900: Loss = -12292.237546952196
Iteration 7000: Loss = -12292.237632147062
1
Iteration 7100: Loss = -12292.286406332638
2
Iteration 7200: Loss = -12292.237150248558
Iteration 7300: Loss = -12292.23699421449
Iteration 7400: Loss = -12292.238013243363
1
Iteration 7500: Loss = -12292.2367194358
Iteration 7600: Loss = -12292.236858402173
1
Iteration 7700: Loss = -12292.236523101163
Iteration 7800: Loss = -12292.247211644115
1
Iteration 7900: Loss = -12292.236375100856
Iteration 8000: Loss = -12292.321863168268
1
Iteration 8100: Loss = -12292.23636834733
Iteration 8200: Loss = -12292.236144606795
Iteration 8300: Loss = -12292.236931544414
1
Iteration 8400: Loss = -12292.236026656114
Iteration 8500: Loss = -12292.243229922566
1
Iteration 8600: Loss = -12292.235902291974
Iteration 8700: Loss = -12292.237626784487
1
Iteration 8800: Loss = -12292.235962558829
2
Iteration 8900: Loss = -12292.27660570126
3
Iteration 9000: Loss = -12292.240575990683
4
Iteration 9100: Loss = -12292.235719433356
Iteration 9200: Loss = -12292.235622706397
Iteration 9300: Loss = -12292.2687284635
1
Iteration 9400: Loss = -12292.235557791088
Iteration 9500: Loss = -12292.235581566252
1
Iteration 9600: Loss = -12292.238719519337
2
Iteration 9700: Loss = -12292.235660329965
3
Iteration 9800: Loss = -12292.252542658975
4
Iteration 9900: Loss = -12292.235407519538
Iteration 10000: Loss = -12292.235651034323
1
Iteration 10100: Loss = -12292.235340408228
Iteration 10200: Loss = -12292.235622884475
1
Iteration 10300: Loss = -12292.235875118311
2
Iteration 10400: Loss = -12292.275132756396
3
Iteration 10500: Loss = -12292.26039253758
4
Iteration 10600: Loss = -12292.236210369525
5
Iteration 10700: Loss = -12292.271788223112
6
Iteration 10800: Loss = -12292.235303807136
Iteration 10900: Loss = -12292.425630474363
1
Iteration 11000: Loss = -12292.235497595331
2
Iteration 11100: Loss = -12292.378256317897
3
Iteration 11200: Loss = -12292.235224430226
Iteration 11300: Loss = -12292.23580666351
1
Iteration 11400: Loss = -12292.366921154357
2
Iteration 11500: Loss = -12292.235136796773
Iteration 11600: Loss = -12292.235775982996
1
Iteration 11700: Loss = -12292.240514046656
2
Iteration 11800: Loss = -12292.235181592865
3
Iteration 11900: Loss = -12292.319356097607
4
Iteration 12000: Loss = -12292.239421468706
5
Iteration 12100: Loss = -12292.23508138666
Iteration 12200: Loss = -12292.236831403503
1
Iteration 12300: Loss = -12292.235028504128
Iteration 12400: Loss = -12292.235759166762
1
Iteration 12500: Loss = -12292.235018539188
Iteration 12600: Loss = -12292.235542061191
1
Iteration 12700: Loss = -12292.235024544985
2
Iteration 12800: Loss = -12292.243197271233
3
Iteration 12900: Loss = -12292.23500186156
Iteration 13000: Loss = -12292.234995280092
Iteration 13100: Loss = -12292.235331609412
1
Iteration 13200: Loss = -12292.235050874839
2
Iteration 13300: Loss = -12292.235013244835
3
Iteration 13400: Loss = -12292.235334558569
4
Iteration 13500: Loss = -12292.23499509485
Iteration 13600: Loss = -12292.256514022607
1
Iteration 13700: Loss = -12292.23497966406
Iteration 13800: Loss = -12292.235412491405
1
Iteration 13900: Loss = -12292.235033157902
2
Iteration 14000: Loss = -12292.23502398063
3
Iteration 14100: Loss = -12292.26123647414
4
Iteration 14200: Loss = -12292.236440857521
5
Iteration 14300: Loss = -12292.235753713789
6
Iteration 14400: Loss = -12292.234933616148
Iteration 14500: Loss = -12292.241617313752
1
Iteration 14600: Loss = -12292.234957088931
2
Iteration 14700: Loss = -12292.235022090987
3
Iteration 14800: Loss = -12292.234963140705
4
Iteration 14900: Loss = -12292.238432668795
5
Iteration 15000: Loss = -12292.2374407538
6
Iteration 15100: Loss = -12292.337817923515
7
Iteration 15200: Loss = -12292.235127089236
8
Iteration 15300: Loss = -12292.257663520788
9
Iteration 15400: Loss = -12292.234931286537
Iteration 15500: Loss = -12292.236543538604
1
Iteration 15600: Loss = -12292.247180236649
2
Iteration 15700: Loss = -12292.243438772415
3
Iteration 15800: Loss = -12292.270147328738
4
Iteration 15900: Loss = -12292.245609914766
5
Iteration 16000: Loss = -12292.240511273758
6
Iteration 16100: Loss = -12292.234976330888
7
Iteration 16200: Loss = -12292.236024164611
8
Iteration 16300: Loss = -12292.241037504376
9
Iteration 16400: Loss = -12292.406937843733
10
Stopping early at iteration 16400 due to no improvement.
tensor([[ -8.0874,   3.4722],
        [ -9.7629,   5.1477],
        [ -7.6749,   3.0597],
        [ -9.3324,   4.7172],
        [ -8.4702,   3.8550],
        [ -5.8425,   1.2273],
        [ -7.8205,   3.2053],
        [ -9.2323,   4.6171],
        [ -4.0414,  -0.5738],
        [ -5.9285,   1.3133],
        [-10.4619,   5.8467],
        [ -5.6971,   1.0819],
        [ -4.1541,  -0.4611],
        [-10.4566,   5.8414],
        [ -7.4744,   2.8592],
        [ -7.7762,   3.1610],
        [ -7.0800,   2.4648],
        [ -9.4537,   4.8385],
        [ -6.1377,   1.5225],
        [ -8.4098,   3.7945],
        [ -7.6507,   3.0354],
        [ -9.5830,   4.9678],
        [ -6.5114,   1.8962],
        [ -5.2302,   0.6149],
        [-11.5966,   6.9814],
        [ -6.4913,   1.8761],
        [ -8.1574,   3.5422],
        [ -8.9292,   4.3140],
        [ -7.0091,   2.3939],
        [ -4.8951,   0.2799],
        [ -8.3801,   3.7649],
        [ -5.9009,   1.2857],
        [ -6.7706,   2.1553],
        [ -7.8952,   3.2800],
        [ -6.6205,   2.0053],
        [ -5.6238,   1.0085],
        [-10.7532,   6.1380],
        [ -8.8726,   4.2574],
        [ -8.3691,   3.7539],
        [ -5.5314,   0.9161],
        [ -8.9867,   4.3715],
        [ -5.0295,   0.4143],
        [ -5.8683,   1.2531],
        [ -8.5023,   3.8871],
        [ -7.3619,   2.7467],
        [ -8.0816,   3.4664],
        [ -5.9109,   1.2956],
        [ -8.5527,   3.9374],
        [-10.5330,   5.9178],
        [ -7.1621,   2.5469],
        [ -7.7127,   3.0974],
        [ -9.1535,   4.5383],
        [ -7.8718,   3.2566],
        [ -5.4576,   0.8424],
        [ -9.9837,   5.3685],
        [ -1.3222,  -3.2930],
        [-10.1029,   5.4877],
        [ -5.0762,   0.4610],
        [ -8.9121,   4.2969],
        [ -6.4823,   1.8671],
        [ -6.0735,   1.4582],
        [ -7.3705,   2.7553],
        [ -5.7152,   1.0999],
        [ -4.1479,  -0.4673],
        [ -3.6969,  -0.9184],
        [ -6.4348,   1.8196],
        [ -7.8892,   3.2740],
        [ -8.2057,   3.5905],
        [ -7.2666,   2.6514],
        [ -9.3221,   4.7069],
        [ -9.2659,   4.6507],
        [ -9.2027,   4.5875],
        [ -5.3435,   0.7283],
        [ -5.8747,   1.2595],
        [ -9.5923,   4.9770],
        [ -6.1195,   1.5043],
        [ -8.5979,   3.9827],
        [ -8.3014,   3.6862],
        [ -8.1674,   3.5521],
        [ -4.2816,  -0.3336],
        [ -4.8002,   0.1850],
        [-10.4498,   5.8346],
        [ -5.9937,   1.3785],
        [ -7.2267,   2.6114],
        [ -6.5948,   1.9795],
        [ -6.8914,   2.2761],
        [ -7.9415,   3.3262],
        [ -6.6680,   2.0527],
        [ -8.5083,   3.8931],
        [ -7.5404,   2.9252],
        [ -9.3077,   4.6925],
        [ -4.3488,  -0.2664],
        [ -5.3379,   0.7227],
        [ -8.3287,   3.7134],
        [ -7.4773,   2.8621],
        [ -4.8588,   0.2436],
        [ -4.8321,   0.2169],
        [ -4.1440,  -0.4712],
        [ -5.4627,   0.8475],
        [ -7.4312,   2.8160]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.6214e-07, 1.0000e+00],
        [1.0000e+00, 1.0673e-07]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0116, 0.9884], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1904, 0.1510],
         [0.7210, 0.2041]],

        [[0.6471, 0.1838],
         [0.0994, 0.6662]],

        [[0.8163, 0.1119],
         [0.7644, 0.0126]],

        [[0.4460, 0.2242],
         [0.8952, 0.3613]],

        [[0.8940, 0.2552],
         [0.0735, 0.8916]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
Global Adjusted Rand Index: -0.0019360173777129585
Average Adjusted Rand Index: -0.0006177931860792093
Iteration 0: Loss = -27775.95175334816
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.2610,    nan]],

        [[0.5617,    nan],
         [0.4147, 0.1858]],

        [[0.6142,    nan],
         [0.8387, 0.7231]],

        [[0.6677,    nan],
         [0.9301, 0.3165]],

        [[0.5691,    nan],
         [0.7901, 0.5849]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27776.99793221202
Iteration 100: Loss = -12302.349246547003
Iteration 200: Loss = -12299.287399972105
Iteration 300: Loss = -12298.117095896705
Iteration 400: Loss = -12297.439269669425
Iteration 500: Loss = -12297.010195890185
Iteration 600: Loss = -12296.716997633335
Iteration 700: Loss = -12296.508307755284
Iteration 800: Loss = -12296.354501478829
Iteration 900: Loss = -12296.237499108
Iteration 1000: Loss = -12296.146287046382
Iteration 1100: Loss = -12296.073553474076
Iteration 1200: Loss = -12296.01456788995
Iteration 1300: Loss = -12295.965954804295
Iteration 1400: Loss = -12295.925341890308
Iteration 1500: Loss = -12295.890980393224
Iteration 1600: Loss = -12295.861728487289
Iteration 1700: Loss = -12295.836487489976
Iteration 1800: Loss = -12295.814513954401
Iteration 1900: Loss = -12295.795376091944
Iteration 2000: Loss = -12295.778403142596
Iteration 2100: Loss = -12295.763356209984
Iteration 2200: Loss = -12295.749795527396
Iteration 2300: Loss = -12295.737594238644
Iteration 2400: Loss = -12295.726384694206
Iteration 2500: Loss = -12295.716125169673
Iteration 2600: Loss = -12295.706577354575
Iteration 2700: Loss = -12295.697562867394
Iteration 2800: Loss = -12295.68893900266
Iteration 2900: Loss = -12295.680568772601
Iteration 3000: Loss = -12295.672386867875
Iteration 3100: Loss = -12295.664162799843
Iteration 3200: Loss = -12295.655764841627
Iteration 3300: Loss = -12295.647099327045
Iteration 3400: Loss = -12295.700426721534
1
Iteration 3500: Loss = -12295.628173024848
Iteration 3600: Loss = -12295.617622203754
Iteration 3700: Loss = -12295.60632721719
Iteration 3800: Loss = -12295.594053104169
Iteration 3900: Loss = -12295.580443579833
Iteration 4000: Loss = -12295.565680304402
Iteration 4100: Loss = -12295.551912921117
Iteration 4200: Loss = -12295.531510279789
Iteration 4300: Loss = -12295.511642765157
Iteration 4400: Loss = -12295.489547437548
Iteration 4500: Loss = -12295.467739042664
Iteration 4600: Loss = -12295.437085767502
Iteration 4700: Loss = -12295.406282647895
Iteration 4800: Loss = -12295.372377067419
Iteration 4900: Loss = -12295.336111961617
Iteration 5000: Loss = -12295.298302927455
Iteration 5100: Loss = -12295.261409744406
Iteration 5200: Loss = -12295.228378239435
Iteration 5300: Loss = -12295.195297752427
Iteration 5400: Loss = -12295.16586402443
Iteration 5500: Loss = -12295.136557171196
Iteration 5600: Loss = -12295.10736474673
Iteration 5700: Loss = -12295.069504111972
Iteration 5800: Loss = -12295.029253846762
Iteration 5900: Loss = -12295.11042748011
1
Iteration 6000: Loss = -12294.941786283367
Iteration 6100: Loss = -12294.895262201535
Iteration 6200: Loss = -12294.839029374458
Iteration 6300: Loss = -12294.777464791374
Iteration 6400: Loss = -12294.697520194031
Iteration 6500: Loss = -12294.600164666334
Iteration 6600: Loss = -12294.473176760419
Iteration 6700: Loss = -12294.314415355688
Iteration 6800: Loss = -12294.031197729024
Iteration 6900: Loss = -12293.985351067
Iteration 7000: Loss = -12293.935022149642
Iteration 7100: Loss = -12293.91307855761
Iteration 7200: Loss = -12293.89777327633
Iteration 7300: Loss = -12293.885445783551
Iteration 7400: Loss = -12293.874801253714
Iteration 7500: Loss = -12293.865185620423
Iteration 7600: Loss = -12293.894831776786
1
Iteration 7700: Loss = -12293.844240093176
Iteration 7800: Loss = -12293.838922291436
Iteration 7900: Loss = -12293.795050101344
Iteration 8000: Loss = -12293.713204011627
Iteration 8100: Loss = -12293.51444945495
Iteration 8200: Loss = -12293.351731695373
Iteration 8300: Loss = -12293.282886389838
Iteration 8400: Loss = -12293.23802911029
Iteration 8500: Loss = -12293.194242539425
Iteration 8600: Loss = -12293.121434123572
Iteration 8700: Loss = -12292.92877792714
Iteration 8800: Loss = -12292.601742504965
Iteration 8900: Loss = -12292.475205628278
Iteration 9000: Loss = -12292.554357456263
1
Iteration 9100: Loss = -12292.384736750459
Iteration 9200: Loss = -12292.412252638884
1
Iteration 9300: Loss = -12292.297916585785
Iteration 9400: Loss = -12292.28462513499
Iteration 9500: Loss = -12292.271195378662
Iteration 9600: Loss = -12292.341000575185
1
Iteration 9700: Loss = -12292.25887934891
Iteration 9800: Loss = -12292.498900825025
1
Iteration 9900: Loss = -12292.25226270424
Iteration 10000: Loss = -12292.24998768819
Iteration 10100: Loss = -12292.24856969941
Iteration 10200: Loss = -12292.246704052746
Iteration 10300: Loss = -12292.245477705346
Iteration 10400: Loss = -12292.244523085265
Iteration 10500: Loss = -12292.243558219254
Iteration 10600: Loss = -12292.242777617867
Iteration 10700: Loss = -12292.242158111261
Iteration 10800: Loss = -12292.241491419434
Iteration 10900: Loss = -12292.24099155513
Iteration 11000: Loss = -12292.243445270136
1
Iteration 11100: Loss = -12292.240113611557
Iteration 11200: Loss = -12292.239731609898
Iteration 11300: Loss = -12292.331278249481
1
Iteration 11400: Loss = -12292.239120848246
Iteration 11500: Loss = -12292.238907275267
Iteration 11600: Loss = -12292.238533209684
Iteration 11700: Loss = -12292.286956064
1
Iteration 11800: Loss = -12292.238135078107
Iteration 11900: Loss = -12292.237918258974
Iteration 12000: Loss = -12292.243836479378
1
Iteration 12100: Loss = -12292.237563692996
Iteration 12200: Loss = -12292.237463815723
Iteration 12300: Loss = -12292.26460284587
1
Iteration 12400: Loss = -12292.237137196202
Iteration 12500: Loss = -12292.35805569125
1
Iteration 12600: Loss = -12292.237684177515
2
Iteration 12700: Loss = -12292.236793797625
Iteration 12800: Loss = -12292.237610000459
1
Iteration 12900: Loss = -12292.236602427975
Iteration 13000: Loss = -12292.335112681061
1
Iteration 13100: Loss = -12292.236437218748
Iteration 13200: Loss = -12292.236899542437
1
Iteration 13300: Loss = -12292.236297403246
Iteration 13400: Loss = -12292.236195725076
Iteration 13500: Loss = -12292.243183235198
1
Iteration 13600: Loss = -12292.236057726408
Iteration 13700: Loss = -12292.236364459151
1
Iteration 13800: Loss = -12292.236184683681
2
Iteration 13900: Loss = -12292.236057560802
Iteration 14000: Loss = -12292.335320081798
1
Iteration 14100: Loss = -12292.235779256718
Iteration 14200: Loss = -12292.237202531032
1
Iteration 14300: Loss = -12292.297529498988
2
Iteration 14400: Loss = -12292.235672891835
Iteration 14500: Loss = -12292.244243411234
1
Iteration 14600: Loss = -12292.236387173956
2
Iteration 14700: Loss = -12292.235738666845
3
Iteration 14800: Loss = -12292.336833846566
4
Iteration 14900: Loss = -12292.235977066155
5
Iteration 15000: Loss = -12292.238564609497
6
Iteration 15100: Loss = -12292.23544570281
Iteration 15200: Loss = -12292.235609005253
1
Iteration 15300: Loss = -12292.235424542328
Iteration 15400: Loss = -12292.236041527969
1
Iteration 15500: Loss = -12292.245593176805
2
Iteration 15600: Loss = -12292.235333924033
Iteration 15700: Loss = -12292.265694941696
1
Iteration 15800: Loss = -12292.264758149087
2
Iteration 15900: Loss = -12292.433729597567
3
Iteration 16000: Loss = -12292.23525282871
Iteration 16100: Loss = -12292.254953817732
1
Iteration 16200: Loss = -12292.249562593788
2
Iteration 16300: Loss = -12292.237347227807
3
Iteration 16400: Loss = -12292.235213080017
Iteration 16500: Loss = -12292.313390663612
1
Iteration 16600: Loss = -12292.235187790735
Iteration 16700: Loss = -12292.235350409106
1
Iteration 16800: Loss = -12292.240879902456
2
Iteration 16900: Loss = -12292.235137836109
Iteration 17000: Loss = -12292.271562139169
1
Iteration 17100: Loss = -12292.23510844325
Iteration 17200: Loss = -12292.276280802647
1
Iteration 17300: Loss = -12292.235085136934
Iteration 17400: Loss = -12292.23547969311
1
Iteration 17500: Loss = -12292.235136204605
2
Iteration 17600: Loss = -12292.235250815591
3
Iteration 17700: Loss = -12292.238436140991
4
Iteration 17800: Loss = -12292.235185283469
5
Iteration 17900: Loss = -12292.314430019278
6
Iteration 18000: Loss = -12292.236051412958
7
Iteration 18100: Loss = -12292.23503458068
Iteration 18200: Loss = -12292.235615128258
1
Iteration 18300: Loss = -12292.236094801876
2
Iteration 18400: Loss = -12292.23507744979
3
Iteration 18500: Loss = -12292.235518071257
4
Iteration 18600: Loss = -12292.243296030956
5
Iteration 18700: Loss = -12292.2350136101
Iteration 18800: Loss = -12292.241525299925
1
Iteration 18900: Loss = -12292.234976193773
Iteration 19000: Loss = -12292.259863928417
1
Iteration 19100: Loss = -12292.235167705749
2
Iteration 19200: Loss = -12292.23532452498
3
Iteration 19300: Loss = -12292.25002783901
4
Iteration 19400: Loss = -12292.236494283576
5
Iteration 19500: Loss = -12292.236119209014
6
Iteration 19600: Loss = -12292.236237545436
7
Iteration 19700: Loss = -12292.260612165326
8
Iteration 19800: Loss = -12292.234980955334
9
Iteration 19900: Loss = -12292.235809938486
10
Stopping early at iteration 19900 due to no improvement.
tensor([[-7.1576,  4.4215],
        [-8.5800,  6.6022],
        [-6.0812,  4.6571],
        [-8.0757,  6.2144],
        [-7.6542,  4.7293],
        [-4.2286,  2.8400],
        [-6.3817,  4.6671],
        [-7.6775,  6.2869],
        [-2.6975,  0.7498],
        [-4.3469,  2.8698],
        [-9.2936,  7.2464],
        [-4.3985,  2.3644],
        [-2.7809,  0.8959],
        [-9.0901,  6.8395],
        [-6.1146,  4.2297],
        [-6.2351,  4.7120],
        [-5.5575,  3.9704],
        [-7.9753,  6.4906],
        [-4.5123,  3.1249],
        [-6.8239,  5.4018],
        [-6.4225,  4.2719],
        [-8.1122,  6.4964],
        [-4.9290,  3.4769],
        [-3.7818,  2.0604],
        [-9.3530,  7.7619],
        [-4.8986,  3.4697],
        [-6.7658,  4.9371],
        [-7.5857,  5.7967],
        [-5.4442,  3.9586],
        [-3.2717,  1.8816],
        [-6.9302,  5.2520],
        [-4.3856,  2.7934],
        [-5.3296,  3.5988],
        [-6.4429,  4.7461],
        [-5.2371,  3.3904],
        [-4.3131,  2.2968],
        [-9.1579,  7.3709],
        [-7.5730,  5.6180],
        [-6.7723,  5.3663],
        [-4.5097,  1.9285],
        [-7.9416,  5.5839],
        [-3.5167,  1.8998],
        [-4.2954,  2.8202],
        [-7.2953,  5.1202],
        [-5.8275,  4.2873],
        [-6.4936,  5.0808],
        [-4.3041,  2.9052],
        [-7.0871,  5.4516],
        [-9.4512,  7.1375],
        [-5.5861,  4.1230],
        [-6.4658,  4.3533],
        [-7.6380,  6.0873],
        [-6.2828,  4.8521],
        [-3.8585,  2.4148],
        [-8.5018,  6.9930],
        [-0.6998, -2.6831],
        [-8.3551,  6.9071],
        [-3.4663,  2.0603],
        [-7.6516,  5.6019],
        [-5.1182,  3.2308],
        [-4.5942,  2.9242],
        [-6.3139,  3.8124],
        [-4.2970,  2.4911],
        [-2.6137,  1.0441],
        [-2.2216,  0.5518],
        [-5.1523,  3.0961],
        [-6.6365,  4.5230],
        [-7.0514,  4.7722],
        [-6.2620,  3.6556],
        [-9.4239,  4.8087],
        [-7.6245,  6.2093],
        [-7.6970,  6.2337],
        [-3.7584,  2.2896],
        [-4.2854,  2.8236],
        [-7.9623,  6.5473],
        [-4.9843,  2.6250],
        [-7.9104,  4.6724],
        [-6.8947,  5.1257],
        [-6.7529,  4.9831],
        [-2.6600,  1.2721],
        [-3.1809,  1.7938],
        [-9.4072,  7.7277],
        [-5.1922,  2.1761],
        [-5.6316,  4.2067],
        [-5.0252,  3.5368],
        [-5.6518,  3.5082],
        [-6.3581,  4.9288],
        [-5.2291,  3.4714],
        [-7.0876,  5.3857],
        [-7.5427,  2.9275],
        [-7.7445,  6.3165],
        [-4.0068,  0.0741],
        [-3.7595,  2.2883],
        [-7.4106,  4.6736],
        [-6.2427,  4.1026],
        [-3.6153,  1.4663],
        [-3.7061,  1.3231],
        [-3.5377,  0.1106],
        [-3.9956,  2.2950],
        [-6.0829,  4.1616]], dtype=torch.float64, requires_grad=True)
pi: tensor([[4.3023e-07, 1.0000e+00],
        [1.0000e+00, 1.3400e-07]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0114, 0.9886], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1905, 0.1503],
         [0.2610, 0.2026]],

        [[0.5617, 0.1835],
         [0.4147, 0.1858]],

        [[0.6142, 0.1111],
         [0.8387, 0.7231]],

        [[0.6677, 0.2256],
         [0.9301, 0.3165]],

        [[0.5691, 0.2572],
         [0.7901, 0.5849]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
Global Adjusted Rand Index: -0.0019360173777129585
Average Adjusted Rand Index: -0.0006177931860792093
Iteration 0: Loss = -28792.356292138218
Iteration 10: Loss = -12296.024222010736
Iteration 20: Loss = -12296.024222010736
1
Iteration 30: Loss = -12296.024222010736
2
Iteration 40: Loss = -12296.024222010738
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[1.0000e+00, 1.9277e-16],
        [1.0000e+00, 4.2877e-19]], dtype=torch.float64)
alpha: tensor([1.0000e+00, 1.8779e-16])
beta: tensor([[[0.1954, 0.2180],
         [0.7233, 0.1860]],

        [[0.2362, 0.1764],
         [0.1418, 0.6920]],

        [[0.3758, 0.2568],
         [0.6966, 0.1079]],

        [[0.5137, 0.1090],
         [0.0238, 0.2810]],

        [[0.1036, 0.2460],
         [0.2055, 0.6205]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -28792.507937935356
Iteration 100: Loss = -12314.192225305018
Iteration 200: Loss = -12300.389883273472
Iteration 300: Loss = -12298.324190646526
Iteration 400: Loss = -12297.339940796184
Iteration 500: Loss = -12296.727860855355
Iteration 600: Loss = -12296.342143447568
Iteration 700: Loss = -12296.085215093186
Iteration 800: Loss = -12295.947187867017
Iteration 900: Loss = -12295.869040983689
Iteration 1000: Loss = -12295.8173870457
Iteration 1100: Loss = -12295.778534051135
Iteration 1200: Loss = -12295.749383034552
Iteration 1300: Loss = -12295.727217529922
Iteration 1400: Loss = -12295.70965356619
Iteration 1500: Loss = -12295.695116273451
Iteration 1600: Loss = -12295.682643371181
Iteration 1700: Loss = -12295.671433951182
Iteration 1800: Loss = -12295.660815942436
Iteration 1900: Loss = -12295.651238117143
Iteration 2000: Loss = -12295.642300400863
Iteration 2100: Loss = -12295.633504901594
Iteration 2200: Loss = -12295.62457270259
Iteration 2300: Loss = -12295.615124259162
Iteration 2400: Loss = -12295.604932655684
Iteration 2500: Loss = -12295.593506912228
Iteration 2600: Loss = -12295.580538151722
Iteration 2700: Loss = -12295.565770876243
Iteration 2800: Loss = -12295.548589835036
Iteration 2900: Loss = -12295.52865060162
Iteration 3000: Loss = -12295.50605740371
Iteration 3100: Loss = -12295.481519757172
Iteration 3200: Loss = -12295.45591824019
Iteration 3300: Loss = -12295.429805160513
Iteration 3400: Loss = -12295.40291334893
Iteration 3500: Loss = -12295.374907945808
Iteration 3600: Loss = -12295.345382342426
Iteration 3700: Loss = -12295.314454150963
Iteration 3800: Loss = -12295.282543702955
Iteration 3900: Loss = -12295.250154701991
Iteration 4000: Loss = -12295.217691467444
Iteration 4100: Loss = -12295.185242441505
Iteration 4200: Loss = -12295.152470050954
Iteration 4300: Loss = -12295.118172816945
Iteration 4400: Loss = -12295.079967671705
Iteration 4500: Loss = -12295.02022732163
Iteration 4600: Loss = -12294.967431683937
Iteration 4700: Loss = -12294.90466567399
Iteration 4800: Loss = -12294.823157637038
Iteration 4900: Loss = -12294.705877749391
Iteration 5000: Loss = -12294.532925781677
Iteration 5100: Loss = -12294.335322135878
Iteration 5200: Loss = -12294.194331365397
Iteration 5300: Loss = -12294.101733671145
Iteration 5400: Loss = -12294.037028195346
Iteration 5500: Loss = -12293.990519123538
Iteration 5600: Loss = -12293.956796320557
Iteration 5700: Loss = -12293.931867308835
Iteration 5800: Loss = -12293.912846349727
Iteration 5900: Loss = -12293.897701715512
Iteration 6000: Loss = -12293.88479213494
Iteration 6100: Loss = -12293.873204924328
Iteration 6200: Loss = -12293.861909552495
Iteration 6300: Loss = -12293.849783902362
Iteration 6400: Loss = -12293.835357332366
Iteration 6500: Loss = -12293.818183426161
Iteration 6600: Loss = -12293.779991711477
Iteration 6700: Loss = -12293.701529704964
Iteration 6800: Loss = -12293.54117958175
Iteration 6900: Loss = -12293.3802637295
Iteration 7000: Loss = -12293.301273733343
Iteration 7100: Loss = -12293.256200721902
Iteration 7200: Loss = -12293.220204162066
Iteration 7300: Loss = -12293.175498384651
Iteration 7400: Loss = -12293.077202174927
Iteration 7500: Loss = -12292.837131198192
Iteration 7600: Loss = -12292.601268450646
Iteration 7700: Loss = -12292.484654469603
Iteration 7800: Loss = -12292.419106684292
Iteration 7900: Loss = -12292.373960265755
Iteration 8000: Loss = -12292.339005942438
Iteration 8100: Loss = -12292.312575307456
Iteration 8200: Loss = -12292.293326753233
Iteration 8300: Loss = -12292.28031636059
Iteration 8400: Loss = -12292.347840827244
1
Iteration 8500: Loss = -12292.264792039663
Iteration 8600: Loss = -12292.259972357755
Iteration 8700: Loss = -12292.257187537789
Iteration 8800: Loss = -12292.253549147548
Iteration 8900: Loss = -12292.253526990085
Iteration 9000: Loss = -12292.32565123275
1
Iteration 9100: Loss = -12292.247808219921
Iteration 9200: Loss = -12292.257162819502
1
Iteration 9300: Loss = -12292.245378629568
Iteration 9400: Loss = -12292.244496322483
Iteration 9500: Loss = -12292.243619123932
Iteration 9600: Loss = -12292.242859194219
Iteration 9700: Loss = -12292.242306041811
Iteration 9800: Loss = -12292.343465056787
1
Iteration 9900: Loss = -12292.241108595663
Iteration 10000: Loss = -12292.246029729447
1
Iteration 10100: Loss = -12292.240974227108
Iteration 10200: Loss = -12292.328841454282
1
Iteration 10300: Loss = -12292.242196855512
2
Iteration 10400: Loss = -12292.239321476043
Iteration 10500: Loss = -12292.239069299347
Iteration 10600: Loss = -12292.249499643209
1
Iteration 10700: Loss = -12292.238474015612
Iteration 10800: Loss = -12292.246135227362
1
Iteration 10900: Loss = -12292.238073560116
Iteration 11000: Loss = -12292.249357471383
1
Iteration 11100: Loss = -12292.237675166087
Iteration 11200: Loss = -12292.243214268217
1
Iteration 11300: Loss = -12292.237321920016
Iteration 11400: Loss = -12292.23721677018
Iteration 11500: Loss = -12292.237355959891
1
Iteration 11600: Loss = -12292.23694059079
Iteration 11700: Loss = -12292.236843010382
Iteration 11800: Loss = -12292.237261637456
1
Iteration 11900: Loss = -12292.236659678198
Iteration 12000: Loss = -12292.236542503491
Iteration 12100: Loss = -12292.240011517157
1
Iteration 12200: Loss = -12292.236390289814
Iteration 12300: Loss = -12292.236301598545
Iteration 12400: Loss = -12292.249664731577
1
Iteration 12500: Loss = -12292.2362099281
Iteration 12600: Loss = -12292.236148141688
Iteration 12700: Loss = -12292.413084653404
1
Iteration 12800: Loss = -12292.25721899616
2
Iteration 12900: Loss = -12292.235935873034
Iteration 13000: Loss = -12292.237201321163
1
Iteration 13100: Loss = -12292.23583128271
Iteration 13200: Loss = -12292.293276618864
1
Iteration 13300: Loss = -12292.23572651832
Iteration 13400: Loss = -12292.250363120225
1
Iteration 13500: Loss = -12292.235644872677
Iteration 13600: Loss = -12292.23568839917
1
Iteration 13700: Loss = -12292.235615641966
Iteration 13800: Loss = -12292.235562361308
Iteration 13900: Loss = -12292.335478733366
1
Iteration 14000: Loss = -12292.235491183366
Iteration 14100: Loss = -12292.235863188303
1
Iteration 14200: Loss = -12292.238188012994
2
Iteration 14300: Loss = -12292.235442312664
Iteration 14400: Loss = -12292.235926771864
1
Iteration 14500: Loss = -12292.377685713549
2
Iteration 14600: Loss = -12292.235353692597
Iteration 14700: Loss = -12292.26274151941
1
Iteration 14800: Loss = -12292.235272909342
Iteration 14900: Loss = -12292.235365385312
1
Iteration 15000: Loss = -12292.235250834516
Iteration 15100: Loss = -12292.235564599383
1
Iteration 15200: Loss = -12292.23520066827
Iteration 15300: Loss = -12292.524986120663
1
Iteration 15400: Loss = -12292.235193686862
Iteration 15500: Loss = -12292.235509423519
1
Iteration 15600: Loss = -12292.235336290769
2
Iteration 15700: Loss = -12292.237880350674
3
Iteration 15800: Loss = -12292.247336423918
4
Iteration 15900: Loss = -12292.237916152393
5
Iteration 16000: Loss = -12292.235393138171
6
Iteration 16100: Loss = -12292.235200993455
7
Iteration 16200: Loss = -12292.248517745682
8
Iteration 16300: Loss = -12292.235438552949
9
Iteration 16400: Loss = -12292.337569054795
10
Stopping early at iteration 16400 due to no improvement.
tensor([[-6.5710,  5.0249],
        [-8.0961,  6.6625],
        [-6.1103,  4.6510],
        [-7.9090,  6.3049],
        [-6.9858,  5.4703],
        [-4.4651,  2.6021],
        [-6.5004,  4.5410],
        [-7.9323,  5.8851],
        [-2.6727,  0.7912],
        [-4.4624,  2.7568],
        [-8.6380,  6.6022],
        [-4.1386,  2.6254],
        [-2.5369,  1.1503],
        [-8.2100,  6.6192],
        [-6.1542,  4.1906],
        [-6.2381,  4.7602],
        [-5.4704,  4.0752],
        [-8.1150,  6.3721],
        [-4.8356,  2.8154],
        [-6.8774,  5.4164],
        [-6.3401,  4.4066],
        [-7.8704,  6.4782],
        [-4.9827,  3.4234],
        [-4.9241,  0.9232],
        [-8.9770,  7.2718],
        [-5.0533,  3.3182],
        [-6.7876,  4.9263],
        [-7.5590,  5.8270],
        [-5.5991,  3.8080],
        [-4.0086,  1.1596],
        [-6.9735,  5.2912],
        [-5.3010,  1.8789],
        [-5.7814,  3.1508],
        [-6.3222,  4.9210],
        [-5.0084,  3.6215],
        [-4.1845,  2.4276],
        [-8.3176,  6.9291],
        [-7.5316,  5.7427],
        [-7.3168,  4.8240],
        [-4.9572,  1.4899],
        [-7.6481,  5.9056],
        [-3.4095,  2.0179],
        [-5.2505,  1.8693],
        [-7.4526,  4.9911],
        [-5.9346,  4.1828],
        [-6.7233,  4.8683],
        [-4.6664,  2.5421],
        [-7.0442,  5.4845],
        [-8.3128,  6.8287],
        [-5.7809,  3.9320],
        [-7.1738,  3.6554],
        [-7.6082,  6.1446],
        [-6.3146,  4.8797],
        [-3.9673,  2.3107],
        [-8.0814,  6.6114],
        [ 0.1017, -1.8677],
        [-8.0960,  6.7034],
        [-4.0946,  1.4407],
        [-7.3602,  5.9665],
        [-5.3404,  3.0115],
        [-6.0696,  1.4544],
        [-6.1786,  3.9725],
        [-4.7050,  2.0915],
        [-2.8776,  0.7992],
        [-2.2225,  0.5557],
        [-4.8430,  3.4064],
        [-6.2853,  4.8913],
        [-6.6552,  5.2420],
        [-5.6746,  4.2604],
        [-7.6715,  6.2778],
        [-7.5950,  6.1314],
        [-7.5939,  6.1724],
        [-4.1991,  1.8686],
        [-5.0705,  2.0452],
        [-9.6707,  5.3625],
        [-6.0030,  1.6135],
        [-7.7379,  4.8790],
        [-6.8630,  5.1637],
        [-7.1602,  4.6786],
        [-2.8601,  1.0838],
        [-4.3239,  0.6591],
        [-8.4249,  6.8405],
        [-4.4474,  2.9238],
        [-5.6831,  4.1663],
        [-5.1891,  3.3730],
        [-5.3244,  3.8391],
        [-6.5877,  4.7297],
        [-5.0429,  3.6565],
        [-7.1068,  5.3510],
        [-6.0136,  4.4635],
        [-7.6587,  6.2646],
        [-2.7342,  1.3470],
        [-3.7780,  2.2715],
        [-6.7486,  5.3572],
        [-5.9005,  4.4631],
        [-3.2621,  1.8337],
        [-3.7635,  1.2789],
        [-2.8248,  0.8433],
        [-4.1606,  2.1397],
        [-5.8285,  4.4339]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.7443e-06, 1.0000e+00],
        [1.0000e+00, 4.1965e-07]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0116, 0.9884], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1904, 0.1508],
         [0.7233, 0.2040]],

        [[0.2362, 0.1836],
         [0.1418, 0.6920]],

        [[0.3758, 0.1116],
         [0.6966, 0.1079]],

        [[0.5137, 0.2247],
         [0.0238, 0.2810]],

        [[0.1036, 0.2555],
         [0.2055, 0.6205]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
Global Adjusted Rand Index: -0.0019360173777129585
Average Adjusted Rand Index: -0.0006177931860792093
Iteration 0: Loss = -26111.280973869027
Iteration 10: Loss = -12296.024222207858
Iteration 20: Loss = -12296.024222681182
1
Iteration 30: Loss = -12296.024225074512
2
Iteration 40: Loss = -12296.024236230598
3
Stopping early at iteration 39 due to no improvement.
pi: tensor([[1.0000e+00, 5.9422e-10],
        [1.0000e+00, 6.5366e-20]], dtype=torch.float64)
alpha: tensor([1.0000e+00, 5.7887e-10])
beta: tensor([[[0.1954, 0.2180],
         [0.1927, 0.1860]],

        [[0.8677, 0.1764],
         [0.9638, 0.8635]],

        [[0.4133, 0.2568],
         [0.2673, 0.7288]],

        [[0.3955, 0.1090],
         [0.6050, 0.6765]],

        [[0.6293, 0.2460],
         [0.7169, 0.3447]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26110.131294562907
Iteration 100: Loss = -12309.15623733765
Iteration 200: Loss = -12298.183269293006
Iteration 300: Loss = -12296.965226537313
Iteration 400: Loss = -12296.484509396407
Iteration 500: Loss = -12296.219009639717
Iteration 600: Loss = -12296.049900239297
Iteration 700: Loss = -12295.931928138985
Iteration 800: Loss = -12295.844574553248
Iteration 900: Loss = -12295.77750087732
Iteration 1000: Loss = -12295.723781013727
Iteration 1100: Loss = -12295.679194579612
Iteration 1200: Loss = -12295.641075007143
Iteration 1300: Loss = -12295.607427143877
Iteration 1400: Loss = -12295.576851529704
Iteration 1500: Loss = -12295.548112905184
Iteration 1600: Loss = -12295.520372980227
Iteration 1700: Loss = -12295.492880009759
Iteration 1800: Loss = -12295.465051276306
Iteration 1900: Loss = -12295.436571265027
Iteration 2000: Loss = -12295.4069835452
Iteration 2100: Loss = -12295.376304873049
Iteration 2200: Loss = -12295.344140476363
Iteration 2300: Loss = -12295.31016870021
Iteration 2400: Loss = -12295.274409261592
Iteration 2500: Loss = -12295.236841660526
Iteration 2600: Loss = -12295.197783542148
Iteration 2700: Loss = -12295.157841594364
Iteration 2800: Loss = -12295.117886412545
Iteration 2900: Loss = -12295.078905572822
Iteration 3000: Loss = -12295.041479623089
Iteration 3100: Loss = -12295.005864792789
Iteration 3200: Loss = -12294.971910592121
Iteration 3300: Loss = -12294.939007008625
Iteration 3400: Loss = -12294.906416644224
Iteration 3500: Loss = -12294.872694221463
Iteration 3600: Loss = -12294.83656776259
Iteration 3700: Loss = -12294.796264650438
Iteration 3800: Loss = -12294.749305081195
Iteration 3900: Loss = -12294.692660660821
Iteration 4000: Loss = -12294.623985532955
Iteration 4100: Loss = -12294.54619297425
Iteration 4200: Loss = -12294.473878297373
Iteration 4300: Loss = -12294.422483911407
Iteration 4400: Loss = -12294.199745643284
Iteration 4500: Loss = -12294.098675632806
Iteration 4600: Loss = -12294.03784350708
Iteration 4700: Loss = -12293.99644353134
Iteration 4800: Loss = -12293.96661206328
Iteration 4900: Loss = -12293.94383287048
Iteration 5000: Loss = -12293.957682365985
1
Iteration 5100: Loss = -12293.912359536358
Iteration 5200: Loss = -12293.900767300276
Iteration 5300: Loss = -12293.89127838855
Iteration 5400: Loss = -12293.882237851105
Iteration 5500: Loss = -12293.874967432897
Iteration 5600: Loss = -12293.86650132259
Iteration 5700: Loss = -12293.899016878559
1
Iteration 5800: Loss = -12293.849919545866
Iteration 5900: Loss = -12293.839339480875
Iteration 6000: Loss = -12293.825318292902
Iteration 6100: Loss = -12293.802455863608
Iteration 6200: Loss = -12293.757424775842
Iteration 6300: Loss = -12293.653345627345
Iteration 6400: Loss = -12293.482007622884
Iteration 6500: Loss = -12293.358886501066
Iteration 6600: Loss = -12293.29500707282
Iteration 6700: Loss = -12293.25462861544
Iteration 6800: Loss = -12293.219777146756
Iteration 6900: Loss = -12293.173644660603
Iteration 7000: Loss = -12293.072758750124
Iteration 7100: Loss = -12292.84414934301
Iteration 7200: Loss = -12292.612032666535
Iteration 7300: Loss = -12292.491075990289
Iteration 7400: Loss = -12292.423220269704
Iteration 7500: Loss = -12292.37671029799
Iteration 7600: Loss = -12292.341117165144
Iteration 7700: Loss = -12292.313659217589
Iteration 7800: Loss = -12292.293963852211
Iteration 7900: Loss = -12292.300136820253
1
Iteration 8000: Loss = -12292.271225821525
Iteration 8100: Loss = -12292.264619616832
Iteration 8200: Loss = -12292.259796463686
Iteration 8300: Loss = -12292.256151059482
Iteration 8400: Loss = -12292.253267647027
Iteration 8500: Loss = -12292.25099510734
Iteration 8600: Loss = -12292.419170980966
1
Iteration 8700: Loss = -12292.247659784809
Iteration 8800: Loss = -12292.246334548177
Iteration 8900: Loss = -12292.245262897299
Iteration 9000: Loss = -12292.246375974102
1
Iteration 9100: Loss = -12292.243521020166
Iteration 9200: Loss = -12292.24278122737
Iteration 9300: Loss = -12292.251366405128
1
Iteration 9400: Loss = -12292.241600744599
Iteration 9500: Loss = -12292.244120244464
1
Iteration 9600: Loss = -12292.24066746097
Iteration 9700: Loss = -12292.240473198144
Iteration 9800: Loss = -12292.242248616209
1
Iteration 9900: Loss = -12292.347425986774
2
Iteration 10000: Loss = -12292.239350412894
Iteration 10100: Loss = -12292.238986644994
Iteration 10200: Loss = -12292.23905245792
1
Iteration 10300: Loss = -12292.238638343018
Iteration 10400: Loss = -12292.23838562085
Iteration 10500: Loss = -12292.246826249686
1
Iteration 10600: Loss = -12292.237971507755
Iteration 10700: Loss = -12292.240457020958
1
Iteration 10800: Loss = -12292.317171691227
2
Iteration 10900: Loss = -12292.237455043794
Iteration 11000: Loss = -12292.237278201896
Iteration 11100: Loss = -12292.250347432991
1
Iteration 11200: Loss = -12292.237050993517
Iteration 11300: Loss = -12292.259294454258
1
Iteration 11400: Loss = -12292.237032779236
Iteration 11500: Loss = -12292.236790661203
Iteration 11600: Loss = -12292.271189327343
1
Iteration 11700: Loss = -12292.23650424008
Iteration 11800: Loss = -12292.279569520953
1
Iteration 11900: Loss = -12292.23700669413
2
Iteration 12000: Loss = -12292.504994681636
3
Iteration 12100: Loss = -12292.236224334361
Iteration 12200: Loss = -12292.348860299659
1
Iteration 12300: Loss = -12292.236311708128
2
Iteration 12400: Loss = -12292.236087791349
Iteration 12500: Loss = -12292.23609608724
1
Iteration 12600: Loss = -12292.296387064618
2
Iteration 12700: Loss = -12292.237183998745
3
Iteration 12800: Loss = -12292.25317165173
4
Iteration 12900: Loss = -12292.236759718915
5
Iteration 13000: Loss = -12292.235829297275
Iteration 13100: Loss = -12292.24221063033
1
Iteration 13200: Loss = -12292.27296693634
2
Iteration 13300: Loss = -12292.235645890763
Iteration 13400: Loss = -12292.238935308796
1
Iteration 13500: Loss = -12292.23556554427
Iteration 13600: Loss = -12292.27002996748
1
Iteration 13700: Loss = -12292.235464425497
Iteration 13800: Loss = -12292.265196693048
1
Iteration 13900: Loss = -12292.235446197119
Iteration 14000: Loss = -12292.23538986011
Iteration 14100: Loss = -12292.23591013246
1
Iteration 14200: Loss = -12292.235361821884
Iteration 14300: Loss = -12292.237475489073
1
Iteration 14400: Loss = -12292.235376454622
2
Iteration 14500: Loss = -12292.23529352665
Iteration 14600: Loss = -12292.263553603161
1
Iteration 14700: Loss = -12292.259871808568
2
Iteration 14800: Loss = -12292.3378941598
3
Iteration 14900: Loss = -12292.235293689304
4
Iteration 15000: Loss = -12292.259188211847
5
Iteration 15100: Loss = -12292.235173724777
Iteration 15200: Loss = -12292.293941260403
1
Iteration 15300: Loss = -12292.235166350978
Iteration 15400: Loss = -12292.23569677123
1
Iteration 15500: Loss = -12292.398069582912
2
Iteration 15600: Loss = -12292.235144659684
Iteration 15700: Loss = -12292.237015326875
1
Iteration 15800: Loss = -12292.23510637682
Iteration 15900: Loss = -12292.248203929023
1
Iteration 16000: Loss = -12292.235073957301
Iteration 16100: Loss = -12292.520881638904
1
Iteration 16200: Loss = -12292.23506042097
Iteration 16300: Loss = -12292.235051890286
Iteration 16400: Loss = -12292.236952918465
1
Iteration 16500: Loss = -12292.23502004805
Iteration 16600: Loss = -12292.254734734172
1
Iteration 16700: Loss = -12292.236198222641
2
Iteration 16800: Loss = -12292.249872815892
3
Iteration 16900: Loss = -12292.257145094884
4
Iteration 17000: Loss = -12292.297774267803
5
Iteration 17100: Loss = -12292.235073210384
6
Iteration 17200: Loss = -12292.235195952198
7
Iteration 17300: Loss = -12292.237813980279
8
Iteration 17400: Loss = -12292.251183507286
9
Iteration 17500: Loss = -12292.235059153487
10
Stopping early at iteration 17500 due to no improvement.
tensor([[-6.8515,  4.7412],
        [-8.7694,  6.3997],
        [-6.0752,  4.6752],
        [-7.9121,  6.5195],
        [-6.9555,  5.4471],
        [-4.6579,  2.4111],
        [-6.5960,  4.4504],
        [-7.8406,  6.1294],
        [-2.4309,  1.0164],
        [-4.6735,  2.5428],
        [-9.0412,  7.1532],
        [-4.0860,  2.6772],
        [-2.6570,  1.0198],
        [-8.8037,  7.0056],
        [-6.1535,  4.1957],
        [-6.3914,  4.5616],
        [-5.5591,  3.9729],
        [-8.6295,  5.7391],
        [-5.1128,  2.5273],
        [-6.8443,  5.4562],
        [-6.1081,  4.5915],
        [-8.0304,  6.5538],
        [-5.0121,  3.3939],
        [-4.0738,  1.7678],
        [-9.8980,  7.4059],
        [-4.9945,  3.3772],
        [-6.8560,  4.8713],
        [-7.7981,  5.5820],
        [-5.6086,  3.7958],
        [-3.3052,  1.8472],
        [-7.0048,  5.2202],
        [-5.8505,  1.3291],
        [-5.2930,  3.6341],
        [-6.6767,  4.5338],
        [-5.6262,  3.0011],
        [-4.2104,  2.3994],
        [-8.9610,  7.2948],
        [-7.7295,  5.5038],
        [-7.1141,  5.0867],
        [-4.1247,  2.3151],
        [-7.5314,  6.0593],
        [-3.4043,  2.0108],
        [-4.3009,  2.8150],
        [-7.2705,  5.2479],
        [-5.8056,  4.3116],
        [-6.4934,  5.0892],
        [-4.3059,  2.9038],
        [-7.1927,  5.3955],
        [-9.5579,  6.8329],
        [-5.6701,  4.0433],
        [-6.6741,  4.1641],
        [-7.9152,  5.8377],
        [-6.2686,  4.8794],
        [-4.0068,  2.2652],
        [-8.2465,  6.7595],
        [ 0.2218, -1.7624],
        [-8.2409,  6.5974],
        [-3.7343,  1.7929],
        [-8.3965,  5.0025],
        [-4.8831,  3.4666],
        [-4.4706,  3.0494],
        [-5.7630,  4.3727],
        [-4.2964,  2.4906],
        [-2.5599,  1.0965],
        [-2.1442,  0.6315],
        [-4.8190,  3.4314],
        [-6.3392,  4.8619],
        [-6.6206,  5.2119],
        [-6.5145,  3.4070],
        [-7.9964,  6.2420],
        [-7.7586,  6.2188],
        [-7.7909,  6.2387],
        [-3.8942,  2.1533],
        [-5.2231,  1.8859],
        [-8.7176,  5.9895],
        [-4.7997,  2.8110],
        [-6.9738,  5.5718],
        [-7.0560,  4.9321],
        [-6.6276,  5.1461],
        [-2.9167,  1.0158],
        [-3.2915,  1.6843],
        [-8.7938,  7.3310],
        [-4.4551,  2.9135],
        [-6.1320,  3.7109],
        [-5.1366,  3.4240],
        [-5.4474,  3.7136],
        [-6.4271,  4.8658],
        [-5.2452,  3.4545],
        [-7.1868,  5.3137],
        [-6.9484,  3.5277],
        [-7.6961,  6.3097],
        [-3.1610,  0.9228],
        [-4.0717,  1.9751],
        [-7.0872,  4.9976],
        [-5.9773,  4.3728],
        [-3.2644,  1.8167],
        [-3.2224,  1.8059],
        [-2.5400,  1.1068],
        [-3.8789,  2.4118],
        [-5.8351,  4.4184]], dtype=torch.float64, requires_grad=True)
pi: tensor([[6.9931e-07, 1.0000e+00],
        [1.0000e+00, 2.6275e-07]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0114, 0.9886], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1905, 0.1505],
         [0.1927, 0.2025]],

        [[0.8677, 0.1833],
         [0.9638, 0.8635]],

        [[0.4133, 0.1111],
         [0.2673, 0.7288]],

        [[0.3955, 0.2254],
         [0.6050, 0.6765]],

        [[0.6293, 0.2574],
         [0.7169, 0.3447]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
Global Adjusted Rand Index: -0.0019360173777129585
Average Adjusted Rand Index: -0.0006177931860792093
Iteration 0: Loss = -26406.17734359716
Iteration 10: Loss = -12296.024282831235
Iteration 20: Loss = -12296.024394416183
1
Iteration 30: Loss = -12296.024279344072
Iteration 40: Loss = -12296.024253150461
Iteration 50: Loss = -12296.024102079928
Iteration 60: Loss = -12296.023840441618
Iteration 70: Loss = -12296.022416582453
Iteration 80: Loss = -12296.016256457573
Iteration 90: Loss = -12295.989862521708
Iteration 100: Loss = -12295.90111165578
Iteration 110: Loss = -12295.735529720334
Iteration 120: Loss = -12295.58045064109
Iteration 130: Loss = -12295.459277021606
Iteration 140: Loss = -12295.33975206999
Iteration 150: Loss = -12295.229695117558
Iteration 160: Loss = -12295.143210529899
Iteration 170: Loss = -12295.076472277731
Iteration 180: Loss = -12295.023454433145
Iteration 190: Loss = -12294.980058923766
Iteration 200: Loss = -12294.943697515548
Iteration 210: Loss = -12294.912797004179
Iteration 220: Loss = -12294.886087762037
Iteration 230: Loss = -12294.86281845638
Iteration 240: Loss = -12294.842273555174
Iteration 250: Loss = -12294.824019661386
Iteration 260: Loss = -12294.807728772812
Iteration 270: Loss = -12294.792976064196
Iteration 280: Loss = -12294.779706787338
Iteration 290: Loss = -12294.767711866445
Iteration 300: Loss = -12294.756907843306
Iteration 310: Loss = -12294.747152296008
Iteration 320: Loss = -12294.73839583508
Iteration 330: Loss = -12294.730560929816
Iteration 340: Loss = -12294.723493215863
Iteration 350: Loss = -12294.717137214184
Iteration 360: Loss = -12294.711437356736
Iteration 370: Loss = -12294.70632970075
Iteration 380: Loss = -12294.70170290613
Iteration 390: Loss = -12294.697617563863
Iteration 400: Loss = -12294.693771509144
Iteration 410: Loss = -12294.690365048516
Iteration 420: Loss = -12294.687200168713
Iteration 430: Loss = -12294.684316948673
Iteration 440: Loss = -12294.68169092034
Iteration 450: Loss = -12294.67929439573
Iteration 460: Loss = -12294.677042253428
Iteration 470: Loss = -12294.674949600174
Iteration 480: Loss = -12294.673017395826
Iteration 490: Loss = -12294.671284318494
Iteration 500: Loss = -12294.66961153941
Iteration 510: Loss = -12294.668041126937
Iteration 520: Loss = -12294.666621038881
Iteration 530: Loss = -12294.665230634817
Iteration 540: Loss = -12294.66398414076
Iteration 550: Loss = -12294.66276347986
Iteration 560: Loss = -12294.661662851548
Iteration 570: Loss = -12294.66061335383
Iteration 580: Loss = -12294.659627938248
Iteration 590: Loss = -12294.658714851612
Iteration 600: Loss = -12294.6578415139
Iteration 610: Loss = -12294.657040185393
Iteration 620: Loss = -12294.656274063953
Iteration 630: Loss = -12294.655528076031
Iteration 640: Loss = -12294.654818594241
Iteration 650: Loss = -12294.654174181966
Iteration 660: Loss = -12294.653561016228
Iteration 670: Loss = -12294.652990094284
Iteration 680: Loss = -12294.652418482112
Iteration 690: Loss = -12294.651913290487
Iteration 700: Loss = -12294.65143031815
Iteration 710: Loss = -12294.650960163157
Iteration 720: Loss = -12294.650509351095
Iteration 730: Loss = -12294.650120252467
Iteration 740: Loss = -12294.64973627811
Iteration 750: Loss = -12294.649322034511
Iteration 760: Loss = -12294.648975106247
Iteration 770: Loss = -12294.648670086704
Iteration 780: Loss = -12294.648303630822
Iteration 790: Loss = -12294.64803541079
Iteration 800: Loss = -12294.647740405773
Iteration 810: Loss = -12294.647520346805
Iteration 820: Loss = -12294.64725123073
Iteration 830: Loss = -12294.647003897104
Iteration 840: Loss = -12294.646739143003
Iteration 850: Loss = -12294.646534218029
Iteration 860: Loss = -12294.646337104608
Iteration 870: Loss = -12294.646169673055
Iteration 880: Loss = -12294.645976918064
Iteration 890: Loss = -12294.64577281933
Iteration 900: Loss = -12294.645603000125
Iteration 910: Loss = -12294.6454027311
Iteration 920: Loss = -12294.645281412995
Iteration 930: Loss = -12294.645143306989
Iteration 940: Loss = -12294.645035808702
Iteration 950: Loss = -12294.644911889922
Iteration 960: Loss = -12294.644799618523
Iteration 970: Loss = -12294.64465367522
Iteration 980: Loss = -12294.644555045163
Iteration 990: Loss = -12294.64441492776
Iteration 1000: Loss = -12294.644360968347
Iteration 1010: Loss = -12294.644181418465
Iteration 1020: Loss = -12294.644138022171
Iteration 1030: Loss = -12294.64408588192
Iteration 1040: Loss = -12294.644003392277
Iteration 1050: Loss = -12294.643927168807
Iteration 1060: Loss = -12294.643842549873
Iteration 1070: Loss = -12294.643723999472
Iteration 1080: Loss = -12294.6436890113
Iteration 1090: Loss = -12294.643640641578
Iteration 1100: Loss = -12294.643547920448
Iteration 1110: Loss = -12294.643506359873
Iteration 1120: Loss = -12294.643463651952
Iteration 1130: Loss = -12294.643402993715
Iteration 1140: Loss = -12294.64337157178
Iteration 1150: Loss = -12294.64330075244
Iteration 1160: Loss = -12294.643244212835
Iteration 1170: Loss = -12294.643222984885
Iteration 1180: Loss = -12294.643181578129
Iteration 1190: Loss = -12294.643159318024
Iteration 1200: Loss = -12294.64308660661
Iteration 1210: Loss = -12294.643088359717
1
Iteration 1220: Loss = -12294.643051407442
Iteration 1230: Loss = -12294.643036703828
Iteration 1240: Loss = -12294.642996215844
Iteration 1250: Loss = -12294.642956900325
Iteration 1260: Loss = -12294.642935200756
Iteration 1270: Loss = -12294.642882401487
Iteration 1280: Loss = -12294.642903467868
1
Iteration 1290: Loss = -12294.642856382656
Iteration 1300: Loss = -12294.642826866093
Iteration 1310: Loss = -12294.642811491409
Iteration 1320: Loss = -12294.642768134687
Iteration 1330: Loss = -12294.64275075526
Iteration 1340: Loss = -12294.642731594453
Iteration 1350: Loss = -12294.642755562107
1
Iteration 1360: Loss = -12294.642690216162
Iteration 1370: Loss = -12294.642709947459
1
Iteration 1380: Loss = -12294.642664080993
Iteration 1390: Loss = -12294.642678261347
1
Iteration 1400: Loss = -12294.642635305327
Iteration 1410: Loss = -12294.642643808174
1
Iteration 1420: Loss = -12294.642604518294
Iteration 1430: Loss = -12294.642625986906
1
Iteration 1440: Loss = -12294.642624872491
2
Iteration 1450: Loss = -12294.642577368459
Iteration 1460: Loss = -12294.642614379225
1
Iteration 1470: Loss = -12294.642600896923
2
Iteration 1480: Loss = -12294.642584911
3
Stopping early at iteration 1479 due to no improvement.
pi: tensor([[0.1542, 0.8458],
        [0.1227, 0.8773]], dtype=torch.float64)
alpha: tensor([0.1273, 0.8727])
beta: tensor([[[0.2083, 0.2085],
         [0.7262, 0.1936]],

        [[0.6903, 0.1938],
         [0.8302, 0.7923]],

        [[0.2821, 0.2042],
         [0.8522, 0.3446]],

        [[0.1837, 0.1805],
         [0.7596, 0.3412]],

        [[0.5581, 0.2162],
         [0.1514, 0.6861]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -26324.563443636514
Iteration 100: Loss = -12321.871594012073
Iteration 200: Loss = -12253.368505331491
Iteration 300: Loss = -12239.99411307083
Iteration 400: Loss = -12234.10394045343
Iteration 500: Loss = -12233.156391986937
Iteration 600: Loss = -12232.6503434229
Iteration 700: Loss = -12232.33052859555
Iteration 800: Loss = -12232.08756389113
Iteration 900: Loss = -12231.851324862457
Iteration 1000: Loss = -12231.622417759461
Iteration 1100: Loss = -12231.3871650782
Iteration 1200: Loss = -12231.187199447333
Iteration 1300: Loss = -12227.320854579242
Iteration 1400: Loss = -12222.054297804152
Iteration 1500: Loss = -12208.2007929716
Iteration 1600: Loss = -12179.705751297986
Iteration 1700: Loss = -12122.779255379039
Iteration 1800: Loss = -11981.346833207366
Iteration 1900: Loss = -11880.801521458236
Iteration 2000: Loss = -11854.46811363611
Iteration 2100: Loss = -11832.930446971985
Iteration 2200: Loss = -11832.784087305632
Iteration 2300: Loss = -11832.710696223738
Iteration 2400: Loss = -11832.657603537058
Iteration 2500: Loss = -11832.61650915124
Iteration 2600: Loss = -11832.586317288387
Iteration 2700: Loss = -11832.202008332433
Iteration 2800: Loss = -11812.111636360465
Iteration 2900: Loss = -11812.046711802888
Iteration 3000: Loss = -11812.032628116462
Iteration 3100: Loss = -11812.020733861498
Iteration 3200: Loss = -11812.010114860652
Iteration 3300: Loss = -11811.99961430406
Iteration 3400: Loss = -11811.990724006315
Iteration 3500: Loss = -11811.975870813369
Iteration 3600: Loss = -11797.004458691881
Iteration 3700: Loss = -11796.986075990038
Iteration 3800: Loss = -11796.977783038124
Iteration 3900: Loss = -11796.97251892374
Iteration 4000: Loss = -11796.968289140912
Iteration 4100: Loss = -11796.964427004717
Iteration 4200: Loss = -11796.961096400077
Iteration 4300: Loss = -11796.958056718762
Iteration 4400: Loss = -11796.95524815789
Iteration 4500: Loss = -11796.952728387028
Iteration 4600: Loss = -11796.950408134884
Iteration 4700: Loss = -11796.948600250582
Iteration 4800: Loss = -11796.945876319467
Iteration 4900: Loss = -11796.941848611163
Iteration 5000: Loss = -11796.94010263078
Iteration 5100: Loss = -11796.938443288602
Iteration 5200: Loss = -11796.936680354318
Iteration 5300: Loss = -11796.934688099678
Iteration 5400: Loss = -11796.93255378
Iteration 5500: Loss = -11796.928753079867
Iteration 5600: Loss = -11796.927105668294
Iteration 5700: Loss = -11796.92639412174
Iteration 5800: Loss = -11796.925274773615
Iteration 5900: Loss = -11796.924376238343
Iteration 6000: Loss = -11796.923643898872
Iteration 6100: Loss = -11796.922964870406
Iteration 6200: Loss = -11796.92271593449
Iteration 6300: Loss = -11796.921403134183
Iteration 6400: Loss = -11796.920682440676
Iteration 6500: Loss = -11796.919327771453
Iteration 6600: Loss = -11796.838323632172
Iteration 6700: Loss = -11794.186851724879
Iteration 6800: Loss = -11794.182758336225
Iteration 6900: Loss = -11794.182811249937
1
Iteration 7000: Loss = -11794.180770636834
Iteration 7100: Loss = -11794.180155483695
Iteration 7200: Loss = -11794.179731149008
Iteration 7300: Loss = -11794.179341492623
Iteration 7400: Loss = -11794.179072569266
Iteration 7500: Loss = -11794.17863088726
Iteration 7600: Loss = -11794.179390732588
1
Iteration 7700: Loss = -11794.178043390437
Iteration 7800: Loss = -11794.177798672174
Iteration 7900: Loss = -11794.179766350286
1
Iteration 8000: Loss = -11794.180107779892
2
Iteration 8100: Loss = -11794.197328067457
3
Iteration 8200: Loss = -11794.17871192522
4
Iteration 8300: Loss = -11794.181281182842
5
Iteration 8400: Loss = -11794.173119347732
Iteration 8500: Loss = -11794.11347226075
Iteration 8600: Loss = -11794.11345126439
Iteration 8700: Loss = -11794.113484978818
1
Iteration 8800: Loss = -11794.120509918599
2
Iteration 8900: Loss = -11794.139246522735
3
Iteration 9000: Loss = -11794.116325268942
4
Iteration 9100: Loss = -11794.113216395393
Iteration 9200: Loss = -11794.122610778797
1
Iteration 9300: Loss = -11794.11309637645
Iteration 9400: Loss = -11794.114839200485
1
Iteration 9500: Loss = -11794.117640344659
2
Iteration 9600: Loss = -11794.126438516008
3
Iteration 9700: Loss = -11794.11282977921
Iteration 9800: Loss = -11794.114560069438
1
Iteration 9900: Loss = -11794.112352604307
Iteration 10000: Loss = -11794.114771827215
1
Iteration 10100: Loss = -11794.112675457083
2
Iteration 10200: Loss = -11794.116671496266
3
Iteration 10300: Loss = -11794.112325614426
Iteration 10400: Loss = -11794.120812657795
1
Iteration 10500: Loss = -11794.112011288036
Iteration 10600: Loss = -11794.111447295676
Iteration 10700: Loss = -11794.113382317566
1
Iteration 10800: Loss = -11794.11196576123
2
Iteration 10900: Loss = -11794.112675662958
3
Iteration 11000: Loss = -11794.120342489196
4
Iteration 11100: Loss = -11794.111183199298
Iteration 11200: Loss = -11794.111917782728
1
Iteration 11300: Loss = -11794.111100424647
Iteration 11400: Loss = -11794.111632207718
1
Iteration 11500: Loss = -11794.114562346027
2
Iteration 11600: Loss = -11794.129088643218
3
Iteration 11700: Loss = -11794.11126922629
4
Iteration 11800: Loss = -11794.113344352074
5
Iteration 11900: Loss = -11794.110587998308
Iteration 12000: Loss = -11794.120643455733
1
Iteration 12100: Loss = -11794.110151975505
Iteration 12200: Loss = -11794.10980541628
Iteration 12300: Loss = -11794.11453117526
1
Iteration 12400: Loss = -11794.119891519827
2
Iteration 12500: Loss = -11792.871461131037
Iteration 12600: Loss = -11792.646027939662
Iteration 12700: Loss = -11792.646383652951
1
Iteration 12800: Loss = -11792.649257196697
2
Iteration 12900: Loss = -11792.643503622481
Iteration 13000: Loss = -11792.642829921413
Iteration 13100: Loss = -11792.642420453058
Iteration 13200: Loss = -11792.645835305702
1
Iteration 13300: Loss = -11792.64318642983
2
Iteration 13400: Loss = -11792.650406604245
3
Iteration 13500: Loss = -11792.642073226249
Iteration 13600: Loss = -11792.642430489183
1
Iteration 13700: Loss = -11792.699816563156
2
Iteration 13800: Loss = -11792.712384272523
3
Iteration 13900: Loss = -11792.67668712733
4
Iteration 14000: Loss = -11792.64197679432
Iteration 14100: Loss = -11792.646077425632
1
Iteration 14200: Loss = -11792.648471187029
2
Iteration 14300: Loss = -11792.653215948412
3
Iteration 14400: Loss = -11792.641615765271
Iteration 14500: Loss = -11792.644195508954
1
Iteration 14600: Loss = -11792.662251735013
2
Iteration 14700: Loss = -11792.693225299916
3
Iteration 14800: Loss = -11792.643569028081
4
Iteration 14900: Loss = -11792.64889530836
5
Iteration 15000: Loss = -11792.642366766808
6
Iteration 15100: Loss = -11792.644605160376
7
Iteration 15200: Loss = -11792.644611241762
8
Iteration 15300: Loss = -11792.64226810684
9
Iteration 15400: Loss = -11792.654755491494
10
Stopping early at iteration 15400 due to no improvement.
tensor([[  8.5350,  -9.9948],
        [-10.0243,   7.6158],
        [  8.3650,  -9.7955],
        [ -9.3110,   7.1299],
        [ -9.5473,   8.1460],
        [ -9.8920,   8.3231],
        [ -9.4116,   7.6247],
        [  4.3831,  -8.2647],
        [  7.1905,  -8.5902],
        [  4.1009,  -5.4959],
        [ -6.2884,   4.7989],
        [ -6.3660,   4.8745],
        [  8.4910, -10.0583],
        [ -7.7169,   6.1540],
        [ -9.0094,   7.5615],
        [  7.0418,  -8.4294],
        [ -7.6568,   5.9281],
        [-10.0054,   6.7395],
        [ -9.3202,   7.9215],
        [ -7.1661,   5.7721],
        [-10.4261,   7.0159],
        [  8.6742, -11.4921],
        [ -9.5770,   8.1601],
        [ -5.7599,   4.3585],
        [  7.8477,  -9.4538],
        [ -8.4700,   7.0639],
        [ -8.0494,   6.5727],
        [ -6.7620,   4.2316],
        [ -9.6734,   7.4696],
        [ -9.7462,   7.5266],
        [  8.2577, -10.4500],
        [ -6.6597,   4.5597],
        [  7.7348,  -9.1915],
        [  8.9976, -10.4385],
        [  8.0623,  -9.9975],
        [ -5.9442,   4.4938],
        [  8.2474,  -9.6505],
        [ -8.8078,   7.2414],
        [ -4.4425,   1.3940],
        [  9.1230, -10.6809],
        [ -8.6298,   7.0824],
        [  2.6999,  -4.1405],
        [ -8.2457,   6.8575],
        [  6.5789,  -8.3614],
        [ -7.5655,   5.9866],
        [ -9.0325,   6.2644],
        [ -8.7773,   6.0100],
        [ -8.5032,   7.0733],
        [  7.3398,  -8.7506],
        [  7.3439,  -9.2899],
        [ -7.7585,   5.9434],
        [  8.2303, -12.7629],
        [  9.2997, -10.7744],
        [  7.8670, -10.5260],
        [ -9.8483,   6.6257],
        [ -8.8145,   6.2108],
        [  7.4660,  -9.3587],
        [ -9.1249,   6.6854],
        [  3.1783,  -4.7854],
        [ -0.8842,  -0.5039],
        [-10.0125,   7.7402],
        [  6.1994,  -7.7244],
        [  8.0601, -12.1597],
        [ -5.6266,   3.4826],
        [ -8.4017,   6.2894],
        [  6.1074,  -7.4969],
        [  5.2763,  -6.8234],
        [  2.5373,  -4.3335],
        [ -3.9311,   2.1324],
        [ -5.7223,   3.9846],
        [ -9.0202,   7.3560],
        [  8.9508, -10.4102],
        [ -4.2382,   2.7789],
        [ -8.9448,   7.5534],
        [ -0.1894,  -1.2412],
        [  8.4710,  -9.9847],
        [  8.0774,  -9.6278],
        [ -8.3712,   6.9429],
        [ -8.0277,   6.4900],
        [  7.8719,  -9.2605],
        [ -8.3440,   6.3910],
        [ -8.0257,   3.4104],
        [  8.4659, -11.1909],
        [ -9.8112,   7.9277],
        [ -8.3488,   6.5905],
        [-10.7689,   7.3992],
        [ -9.2222,   6.9691],
        [  9.3948, -10.8393],
        [  4.2648,  -5.7979],
        [ -8.4560,   6.9894],
        [  8.1365, -11.4365],
        [  9.0659, -10.4965],
        [  8.4014,  -9.7880],
        [  6.9371, -10.8695],
        [  8.7166, -10.5505],
        [  0.7198,  -2.1093],
        [ -6.2372,   4.8481],
        [  7.8645,  -9.3140],
        [ -9.4112,   7.9163],
        [ -2.4006,   0.8820]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7677, 0.2323],
        [0.2517, 0.7483]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4415, 0.5585], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3003, 0.0990],
         [0.7262, 0.2931]],

        [[0.6903, 0.1011],
         [0.8302, 0.7923]],

        [[0.2821, 0.0953],
         [0.8522, 0.3446]],

        [[0.1837, 0.0933],
         [0.7596, 0.3412]],

        [[0.5581, 0.1070],
         [0.1514, 0.6861]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 1
tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 4
tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840320165194881
Average Adjusted Rand Index: 0.9839998119331363
11797.16589123401
new:  [-0.0019360173777129585, -0.0019360173777129585, -0.0019360173777129585, 0.9840320165194881] [-0.0006177931860792093, -0.0006177931860792093, -0.0006177931860792093, 0.9839998119331363] [12292.235809938486, 12292.337569054795, 12292.235059153487, 11792.654755491494]
prior:  [0.0, 0.0, 0.0, 0.0] [0.0, 0.0, 0.0, 0.0] [nan, 12296.024222010738, 12296.024236230598, 12294.642584911]
-----------------------------------------------------------------------------------------
This iteration is 18
True Objective function: Loss = -11972.837437852699
Iteration 0: Loss = -27124.075194033347
Iteration 10: Loss = -12473.677754782471
Iteration 20: Loss = -12473.6006906671
Iteration 30: Loss = -12473.59958715597
Iteration 40: Loss = -12473.59975456111
1
Iteration 50: Loss = -12473.599860038914
2
Iteration 60: Loss = -12473.599848310265
3
Stopping early at iteration 59 due to no improvement.
pi: tensor([[0.4637, 0.5363],
        [0.9971, 0.0029]], dtype=torch.float64)
alpha: tensor([0.6502, 0.3498])
beta: tensor([[[0.1996, 0.2044],
         [0.3367, 0.2024]],

        [[0.9690, 0.1891],
         [0.5449, 0.9115]],

        [[0.7589, 0.2017],
         [0.0147, 0.0669]],

        [[0.8310, 0.2033],
         [0.7674, 0.9451]],

        [[0.9814, 0.2065],
         [0.0764, 0.8251]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -27067.973459496574
Iteration 100: Loss = -12487.308561857026
Iteration 200: Loss = -12478.258947323346
Iteration 300: Loss = -12476.166247879723
Iteration 400: Loss = -12475.429283721758
Iteration 500: Loss = -12475.098417751953
Iteration 600: Loss = -12474.903163455405
Iteration 700: Loss = -12474.110363939517
Iteration 800: Loss = -12473.552058669335
Iteration 900: Loss = -12473.018460748139
Iteration 1000: Loss = -12472.613827670752
Iteration 1100: Loss = -12472.399718049954
Iteration 1200: Loss = -12472.287476459604
Iteration 1300: Loss = -12472.226331128062
Iteration 1400: Loss = -12472.185369935314
Iteration 1500: Loss = -12472.148671303283
Iteration 1600: Loss = -12472.1075468306
Iteration 1700: Loss = -12472.044223405055
Iteration 1800: Loss = -12471.913056397983
Iteration 1900: Loss = -12471.659523478967
Iteration 2000: Loss = -12471.354734249828
Iteration 2100: Loss = -12471.115069873522
Iteration 2200: Loss = -12470.956987436297
Iteration 2300: Loss = -12470.8557584325
Iteration 2400: Loss = -12470.788952524883
Iteration 2500: Loss = -12470.742960943751
Iteration 2600: Loss = -12470.709929406426
Iteration 2700: Loss = -12470.685259798245
Iteration 2800: Loss = -12470.666480416958
Iteration 2900: Loss = -12470.651759416362
Iteration 3000: Loss = -12470.6400362477
Iteration 3100: Loss = -12470.630532684028
Iteration 3200: Loss = -12470.622765470182
Iteration 3300: Loss = -12470.616274410722
Iteration 3400: Loss = -12470.610509107948
Iteration 3500: Loss = -12470.60552801673
Iteration 3600: Loss = -12470.601503634809
Iteration 3700: Loss = -12470.598023627981
Iteration 3800: Loss = -12470.594904799224
Iteration 3900: Loss = -12470.592143166468
Iteration 4000: Loss = -12470.589700572864
Iteration 4100: Loss = -12470.587343697693
Iteration 4200: Loss = -12470.58509037872
Iteration 4300: Loss = -12470.582744842939
Iteration 4400: Loss = -12470.580031031395
Iteration 4500: Loss = -12470.56619542617
Iteration 4600: Loss = -12470.55243266173
Iteration 4700: Loss = -12470.538845854577
Iteration 4800: Loss = -12470.520084739119
Iteration 4900: Loss = -12470.502560034683
Iteration 5000: Loss = -12470.486844873809
Iteration 5100: Loss = -12470.475153419258
Iteration 5200: Loss = -12470.44732502211
Iteration 5300: Loss = -12470.423454135984
Iteration 5400: Loss = -12470.416582453296
Iteration 5500: Loss = -12470.289297927848
Iteration 5600: Loss = -12470.287164829148
Iteration 5700: Loss = -12470.28467786257
Iteration 5800: Loss = -12470.282882769403
Iteration 5900: Loss = -12470.281616243286
Iteration 6000: Loss = -12470.277530258463
Iteration 6100: Loss = -12470.273506208314
Iteration 6200: Loss = -12470.269977937467
Iteration 6300: Loss = -12470.262499765004
Iteration 6400: Loss = -12470.258515283926
Iteration 6500: Loss = -12470.256607947807
Iteration 6600: Loss = -12470.2488418725
Iteration 6700: Loss = -12470.247679259102
Iteration 6800: Loss = -12470.246038136267
Iteration 6900: Loss = -12470.245917288608
Iteration 7000: Loss = -12470.242588296018
Iteration 7100: Loss = -12470.242137316935
Iteration 7200: Loss = -12470.241060961238
Iteration 7300: Loss = -12470.240506682007
Iteration 7400: Loss = -12470.240336621553
Iteration 7500: Loss = -12470.23799464395
Iteration 7600: Loss = -12470.237942369426
Iteration 7700: Loss = -12470.237416028122
Iteration 7800: Loss = -12470.23738226004
Iteration 7900: Loss = -12470.237297207592
Iteration 8000: Loss = -12470.237038722928
Iteration 8100: Loss = -12470.235956292521
Iteration 8200: Loss = -12470.235538670548
Iteration 8300: Loss = -12470.234962189586
Iteration 8400: Loss = -12470.242604307186
1
Iteration 8500: Loss = -12470.239221051901
2
Iteration 8600: Loss = -12470.235850865056
3
Iteration 8700: Loss = -12470.235119378996
4
Iteration 8800: Loss = -12470.247039574071
5
Iteration 8900: Loss = -12470.234914692523
Iteration 9000: Loss = -12470.235086425062
1
Iteration 9100: Loss = -12470.234914119394
Iteration 9200: Loss = -12470.234168140316
Iteration 9300: Loss = -12470.2328927881
Iteration 9400: Loss = -12470.233319238485
1
Iteration 9500: Loss = -12470.232875634332
Iteration 9600: Loss = -12470.232946129825
1
Iteration 9700: Loss = -12470.232982592586
2
Iteration 9800: Loss = -12470.23290281247
3
Iteration 9900: Loss = -12470.23288063909
4
Iteration 10000: Loss = -12470.23284916588
Iteration 10100: Loss = -12470.232797673618
Iteration 10200: Loss = -12470.232096434504
Iteration 10300: Loss = -12470.232375038611
1
Iteration 10400: Loss = -12470.232024092078
Iteration 10500: Loss = -12470.232009897258
Iteration 10600: Loss = -12470.231920740187
Iteration 10700: Loss = -12470.231224389847
Iteration 10800: Loss = -12470.228567183722
Iteration 10900: Loss = -12470.228282439244
Iteration 11000: Loss = -12470.233406920035
1
Iteration 11100: Loss = -12470.228257936042
Iteration 11200: Loss = -12470.228238120751
Iteration 11300: Loss = -12470.22825378255
1
Iteration 11400: Loss = -12470.227529465563
Iteration 11500: Loss = -12470.227452140964
Iteration 11600: Loss = -12470.227463053785
1
Iteration 11700: Loss = -12470.23410544254
2
Iteration 11800: Loss = -12470.227434002098
Iteration 11900: Loss = -12470.22742466787
Iteration 12000: Loss = -12470.227422985612
Iteration 12100: Loss = -12470.22769036621
1
Iteration 12200: Loss = -12470.227444076767
2
Iteration 12300: Loss = -12470.227381753495
Iteration 12400: Loss = -12470.226706366911
Iteration 12500: Loss = -12470.267550513357
1
Iteration 12600: Loss = -12470.226683728833
Iteration 12700: Loss = -12470.226659056396
Iteration 12800: Loss = -12470.251490007924
1
Iteration 12900: Loss = -12470.226233386857
Iteration 13000: Loss = -12470.226447987818
1
Iteration 13100: Loss = -12470.226531903052
2
Iteration 13200: Loss = -12470.52555480813
3
Iteration 13300: Loss = -12470.226222194646
Iteration 13400: Loss = -12470.267351896642
1
Iteration 13500: Loss = -12470.226255045103
2
Iteration 13600: Loss = -12470.245839446157
3
Iteration 13700: Loss = -12470.225982963022
Iteration 13800: Loss = -12470.225957733535
Iteration 13900: Loss = -12470.226088343734
1
Iteration 14000: Loss = -12470.225965597834
2
Iteration 14100: Loss = -12470.235643801017
3
Iteration 14200: Loss = -12470.224861425635
Iteration 14300: Loss = -12470.22484683301
Iteration 14400: Loss = -12470.225170686774
1
Iteration 14500: Loss = -12470.224855083592
2
Iteration 14600: Loss = -12470.224782304203
Iteration 14700: Loss = -12470.245543022887
1
Iteration 14800: Loss = -12470.224776956631
Iteration 14900: Loss = -12470.22475456962
Iteration 15000: Loss = -12470.225126910012
1
Iteration 15100: Loss = -12470.224829297526
2
Iteration 15200: Loss = -12470.224728541132
Iteration 15300: Loss = -12470.22475817964
1
Iteration 15400: Loss = -12470.224801837257
2
Iteration 15500: Loss = -12470.224782201756
3
Iteration 15600: Loss = -12470.224729895994
4
Iteration 15700: Loss = -12470.225852187665
5
Iteration 15800: Loss = -12470.22473336575
6
Iteration 15900: Loss = -12470.224732660401
7
Iteration 16000: Loss = -12470.23442999613
8
Iteration 16100: Loss = -12470.224769159842
9
Iteration 16200: Loss = -12470.2247605739
10
Stopping early at iteration 16200 due to no improvement.
tensor([[-10.1912,   5.5760],
        [-10.4200,   5.8048],
        [-11.4561,   6.8408],
        [ -7.2018,   2.5866],
        [-11.9719,   7.3566],
        [ -9.9025,   5.2873],
        [-11.4368,   6.8215],
        [-11.5988,   6.9836],
        [-12.1633,   7.5481],
        [-11.6428,   7.0276],
        [-11.2422,   6.6269],
        [-11.9771,   7.3619],
        [-10.9010,   6.2858],
        [-12.2163,   7.6011],
        [ -8.7233,   4.1081],
        [ -6.3710,   1.7558],
        [-11.4456,   6.8303],
        [-11.9526,   7.3374],
        [-11.4223,   6.8071],
        [-11.8013,   7.1861],
        [-11.1690,   6.5538],
        [ -1.2842,  -3.3310],
        [-12.1498,   7.5346],
        [-10.7404,   6.1252],
        [-10.8363,   6.2211],
        [-10.0764,   5.4611],
        [-11.9325,   7.3173],
        [-10.0389,   5.4237],
        [-11.2772,   6.6620],
        [ -9.9901,   5.3748],
        [ -7.6406,   3.0254],
        [-11.2774,   6.6622],
        [-11.3200,   6.7048],
        [-10.9640,   6.3487],
        [-11.3909,   6.7757],
        [ -8.7481,   4.1329],
        [-11.8283,   7.2130],
        [ -9.7180,   5.1028],
        [-10.3862,   5.7710],
        [-11.3659,   6.7507],
        [-11.4030,   6.7878],
        [ -6.8177,   2.2024],
        [ -7.5344,   2.9192],
        [ -9.6969,   5.0816],
        [-12.0049,   7.3897],
        [-10.6876,   6.0724],
        [-10.0395,   5.4243],
        [-11.8751,   7.2599],
        [-11.5183,   6.9031],
        [-11.6145,   6.9993],
        [-12.1736,   7.5584],
        [-11.0380,   6.4228],
        [ -7.4108,   2.7956],
        [-11.7142,   7.0989],
        [ -6.9548,   2.3395],
        [-11.1818,   6.5666],
        [-11.7358,   7.1206],
        [ -9.0539,   4.4386],
        [-10.4355,   5.8203],
        [-10.9437,   6.3285],
        [ -8.8317,   4.2165],
        [-11.4676,   6.8524],
        [-11.7496,   7.1344],
        [ -9.9894,   5.3742],
        [-11.3468,   6.7316],
        [ -7.5308,   2.9155],
        [-11.4146,   6.7993],
        [-11.5642,   6.9490],
        [-10.7191,   6.1038],
        [-10.3126,   5.6973],
        [-11.3647,   6.7495],
        [-11.9162,   7.3010],
        [-11.6840,   7.0688],
        [ -8.2217,   3.6065],
        [-10.5255,   5.9103],
        [-10.8036,   6.1883],
        [-10.3973,   5.7821],
        [-11.7095,   7.0943],
        [-11.6752,   7.0600],
        [ -0.9010,  -3.7142],
        [-10.6895,   6.0743],
        [-11.9808,   7.3655],
        [-11.6785,   7.0633],
        [-12.0301,   7.4149],
        [-11.9040,   7.2888],
        [ -8.2338,   3.6186],
        [ -7.6925,   3.0773],
        [-10.3410,   5.7258],
        [-11.0225,   6.4072],
        [-11.5560,   6.9408],
        [-11.9022,   7.2869],
        [-10.5144,   5.8992],
        [-11.9217,   7.3064],
        [-11.5465,   6.9312],
        [  0.7361,  -5.3514],
        [ -0.5038,  -4.1114],
        [-11.2819,   6.6666],
        [-11.4717,   6.8565],
        [ -0.0996,  -4.5157],
        [-11.6627,   7.0475]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.1762, 0.8238],
        [0.0028, 0.9972]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0479, 0.9521], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.9952, 0.2757],
         [0.3367, 0.2015]],

        [[0.9690, 0.1003],
         [0.5449, 0.9115]],

        [[0.7589, 0.1690],
         [0.0147, 0.0669]],

        [[0.8310, 0.2400],
         [0.7674, 0.9451]],

        [[0.9814, 0.2962],
         [0.0764, 0.8251]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.011562904917628186
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.00776683106263838
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00038155393280398057
Average Adjusted Rand Index: -0.0007592147709979612
Iteration 0: Loss = -24865.137985951103
Iteration 10: Loss = -12474.241695863735
Iteration 20: Loss = -12473.599461468148
Iteration 30: Loss = -12473.16227694071
Iteration 40: Loss = -12472.910372246706
Iteration 50: Loss = -12472.817606278382
Iteration 60: Loss = -12472.779370488899
Iteration 70: Loss = -12472.762138656273
Iteration 80: Loss = -12472.753757969487
Iteration 90: Loss = -12472.749495930519
Iteration 100: Loss = -12472.747213201324
Iteration 110: Loss = -12472.745943572394
Iteration 120: Loss = -12472.745204392033
Iteration 130: Loss = -12472.744746214965
Iteration 140: Loss = -12472.744494482844
Iteration 150: Loss = -12472.744337764812
Iteration 160: Loss = -12472.744172960738
Iteration 170: Loss = -12472.744149530166
Iteration 180: Loss = -12472.744165956463
1
Iteration 190: Loss = -12472.744066394345
Iteration 200: Loss = -12472.74402903678
Iteration 210: Loss = -12472.744011894874
Iteration 220: Loss = -12472.74403916871
1
Iteration 230: Loss = -12472.744032342389
2
Iteration 240: Loss = -12472.743992896005
Iteration 250: Loss = -12472.744016717797
1
Iteration 260: Loss = -12472.744019552876
2
Iteration 270: Loss = -12472.743987526004
Iteration 280: Loss = -12472.74402198651
1
Iteration 290: Loss = -12472.744018568508
2
Iteration 300: Loss = -12472.744021994635
3
Stopping early at iteration 299 due to no improvement.
pi: tensor([[0.9616, 0.0384],
        [0.8805, 0.1195]], dtype=torch.float64)
alpha: tensor([0.9579, 0.0421])
beta: tensor([[[0.1993, 0.2632],
         [0.6453, 0.2494]],

        [[0.3779, 0.1372],
         [0.9842, 0.6081]],

        [[0.1498, 0.2082],
         [0.5005, 0.2268]],

        [[0.4287, 0.2198],
         [0.8498, 0.1886]],

        [[0.7444, 0.2381],
         [0.2527, 0.5601]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: -0.00491035742834304
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0004811949056573946
Average Adjusted Rand Index: -0.000982071485668608
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24864.883337968397
Iteration 100: Loss = -12524.047571280156
Iteration 200: Loss = -12491.079641375092
Iteration 300: Loss = -12481.480431908254
Iteration 400: Loss = -12478.834306030269
Iteration 500: Loss = -12469.248692164887
Iteration 600: Loss = -12393.454117039748
Iteration 700: Loss = -12392.793815345236
Iteration 800: Loss = -12392.516478113115
Iteration 900: Loss = -12392.248167027663
Iteration 1000: Loss = -12392.11326125542
Iteration 1100: Loss = -12392.016222128765
Iteration 1200: Loss = -12391.942498559289
Iteration 1300: Loss = -12391.88143318108
Iteration 1400: Loss = -12391.827369957933
Iteration 1500: Loss = -12391.851707014237
1
Iteration 1600: Loss = -12391.738950346882
Iteration 1700: Loss = -12391.701470665115
Iteration 1800: Loss = -12391.665744078568
Iteration 1900: Loss = -12391.634927077064
Iteration 2000: Loss = -12391.61181867398
Iteration 2100: Loss = -12391.590033872808
Iteration 2200: Loss = -12391.572409727862
Iteration 2300: Loss = -12391.234113128507
Iteration 2400: Loss = -12390.976135857973
Iteration 2500: Loss = -12390.886407841255
Iteration 2600: Loss = -12390.82112676026
Iteration 2700: Loss = -12390.759589909567
Iteration 2800: Loss = -12390.710971699611
Iteration 2900: Loss = -12390.69470705911
Iteration 3000: Loss = -12390.672521456798
Iteration 3100: Loss = -12390.663316297245
Iteration 3200: Loss = -12390.938308936584
1
Iteration 3300: Loss = -12390.64333884585
Iteration 3400: Loss = -12390.628989202783
Iteration 3500: Loss = -12390.614816668181
Iteration 3600: Loss = -12390.5949221684
Iteration 3700: Loss = -12390.585335611651
Iteration 3800: Loss = -12390.577649145165
Iteration 3900: Loss = -12390.571152664701
Iteration 4000: Loss = -12367.486614774234
Iteration 4100: Loss = -12332.312251042069
Iteration 4200: Loss = -12202.28127949301
Iteration 4300: Loss = -12074.800835387276
Iteration 4400: Loss = -12054.337236541605
Iteration 4500: Loss = -12042.038989874205
Iteration 4600: Loss = -12042.007583836503
Iteration 4700: Loss = -12033.720010021907
Iteration 4800: Loss = -12033.705627165706
Iteration 4900: Loss = -12033.697308445015
Iteration 5000: Loss = -12033.6891637713
Iteration 5100: Loss = -12033.626340665925
Iteration 5200: Loss = -12023.932940331953
Iteration 5300: Loss = -12023.926665664141
Iteration 5400: Loss = -12023.923176517295
Iteration 5500: Loss = -12023.922254892957
Iteration 5600: Loss = -12023.918132974335
Iteration 5700: Loss = -12023.9197900527
1
Iteration 5800: Loss = -12023.914435365932
Iteration 5900: Loss = -12023.912766777134
Iteration 6000: Loss = -12023.91113261444
Iteration 6100: Loss = -12023.909253563153
Iteration 6200: Loss = -12020.532754341983
Iteration 6300: Loss = -12020.06316449062
Iteration 6400: Loss = -12014.460936279005
Iteration 6500: Loss = -12014.451349498026
Iteration 6600: Loss = -12014.44506687747
Iteration 6700: Loss = -12014.461482153903
1
Iteration 6800: Loss = -12014.44494239275
Iteration 6900: Loss = -12014.443810641525
Iteration 7000: Loss = -12014.451163354188
1
Iteration 7100: Loss = -12014.522519524484
2
Iteration 7200: Loss = -12014.44147915343
Iteration 7300: Loss = -12014.443740540648
1
Iteration 7400: Loss = -12014.441912303828
2
Iteration 7500: Loss = -12014.441315355363
Iteration 7600: Loss = -12011.552310747065
Iteration 7700: Loss = -12011.57522651589
1
Iteration 7800: Loss = -12011.542680846896
Iteration 7900: Loss = -12003.674264680112
Iteration 8000: Loss = -12003.67184420797
Iteration 8100: Loss = -12003.673376041557
1
Iteration 8200: Loss = -12003.677992846287
2
Iteration 8300: Loss = -12003.67610662133
3
Iteration 8400: Loss = -12003.67886211273
4
Iteration 8500: Loss = -12003.67670894266
5
Iteration 8600: Loss = -12003.672147301188
6
Iteration 8700: Loss = -12003.670648971
Iteration 8800: Loss = -12003.67044573891
Iteration 8900: Loss = -12003.680163345365
1
Iteration 9000: Loss = -12003.714395591644
2
Iteration 9100: Loss = -12003.669134147938
Iteration 9200: Loss = -12003.674234241997
1
Iteration 9300: Loss = -12003.674958901509
2
Iteration 9400: Loss = -12003.694350997092
3
Iteration 9500: Loss = -12003.664951367125
Iteration 9600: Loss = -12003.624597815498
Iteration 9700: Loss = -12003.624778982852
1
Iteration 9800: Loss = -12003.62513365388
2
Iteration 9900: Loss = -12003.61152574622
Iteration 10000: Loss = -11994.979301187455
Iteration 10100: Loss = -11994.95904321605
Iteration 10200: Loss = -11994.968894300697
1
Iteration 10300: Loss = -11994.974387265887
2
Iteration 10400: Loss = -11994.983336772326
3
Iteration 10500: Loss = -11994.972150345604
4
Iteration 10600: Loss = -11994.958717916046
Iteration 10700: Loss = -11994.958584598788
Iteration 10800: Loss = -11994.959491910373
1
Iteration 10900: Loss = -11994.95818736241
Iteration 11000: Loss = -11994.968780189136
1
Iteration 11100: Loss = -11994.966725073433
2
Iteration 11200: Loss = -11994.947516401506
Iteration 11300: Loss = -11994.964501748638
1
Iteration 11400: Loss = -11994.948651314804
2
Iteration 11500: Loss = -11994.948295649352
3
Iteration 11600: Loss = -11994.97949389213
4
Iteration 11700: Loss = -11994.993267025136
5
Iteration 11800: Loss = -11994.978431719255
6
Iteration 11900: Loss = -11994.950688804856
7
Iteration 12000: Loss = -11994.96296063814
8
Iteration 12100: Loss = -11994.966836402462
9
Iteration 12200: Loss = -11994.959002859314
10
Stopping early at iteration 12200 due to no improvement.
tensor([[  6.3331,  -7.7664],
        [ -9.2367,   7.6895],
        [  6.4354,  -7.8444],
        [-10.3242,   7.8147],
        [  6.8559,  -8.6083],
        [ -6.5423,   4.5683],
        [  5.7318,  -8.6547],
        [  4.7127,  -6.1603],
        [  5.9562,  -7.4410],
        [  3.3361,  -4.9023],
        [ -9.6126,   6.1189],
        [ -9.6706,   7.9045],
        [ -7.9734,   5.7947],
        [ -8.7891,   6.8922],
        [-10.4110,   7.9238],
        [ -9.9872,   6.7809],
        [  3.3089,  -4.8315],
        [  5.3878,  -6.7862],
        [ -8.9728,   7.2959],
        [  7.5127, -10.0328],
        [  5.0234,  -7.2136],
        [ -8.6797,   7.2922],
        [ -9.5824,   8.1727],
        [ -8.9983,   7.0918],
        [ -9.0719,   7.6789],
        [ -9.6149,   7.8102],
        [  6.7717,  -8.1592],
        [ -7.5102,   6.0624],
        [ -9.4750,   7.4195],
        [-10.6710,   7.7585],
        [ -8.8936,   7.2147],
        [ -3.6353,   2.0474],
        [ -8.2708,   6.8845],
        [ -8.1252,   6.7065],
        [ -4.3268,   2.6702],
        [ -8.9694,   7.2260],
        [  5.9542,  -7.4925],
        [ -9.6483,   8.2411],
        [ -9.2075,   7.2484],
        [ -8.7809,   7.2675],
        [ -8.4847,   7.0556],
        [ -9.0215,   7.0378],
        [  6.3722,  -8.9438],
        [ -9.9023,   8.2527],
        [  3.9319,  -6.9294],
        [  5.4762,  -8.9211],
        [ -9.4042,   7.9257],
        [ -9.4481,   6.9545],
        [ -9.7600,   8.3077],
        [  7.3353,  -9.1005],
        [  1.8359,  -3.6645],
        [ -9.9431,   7.9837],
        [  7.5737, -10.3748],
        [  6.5444,  -8.1554],
        [ -8.2010,   6.7138],
        [  6.9445,  -8.3933],
        [  5.2862,  -6.9012],
        [  1.6244,  -4.5990],
        [  5.9710,  -7.4019],
        [ -9.4042,   7.5504],
        [ -9.7631,   7.4925],
        [ -8.3824,   6.5931],
        [ -7.0136,   5.1739],
        [-10.5805,   7.0209],
        [-10.9591,   7.6737],
        [ -8.7701,   7.2390],
        [  6.5883,  -8.8083],
        [ -8.6207,   7.0608],
        [ -9.4461,   7.3731],
        [ -9.5871,   8.2001],
        [ -8.7711,   7.3302],
        [  7.1024,  -8.5479],
        [  5.2644,  -6.8802],
        [ -5.6382,   4.1182],
        [  7.1938,  -8.5826],
        [  4.6272,  -6.2355],
        [  5.3406,  -6.7277],
        [ -8.9436,   7.3108],
        [ -9.7356,   8.2863],
        [-10.0821,   8.2777],
        [  6.7880,  -8.2716],
        [  0.9722,  -3.2700],
        [  6.3843,  -7.7997],
        [ -9.2580,   7.8528],
        [ -8.9479,   7.4027],
        [ -7.1734,   5.4585],
        [  3.2315,  -4.9142],
        [ -8.5147,   7.1267],
        [  6.4696,  -7.8636],
        [ -8.8347,   7.4249],
        [  4.9835,  -7.1481],
        [  3.3844,  -4.7725],
        [  6.2491,  -7.7649],
        [  2.6134,  -4.2072],
        [  4.6557,  -6.1280],
        [ -9.0922,   6.8419],
        [ -8.7726,   7.3480],
        [  7.0587,  -9.3601],
        [ -9.9605,   8.0418],
        [-10.0493,   6.7133]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7228, 0.2772],
        [0.2654, 0.7346]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4104, 0.5896], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3006, 0.1021],
         [0.6453, 0.3023]],

        [[0.3779, 0.1073],
         [0.9842, 0.6081]],

        [[0.1498, 0.0981],
         [0.5005, 0.2268]],

        [[0.4287, 0.1005],
         [0.8498, 0.1886]],

        [[0.7444, 0.1109],
         [0.2527, 0.5601]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9207907017983009
time is 1
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 98
Adjusted Rand Index: 0.9208056373456518
time is 2
tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9681922444705638
Average Adjusted Rand Index: 0.9683192678287906
Iteration 0: Loss = -17423.80632777952
Iteration 10: Loss = -12473.119164398076
Iteration 20: Loss = -12473.073548223001
Iteration 30: Loss = -12473.059854056226
Iteration 40: Loss = -12473.049526501612
Iteration 50: Loss = -12473.04078990426
Iteration 60: Loss = -12473.032915759775
Iteration 70: Loss = -12473.025873235712
Iteration 80: Loss = -12473.019444992655
Iteration 90: Loss = -12473.01351234787
Iteration 100: Loss = -12473.008046635594
Iteration 110: Loss = -12473.002910157982
Iteration 120: Loss = -12472.998306372087
Iteration 130: Loss = -12472.993971158945
Iteration 140: Loss = -12472.99000326818
Iteration 150: Loss = -12472.986347988419
Iteration 160: Loss = -12472.98298096695
Iteration 170: Loss = -12472.979780891992
Iteration 180: Loss = -12472.976891016518
Iteration 190: Loss = -12472.974090484993
Iteration 200: Loss = -12472.971452010408
Iteration 210: Loss = -12472.968895511309
Iteration 220: Loss = -12472.966323022174
Iteration 230: Loss = -12472.963719023293
Iteration 240: Loss = -12472.96090390329
Iteration 250: Loss = -12472.957834274932
Iteration 260: Loss = -12472.954210415668
Iteration 270: Loss = -12472.949732140183
Iteration 280: Loss = -12472.94384944746
Iteration 290: Loss = -12472.935220603604
Iteration 300: Loss = -12472.921797798765
Iteration 310: Loss = -12472.898761767568
Iteration 320: Loss = -12472.859845591483
Iteration 330: Loss = -12472.807950475057
Iteration 340: Loss = -12472.76692722847
Iteration 350: Loss = -12472.747915259013
Iteration 360: Loss = -12472.74182119207
Iteration 370: Loss = -12472.74072127125
Iteration 380: Loss = -12472.74113266076
1
Iteration 390: Loss = -12472.741870828833
2
Iteration 400: Loss = -12472.74252715076
3
Stopping early at iteration 399 due to no improvement.
pi: tensor([[0.9609, 0.0391],
        [0.8812, 0.1188]], dtype=torch.float64)
alpha: tensor([0.9573, 0.0427])
beta: tensor([[[0.1993, 0.2628],
         [0.9578, 0.2490]],

        [[0.1378, 0.1376],
         [0.8053, 0.0169]],

        [[0.7919, 0.2083],
         [0.1921, 0.0362]],

        [[0.6558, 0.2197],
         [0.0953, 0.5542]],

        [[0.0854, 0.2379],
         [0.2773, 0.3441]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: -0.00491035742834304
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0004811949056573946
Average Adjusted Rand Index: -0.000982071485668608
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17423.503245010827
Iteration 100: Loss = -12484.748105522714
Iteration 200: Loss = -12444.271202694485
Iteration 300: Loss = -12307.053473379916
Iteration 400: Loss = -12270.712509221796
Iteration 500: Loss = -12230.426532913381
Iteration 600: Loss = -12105.79563189578
Iteration 700: Loss = -12088.260984331342
Iteration 800: Loss = -12026.018245553412
Iteration 900: Loss = -12014.829354341211
Iteration 1000: Loss = -12014.678809973764
Iteration 1100: Loss = -12014.628376995024
Iteration 1200: Loss = -12014.595508957593
Iteration 1300: Loss = -12014.572305607422
Iteration 1400: Loss = -12014.555007517305
Iteration 1500: Loss = -12014.541653732833
Iteration 1600: Loss = -12014.53107836408
Iteration 1700: Loss = -12014.522505127
Iteration 1800: Loss = -12014.515419807294
Iteration 1900: Loss = -12014.509369687205
Iteration 2000: Loss = -12014.503829971705
Iteration 2100: Loss = -12014.496353738443
Iteration 2200: Loss = -12014.491648712097
Iteration 2300: Loss = -12014.488286779551
Iteration 2400: Loss = -12014.485297657648
Iteration 2500: Loss = -12014.482442283825
Iteration 2600: Loss = -12014.479396628742
Iteration 2700: Loss = -12014.473214132264
Iteration 2800: Loss = -12008.046753185072
Iteration 2900: Loss = -12008.04559991944
Iteration 3000: Loss = -12008.04219858061
Iteration 3100: Loss = -12008.143765884164
1
Iteration 3200: Loss = -12008.039315482602
Iteration 3300: Loss = -12008.038017582388
Iteration 3400: Loss = -12008.121769113044
1
Iteration 3500: Loss = -12008.035698568949
Iteration 3600: Loss = -12008.037988366079
1
Iteration 3700: Loss = -12008.034244387716
Iteration 3800: Loss = -12008.008982222576
Iteration 3900: Loss = -12004.14241255183
Iteration 4000: Loss = -12004.131185611197
Iteration 4100: Loss = -12004.130001279755
Iteration 4200: Loss = -12004.129234844959
Iteration 4300: Loss = -12004.131282585566
1
Iteration 4400: Loss = -12004.127350565754
Iteration 4500: Loss = -12004.126150339132
Iteration 4600: Loss = -12004.125418019823
Iteration 4700: Loss = -12004.13301682772
1
Iteration 4800: Loss = -12004.124447579883
Iteration 4900: Loss = -12004.128138576323
1
Iteration 5000: Loss = -12004.123798413497
Iteration 5100: Loss = -12004.15346090322
1
Iteration 5200: Loss = -12004.12338292947
Iteration 5300: Loss = -12004.122960114542
Iteration 5400: Loss = -12004.1277781747
1
Iteration 5500: Loss = -12004.122170608487
Iteration 5600: Loss = -12004.121466007013
Iteration 5700: Loss = -12004.11443425018
Iteration 5800: Loss = -12000.716088556828
Iteration 5900: Loss = -12000.715501272189
Iteration 6000: Loss = -12000.719667912337
1
Iteration 6100: Loss = -12000.717018713163
2
Iteration 6200: Loss = -12000.72618145553
3
Iteration 6300: Loss = -12000.709288918204
Iteration 6400: Loss = -12000.709565539628
1
Iteration 6500: Loss = -12000.70905503088
Iteration 6600: Loss = -12000.708718005484
Iteration 6700: Loss = -12000.876554915954
1
Iteration 6800: Loss = -12000.708777222768
2
Iteration 6900: Loss = -12000.783208144563
3
Iteration 7000: Loss = -12000.708335168672
Iteration 7100: Loss = -12000.727970015721
1
Iteration 7200: Loss = -12000.708214662554
Iteration 7300: Loss = -12000.717374328924
1
Iteration 7400: Loss = -12000.712365984076
2
Iteration 7500: Loss = -12000.708672737004
3
Iteration 7600: Loss = -12000.708606818815
4
Iteration 7700: Loss = -12000.814773899545
5
Iteration 7800: Loss = -12000.709528519481
6
Iteration 7900: Loss = -12000.718491156062
7
Iteration 8000: Loss = -12000.707773696075
Iteration 8100: Loss = -12000.70794543546
1
Iteration 8200: Loss = -12000.710201004636
2
Iteration 8300: Loss = -12000.754220135012
3
Iteration 8400: Loss = -12000.70651056378
Iteration 8500: Loss = -12000.680291383931
Iteration 8600: Loss = -12000.725732299788
1
Iteration 8700: Loss = -12000.679658101257
Iteration 8800: Loss = -12000.68124776342
1
Iteration 8900: Loss = -12000.698205717328
2
Iteration 9000: Loss = -12000.651567458897
Iteration 9100: Loss = -12000.655652979895
1
Iteration 9200: Loss = -12000.658738566815
2
Iteration 9300: Loss = -12000.652170322548
3
Iteration 9400: Loss = -12000.653273716416
4
Iteration 9500: Loss = -12000.651882919023
5
Iteration 9600: Loss = -12000.658797285969
6
Iteration 9700: Loss = -12000.650513378172
Iteration 9800: Loss = -12000.61924482906
Iteration 9900: Loss = -12000.622961209503
1
Iteration 10000: Loss = -12000.626604185434
2
Iteration 10100: Loss = -12000.618901011623
Iteration 10200: Loss = -12000.638902006089
1
Iteration 10300: Loss = -12000.618783071497
Iteration 10400: Loss = -12000.624836018025
1
Iteration 10500: Loss = -12000.61969161515
2
Iteration 10600: Loss = -12000.621669326054
3
Iteration 10700: Loss = -12000.626671374219
4
Iteration 10800: Loss = -11997.924138352979
Iteration 10900: Loss = -11997.95116694499
1
Iteration 11000: Loss = -11997.932787993434
2
Iteration 11100: Loss = -11997.923062732692
Iteration 11200: Loss = -11998.002155256128
1
Iteration 11300: Loss = -11997.945824144155
2
Iteration 11400: Loss = -11998.013899036103
3
Iteration 11500: Loss = -11997.963838243675
4
Iteration 11600: Loss = -11997.986275573632
5
Iteration 11700: Loss = -11997.927350522763
6
Iteration 11800: Loss = -11997.926180868366
7
Iteration 11900: Loss = -11997.925652301172
8
Iteration 12000: Loss = -11997.996419287567
9
Iteration 12100: Loss = -11997.924378459069
10
Stopping early at iteration 12100 due to no improvement.
tensor([[ -7.8255,   6.2391],
        [  6.7974,  -8.6589],
        [ -8.0323,   6.0515],
        [  6.9048,  -8.4261],
        [ -8.4399,   6.8782],
        [  4.2778,  -5.8470],
        [ -7.9044,   6.2674],
        [ -5.8620,   3.7124],
        [ -6.9133,   5.3487],
        [ -4.1948,   2.7112],
        [  6.4939,  -8.8295],
        [  7.2729, -10.1182],
        [  6.5518,  -8.2874],
        [  1.2943,  -2.7399],
        [  4.0302,  -5.4722],
        [  8.0169,  -9.5043],
        [ -5.5333,   3.8875],
        [ -6.7563,   4.1756],
        [  5.7618, -10.3770],
        [ -6.3995,   4.8907],
        [ -6.3338,   4.6326],
        [  6.9581,  -8.4057],
        [  6.5081,  -8.0620],
        [  3.0716,  -5.1969],
        [  5.3608,  -6.8426],
        [  7.2953,  -9.6527],
        [ -9.1588,   6.8397],
        [  5.8858,  -9.0645],
        [  7.0586,  -8.4486],
        [  6.2888,  -7.9036],
        [  7.3156,  -8.8653],
        [  2.4821,  -4.7287],
        [  5.4761,  -8.0474],
        [  5.3834,  -6.7805],
        [  2.0612,  -6.5360],
        [  7.3302,  -8.7467],
        [ -7.7554,   6.3038],
        [  7.1989,  -8.5938],
        [  7.6042, -11.4817],
        [  2.4184,  -4.4745],
        [  5.4577,  -7.6975],
        [  6.1030,  -7.5390],
        [ -7.7898,   6.2674],
        [  7.6885,  -9.0869],
        [ -8.2708,   3.9382],
        [ -8.7989,   5.9633],
        [  2.6329,  -4.1770],
        [ -2.3399,   0.9165],
        [  4.9828,  -7.2631],
        [ -8.1683,   5.7214],
        [ -4.1212,   2.6577],
        [  7.0229,  -8.4338],
        [ -4.9799,   3.5565],
        [ -8.1207,   6.2290],
        [ -2.2781,   0.8904],
        [ -8.5682,   6.4808],
        [ -7.5539,   5.9743],
        [ -3.8066,   2.1596],
        [ -7.1269,   5.5874],
        [  5.7465,  -8.1289],
        [  7.6629,  -9.1821],
        [  7.0929,  -8.4959],
        [  4.6247,  -6.1613],
        [  3.4006,  -5.2364],
        [  7.1250,  -9.8410],
        [  7.6129,  -9.0058],
        [ -8.7249,   7.3315],
        [  5.7585,  -9.5039],
        [  5.4447,  -6.8509],
        [  6.7852,  -8.2881],
        [  2.6069,  -7.0297],
        [-10.2159,   7.0000],
        [ -6.3511,   4.5494],
        [  3.5797,  -5.1379],
        [ -8.7546,   6.4166],
        [ -7.0818,   2.4666],
        [ -7.5588,   5.9582],
        [  7.0802,  -8.8107],
        [  6.0199,  -7.4384],
        [  7.6856,  -9.4258],
        [ -8.3785,   6.7751],
        [ -8.3315,   3.7163],
        [ -7.1116,   5.5010],
        [  3.3715,  -4.7598],
        [  3.1191,  -5.0006],
        [  6.0580,  -7.5788],
        [ -4.5407,   2.2863],
        [  6.4509,  -9.4809],
        [ -7.8716,   5.9679],
        [  6.9718,  -8.4346],
        [ -7.5874,   5.8751],
        [ -6.3062,   3.1289],
        [ -8.0714,   6.4439],
        [ -4.7675,   3.3132],
        [ -6.9715,   5.1473],
        [  7.2008,  -9.0793],
        [  8.3338,  -9.7530],
        [ -8.4703,   6.5523],
        [  6.7581,  -9.1127],
        [  3.3654,  -4.7681]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.3667, 0.6333],
        [0.5877, 0.4123]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5706, 0.4294], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3050, 0.1009],
         [0.9578, 0.3023]],

        [[0.1378, 0.0998],
         [0.8053, 0.0169]],

        [[0.7919, 0.0981],
         [0.1921, 0.0362]],

        [[0.6558, 0.1010],
         [0.0953, 0.5542]],

        [[0.0854, 0.1108],
         [0.2773, 0.3441]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 2
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.039699141508821735
Average Adjusted Rand Index: 0.992
Iteration 0: Loss = -20839.160707404826
Iteration 10: Loss = -12473.971543645199
Iteration 20: Loss = -12473.315025645872
Iteration 30: Loss = -12472.981436646935
Iteration 40: Loss = -12472.848239532083
Iteration 50: Loss = -12472.792620808239
Iteration 60: Loss = -12472.76801721071
Iteration 70: Loss = -12472.756607244257
Iteration 80: Loss = -12472.750953669325
Iteration 90: Loss = -12472.748022717624
Iteration 100: Loss = -12472.746402688488
Iteration 110: Loss = -12472.745455135737
Iteration 120: Loss = -12472.744916559795
Iteration 130: Loss = -12472.744582517678
Iteration 140: Loss = -12472.74437341967
Iteration 150: Loss = -12472.744215656563
Iteration 160: Loss = -12472.744169875728
Iteration 170: Loss = -12472.744141192321
Iteration 180: Loss = -12472.744060586914
Iteration 190: Loss = -12472.744051797572
Iteration 200: Loss = -12472.744011346465
Iteration 210: Loss = -12472.74402224019
1
Iteration 220: Loss = -12472.7440584409
2
Iteration 230: Loss = -12472.744037057691
3
Stopping early at iteration 229 due to no improvement.
pi: tensor([[0.9616, 0.0384],
        [0.8805, 0.1195]], dtype=torch.float64)
alpha: tensor([0.9579, 0.0421])
beta: tensor([[[0.1993, 0.2632],
         [0.2421, 0.2494]],

        [[0.6383, 0.1372],
         [0.6395, 0.3519]],

        [[0.4283, 0.2082],
         [0.5129, 0.9708]],

        [[0.6582, 0.2198],
         [0.4533, 0.2157]],

        [[0.5320, 0.2381],
         [0.1444, 0.9670]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: -0.00491035742834304
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0004811949056573946
Average Adjusted Rand Index: -0.000982071485668608
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20838.853435037596
Iteration 100: Loss = -12475.561999267004
Iteration 200: Loss = -12474.227885132153
Iteration 300: Loss = -12473.839965327767
Iteration 400: Loss = -12473.610859058921
Iteration 500: Loss = -12473.42925090282
Iteration 600: Loss = -12473.184187528503
Iteration 700: Loss = -12472.583707092734
Iteration 800: Loss = -12472.46672157586
Iteration 900: Loss = -12472.385414749651
Iteration 1000: Loss = -12472.285225444859
Iteration 1100: Loss = -12472.166415034782
Iteration 1200: Loss = -12472.055929514638
Iteration 1300: Loss = -12471.977086425693
Iteration 1400: Loss = -12471.928219032683
Iteration 1500: Loss = -12471.897056163787
Iteration 1600: Loss = -12471.874727855837
Iteration 1700: Loss = -12471.8584649055
Iteration 1800: Loss = -12471.8481313066
Iteration 1900: Loss = -12471.838553467729
Iteration 2000: Loss = -12471.828644025663
Iteration 2100: Loss = -12471.818471699446
Iteration 2200: Loss = -12471.80822722771
Iteration 2300: Loss = -12471.798372247042
Iteration 2400: Loss = -12471.789380100929
Iteration 2500: Loss = -12471.781421600033
Iteration 2600: Loss = -12471.77434386356
Iteration 2700: Loss = -12471.767868386116
Iteration 2800: Loss = -12471.760901513257
Iteration 2900: Loss = -12471.753283258091
Iteration 3000: Loss = -12471.743732205809
Iteration 3100: Loss = -12471.723660776446
Iteration 3200: Loss = -12471.651088968556
Iteration 3300: Loss = -12471.497120808719
Iteration 3400: Loss = -12471.379612357776
Iteration 3500: Loss = -12471.288752703893
Iteration 3600: Loss = -12471.221815894323
Iteration 3700: Loss = -12471.175488906003
Iteration 3800: Loss = -12471.143801373799
Iteration 3900: Loss = -12471.121818074902
Iteration 4000: Loss = -12471.106067637564
Iteration 4100: Loss = -12471.094412950302
Iteration 4200: Loss = -12471.085523452608
Iteration 4300: Loss = -12471.07856659365
Iteration 4400: Loss = -12471.072986959334
Iteration 4500: Loss = -12471.068432762637
Iteration 4600: Loss = -12471.064614036122
Iteration 4700: Loss = -12471.061437255106
Iteration 4800: Loss = -12471.058716746109
Iteration 4900: Loss = -12471.056343673272
Iteration 5000: Loss = -12471.05434887985
Iteration 5100: Loss = -12471.056887373074
1
Iteration 5200: Loss = -12471.050981362605
Iteration 5300: Loss = -12471.049599782667
Iteration 5400: Loss = -12471.049618309702
1
Iteration 5500: Loss = -12471.047204927016
Iteration 5600: Loss = -12471.046220356668
Iteration 5700: Loss = -12471.045362511783
Iteration 5800: Loss = -12471.04445245751
Iteration 5900: Loss = -12471.060572495517
1
Iteration 6000: Loss = -12471.043021324194
Iteration 6100: Loss = -12471.042383640062
Iteration 6200: Loss = -12471.041932023096
Iteration 6300: Loss = -12471.041201463051
Iteration 6400: Loss = -12471.055039358169
1
Iteration 6500: Loss = -12471.040192323986
Iteration 6600: Loss = -12471.039717893138
Iteration 6700: Loss = -12471.039603399078
Iteration 6800: Loss = -12471.038895231608
Iteration 6900: Loss = -12471.038820194057
Iteration 7000: Loss = -12471.038167143552
Iteration 7100: Loss = -12471.037963691118
Iteration 7200: Loss = -12471.063515134507
1
Iteration 7300: Loss = -12471.03731044415
Iteration 7400: Loss = -12471.037431922765
1
Iteration 7500: Loss = -12471.036849750646
Iteration 7600: Loss = -12471.036597213464
Iteration 7700: Loss = -12471.036415043363
Iteration 7800: Loss = -12471.036201911778
Iteration 7900: Loss = -12471.036028162755
Iteration 8000: Loss = -12471.068065203766
1
Iteration 8100: Loss = -12471.035698280344
Iteration 8200: Loss = -12471.035575810896
Iteration 8300: Loss = -12471.03955139475
1
Iteration 8400: Loss = -12471.035325350322
Iteration 8500: Loss = -12471.0351872047
Iteration 8600: Loss = -12471.05992634702
1
Iteration 8700: Loss = -12471.034972447625
Iteration 8800: Loss = -12471.034851512504
Iteration 8900: Loss = -12471.034861819528
1
Iteration 9000: Loss = -12471.034655132562
Iteration 9100: Loss = -12471.034559478443
Iteration 9200: Loss = -12471.035181256471
1
Iteration 9300: Loss = -12471.03438853413
Iteration 9400: Loss = -12471.034265225828
Iteration 9500: Loss = -12471.670512976327
1
Iteration 9600: Loss = -12471.034095998057
Iteration 9700: Loss = -12471.034033712629
Iteration 9800: Loss = -12471.033995906238
Iteration 9900: Loss = -12471.033939698706
Iteration 10000: Loss = -12471.033894517468
Iteration 10100: Loss = -12471.034056875924
1
Iteration 10200: Loss = -12471.033756399125
Iteration 10300: Loss = -12471.033679798911
Iteration 10400: Loss = -12471.044625711298
1
Iteration 10500: Loss = -12471.033611037914
Iteration 10600: Loss = -12471.033524006356
Iteration 10700: Loss = -12471.21086761109
1
Iteration 10800: Loss = -12471.033256263512
Iteration 10900: Loss = -12471.033215052574
Iteration 11000: Loss = -12471.03317559671
Iteration 11100: Loss = -12471.033890696192
1
Iteration 11200: Loss = -12471.033158866754
Iteration 11300: Loss = -12471.033110978842
Iteration 11400: Loss = -12471.034777553132
1
Iteration 11500: Loss = -12471.033048998217
Iteration 11600: Loss = -12471.033035292714
Iteration 11700: Loss = -12471.03300138757
Iteration 11800: Loss = -12471.032994081217
Iteration 11900: Loss = -12471.032927449885
Iteration 12000: Loss = -12471.032929281719
1
Iteration 12100: Loss = -12471.032919189727
Iteration 12200: Loss = -12471.032906424007
Iteration 12300: Loss = -12471.032876723819
Iteration 12400: Loss = -12471.032972224224
1
Iteration 12500: Loss = -12471.032855372338
Iteration 12600: Loss = -12471.047306249366
1
Iteration 12700: Loss = -12471.032799142631
Iteration 12800: Loss = -12471.03278232158
Iteration 12900: Loss = -12471.055214530405
1
Iteration 13000: Loss = -12471.032799522492
2
Iteration 13100: Loss = -12471.032789692303
3
Iteration 13200: Loss = -12471.04452187404
4
Iteration 13300: Loss = -12471.0327647414
Iteration 13400: Loss = -12471.032732038268
Iteration 13500: Loss = -12471.044975477571
1
Iteration 13600: Loss = -12471.032753175494
2
Iteration 13700: Loss = -12471.032721811775
Iteration 13800: Loss = -12471.243060822435
1
Iteration 13900: Loss = -12471.032451869509
Iteration 14000: Loss = -12471.032449719702
Iteration 14100: Loss = -12471.283316565556
1
Iteration 14200: Loss = -12471.032439803004
Iteration 14300: Loss = -12471.032413755522
Iteration 14400: Loss = -12471.032374955894
Iteration 14500: Loss = -12471.032489888546
1
Iteration 14600: Loss = -12471.03237130245
Iteration 14700: Loss = -12471.032369705015
Iteration 14800: Loss = -12471.032767582863
1
Iteration 14900: Loss = -12471.03244960994
2
Iteration 15000: Loss = -12471.032381954708
3
Iteration 15100: Loss = -12471.032350792446
Iteration 15200: Loss = -12471.032499956866
1
Iteration 15300: Loss = -12471.03233721237
Iteration 15400: Loss = -12471.0323447889
1
Iteration 15500: Loss = -12471.039510630208
2
Iteration 15600: Loss = -12471.032349679137
3
Iteration 15700: Loss = -12471.032337970542
4
Iteration 15800: Loss = -12471.034799272895
5
Iteration 15900: Loss = -12471.032316346947
Iteration 16000: Loss = -12471.03228271749
Iteration 16100: Loss = -12471.032405499604
1
Iteration 16200: Loss = -12471.03233092406
2
Iteration 16300: Loss = -12471.03232154681
3
Iteration 16400: Loss = -12471.035164654168
4
Iteration 16500: Loss = -12471.03229109863
5
Iteration 16600: Loss = -12471.032306050683
6
Iteration 16700: Loss = -12471.04670321387
7
Iteration 16800: Loss = -12471.032308220252
8
Iteration 16900: Loss = -12471.032263750036
Iteration 17000: Loss = -12471.126584953945
1
Iteration 17100: Loss = -12471.032348833945
2
Iteration 17200: Loss = -12471.032458460564
3
Iteration 17300: Loss = -12471.032411474524
4
Iteration 17400: Loss = -12471.032332804585
5
Iteration 17500: Loss = -12471.037865543785
6
Iteration 17600: Loss = -12471.032321389308
7
Iteration 17700: Loss = -12471.032382827376
8
Iteration 17800: Loss = -12471.03234901158
9
Iteration 17900: Loss = -12471.032257506615
Iteration 18000: Loss = -12471.567418163831
1
Iteration 18100: Loss = -12471.032298707309
2
Iteration 18200: Loss = -12471.032286899006
3
Iteration 18300: Loss = -12471.102502146026
4
Iteration 18400: Loss = -12471.032285060031
5
Iteration 18500: Loss = -12471.032275727659
6
Iteration 18600: Loss = -12471.036427845376
7
Iteration 18700: Loss = -12471.032311000115
8
Iteration 18800: Loss = -12471.032282010336
9
Iteration 18900: Loss = -12471.050448558826
10
Stopping early at iteration 18900 due to no improvement.
tensor([[  2.3372,  -4.1354],
        [  2.4526,  -4.2524],
        [  3.1851,  -4.8399],
        [  2.7234,  -5.1657],
        [  2.0592,  -4.2403],
        [ -1.6230,   0.2163],
        [  0.8455,  -4.0252],
        [  1.7582,  -3.1451],
        [  1.2992,  -4.2176],
        [  3.3615,  -6.2047],
        [  1.0453,  -2.5258],
        [  1.2372,  -3.5365],
        [  1.3343,  -5.2986],
        [  3.0575,  -4.6634],
        [  0.6908,  -3.0041],
        [  0.8474,  -4.3651],
        [  1.8949,  -3.9954],
        [  1.5455,  -3.3124],
        [  1.4174,  -3.1698],
        [  2.5274,  -6.0678],
        [  4.7693,  -7.1595],
        [  3.6173,  -5.3232],
        [  0.0554,  -4.6706],
        [  1.4850,  -2.8730],
        [  3.8987,  -5.2880],
        [ -1.9598,   0.1206],
        [  2.1749,  -3.6815],
        [  1.9374,  -3.8146],
        [ -1.0914,  -0.5032],
        [ -0.7814,  -0.7563],
        [  1.1889,  -2.7498],
        [  1.5707,  -5.4396],
        [  2.9335,  -4.3228],
        [  3.2108,  -4.6026],
        [  3.1164,  -4.7000],
        [ -0.8044,  -0.7670],
        [  3.2500,  -5.0614],
        [  2.0011,  -3.8438],
        [  2.2961,  -3.6977],
        [  2.1318,  -4.6745],
        [  6.3108, -10.9261],
        [  0.6733,  -2.0694],
        [  1.7960,  -3.5955],
        [  2.0807,  -3.5656],
        [  1.5350,  -3.1966],
        [  1.5976,  -3.1256],
        [  2.3390,  -3.7765],
        [  2.6593,  -4.4204],
        [ -0.1159,  -4.3692],
        [  3.7507,  -5.4066],
        [  3.8758,  -6.2679],
        [  0.8727,  -2.4399],
        [  2.8808,  -5.2990],
        [  1.4258,  -2.8731],
        [  3.3497,  -4.7789],
        [  0.8464,  -2.2864],
        [ -0.5347,  -4.0744],
        [  4.2685,  -6.6843],
        [  1.7856,  -3.4022],
        [  4.3100,  -6.8055],
        [ -1.9225,  -2.6927],
        [  1.9154,  -3.3395],
        [  0.9702,  -2.4343],
        [  2.3114,  -3.7027],
        [  0.7115,  -2.6318],
        [  2.9108,  -4.4234],
        [  3.3109,  -5.6799],
        [ -0.1572,  -2.3724],
        [  3.1061,  -4.4926],
        [  6.8983,  -8.8309],
        [  1.7613,  -3.6221],
        [  1.5614,  -2.9982],
        [  1.3809,  -3.0262],
        [  2.2399,  -4.0659],
        [  2.3957,  -3.9778],
        [  0.7056,  -4.6828],
        [  1.4922,  -3.2225],
        [  0.3325,  -2.2591],
        [  2.2845,  -3.7649],
        [ -0.7909,  -1.4280],
        [  2.1150,  -4.8700],
        [  0.6011,  -4.2304],
        [  3.1613,  -4.5571],
        [  4.1539,  -5.7700],
        [  4.3015,  -6.2795],
        [  2.2847,  -3.8829],
        [  0.0478,  -1.8689],
        [  3.2090,  -4.5960],
        [  1.4043,  -2.8653],
        [  1.1021,  -4.0183],
        [  3.2820,  -5.4569],
        [  0.8370,  -2.2742],
        [  2.4688,  -4.5742],
        [  2.3395,  -5.5494],
        [ -2.5559,   0.6386],
        [  0.1789,  -1.6394],
        [  0.3783,  -2.7675],
        [  2.6401,  -4.1463],
        [  1.7852,  -3.2060],
        [  2.8873,  -4.7858]], dtype=torch.float64, requires_grad=True)
pi: tensor([[1.0000e+00, 1.9247e-07],
        [1.8509e-06, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9388, 0.0612], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2038, 0.2637],
         [0.2421, 0.2064]],

        [[0.6383, 0.1287],
         [0.6395, 0.3519]],

        [[0.4283, 0.2084],
         [0.5129, 0.9708]],

        [[0.6582, 0.1824],
         [0.4533, 0.2157]],

        [[0.5320, 0.1970],
         [0.1444, 0.9670]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.01405713152211082
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.0008263300397341679
time is 2
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.002193311857154732
time is 3
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0010149900615556472
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.009696969696969697
Global Adjusted Rand Index: 0.00047552766424858366
Average Adjusted Rand Index: -0.0012729627320948755
Iteration 0: Loss = -33322.765854632584
Iteration 10: Loss = -12473.50218082545
Iteration 20: Loss = -12473.098257826017
Iteration 30: Loss = -12473.005506961776
Iteration 40: Loss = -12472.924925340187
Iteration 50: Loss = -12472.813420875791
Iteration 60: Loss = -12472.764538013216
Iteration 70: Loss = -12472.752721575584
Iteration 80: Loss = -12472.748845904167
Iteration 90: Loss = -12472.74689577421
Iteration 100: Loss = -12472.74580092343
Iteration 110: Loss = -12472.745181669783
Iteration 120: Loss = -12472.744753178597
Iteration 130: Loss = -12472.744485361583
Iteration 140: Loss = -12472.74433708136
Iteration 150: Loss = -12472.744198504251
Iteration 160: Loss = -12472.744160376556
Iteration 170: Loss = -12472.744070654833
Iteration 180: Loss = -12472.744040632437
Iteration 190: Loss = -12472.744107071896
1
Iteration 200: Loss = -12472.744069374347
2
Iteration 210: Loss = -12472.74404751626
3
Stopping early at iteration 209 due to no improvement.
pi: tensor([[0.9616, 0.0384],
        [0.8805, 0.1195]], dtype=torch.float64)
alpha: tensor([0.9579, 0.0421])
beta: tensor([[[0.1993, 0.2632],
         [0.6227, 0.2494]],

        [[0.3706, 0.1372],
         [0.2024, 0.4415]],

        [[0.3840, 0.2082],
         [0.6395, 0.7741]],

        [[0.9916, 0.2198],
         [0.5361, 0.0591]],

        [[0.8579, 0.2381],
         [0.7110, 0.5066]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: -0.00491035742834304
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0004811949056573946
Average Adjusted Rand Index: -0.000982071485668608
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -33322.62998737317
Iteration 100: Loss = -12558.316172945519
Iteration 200: Loss = -12510.874851461978
Iteration 300: Loss = -12490.360381180135
Iteration 400: Loss = -12481.937371669752
Iteration 500: Loss = -12478.029546561025
Iteration 600: Loss = -12476.897546582923
Iteration 700: Loss = -12476.223490032799
Iteration 800: Loss = -12475.771179744865
Iteration 900: Loss = -12475.451463258149
Iteration 1000: Loss = -12475.234647920402
Iteration 1100: Loss = -12475.081719829883
Iteration 1200: Loss = -12474.969451858113
Iteration 1300: Loss = -12474.88157004407
Iteration 1400: Loss = -12474.808114388446
Iteration 1500: Loss = -12474.741280551254
Iteration 1600: Loss = -12474.672832607874
Iteration 1700: Loss = -12474.60164138505
Iteration 1800: Loss = -12474.531618442463
Iteration 1900: Loss = -12474.443854769828
Iteration 2000: Loss = -12474.35547764065
Iteration 2100: Loss = -12474.290664536478
Iteration 2200: Loss = -12474.197065297438
Iteration 2300: Loss = -12473.964423052483
Iteration 2400: Loss = -12473.699350944244
Iteration 2500: Loss = -12473.567532348294
Iteration 2600: Loss = -12473.48560619307
Iteration 2700: Loss = -12473.291902194307
Iteration 2800: Loss = -12472.8925679065
Iteration 2900: Loss = -12472.782325950651
Iteration 3000: Loss = -12472.69561495951
Iteration 3100: Loss = -12472.599164396779
Iteration 3200: Loss = -12472.49933386938
Iteration 3300: Loss = -12472.403004981688
Iteration 3400: Loss = -12472.310577245797
Iteration 3500: Loss = -12472.228926156065
Iteration 3600: Loss = -12472.15334050308
Iteration 3700: Loss = -12472.060788855986
Iteration 3800: Loss = -12472.004962945288
Iteration 3900: Loss = -12471.972683711598
Iteration 4000: Loss = -12471.955487580746
Iteration 4100: Loss = -12471.94416211127
Iteration 4200: Loss = -12471.932125891759
Iteration 4300: Loss = -12471.90167619025
Iteration 4400: Loss = -12471.854699627767
Iteration 4500: Loss = -12471.84927469016
Iteration 4600: Loss = -12471.843239662825
Iteration 4700: Loss = -12471.83573284004
Iteration 4800: Loss = -12471.825336373679
Iteration 4900: Loss = -12471.808428335906
Iteration 5000: Loss = -12471.779311283026
Iteration 5100: Loss = -12471.711176735624
Iteration 5200: Loss = -12471.556159840362
Iteration 5300: Loss = -12471.33456675554
Iteration 5400: Loss = -12471.108153591405
Iteration 5500: Loss = -12470.914754101463
Iteration 5600: Loss = -12470.824370866865
Iteration 5700: Loss = -12470.651039120277
Iteration 5800: Loss = -12470.566995208135
Iteration 5900: Loss = -12470.52046788606
Iteration 6000: Loss = -12470.456049992794
Iteration 6100: Loss = -12470.419393276188
Iteration 6200: Loss = -12470.390055648893
Iteration 6300: Loss = -12470.375920637909
Iteration 6400: Loss = -12470.346907657373
Iteration 6500: Loss = -12470.331891290794
Iteration 6600: Loss = -12470.342399159694
1
Iteration 6700: Loss = -12470.309207381995
Iteration 6800: Loss = -12470.300635502334
Iteration 6900: Loss = -12470.30954236205
1
Iteration 7000: Loss = -12470.287254169396
Iteration 7100: Loss = -12470.2820235787
Iteration 7200: Loss = -12470.277535203328
Iteration 7300: Loss = -12470.273798802637
Iteration 7400: Loss = -12470.270105320287
Iteration 7500: Loss = -12470.267054513826
Iteration 7600: Loss = -12470.289761906623
1
Iteration 7700: Loss = -12470.262301029177
Iteration 7800: Loss = -12470.260351155937
Iteration 7900: Loss = -12470.25873785562
Iteration 8000: Loss = -12470.257303315397
Iteration 8100: Loss = -12470.255850025387
Iteration 8200: Loss = -12470.254730753475
Iteration 8300: Loss = -12470.291613191524
1
Iteration 8400: Loss = -12470.252708291515
Iteration 8500: Loss = -12470.251828478722
Iteration 8600: Loss = -12470.250812862165
Iteration 8700: Loss = -12470.249210379847
Iteration 8800: Loss = -12470.242022393513
Iteration 8900: Loss = -12470.240435911697
Iteration 9000: Loss = -12470.23881524849
Iteration 9100: Loss = -12470.236588863816
Iteration 9200: Loss = -12470.23375791486
Iteration 9300: Loss = -12470.232029319652
Iteration 9400: Loss = -12470.243368335332
1
Iteration 9500: Loss = -12470.230461365527
Iteration 9600: Loss = -12470.228490049305
Iteration 9700: Loss = -12470.228243267202
Iteration 9800: Loss = -12470.227578207698
Iteration 9900: Loss = -12470.227376554829
Iteration 10000: Loss = -12470.227175764072
Iteration 10100: Loss = -12470.22781291423
1
Iteration 10200: Loss = -12470.226825769269
Iteration 10300: Loss = -12470.226695823792
Iteration 10400: Loss = -12470.507207779712
1
Iteration 10500: Loss = -12470.226181933149
Iteration 10600: Loss = -12470.226059229379
Iteration 10700: Loss = -12470.225945835862
Iteration 10800: Loss = -12470.226013964826
1
Iteration 10900: Loss = -12470.225695212688
Iteration 11000: Loss = -12470.225653683881
Iteration 11100: Loss = -12470.375872760227
1
Iteration 11200: Loss = -12470.225524499134
Iteration 11300: Loss = -12470.225452680359
Iteration 11400: Loss = -12470.225333429553
Iteration 11500: Loss = -12470.283446160574
1
Iteration 11600: Loss = -12470.225345633184
2
Iteration 11700: Loss = -12470.225286215758
Iteration 11800: Loss = -12470.225209328573
Iteration 11900: Loss = -12470.226584990769
1
Iteration 12000: Loss = -12470.225159792697
Iteration 12100: Loss = -12470.225136232057
Iteration 12200: Loss = -12470.22511683322
Iteration 12300: Loss = -12470.225115577807
Iteration 12400: Loss = -12470.225058375612
Iteration 12500: Loss = -12470.225040267369
Iteration 12600: Loss = -12470.343971907088
1
Iteration 12700: Loss = -12470.224985647877
Iteration 12800: Loss = -12470.224959276306
Iteration 12900: Loss = -12470.224942815297
Iteration 13000: Loss = -12470.227013325564
1
Iteration 13100: Loss = -12470.224862469375
Iteration 13200: Loss = -12470.224825287722
Iteration 13300: Loss = -12470.350838739036
1
Iteration 13400: Loss = -12470.224854612574
2
Iteration 13500: Loss = -12470.224800061316
Iteration 13600: Loss = -12470.355886860309
1
Iteration 13700: Loss = -12470.22478978155
Iteration 13800: Loss = -12470.22493583749
1
Iteration 13900: Loss = -12470.224773339402
Iteration 14000: Loss = -12470.225026896553
1
Iteration 14100: Loss = -12470.224749341176
Iteration 14200: Loss = -12470.227252550147
1
Iteration 14300: Loss = -12470.22473121724
Iteration 14400: Loss = -12470.225642339381
1
Iteration 14500: Loss = -12470.224729747571
Iteration 14600: Loss = -12470.224739079424
1
Iteration 14700: Loss = -12470.736828899717
2
Iteration 14800: Loss = -12470.22470555959
Iteration 14900: Loss = -12470.224713795304
1
Iteration 15000: Loss = -12470.297967376006
2
Iteration 15100: Loss = -12470.224693407987
Iteration 15200: Loss = -12470.22466727073
Iteration 15300: Loss = -12470.232735071655
1
Iteration 15400: Loss = -12470.224685559338
2
Iteration 15500: Loss = -12470.224639677073
Iteration 15600: Loss = -12470.227639655466
1
Iteration 15700: Loss = -12470.224679301806
2
Iteration 15800: Loss = -12470.224823925182
3
Iteration 15900: Loss = -12470.225216361734
4
Iteration 16000: Loss = -12470.224641262157
5
Iteration 16100: Loss = -12470.226611682547
6
Iteration 16200: Loss = -12470.224661158036
7
Iteration 16300: Loss = -12470.224666178696
8
Iteration 16400: Loss = -12470.224701017596
9
Iteration 16500: Loss = -12470.225188256136
10
Stopping early at iteration 16500 due to no improvement.
tensor([[  5.8273,  -8.6375],
        [  6.0512,  -8.3892],
        [  6.5983,  -9.2376],
        [  4.1962,  -5.5849],
        [  7.8320, -10.2736],
        [  6.5705,  -8.0397],
        [  7.4783, -10.8782],
        [  7.6828,  -9.5818],
        [  7.3886,  -9.0769],
        [  7.5309,  -9.0695],
        [  7.5996,  -9.8929],
        [  7.8572,  -9.5888],
        [  6.7374, -10.1221],
        [  7.7261,  -9.1190],
        [  5.5074,  -7.3244],
        [  3.3374,  -4.7850],
        [  7.7047,  -9.1377],
        [  7.8310,  -9.2328],
        [  7.4494,  -8.8616],
        [  7.8028, -10.4304],
        [  6.2751,  -8.9469],
        [ -1.7172,   0.3296],
        [  7.9047,  -9.2912],
        [  7.0502,  -8.9231],
        [  6.5905,  -8.2124],
        [  6.4781,  -7.9563],
        [  7.6565,  -9.0747],
        [  6.5600,  -8.0224],
        [  6.9074,  -8.7616],
        [  6.6082,  -8.2461],
        [  4.3946,  -6.2677],
        [  7.2976,  -8.7244],
        [  7.1256,  -8.8031],
        [  5.9252,  -8.9470],
        [  6.9862,  -8.7735],
        [  5.6206,  -7.2573],
        [  7.5334,  -8.9669],
        [  6.4374,  -8.2318],
        [  6.4177,  -9.6511],
        [  7.3283,  -8.7402],
        [  6.1089,  -9.7154],
        [  3.8094,  -5.2066],
        [  4.0565,  -6.3926],
        [  6.1609,  -8.0133],
        [  7.8704, -10.6524],
        [  6.7686,  -8.2291],
        [  6.6131,  -9.7897],
        [  7.2960,  -8.7835],
        [  6.7230,  -9.2060],
        [  7.5397,  -9.0126],
        [  7.8440,  -9.3547],
        [  7.4069,  -8.7973],
        [  4.3322,  -5.8631],
        [  7.7940,  -9.8830],
        [  3.8726,  -5.4143],
        [  7.4347,  -8.8420],
        [  7.8034,  -9.2314],
        [  5.9851,  -7.4041],
        [  6.4655,  -8.1782],
        [  7.0275,  -8.4685],
        [  4.2126,  -8.8278],
        [  6.6044,  -8.9453],
        [  6.7253,  -9.5136],
        [  6.0095,  -8.2174],
        [  7.2685,  -8.6548],
        [  4.5167,  -5.9248],
        [  7.6520, -10.2083],
        [  7.5927,  -9.0038],
        [  6.9688,  -8.3624],
        [  6.7557,  -8.2222],
        [  7.5931,  -9.0476],
        [  7.6253, -12.2405],
        [  7.4048,  -8.7916],
        [  5.1893,  -6.6385],
        [  6.7203,  -8.1863],
        [  7.0565,  -8.5935],
        [  6.9272,  -8.3533],
        [  7.5021,  -9.0338],
        [  6.7707,  -9.5146],
        [ -2.6144,   0.1988],
        [  6.8620,  -8.2722],
        [  6.0869, -10.2444],
        [  7.9114, -10.4967],
        [  7.3058,  -8.7273],
        [  7.3490,  -8.7417],
        [  5.0658,  -6.7616],
        [  4.5945,  -6.1614],
        [  6.7489,  -8.2092],
        [  7.1119,  -9.6967],
        [  6.1229,  -9.5539],
        [  7.1106,  -8.6967],
        [  6.7663,  -8.1942],
        [  7.8350, -10.3696],
        [  5.6678, -10.2830],
        [ -3.7565,   2.3298],
        [ -3.9905,  -0.3828],
        [  6.3120,  -9.4117],
        [  7.7716,  -9.3456],
        [ -3.3609,   1.0552],
        [  7.9166, -10.5667]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.9972, 0.0028],
        [0.8238, 0.1762]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9521, 0.0479], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2015, 0.2757],
         [0.6227, 0.9952]],

        [[0.3706, 0.1005],
         [0.2024, 0.4415]],

        [[0.3840, 0.1691],
         [0.6395, 0.7741]],

        [[0.9916, 0.2400],
         [0.5361, 0.0591]],

        [[0.8579, 0.2962],
         [0.7110, 0.5066]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.011562904917628186
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.00776683106263838
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00038155393280398057
Average Adjusted Rand Index: -0.0007592147709979612
11972.837437852699
new:  [0.9681922444705638, 0.039699141508821735, 0.00047552766424858366, -0.00038155393280398057] [0.9683192678287906, 0.992, -0.0012729627320948755, -0.0007592147709979612] [11994.959002859314, 11997.924378459069, 12471.050448558826, 12470.225188256136]
prior:  [-0.0004811949056573946, -0.0004811949056573946, -0.0004811949056573946, -0.0004811949056573946] [-0.000982071485668608, -0.000982071485668608, -0.000982071485668608, -0.000982071485668608] [12472.744021994635, 12472.74252715076, 12472.744037057691, 12472.74404751626]
-----------------------------------------------------------------------------------------
This iteration is 19
True Objective function: Loss = -12037.417680168353
Iteration 0: Loss = -24365.848245472254
pi: tensor([[nan, nan],
        [nan, nan]], dtype=torch.float64)
alpha: tensor([nan, nan])
beta: tensor([[[   nan,    nan],
         [0.7410,    nan]],

        [[0.3565,    nan],
         [0.0508, 0.6336]],

        [[0.6179,    nan],
         [0.4792, 0.2061]],

        [[0.1821,    nan],
         [0.7312, 0.0235]],

        [[0.2601,    nan],
         [0.9123, 0.0391]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 64
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
tensor([[-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-0.0101, -4.6052],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101],
        [-4.6052, -0.0101]], dtype=torch.float64) tensor([[[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]],


        [[[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         ...,

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]],

         [[-0.0101, -4.6052],
          [-4.6052, -0.0101]]]], dtype=torch.float64)
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24690.894830887883
Iteration 100: Loss = -12512.978371877643
Iteration 200: Loss = -12508.930795486014
Iteration 300: Loss = -12507.529309445641
Iteration 400: Loss = -12506.925494716075
Iteration 500: Loss = -12506.636474817986
Iteration 600: Loss = -12506.470532507112
Iteration 700: Loss = -12506.361661453155
Iteration 800: Loss = -12506.284532870664
Iteration 900: Loss = -12506.227312331888
Iteration 1000: Loss = -12506.183778077057
Iteration 1100: Loss = -12506.14983135816
Iteration 1200: Loss = -12506.12234686996
Iteration 1300: Loss = -12506.098184333305
Iteration 1400: Loss = -12506.035678512408
Iteration 1500: Loss = -12505.023774577618
Iteration 1600: Loss = -12504.93847132732
Iteration 1700: Loss = -12504.865331899957
Iteration 1800: Loss = -12504.791811382347
Iteration 1900: Loss = -12504.759265859298
Iteration 2000: Loss = -12504.744602118955
Iteration 2100: Loss = -12504.735169799877
Iteration 2200: Loss = -12504.727655123375
Iteration 2300: Loss = -12504.72105376475
Iteration 2400: Loss = -12504.715255760788
Iteration 2500: Loss = -12504.710163934787
Iteration 2600: Loss = -12504.705575746983
Iteration 2700: Loss = -12504.70137958496
Iteration 2800: Loss = -12504.697622904096
Iteration 2900: Loss = -12504.694158312674
Iteration 3000: Loss = -12504.690970531214
Iteration 3100: Loss = -12504.68807646454
Iteration 3200: Loss = -12504.685400043487
Iteration 3300: Loss = -12504.682900098007
Iteration 3400: Loss = -12504.680593924448
Iteration 3500: Loss = -12504.678442641522
Iteration 3600: Loss = -12504.676445115498
Iteration 3700: Loss = -12504.67454451756
Iteration 3800: Loss = -12504.672779027114
Iteration 3900: Loss = -12504.671111785483
Iteration 4000: Loss = -12504.669528010714
Iteration 4100: Loss = -12504.667967373123
Iteration 4200: Loss = -12504.666485164555
Iteration 4300: Loss = -12504.665082679077
Iteration 4400: Loss = -12504.663706560821
Iteration 4500: Loss = -12504.662525770727
Iteration 4600: Loss = -12504.661231529219
Iteration 4700: Loss = -12504.660234814992
Iteration 4800: Loss = -12504.659077721091
Iteration 4900: Loss = -12504.658236998825
Iteration 5000: Loss = -12504.65735507547
Iteration 5100: Loss = -12504.656565348596
Iteration 5200: Loss = -12504.655909958015
Iteration 5300: Loss = -12504.655195537123
Iteration 5400: Loss = -12504.654545206784
Iteration 5500: Loss = -12504.653997179057
Iteration 5600: Loss = -12504.653434201135
Iteration 5700: Loss = -12504.652971388246
Iteration 5800: Loss = -12504.652476011694
Iteration 5900: Loss = -12504.651980783507
Iteration 6000: Loss = -12504.651627368028
Iteration 6100: Loss = -12504.651192156058
Iteration 6200: Loss = -12504.651012450346
Iteration 6300: Loss = -12504.65060885594
Iteration 6400: Loss = -12504.650152654602
Iteration 6500: Loss = -12504.65019898263
1
Iteration 6600: Loss = -12504.649497340375
Iteration 6700: Loss = -12504.649240461582
Iteration 6800: Loss = -12504.648892404683
Iteration 6900: Loss = -12504.64916609447
1
Iteration 7000: Loss = -12504.64842260842
Iteration 7100: Loss = -12504.648200891948
Iteration 7200: Loss = -12504.647956392559
Iteration 7300: Loss = -12504.64775197269
Iteration 7400: Loss = -12504.647531787361
Iteration 7500: Loss = -12504.6472798562
Iteration 7600: Loss = -12504.647169119453
Iteration 7700: Loss = -12504.646838668004
Iteration 7800: Loss = -12504.646614680329
Iteration 7900: Loss = -12504.646259977491
Iteration 8000: Loss = -12504.645806816457
Iteration 8100: Loss = -12504.644927473413
Iteration 8200: Loss = -12504.643078379508
Iteration 8300: Loss = -12504.640599537057
Iteration 8400: Loss = -12504.693987499655
1
Iteration 8500: Loss = -12504.64708994799
2
Iteration 8600: Loss = -12504.647892132687
3
Iteration 8700: Loss = -12504.854439980763
4
Iteration 8800: Loss = -12504.621807484804
Iteration 8900: Loss = -12504.570302901795
Iteration 9000: Loss = -12504.249081709992
Iteration 9100: Loss = -12504.194219469182
Iteration 9200: Loss = -12504.15562064581
Iteration 9300: Loss = -12504.158386928439
1
Iteration 9400: Loss = -12504.131573185427
Iteration 9500: Loss = -12504.105514890487
Iteration 9600: Loss = -12504.061690244109
Iteration 9700: Loss = -12504.06186457228
1
Iteration 9800: Loss = -12504.057986338166
Iteration 9900: Loss = -12504.048257203522
Iteration 10000: Loss = -12504.03595267364
Iteration 10100: Loss = -12504.01946518851
Iteration 10200: Loss = -12504.015189105507
Iteration 10300: Loss = -12504.00542840762
Iteration 10400: Loss = -12503.972319010956
Iteration 10500: Loss = -12503.953352205936
Iteration 10600: Loss = -12503.9084615441
Iteration 10700: Loss = -12503.301863669765
Iteration 10800: Loss = -12223.766748852402
Iteration 10900: Loss = -12154.855788396744
Iteration 11000: Loss = -12104.713156351174
Iteration 11100: Loss = -12104.534751780488
Iteration 11200: Loss = -12093.20772179767
Iteration 11300: Loss = -12093.17293657815
Iteration 11400: Loss = -12083.784008514103
Iteration 11500: Loss = -12083.766328303778
Iteration 11600: Loss = -12083.863374613775
1
Iteration 11700: Loss = -12081.206021629732
Iteration 11800: Loss = -12081.144615279518
Iteration 11900: Loss = -12078.233565368862
Iteration 12000: Loss = -12078.199190291942
Iteration 12100: Loss = -12078.190358356313
Iteration 12200: Loss = -12078.205440555303
1
Iteration 12300: Loss = -12078.187320079558
Iteration 12400: Loss = -12069.819513051396
Iteration 12500: Loss = -12069.825471180815
1
Iteration 12600: Loss = -12069.843143916118
2
Iteration 12700: Loss = -12069.818743684456
Iteration 12800: Loss = -12069.827514869672
1
Iteration 12900: Loss = -12069.825060909076
2
Iteration 13000: Loss = -12069.822373351626
3
Iteration 13100: Loss = -12069.813634544564
Iteration 13200: Loss = -12069.855347429857
1
Iteration 13300: Loss = -12069.791042490673
Iteration 13400: Loss = -12069.74035665492
Iteration 13500: Loss = -12069.732005882719
Iteration 13600: Loss = -12069.732459617238
1
Iteration 13700: Loss = -12069.73570121829
2
Iteration 13800: Loss = -12069.745123643057
3
Iteration 13900: Loss = -12069.790338805447
4
Iteration 14000: Loss = -12069.771139627333
5
Iteration 14100: Loss = -12069.740246578674
6
Iteration 14200: Loss = -12069.729010741821
Iteration 14300: Loss = -12069.753149178154
1
Iteration 14400: Loss = -12069.701492700551
Iteration 14500: Loss = -12069.702831894769
1
Iteration 14600: Loss = -12069.692762190032
Iteration 14700: Loss = -12069.697643896789
1
Iteration 14800: Loss = -12069.688698334408
Iteration 14900: Loss = -12069.723474698878
1
Iteration 15000: Loss = -12069.80166915622
2
Iteration 15100: Loss = -12069.702070804979
3
Iteration 15200: Loss = -12069.686430759857
Iteration 15300: Loss = -12069.692332569235
1
Iteration 15400: Loss = -12069.686653745575
2
Iteration 15500: Loss = -12069.697822957049
3
Iteration 15600: Loss = -12069.68820155086
4
Iteration 15700: Loss = -12069.635909930716
Iteration 15800: Loss = -12069.64128381499
1
Iteration 15900: Loss = -12069.634898482267
Iteration 16000: Loss = -12069.639141646257
1
Iteration 16100: Loss = -12069.63641525169
2
Iteration 16200: Loss = -12069.376511586313
Iteration 16300: Loss = -12063.685246710771
Iteration 16400: Loss = -12063.726510624283
1
Iteration 16500: Loss = -12063.770353415028
2
Iteration 16600: Loss = -12063.669434875997
Iteration 16700: Loss = -12063.674944082415
1
Iteration 16800: Loss = -12063.673530028304
2
Iteration 16900: Loss = -12063.678286250579
3
Iteration 17000: Loss = -12063.674039231133
4
Iteration 17100: Loss = -12063.689297523078
5
Iteration 17200: Loss = -12063.739178400607
6
Iteration 17300: Loss = -12063.755147983793
7
Iteration 17400: Loss = -12063.676825090995
8
Iteration 17500: Loss = -12063.668744697748
Iteration 17600: Loss = -12063.672385521364
1
Iteration 17700: Loss = -12063.668111413474
Iteration 17800: Loss = -12063.667572322203
Iteration 17900: Loss = -12063.67090893829
1
Iteration 18000: Loss = -12063.695939932095
2
Iteration 18100: Loss = -12063.668730185336
3
Iteration 18200: Loss = -12063.671940629809
4
Iteration 18300: Loss = -12063.667529387663
Iteration 18400: Loss = -12063.666435195177
Iteration 18500: Loss = -12063.661854660473
Iteration 18600: Loss = -12063.678932563604
1
Iteration 18700: Loss = -12063.746737744901
2
Iteration 18800: Loss = -12063.661978185444
3
Iteration 18900: Loss = -12063.665543069452
4
Iteration 19000: Loss = -12063.662078659614
5
Iteration 19100: Loss = -12063.659582605253
Iteration 19200: Loss = -12063.659669832658
1
Iteration 19300: Loss = -12063.659619504017
2
Iteration 19400: Loss = -12063.659933007768
3
Iteration 19500: Loss = -12063.733771127707
4
Iteration 19600: Loss = -12063.659125466791
Iteration 19700: Loss = -12063.659024468441
Iteration 19800: Loss = -12063.676422197399
1
Iteration 19900: Loss = -12063.658977000967
tensor([[-11.6877,   7.0725],
        [ -6.2210,   1.6058],
        [ -4.9927,   0.3775],
        [  3.5146,  -8.1298],
        [ -5.1189,   0.5037],
        [ -6.9371,   2.3218],
        [  5.2466,  -9.8618],
        [  3.6702,  -8.2854],
        [  4.5778,  -9.1930],
        [ -2.3114,  -2.3038],
        [  3.4896,  -8.1048],
        [ -6.3351,   1.7199],
        [  0.9983,  -5.6135],
        [  4.7261,  -9.3413],
        [ -8.1181,   3.5028],
        [ -6.8541,   2.2389],
        [  6.0841, -10.6994],
        [  2.2514,  -6.8666],
        [ -9.3542,   4.7390],
        [-10.5416,   5.9263],
        [  3.4436,  -8.0588],
        [-10.9860,   6.3708],
        [ -0.8957,  -3.7195],
        [  3.3467,  -7.9619],
        [-11.9541,   7.3388],
        [  1.4815,  -6.0967],
        [  5.0886,  -9.7038],
        [  3.4995,  -8.1148],
        [-10.8621,   6.2469],
        [ -6.3657,   1.7505],
        [ -5.6197,   1.0045],
        [  2.2534,  -6.8687],
        [ -7.5709,   2.9556],
        [-11.9334,   7.3182],
        [ -1.0090,  -3.6062],
        [  0.3473,  -4.9625],
        [  5.7843, -10.3995],
        [  1.5723,  -6.1875],
        [-10.0150,   5.3998],
        [  0.2695,  -4.8847],
        [-10.9708,   6.3555],
        [ -6.9214,   2.3062],
        [-11.1615,   6.5463],
        [ -6.8567,   2.2414],
        [ -6.2451,   1.6299],
        [  2.8029,  -7.4181],
        [ -9.2530,   4.6378],
        [ -9.0531,   4.4378],
        [  4.9342,  -9.5494],
        [ -9.2740,   4.6588],
        [-10.9971,   6.3819],
        [  2.1680,  -6.7833],
        [ -9.9033,   5.2881],
        [  5.1068,  -9.7220],
        [-12.2953,   7.6800],
        [  3.3597,  -7.9749],
        [  2.7049,  -7.3201],
        [ -6.9389,   2.3237],
        [ -9.2681,   4.6529],
        [ -9.9941,   5.3788],
        [ -6.8267,   2.2114],
        [-10.0434,   5.4282],
        [  5.7618, -10.3771],
        [  0.1049,  -4.7202],
        [  2.1659,  -6.7811],
        [  4.2156,  -8.8309],
        [  4.7709,  -9.3862],
        [  1.0091,  -5.6243],
        [  1.6072,  -6.2224],
        [ -8.1545,   3.5393],
        [ -8.0205,   3.4053],
        [  2.9051,  -7.5203],
        [-12.1144,   7.4992],
        [  3.9872,  -8.6024],
        [ -7.5141,   2.8988],
        [ -4.9991,   0.3839],
        [  2.3111,  -6.9263],
        [  5.0800,  -9.6952],
        [-12.7818,   8.1666],
        [ -9.3666,   4.7514],
        [ -0.2488,  -4.3664],
        [-12.0948,   7.4796],
        [ -9.3682,   4.7530],
        [ -6.2960,   1.6808],
        [ -9.3656,   4.7503],
        [ -8.7242,   4.1090],
        [ -5.6797,   1.0644],
        [ -7.5529,   2.9377],
        [  4.7679,  -9.3831],
        [ -8.7822,   4.1670],
        [ -8.2482,   3.6329],
        [  4.0539,  -8.6691],
        [ -6.7922,   2.1770],
        [-11.2009,   6.5857],
        [ -4.2626,  -0.3526],
        [  2.1603,  -6.7755],
        [  1.5375,  -6.1527],
        [  2.2148,  -6.8300],
        [ -0.8836,  -3.7317],
        [ -9.9662,   5.3509]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.6938, 0.3062],
        [0.4362, 0.5638]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4532, 0.5468], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3014, 0.1084],
         [0.7410, 0.2979]],

        [[0.3565, 0.0969],
         [0.0508, 0.6336]],

        [[0.6179, 0.1000],
         [0.4792, 0.2061]],

        [[0.1821, 0.1036],
         [0.7312, 0.0235]],

        [[0.2601, 0.1111],
         [0.9123, 0.0391]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 1
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.95999388703655
time is 2
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9598385576399676
time is 3
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 4
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.3537915253268172
Average Adjusted Rand Index: 0.9601277603009152
Iteration 0: Loss = -17398.792709525645
Iteration 10: Loss = -12048.989701786964
Iteration 20: Loss = -12020.661256769046
Iteration 30: Loss = -12020.661248581677
Iteration 40: Loss = -12020.661248291326
Iteration 50: Loss = -12020.661248291326
1
Iteration 60: Loss = -12020.661248291326
2
Iteration 70: Loss = -12020.661248291326
3
Stopping early at iteration 69 due to no improvement.
pi: tensor([[0.7807, 0.2193],
        [0.3001, 0.6999]], dtype=torch.float64)
alpha: tensor([0.5732, 0.4268])
beta: tensor([[[0.3001, 0.1053],
         [0.8499, 0.2852]],

        [[0.9587, 0.0971],
         [0.2813, 0.1208]],

        [[0.7772, 0.1000],
         [0.3196, 0.3436]],

        [[0.4995, 0.1036],
         [0.6331, 0.4973]],

        [[0.6085, 0.1112],
         [0.4749, 0.9983]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9598385576399676
time is 3
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 4
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840257224851341
Average Adjusted Rand Index: 0.9839673667319889
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -17398.42855073845
Iteration 100: Loss = -12593.685342685794
Iteration 200: Loss = -12526.235586320676
Iteration 300: Loss = -12157.14947160312
Iteration 400: Loss = -12054.480276908533
Iteration 500: Loss = -12040.198313325902
Iteration 600: Loss = -12037.21114613161
Iteration 700: Loss = -12037.107034202105
Iteration 800: Loss = -12036.98023823291
Iteration 900: Loss = -12036.927386860496
Iteration 1000: Loss = -12036.89083975703
Iteration 1100: Loss = -12036.863248395304
Iteration 1200: Loss = -12036.83770630012
Iteration 1300: Loss = -12027.74087264428
Iteration 1400: Loss = -12027.724805515676
Iteration 1500: Loss = -12027.712155832163
Iteration 1600: Loss = -12027.700847541651
Iteration 1700: Loss = -12027.692621933535
Iteration 1800: Loss = -12027.686302919828
Iteration 1900: Loss = -12027.680890385713
Iteration 2000: Loss = -12027.676332943502
Iteration 2100: Loss = -12027.67230852827
Iteration 2200: Loss = -12027.668893625947
Iteration 2300: Loss = -12027.665789520825
Iteration 2400: Loss = -12027.663082642692
Iteration 2500: Loss = -12027.6606809634
Iteration 2600: Loss = -12027.66071674025
1
Iteration 2700: Loss = -12027.656524827673
Iteration 2800: Loss = -12027.654685812928
Iteration 2900: Loss = -12027.654456907947
Iteration 3000: Loss = -12027.651057464982
Iteration 3100: Loss = -12027.64906007518
Iteration 3200: Loss = -12027.687163312034
1
Iteration 3300: Loss = -12027.646018105866
Iteration 3400: Loss = -12027.6449036239
Iteration 3500: Loss = -12027.643890014755
Iteration 3600: Loss = -12027.643128045951
Iteration 3700: Loss = -12027.64217746285
Iteration 3800: Loss = -12027.641364047036
Iteration 3900: Loss = -12027.642161369506
1
Iteration 4000: Loss = -12027.640022163072
Iteration 4100: Loss = -12027.64064851556
1
Iteration 4200: Loss = -12027.638860062136
Iteration 4300: Loss = -12027.797824932226
1
Iteration 4400: Loss = -12027.637691307815
Iteration 4500: Loss = -12027.751984893608
1
Iteration 4600: Loss = -12027.636798135112
Iteration 4700: Loss = -12027.63797941392
1
Iteration 4800: Loss = -12027.642643642557
2
Iteration 4900: Loss = -12027.638853254804
3
Iteration 5000: Loss = -12027.636420486218
Iteration 5100: Loss = -12027.639667146737
1
Iteration 5200: Loss = -12027.638267071967
2
Iteration 5300: Loss = -12027.634877989663
Iteration 5400: Loss = -12027.634601234276
Iteration 5500: Loss = -12027.650177189
1
Iteration 5600: Loss = -12027.633509406623
Iteration 5700: Loss = -12027.638350322834
1
Iteration 5800: Loss = -12027.636933341288
2
Iteration 5900: Loss = -12027.634614808323
3
Iteration 6000: Loss = -12027.635000803903
4
Iteration 6100: Loss = -12027.723313630679
5
Iteration 6200: Loss = -12027.633171045207
Iteration 6300: Loss = -12018.86888452302
Iteration 6400: Loss = -12018.868591735265
Iteration 6500: Loss = -12018.984050187853
1
Iteration 6600: Loss = -12018.85935140644
Iteration 6700: Loss = -12018.860723723445
1
Iteration 6800: Loss = -12018.859113264927
Iteration 6900: Loss = -12018.859085543432
Iteration 7000: Loss = -12018.86141706088
1
Iteration 7100: Loss = -12018.8911617216
2
Iteration 7200: Loss = -12018.865282442688
3
Iteration 7300: Loss = -12018.861509283362
4
Iteration 7400: Loss = -12018.865476019937
5
Iteration 7500: Loss = -12018.873885361769
6
Iteration 7600: Loss = -12018.90228839032
7
Iteration 7700: Loss = -12018.863163016162
8
Iteration 7800: Loss = -12018.85862907033
Iteration 7900: Loss = -12018.858127018319
Iteration 8000: Loss = -12018.85899992143
1
Iteration 8100: Loss = -12018.858016964377
Iteration 8200: Loss = -12018.862350584166
1
Iteration 8300: Loss = -12018.85778353063
Iteration 8400: Loss = -12018.857967239244
1
Iteration 8500: Loss = -12018.857614806111
Iteration 8600: Loss = -12018.857596779959
Iteration 8700: Loss = -12018.859060565233
1
Iteration 8800: Loss = -12018.923248333302
2
Iteration 8900: Loss = -12018.90432838919
3
Iteration 9000: Loss = -12018.86780789649
4
Iteration 9100: Loss = -12018.85815483421
5
Iteration 9200: Loss = -12018.860763937739
6
Iteration 9300: Loss = -12018.86135329227
7
Iteration 9400: Loss = -12018.871009574523
8
Iteration 9500: Loss = -12018.868268056414
9
Iteration 9600: Loss = -12018.8781460696
10
Stopping early at iteration 9600 due to no improvement.
tensor([[ -8.4962,   5.7919],
        [ -4.7427,   3.1287],
        [ -4.3213,   2.1659],
        [  6.6320,  -8.2016],
        [ -4.5814,   1.7468],
        [ -5.7986,   4.4122],
        [ -5.9426,   4.1184],
        [  5.1056,  -7.3004],
        [  6.6143,  -8.0639],
        [  0.3406,  -2.2073],
        [  6.2118,  -7.9033],
        [ -6.8698,   2.2546],
        [  3.5400,  -5.7014],
        [  6.8177,  -8.2299],
        [ -8.7496,   7.1956],
        [ -5.8338,   4.4473],
        [  6.6154,  -8.5898],
        [  3.9672,  -7.6471],
        [ -7.4868,   6.0983],
        [ -7.6044,   6.1505],
        [  6.7118,  -8.3076],
        [ -8.6962,   6.8868],
        [  2.5171,  -3.9072],
        [  4.3021,  -6.6953],
        [ -7.8716,   6.2956],
        [  2.5550,  -4.4654],
        [  5.2933,  -6.7086],
        [  6.3778,  -7.9040],
        [ -8.0301,   6.6062],
        [ -9.6220,   6.3063],
        [ -5.3183,   3.9272],
        [  5.8957,  -8.0537],
        [ -6.5780,   5.0479],
        [ -9.2688,   5.8608],
        [  0.2749,  -1.7374],
        [  3.3807,  -4.8160],
        [  6.6034,  -8.5664],
        [  4.5467,  -5.9494],
        [ -8.2998,   6.6348],
        [  1.1421,  -3.4343],
        [ -8.3581,   6.2090],
        [ -6.1638,   4.0473],
        [ -8.5445,   6.1297],
        [ -7.1217,   4.6541],
        [ -4.6072,   3.1393],
        [  5.4628,  -8.5263],
        [ -6.7436,   5.3352],
        [ -5.3566,   3.5234],
        [  6.1730,  -7.7843],
        [ -6.6555,   5.2641],
        [ -8.7502,   7.3638],
        [  5.1733,  -6.7017],
        [ -7.9019,   6.4071],
        [  7.4737,  -9.6045],
        [ -8.6463,   7.2515],
        [  5.8563,  -7.4804],
        [  4.0188,  -5.6982],
        [ -5.4024,   3.7842],
        [ -6.4953,   5.0234],
        [ -8.4197,   7.0334],
        [ -7.5027,   4.9183],
        [ -7.8378,   6.4049],
        [  5.8683,  -8.7789],
        [  2.1530,  -3.8492],
        [  5.1763,  -7.3562],
        [  6.9129,  -9.4275],
        [  5.5887,  -7.4555],
        [  3.7328,  -5.2701],
        [  4.1335,  -6.3347],
        [ -8.2769,   5.9452],
        [ -5.0833,   3.6960],
        [  6.6497,  -8.0778],
        [-10.3429,   5.7277],
        [  5.2503,  -6.7639],
        [ -6.5483,   4.7924],
        [ -3.8563,   2.3999],
        [  6.8325,  -8.7042],
        [  7.1159,  -9.1652],
        [ -7.5177,   6.1060],
        [ -8.8957,   7.3389],
        [  1.3196,  -2.7062],
        [ -8.4405,   7.0143],
        [ -8.1568,   6.6760],
        [ -4.3510,   2.8850],
        [ -6.3259,   4.4430],
        [ -6.9548,   5.5635],
        [ -6.3651,   4.5312],
        [ -6.5430,   5.0682],
        [  6.3045,  -7.7505],
        [ -7.2897,   5.5185],
        [ -8.1115,   6.1857],
        [  6.3970,  -7.8574],
        [ -4.6497,   1.9600],
        [ -9.5235,   6.2902],
        [ -1.8109,  -0.3036],
        [  6.2234,  -7.9378],
        [  3.0656,  -4.4530],
        [  5.2259,  -6.6166],
        [  1.8689,  -3.3465],
        [ -8.0367,   6.3623]], dtype=torch.float64, requires_grad=True)
pi: tensor([[0.7027, 0.2973],
        [0.2167, 0.7833]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4502, 0.5498], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2922, 0.1048],
         [0.8499, 0.3064]],

        [[0.9587, 0.0969],
         [0.2813, 0.1208]],

        [[0.7772, 0.1001],
         [0.3196, 0.3436]],

        [[0.4995, 0.1037],
         [0.6331, 0.4973]],

        [[0.6085, 0.1111],
         [0.4749, 0.9983]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.95999388703655
time is 2
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9598385576399676
time is 3
tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 4
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9760861069565875
Average Adjusted Rand Index: 0.9759661441392989
Iteration 0: Loss = -25927.30979638079
Iteration 10: Loss = -12504.914942597487
Iteration 20: Loss = -12504.87289418332
Iteration 30: Loss = -12504.875395940659
1
Iteration 40: Loss = -12504.88427637924
2
Iteration 50: Loss = -12504.890560650492
3
Stopping early at iteration 49 due to no improvement.
pi: tensor([[0.0019, 0.9981],
        [0.0751, 0.9249]], dtype=torch.float64)
alpha: tensor([0.0695, 0.9305])
beta: tensor([[[0.2659, 0.2154],
         [0.4767, 0.1976]],

        [[0.3405, 0.2003],
         [0.6056, 0.5343]],

        [[0.9044, 0.2517],
         [0.7547, 0.5263]],

        [[0.0527, 0.2284],
         [0.4071, 0.0548]],

        [[0.6702, 0.2284],
         [0.5594, 0.7049]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 37
Adjusted Rand Index: -0.008724100327153763
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.001055373160744363
Average Adjusted Rand Index: -0.0017448200654307526
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25926.896654973803
Iteration 100: Loss = -12566.791704114516
Iteration 200: Loss = -12543.548694036619
Iteration 300: Loss = -12527.78775302553
Iteration 400: Loss = -12512.107568627527
Iteration 500: Loss = -12509.638008757995
Iteration 600: Loss = -12508.54711081518
Iteration 700: Loss = -12507.910069881935
Iteration 800: Loss = -12507.491610498855
Iteration 900: Loss = -12507.194135998285
Iteration 1000: Loss = -12506.965984171771
Iteration 1100: Loss = -12506.769534841904
Iteration 1200: Loss = -12506.536797356586
Iteration 1300: Loss = -12506.12632447121
Iteration 1400: Loss = -12505.874320755753
Iteration 1500: Loss = -12505.636126725813
Iteration 1600: Loss = -12505.419137774135
Iteration 1700: Loss = -12505.293862610504
Iteration 1800: Loss = -12505.2022922875
Iteration 1900: Loss = -12505.131226243542
Iteration 2000: Loss = -12505.07483303832
Iteration 2100: Loss = -12505.02879041899
Iteration 2200: Loss = -12504.990162311406
Iteration 2300: Loss = -12504.95711315906
Iteration 2400: Loss = -12504.928478593836
Iteration 2500: Loss = -12504.903245311612
Iteration 2600: Loss = -12504.880584886103
Iteration 2700: Loss = -12504.859705936457
Iteration 2800: Loss = -12504.838686180059
Iteration 2900: Loss = -12504.810248516476
Iteration 3000: Loss = -12504.760703532613
Iteration 3100: Loss = -12504.680536495971
Iteration 3200: Loss = -12504.557909951975
Iteration 3300: Loss = -12504.472905501318
Iteration 3400: Loss = -12504.414276861482
Iteration 3500: Loss = -12504.370306228835
Iteration 3600: Loss = -12504.331305334768
Iteration 3700: Loss = -12504.301011821271
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 20%|        | 20/100 [14:58:57<57:13:13, 2574.92s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 21%|        | 21/100 [15:46:28<58:19:29, 2657.84s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 22%|       | 22/100 [16:34:16<58:57:15, 2720.97s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 23%|       | 23/100 [17:18:27<57:44:44, 2699.80s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 24%|       | 24/100 [18:02:44<56:43:23, 2686.89s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 25%|       | 25/100 [18:46:23<55:33:15, 2666.61s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 26%|       | 26/100 [19:22:07<51:35:26, 2509.82s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 27%|       | 27/100 [20:05:43<51:32:32, 2541.81s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
