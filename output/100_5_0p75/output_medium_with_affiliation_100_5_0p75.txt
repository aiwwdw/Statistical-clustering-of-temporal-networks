nohup: ignoring input
  0%|          | 0/100 [00:00<?, ?it/s]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/adj_generator.py:108: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1720538439675/work/torch/csrc/utils/tensor_new.cpp:278.)
  Y = torch.tensor(Y)
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  1%|          | 1/100 [28:53<47:40:32, 1733.67s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  2%|▏         | 2/100 [49:49<39:32:48, 1452.74s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  3%|▎         | 3/100 [1:11:39<37:23:05, 1387.48s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  4%|▍         | 4/100 [1:30:24<34:14:06, 1283.82s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  5%|▌         | 5/100 [1:51:51<33:54:48, 1285.14s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  6%|▌         | 6/100 [2:14:52<34:24:00, 1317.45s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  7%|▋         | 7/100 [2:34:24<32:48:29, 1269.99s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  8%|▊         | 8/100 [2:58:28<33:52:13, 1325.37s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  9%|▉         | 9/100 [3:17:26<32:01:33, 1266.97s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 10%|█         | 10/100 [3:37:27<31:09:33, 1246.37s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 11%|█         | 11/100 [3:53:41<28:45:22, 1163.17s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 12%|█▏        | 12/100 [4:15:59<29:44:02, 1216.39s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 13%|█▎        | 13/100 [4:40:41<31:20:14, 1296.72s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 14%|█▍        | 14/100 [5:20:12<38:43:57, 1621.36s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 15%|█▌        | 15/100 [5:42:43<36:21:23, 1539.81s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 16%|█▌        | 16/100 [6:09:47<36:31:09, 1565.11s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 17%|█▋        | 17/100 [6:30:15<33:44:51, 1463.76s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 18%|█▊        | 18/100 [6:52:33<32:28:36, 1425.81s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 19%|█▉        | 19/100 [7:15:58<31:56:42, 1419.78s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 20%|██        | 20/100 [7:49:25<35:27:55, 1595.94s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 21%|██        | 21/100 [8:07:49<31:46:53, 1448.27s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
-----------------------------------------------------------------------------------------
This iteration is 0
True Objective function: Loss = -11829.765685323497
Iteration 0: Loss = -12383.07540402453
Iteration 10: Loss = -12381.964417236799
Iteration 20: Loss = -12381.00625007349
Iteration 30: Loss = -12380.809048801571
Iteration 40: Loss = -12380.770983554998
Iteration 50: Loss = -12380.757310496225
Iteration 60: Loss = -12380.74760452117
Iteration 70: Loss = -12380.732890893612
Iteration 80: Loss = -12380.71826314585
Iteration 90: Loss = -12380.71199375955
Iteration 100: Loss = -12380.710863369812
Iteration 110: Loss = -12380.71144260764
1
Iteration 120: Loss = -12380.712361065902
2
Iteration 130: Loss = -12380.713349221009
3
Stopping early at iteration 130 due to no improvement.
pi: tensor([[0.9452, 0.0548],
        [0.9585, 0.0415]], dtype=torch.float64)
alpha: tensor([0.9463, 0.0537])
beta: tensor([[[0.1956, 0.1918],
         [0.9278, 0.2390]],

        [[0.5274, 0.1879],
         [0.7861, 0.5875]],

        [[0.7414, 0.2209],
         [0.3647, 0.4004]],

        [[0.8234, 0.2710],
         [0.4257, 0.7140]],

        [[0.6555, 0.2132],
         [0.9305, 0.0070]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 40
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: -0.009009494263563287
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.001783122334233028
Average Adjusted Rand Index: -0.0018018988527126576
Iteration 0: Loss = -12708.353097875259
Iteration 10: Loss = -12381.595603268064
Iteration 20: Loss = -12381.595467020996
Iteration 30: Loss = -12381.595500660309
1
Iteration 40: Loss = -12381.595442413534
Iteration 50: Loss = -12381.595518697623
1
Iteration 60: Loss = -12381.595582381155
2
Iteration 70: Loss = -12381.595741078878
3
Stopping early at iteration 70 due to no improvement.
pi: tensor([[0.4497, 0.5503],
        [0.4749, 0.5251]], dtype=torch.float64)
alpha: tensor([0.4638, 0.5362])
beta: tensor([[[0.1925, 0.1926],
         [0.7507, 0.2027]],

        [[0.7553, 0.1917],
         [0.6462, 0.6040]],

        [[0.5960, 0.1998],
         [0.9940, 0.0703]],

        [[0.1119, 0.2045],
         [0.3144, 0.5581]],

        [[0.0720, 0.1988],
         [0.6679, 0.9754]]], dtype=torch.float64)
time is 0
tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.005374181991006471
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 62
Adjusted Rand Index: 0.04198753208654199
time is 2
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 61
Adjusted Rand Index: 0.02445479962721342
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 61
Adjusted Rand Index: 0.03219388124931273
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.007301122709623765
Global Adjusted Rand Index: 0.017537026623318592
Average Adjusted Rand Index: 0.020112630736337088
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20973.83089577444
Iteration 100: Loss = -12383.278085036205
Iteration 200: Loss = -12382.467479612144
Iteration 300: Loss = -12382.274441224012
Iteration 400: Loss = -12382.189807945544
Iteration 500: Loss = -12382.121089493243
Iteration 600: Loss = -12382.024813305123
Iteration 700: Loss = -12381.745412181835
Iteration 800: Loss = -12381.24727435532
Iteration 900: Loss = -12380.936255155182
Iteration 1000: Loss = -12380.750642411529
Iteration 1100: Loss = -12380.619599362713
Iteration 1200: Loss = -12380.522094841432
Iteration 1300: Loss = -12380.452069430894
Iteration 1400: Loss = -12380.400070509479
Iteration 1500: Loss = -12380.360013876754
Iteration 1600: Loss = -12380.328521983329
Iteration 1700: Loss = -12380.303994729791
Iteration 1800: Loss = -12380.284827516025
Iteration 1900: Loss = -12380.270028985362
Iteration 2000: Loss = -12380.25886296088
Iteration 2100: Loss = -12380.250473446393
Iteration 2200: Loss = -12380.244191859902
Iteration 2300: Loss = -12380.23951725329
Iteration 2400: Loss = -12380.235975401947
Iteration 2500: Loss = -12380.23321899765
Iteration 2600: Loss = -12380.231106406234
Iteration 2700: Loss = -12380.22940052891
Iteration 2800: Loss = -12380.227961485514
Iteration 2900: Loss = -12380.226758618981
Iteration 3000: Loss = -12380.225539793382
Iteration 3100: Loss = -12380.224561197265
Iteration 3200: Loss = -12380.223556449137
Iteration 3300: Loss = -12380.222597016293
Iteration 3400: Loss = -12380.221644133797
Iteration 3500: Loss = -12380.220737321644
Iteration 3600: Loss = -12380.21990022042
Iteration 3700: Loss = -12380.21902044528
Iteration 3800: Loss = -12380.21820398849
Iteration 3900: Loss = -12380.217384483945
Iteration 4000: Loss = -12380.21662707871
Iteration 4100: Loss = -12380.216011276898
Iteration 4200: Loss = -12380.21503026214
Iteration 4300: Loss = -12380.214340570232
Iteration 4400: Loss = -12380.215907665204
1
Iteration 4500: Loss = -12380.212936028334
Iteration 4600: Loss = -12380.212262552104
Iteration 4700: Loss = -12380.211649580093
Iteration 4800: Loss = -12380.211007741564
Iteration 4900: Loss = -12380.21040525461
Iteration 5000: Loss = -12380.209855399866
Iteration 5100: Loss = -12380.209242748431
Iteration 5200: Loss = -12380.208790040151
Iteration 5300: Loss = -12380.208265068915
Iteration 5400: Loss = -12380.207829828192
Iteration 5500: Loss = -12380.207386191873
Iteration 5600: Loss = -12380.206963958226
Iteration 5700: Loss = -12380.206556032623
Iteration 5800: Loss = -12380.208296791905
1
Iteration 5900: Loss = -12380.205782027631
Iteration 6000: Loss = -12380.20541254741
Iteration 6100: Loss = -12380.205135799659
Iteration 6200: Loss = -12380.204766810248
Iteration 6300: Loss = -12380.204504102434
Iteration 6400: Loss = -12380.204251015095
Iteration 6500: Loss = -12380.2083582947
1
Iteration 6600: Loss = -12380.203689708198
Iteration 6700: Loss = -12380.203466686735
Iteration 6800: Loss = -12380.213246164449
1
Iteration 6900: Loss = -12380.205102916127
2
Iteration 7000: Loss = -12380.202860048768
Iteration 7100: Loss = -12380.394635535991
1
Iteration 7200: Loss = -12380.202438362116
Iteration 7300: Loss = -12380.202333923242
Iteration 7400: Loss = -12380.202209828623
Iteration 7500: Loss = -12380.201912355897
Iteration 7600: Loss = -12380.748231459042
1
Iteration 7700: Loss = -12380.201651335086
Iteration 7800: Loss = -12380.201512997099
Iteration 7900: Loss = -12380.201410690248
Iteration 8000: Loss = -12380.201284607248
Iteration 8100: Loss = -12380.201155206865
Iteration 8200: Loss = -12380.201008418788
Iteration 8300: Loss = -12380.201255345235
1
Iteration 8400: Loss = -12380.200823570764
Iteration 8500: Loss = -12380.200776906444
Iteration 8600: Loss = -12380.20243312202
1
Iteration 8700: Loss = -12380.2005684612
Iteration 8800: Loss = -12380.200472520954
Iteration 8900: Loss = -12380.200507828236
1
Iteration 9000: Loss = -12380.200325446815
Iteration 9100: Loss = -12380.200269585011
Iteration 9200: Loss = -12380.200494726441
1
Iteration 9300: Loss = -12380.20010904041
Iteration 9400: Loss = -12380.20007756576
Iteration 9500: Loss = -12380.200768759907
1
Iteration 9600: Loss = -12380.199963422823
Iteration 9700: Loss = -12380.19992707313
Iteration 9800: Loss = -12380.20148295131
1
Iteration 9900: Loss = -12380.199812152343
Iteration 10000: Loss = -12380.199774992623
Iteration 10100: Loss = -12380.236122209415
1
Iteration 10200: Loss = -12380.199646734078
Iteration 10300: Loss = -12380.19965399751
1
Iteration 10400: Loss = -12380.20153201273
2
Iteration 10500: Loss = -12380.199650994842
3
Iteration 10600: Loss = -12380.19955428453
Iteration 10700: Loss = -12380.206614919874
1
Iteration 10800: Loss = -12380.199483680992
Iteration 10900: Loss = -12380.199634298047
1
Iteration 11000: Loss = -12380.199455463891
Iteration 11100: Loss = -12380.199409692512
Iteration 11200: Loss = -12380.200365320876
1
Iteration 11300: Loss = -12380.199367480795
Iteration 11400: Loss = -12380.201242910367
1
Iteration 11500: Loss = -12380.199350486446
Iteration 11600: Loss = -12380.199327998365
Iteration 11700: Loss = -12380.20106422992
1
Iteration 11800: Loss = -12380.199238414116
Iteration 11900: Loss = -12380.223830720342
1
Iteration 12000: Loss = -12380.199243090115
2
Iteration 12100: Loss = -12380.199251520107
3
Iteration 12200: Loss = -12380.199403262968
4
Iteration 12300: Loss = -12380.199194716086
Iteration 12400: Loss = -12380.199370202774
1
Iteration 12500: Loss = -12380.199151594505
Iteration 12600: Loss = -12380.216174192885
1
Iteration 12700: Loss = -12380.19914230504
Iteration 12800: Loss = -12380.199158797726
1
Iteration 12900: Loss = -12380.19930722505
2
Iteration 13000: Loss = -12380.199128951868
Iteration 13100: Loss = -12380.200597289791
1
Iteration 13200: Loss = -12380.199081839864
Iteration 13300: Loss = -12380.299557202858
1
Iteration 13400: Loss = -12380.199052849935
Iteration 13500: Loss = -12380.199070408366
1
Iteration 13600: Loss = -12380.19913034166
2
Iteration 13700: Loss = -12380.199059047394
3
Iteration 13800: Loss = -12380.199190016496
4
Iteration 13900: Loss = -12380.214205709035
5
Stopping early at iteration 13900 due to no improvement.
pi: tensor([[2.6312e-04, 9.9974e-01],
        [6.4975e-02, 9.3503e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0052, 0.9948], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2742, 0.1498],
         [0.5777, 0.1968]],

        [[0.6994, 0.2027],
         [0.5386, 0.5806]],

        [[0.6154, 0.2288],
         [0.5714, 0.7133]],

        [[0.7005, 0.2664],
         [0.5599, 0.5673]],

        [[0.6314, 0.2218],
         [0.6321, 0.5804]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 60
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: -0.009009494263563287
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.001783122334233028
Average Adjusted Rand Index: -0.0018018988527126576
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19680.44317138599
Iteration 100: Loss = -12382.472877615943
Iteration 200: Loss = -12382.179709103355
Iteration 300: Loss = -12382.127856938681
Iteration 400: Loss = -12382.075304289272
Iteration 500: Loss = -12381.978618825087
Iteration 600: Loss = -12381.692977927612
Iteration 700: Loss = -12381.20980716895
Iteration 800: Loss = -12380.839008477578
Iteration 900: Loss = -12380.541061410817
Iteration 1000: Loss = -12380.378564152004
Iteration 1100: Loss = -12380.317984618865
Iteration 1200: Loss = -12380.288308699537
Iteration 1300: Loss = -12380.267558082913
Iteration 1400: Loss = -12380.252104903262
Iteration 1500: Loss = -12380.240833335809
Iteration 1600: Loss = -12380.233024433612
Iteration 1700: Loss = -12380.22788645472
Iteration 1800: Loss = -12380.224485105673
Iteration 1900: Loss = -12380.22209311462
Iteration 2000: Loss = -12380.22027752484
Iteration 2100: Loss = -12380.218812417123
Iteration 2200: Loss = -12380.217539480387
Iteration 2300: Loss = -12380.216455882875
Iteration 2400: Loss = -12380.215425984843
Iteration 2500: Loss = -12380.214478901638
Iteration 2600: Loss = -12380.213564477664
Iteration 2700: Loss = -12380.212706067827
Iteration 2800: Loss = -12380.211858471299
Iteration 2900: Loss = -12380.211099218079
Iteration 3000: Loss = -12380.210401661241
Iteration 3100: Loss = -12380.209648711207
Iteration 3200: Loss = -12380.209056787748
Iteration 3300: Loss = -12380.20845692601
Iteration 3400: Loss = -12380.207863749898
Iteration 3500: Loss = -12380.207319962537
Iteration 3600: Loss = -12380.206833143273
Iteration 3700: Loss = -12380.208404939103
1
Iteration 3800: Loss = -12380.205895731262
Iteration 3900: Loss = -12380.205479938853
Iteration 4000: Loss = -12380.20572846853
1
Iteration 4100: Loss = -12380.204717583414
Iteration 4200: Loss = -12380.204396338226
Iteration 4300: Loss = -12380.204705074519
1
Iteration 4400: Loss = -12380.203824779348
Iteration 4500: Loss = -12380.203526339521
Iteration 4600: Loss = -12380.203244314249
Iteration 4700: Loss = -12380.203049382944
Iteration 4800: Loss = -12380.204069372363
1
Iteration 4900: Loss = -12380.202613648467
Iteration 5000: Loss = -12380.20242068082
Iteration 5100: Loss = -12380.202243111307
Iteration 5200: Loss = -12380.202051979748
Iteration 5300: Loss = -12380.202805519577
1
Iteration 5400: Loss = -12380.201791299176
Iteration 5500: Loss = -12380.20544128975
1
Iteration 5600: Loss = -12380.201426353653
Iteration 5700: Loss = -12380.201385174782
Iteration 5800: Loss = -12380.201191119508
Iteration 5900: Loss = -12380.201086520452
Iteration 6000: Loss = -12380.200942044432
Iteration 6100: Loss = -12380.200856603637
Iteration 6200: Loss = -12380.201435019295
1
Iteration 6300: Loss = -12380.200652546291
Iteration 6400: Loss = -12380.210575958083
1
Iteration 6500: Loss = -12380.200482322307
Iteration 6600: Loss = -12380.200374853666
Iteration 6700: Loss = -12380.200364667862
Iteration 6800: Loss = -12380.200578588601
1
Iteration 6900: Loss = -12380.200230642635
Iteration 7000: Loss = -12380.201231509342
1
Iteration 7100: Loss = -12380.212378946744
2
Iteration 7200: Loss = -12380.200004307151
Iteration 7300: Loss = -12380.19999124068
Iteration 7400: Loss = -12380.20000156934
1
Iteration 7500: Loss = -12380.19987263298
Iteration 7600: Loss = -12380.801716685044
1
Iteration 7700: Loss = -12380.199788286309
Iteration 7800: Loss = -12380.199744757358
Iteration 7900: Loss = -12380.199903592129
1
Iteration 8000: Loss = -12380.199709689783
Iteration 8100: Loss = -12380.19958589221
Iteration 8200: Loss = -12380.206621715235
1
Iteration 8300: Loss = -12380.199537455002
Iteration 8400: Loss = -12380.199518848858
Iteration 8500: Loss = -12380.19950275437
Iteration 8600: Loss = -12380.19948577474
Iteration 8700: Loss = -12380.19940365729
Iteration 8800: Loss = -12380.199385499234
Iteration 8900: Loss = -12380.200622683813
1
Iteration 9000: Loss = -12380.199373393707
Iteration 9100: Loss = -12380.199324891559
Iteration 9200: Loss = -12380.200898041598
1
Iteration 9300: Loss = -12380.199275748453
Iteration 9400: Loss = -12380.1992827506
1
Iteration 9500: Loss = -12380.199340808662
2
Iteration 9600: Loss = -12380.19922170017
Iteration 9700: Loss = -12380.313934516525
1
Iteration 9800: Loss = -12380.199227856549
2
Iteration 9900: Loss = -12380.199190010684
Iteration 10000: Loss = -12380.202744282038
1
Iteration 10100: Loss = -12380.199165445812
Iteration 10200: Loss = -12380.199145885772
Iteration 10300: Loss = -12380.222914155169
1
Iteration 10400: Loss = -12380.199141906654
Iteration 10500: Loss = -12380.199133841872
Iteration 10600: Loss = -12380.19924453952
1
Iteration 10700: Loss = -12380.199126245263
Iteration 10800: Loss = -12380.199120921001
Iteration 10900: Loss = -12380.200128966264
1
Iteration 11000: Loss = -12380.199072905021
Iteration 11100: Loss = -12380.19906387206
Iteration 11200: Loss = -12380.200030892152
1
Iteration 11300: Loss = -12380.20334464921
2
Iteration 11400: Loss = -12380.199130307135
3
Iteration 11500: Loss = -12380.240192379055
4
Iteration 11600: Loss = -12380.199332100521
5
Stopping early at iteration 11600 due to no improvement.
pi: tensor([[9.3355e-01, 6.6448e-02],
        [9.9974e-01, 2.5697e-04]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9948, 0.0052], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1965, 0.1498],
         [0.5630, 0.2738]],

        [[0.6047, 0.2025],
         [0.5224, 0.5681]],

        [[0.5912, 0.2278],
         [0.5840, 0.7306]],

        [[0.5422, 0.2662],
         [0.6918, 0.7145]],

        [[0.7303, 0.2212],
         [0.5566, 0.7295]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 40
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: -0.009009494263563287
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.001783122334233028
Average Adjusted Rand Index: -0.0018018988527126576
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22658.31112057045
Iteration 100: Loss = -12382.843967948931
Iteration 200: Loss = -12381.913218703394
Iteration 300: Loss = -12381.566462182434
Iteration 400: Loss = -12381.306572031903
Iteration 500: Loss = -12381.070162447533
Iteration 600: Loss = -12380.872688948268
Iteration 700: Loss = -12380.72767228471
Iteration 800: Loss = -12380.625804950349
Iteration 900: Loss = -12380.549405009335
Iteration 1000: Loss = -12380.488487147797
Iteration 1100: Loss = -12380.439106467762
Iteration 1200: Loss = -12380.398720537265
Iteration 1300: Loss = -12380.365862297436
Iteration 1400: Loss = -12380.339274236128
Iteration 1500: Loss = -12380.318072905991
Iteration 1600: Loss = -12380.30125077288
Iteration 1700: Loss = -12380.288018951105
Iteration 1800: Loss = -12380.277498512729
Iteration 1900: Loss = -12380.269231712191
Iteration 2000: Loss = -12380.262633569753
Iteration 2100: Loss = -12380.257386447613
Iteration 2200: Loss = -12380.253191774882
Iteration 2300: Loss = -12380.249825759855
Iteration 2400: Loss = -12380.247092763579
Iteration 2500: Loss = -12380.244783422451
Iteration 2600: Loss = -12380.242862505607
Iteration 2700: Loss = -12380.241132845618
Iteration 2800: Loss = -12380.239634204941
Iteration 2900: Loss = -12380.238255950619
Iteration 3000: Loss = -12380.236977009932
Iteration 3100: Loss = -12380.235759041216
Iteration 3200: Loss = -12380.234598029794
Iteration 3300: Loss = -12380.233518381317
Iteration 3400: Loss = -12380.232322795555
Iteration 3500: Loss = -12380.231251089102
Iteration 3600: Loss = -12380.230137974084
Iteration 3700: Loss = -12380.229067845674
Iteration 3800: Loss = -12380.227973081886
Iteration 3900: Loss = -12380.226886508028
Iteration 4000: Loss = -12380.23136718376
1
Iteration 4100: Loss = -12380.224744418543
Iteration 4200: Loss = -12380.22367824249
Iteration 4300: Loss = -12380.22282329224
Iteration 4400: Loss = -12380.221551170853
Iteration 4500: Loss = -12380.220529622246
Iteration 4600: Loss = -12380.219533006719
Iteration 4700: Loss = -12380.218536228309
Iteration 4800: Loss = -12380.221517870734
1
Iteration 4900: Loss = -12380.21661728083
Iteration 5000: Loss = -12380.215730286633
Iteration 5100: Loss = -12380.215012577544
Iteration 5200: Loss = -12380.21400525723
Iteration 5300: Loss = -12380.213191411967
Iteration 5400: Loss = -12380.212446346002
Iteration 5500: Loss = -12380.211677009178
Iteration 5600: Loss = -12380.224572040315
1
Iteration 5700: Loss = -12380.210331770351
Iteration 5800: Loss = -12380.209651586862
Iteration 5900: Loss = -12380.2092415934
Iteration 6000: Loss = -12380.208497681897
Iteration 6100: Loss = -12380.207912727046
Iteration 6200: Loss = -12380.207410226612
Iteration 6300: Loss = -12380.206934687612
Iteration 6400: Loss = -12380.206572335617
Iteration 6500: Loss = -12380.206056504805
Iteration 6600: Loss = -12380.205639340695
Iteration 6700: Loss = -12380.20527091784
Iteration 6800: Loss = -12380.204898700444
Iteration 6900: Loss = -12380.205271412702
1
Iteration 7000: Loss = -12380.223725434698
2
Iteration 7100: Loss = -12380.203998002778
Iteration 7200: Loss = -12380.204018658344
1
Iteration 7300: Loss = -12380.20342732488
Iteration 7400: Loss = -12380.203194090194
Iteration 7500: Loss = -12380.202990293972
Iteration 7600: Loss = -12380.202759623904
Iteration 7700: Loss = -12380.22099719629
1
Iteration 7800: Loss = -12380.202392970552
Iteration 7900: Loss = -12380.202198545354
Iteration 8000: Loss = -12380.202136212034
Iteration 8100: Loss = -12380.201948839933
Iteration 8200: Loss = -12380.201755279964
Iteration 8300: Loss = -12380.20160256005
Iteration 8400: Loss = -12380.20156226654
Iteration 8500: Loss = -12380.201327207646
Iteration 8600: Loss = -12380.201150059787
Iteration 8700: Loss = -12380.221472498723
1
Iteration 8800: Loss = -12380.200959667878
Iteration 8900: Loss = -12380.200827797345
Iteration 9000: Loss = -12380.26739056713
1
Iteration 9100: Loss = -12380.200674234591
Iteration 9200: Loss = -12380.200570105668
Iteration 9300: Loss = -12380.254588241827
1
Iteration 9400: Loss = -12380.200435152774
Iteration 9500: Loss = -12380.200326487704
Iteration 9600: Loss = -12380.398056488739
1
Iteration 9700: Loss = -12380.20017285402
Iteration 9800: Loss = -12380.200139062672
Iteration 9900: Loss = -12380.200101870289
Iteration 10000: Loss = -12380.201733355803
1
Iteration 10100: Loss = -12380.19999090254
Iteration 10200: Loss = -12380.199910251436
Iteration 10300: Loss = -12380.200374656406
1
Iteration 10400: Loss = -12380.199808782321
Iteration 10500: Loss = -12380.199737564502
Iteration 10600: Loss = -12380.201196033257
1
Iteration 10700: Loss = -12380.199689585987
Iteration 10800: Loss = -12380.199626151021
Iteration 10900: Loss = -12380.203320110313
1
Iteration 11000: Loss = -12380.199570872648
Iteration 11100: Loss = -12380.199544800696
Iteration 11200: Loss = -12380.200554477462
1
Iteration 11300: Loss = -12380.199490141325
Iteration 11400: Loss = -12380.199476086726
Iteration 11500: Loss = -12380.199937053492
1
Iteration 11600: Loss = -12380.199404064379
Iteration 11700: Loss = -12380.199416733944
1
Iteration 11800: Loss = -12380.199341231566
Iteration 11900: Loss = -12380.205189924642
1
Iteration 12000: Loss = -12380.199285056959
Iteration 12100: Loss = -12380.20336002606
1
Iteration 12200: Loss = -12380.199278625158
Iteration 12300: Loss = -12380.210168610844
1
Iteration 12400: Loss = -12380.302339779393
2
Iteration 12500: Loss = -12380.199640666378
3
Iteration 12600: Loss = -12380.199227678215
Iteration 12700: Loss = -12380.19934878205
1
Iteration 12800: Loss = -12380.34748323467
2
Iteration 12900: Loss = -12380.199191607693
Iteration 13000: Loss = -12380.199184152507
Iteration 13100: Loss = -12380.199337718655
1
Iteration 13200: Loss = -12380.199150257627
Iteration 13300: Loss = -12380.201275176636
1
Iteration 13400: Loss = -12380.208723532882
2
Iteration 13500: Loss = -12380.206326711303
3
Iteration 13600: Loss = -12380.199375242968
4
Iteration 13700: Loss = -12380.199215187717
5
Stopping early at iteration 13700 due to no improvement.
pi: tensor([[3.6902e-04, 9.9963e-01],
        [6.6346e-02, 9.3365e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0052, 0.9948], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2738, 0.1498],
         [0.7234, 0.1966]],

        [[0.5392, 0.2024],
         [0.5166, 0.7069]],

        [[0.6732, 0.2277],
         [0.7085, 0.6613]],

        [[0.5904, 0.2662],
         [0.6345, 0.7067]],

        [[0.6769, 0.2212],
         [0.6320, 0.5805]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 60
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: -0.009009494263563287
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.001783122334233028
Average Adjusted Rand Index: -0.0018018988527126576
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22529.210320482125
Iteration 100: Loss = -12382.705319518163
Iteration 200: Loss = -12382.146303684049
Iteration 300: Loss = -12381.796099538104
Iteration 400: Loss = -12381.320094392318
Iteration 500: Loss = -12380.985508920634
Iteration 600: Loss = -12380.781404144893
Iteration 700: Loss = -12380.6596288368
Iteration 800: Loss = -12380.576652350106
Iteration 900: Loss = -12380.511177054823
Iteration 1000: Loss = -12380.4575335854
Iteration 1100: Loss = -12380.41286078739
Iteration 1200: Loss = -12380.375520292075
Iteration 1300: Loss = -12380.344583925213
Iteration 1400: Loss = -12380.319504319215
Iteration 1500: Loss = -12380.299804026388
Iteration 1600: Loss = -12380.284603852842
Iteration 1700: Loss = -12380.273093689824
Iteration 1800: Loss = -12380.264470944863
Iteration 1900: Loss = -12380.258020949383
Iteration 2000: Loss = -12380.25318951613
Iteration 2100: Loss = -12380.249547746918
Iteration 2200: Loss = -12380.24668860699
Iteration 2300: Loss = -12380.244398389352
Iteration 2400: Loss = -12380.242492985011
Iteration 2500: Loss = -12380.24095954124
Iteration 2600: Loss = -12380.239498941823
Iteration 2700: Loss = -12380.238216581456
Iteration 2800: Loss = -12380.237034387013
Iteration 2900: Loss = -12380.23589058769
Iteration 3000: Loss = -12380.234739889023
Iteration 3100: Loss = -12380.233585559594
Iteration 3200: Loss = -12380.232490839793
Iteration 3300: Loss = -12380.231393147127
Iteration 3400: Loss = -12380.230255680573
Iteration 3500: Loss = -12380.229125643575
Iteration 3600: Loss = -12380.228075240011
Iteration 3700: Loss = -12380.226903230589
Iteration 3800: Loss = -12380.22583423169
Iteration 3900: Loss = -12380.224711227818
Iteration 4000: Loss = -12380.224086903636
Iteration 4100: Loss = -12380.222508675826
Iteration 4200: Loss = -12380.221414969179
Iteration 4300: Loss = -12380.225214336982
1
Iteration 4400: Loss = -12380.219317661675
Iteration 4500: Loss = -12380.218253405672
Iteration 4600: Loss = -12380.217439716338
Iteration 4700: Loss = -12380.21631032483
Iteration 4800: Loss = -12380.215354829863
Iteration 4900: Loss = -12380.214468952427
Iteration 5000: Loss = -12380.213608034433
Iteration 5100: Loss = -12380.2127660211
Iteration 5200: Loss = -12380.212007202199
Iteration 5300: Loss = -12380.211239097851
Iteration 5400: Loss = -12380.213522696806
1
Iteration 5500: Loss = -12380.209853855924
Iteration 5600: Loss = -12380.209166061954
Iteration 5700: Loss = -12380.208647257605
Iteration 5800: Loss = -12380.20801011692
Iteration 5900: Loss = -12380.207493040687
Iteration 6000: Loss = -12380.212489952619
1
Iteration 6100: Loss = -12380.206518541148
Iteration 6200: Loss = -12380.206060543627
Iteration 6300: Loss = -12380.205716487555
Iteration 6400: Loss = -12380.205220271524
Iteration 6500: Loss = -12380.204878903784
Iteration 6600: Loss = -12380.204540567043
Iteration 6700: Loss = -12380.204191376091
Iteration 6800: Loss = -12380.203999850239
Iteration 6900: Loss = -12380.203624685737
Iteration 7000: Loss = -12380.203453113008
Iteration 7100: Loss = -12380.20335334124
Iteration 7200: Loss = -12380.203727634542
1
Iteration 7300: Loss = -12380.32587009717
2
Iteration 7400: Loss = -12380.202485283551
Iteration 7500: Loss = -12380.562183931192
1
Iteration 7600: Loss = -12380.202125293798
Iteration 7700: Loss = -12380.201925020852
Iteration 7800: Loss = -12380.201950518807
1
Iteration 7900: Loss = -12380.201632675155
Iteration 8000: Loss = -12380.33189557673
1
Iteration 8100: Loss = -12380.201374259466
Iteration 8200: Loss = -12380.201246681972
Iteration 8300: Loss = -12380.239945231539
1
Iteration 8400: Loss = -12380.20100381821
Iteration 8500: Loss = -12380.200936352878
Iteration 8600: Loss = -12380.238728383056
1
Iteration 8700: Loss = -12380.200677598397
Iteration 8800: Loss = -12380.200611621547
Iteration 8900: Loss = -12380.200531175327
Iteration 9000: Loss = -12380.200472451173
Iteration 9100: Loss = -12380.200356889403
Iteration 9200: Loss = -12380.200298670447
Iteration 9300: Loss = -12380.200259382103
Iteration 9400: Loss = -12380.200146353733
Iteration 9500: Loss = -12380.200105640986
Iteration 9600: Loss = -12380.200398975529
1
Iteration 9700: Loss = -12380.199972424274
Iteration 9800: Loss = -12380.199942032168
Iteration 9900: Loss = -12380.212893757516
1
Iteration 10000: Loss = -12380.199819888907
Iteration 10100: Loss = -12380.19979452768
Iteration 10200: Loss = -12380.349287221905
1
Iteration 10300: Loss = -12380.199687449189
Iteration 10400: Loss = -12380.199685376358
Iteration 10500: Loss = -12380.215371888486
1
Iteration 10600: Loss = -12380.199580133283
Iteration 10700: Loss = -12380.199567144737
Iteration 10800: Loss = -12380.203775527665
1
Iteration 10900: Loss = -12380.19979220814
2
Iteration 11000: Loss = -12380.199556042584
Iteration 11100: Loss = -12380.565907504684
1
Iteration 11200: Loss = -12380.199469316132
Iteration 11300: Loss = -12380.364427518432
1
Iteration 11400: Loss = -12380.19937314538
Iteration 11500: Loss = -12380.199510196557
1
Iteration 11600: Loss = -12380.284533838363
2
Iteration 11700: Loss = -12380.199301428202
Iteration 11800: Loss = -12380.206007693565
1
Iteration 11900: Loss = -12380.199266510683
Iteration 12000: Loss = -12380.200079334292
1
Iteration 12100: Loss = -12380.199243371031
Iteration 12200: Loss = -12380.19943444295
1
Iteration 12300: Loss = -12380.199193496333
Iteration 12400: Loss = -12380.199850331639
1
Iteration 12500: Loss = -12380.199167723293
Iteration 12600: Loss = -12380.224830358193
1
Iteration 12700: Loss = -12380.199138770962
Iteration 12800: Loss = -12380.299125223124
1
Iteration 12900: Loss = -12380.203449994608
2
Iteration 13000: Loss = -12380.199143511
3
Iteration 13100: Loss = -12380.200741444007
4
Iteration 13200: Loss = -12380.207539423081
5
Stopping early at iteration 13200 due to no improvement.
pi: tensor([[3.9825e-04, 9.9960e-01],
        [6.6544e-02, 9.3346e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0052, 0.9948], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2738, 0.1498],
         [0.5282, 0.1963]],

        [[0.6912, 0.2027],
         [0.6730, 0.6373]],

        [[0.6205, 0.2279],
         [0.7212, 0.5499]],

        [[0.5913, 0.2659],
         [0.5235, 0.6806]],

        [[0.6998, 0.2214],
         [0.5998, 0.6468]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 60
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: -0.009009494263563287
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.001783122334233028
Average Adjusted Rand Index: -0.0018018988527126576
11829.765685323497
[-0.001783122334233028, -0.001783122334233028, -0.001783122334233028, -0.001783122334233028] [-0.0018018988527126576, -0.0018018988527126576, -0.0018018988527126576, -0.0018018988527126576] [12380.214205709035, 12380.199332100521, 12380.199215187717, 12380.207539423081]
-----------------------------------------------------------------------------------------
This iteration is 1
True Objective function: Loss = -11752.668410444872
Iteration 0: Loss = -12252.432765753087
Iteration 10: Loss = -12185.820829405951
Iteration 20: Loss = -12185.69585457539
Iteration 30: Loss = -12185.644226577466
Iteration 40: Loss = -12185.616772459654
Iteration 50: Loss = -12185.603304174883
Iteration 60: Loss = -12185.59511353717
Iteration 70: Loss = -12185.588719378535
Iteration 80: Loss = -12185.583351739113
Iteration 90: Loss = -12185.578734375482
Iteration 100: Loss = -12185.574803395179
Iteration 110: Loss = -12185.571368593928
Iteration 120: Loss = -12185.5683768221
Iteration 130: Loss = -12185.565787892137
Iteration 140: Loss = -12185.563570561384
Iteration 150: Loss = -12185.561563408293
Iteration 160: Loss = -12185.55980516029
Iteration 170: Loss = -12185.55828785978
Iteration 180: Loss = -12185.556869272377
Iteration 190: Loss = -12185.555674173902
Iteration 200: Loss = -12185.554654439296
Iteration 210: Loss = -12185.553671189338
Iteration 220: Loss = -12185.552771753586
Iteration 230: Loss = -12185.552018251423
Iteration 240: Loss = -12185.551393449614
Iteration 250: Loss = -12185.550764550238
Iteration 260: Loss = -12185.550260414077
Iteration 270: Loss = -12185.54978150682
Iteration 280: Loss = -12185.549338455656
Iteration 290: Loss = -12185.548991880709
Iteration 300: Loss = -12185.54868888808
Iteration 310: Loss = -12185.548362370626
Iteration 320: Loss = -12185.548150786828
Iteration 330: Loss = -12185.54789977808
Iteration 340: Loss = -12185.547708457985
Iteration 350: Loss = -12185.547561436037
Iteration 360: Loss = -12185.547433766502
Iteration 370: Loss = -12185.54728567128
Iteration 380: Loss = -12185.547175765341
Iteration 390: Loss = -12185.54710828625
Iteration 400: Loss = -12185.547006721343
Iteration 410: Loss = -12185.547021534454
1
Iteration 420: Loss = -12185.546923164222
Iteration 430: Loss = -12185.546899243094
Iteration 440: Loss = -12185.546890872638
Iteration 450: Loss = -12185.54684897807
Iteration 460: Loss = -12185.546856975685
1
Iteration 470: Loss = -12185.54683851135
Iteration 480: Loss = -12185.546862777084
1
Iteration 490: Loss = -12185.54685320843
2
Iteration 500: Loss = -12185.546880673972
3
Stopping early at iteration 500 due to no improvement.
pi: tensor([[0.2038, 0.7962],
        [0.1703, 0.8297]], dtype=torch.float64)
alpha: tensor([0.1759, 0.8241])
beta: tensor([[[0.1845, 0.1969],
         [0.7477, 0.1940]],

        [[0.5436, 0.1838],
         [0.3660, 0.3303]],

        [[0.3642, 0.1917],
         [0.1188, 0.2299]],

        [[0.8119, 0.1858],
         [0.2069, 0.6493]],

        [[0.2955, 0.1878],
         [0.5597, 0.8124]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -12271.168413738977
Iteration 10: Loss = -12185.864475873017
Iteration 20: Loss = -12185.771210348683
Iteration 30: Loss = -12185.733807286444
Iteration 40: Loss = -12185.721248530419
Iteration 50: Loss = -12185.717680026166
Iteration 60: Loss = -12185.714906128676
Iteration 70: Loss = -12185.711040577566
Iteration 80: Loss = -12185.704479683032
Iteration 90: Loss = -12185.691534271964
Iteration 100: Loss = -12185.668412604577
Iteration 110: Loss = -12185.645773319236
Iteration 120: Loss = -12185.633762061932
Iteration 130: Loss = -12185.626059128273
Iteration 140: Loss = -12185.619652347385
Iteration 150: Loss = -12185.614011637015
Iteration 160: Loss = -12185.608935634276
Iteration 170: Loss = -12185.604416776492
Iteration 180: Loss = -12185.600439585694
Iteration 190: Loss = -12185.596894490503
Iteration 200: Loss = -12185.593694912603
Iteration 210: Loss = -12185.590872084687
Iteration 220: Loss = -12185.588335958364
Iteration 230: Loss = -12185.586010430015
Iteration 240: Loss = -12185.583971537877
Iteration 250: Loss = -12185.58204995608
Iteration 260: Loss = -12185.580382690508
Iteration 270: Loss = -12185.578845460255
Iteration 280: Loss = -12185.577424706484
Iteration 290: Loss = -12185.57611610477
Iteration 300: Loss = -12185.574939133972
Iteration 310: Loss = -12185.573797651712
Iteration 320: Loss = -12185.572774853414
Iteration 330: Loss = -12185.57180388949
Iteration 340: Loss = -12185.570894075678
Iteration 350: Loss = -12185.569952522506
Iteration 360: Loss = -12185.569100381217
Iteration 370: Loss = -12185.568304389142
Iteration 380: Loss = -12185.567526568393
Iteration 390: Loss = -12185.56678084302
Iteration 400: Loss = -12185.566015600001
Iteration 410: Loss = -12185.565248432005
Iteration 420: Loss = -12185.564568959499
Iteration 430: Loss = -12185.563858917318
Iteration 440: Loss = -12185.563173029706
Iteration 450: Loss = -12185.562466881016
Iteration 460: Loss = -12185.561785314096
Iteration 470: Loss = -12185.56116947622
Iteration 480: Loss = -12185.56055744397
Iteration 490: Loss = -12185.559886179693
Iteration 500: Loss = -12185.559286830565
Iteration 510: Loss = -12185.558705271213
Iteration 520: Loss = -12185.558099852187
Iteration 530: Loss = -12185.557508167674
Iteration 540: Loss = -12185.556975001558
Iteration 550: Loss = -12185.556416625192
Iteration 560: Loss = -12185.555889323114
Iteration 570: Loss = -12185.555371923405
Iteration 580: Loss = -12185.554914242479
Iteration 590: Loss = -12185.554372325323
Iteration 600: Loss = -12185.554003543946
Iteration 610: Loss = -12185.553542661577
Iteration 620: Loss = -12185.553140434671
Iteration 630: Loss = -12185.552758062753
Iteration 640: Loss = -12185.552396913934
Iteration 650: Loss = -12185.552035025836
Iteration 660: Loss = -12185.551696967656
Iteration 670: Loss = -12185.551402201732
Iteration 680: Loss = -12185.551066832075
Iteration 690: Loss = -12185.550842012883
Iteration 700: Loss = -12185.550573215876
Iteration 710: Loss = -12185.550363432441
Iteration 720: Loss = -12185.550129036923
Iteration 730: Loss = -12185.549978116966
Iteration 740: Loss = -12185.549779507435
Iteration 750: Loss = -12185.549622529632
Iteration 760: Loss = -12185.549458114749
Iteration 770: Loss = -12185.549331062275
Iteration 780: Loss = -12185.54919528968
Iteration 790: Loss = -12185.54904237419
Iteration 800: Loss = -12185.548957419973
Iteration 810: Loss = -12185.548890582502
Iteration 820: Loss = -12185.548784485796
Iteration 830: Loss = -12185.548727517002
Iteration 840: Loss = -12185.548640053032
Iteration 850: Loss = -12185.548599687156
Iteration 860: Loss = -12185.548560919206
Iteration 870: Loss = -12185.548525624648
Iteration 880: Loss = -12185.548477177104
Iteration 890: Loss = -12185.54843633141
Iteration 900: Loss = -12185.54843357642
Iteration 910: Loss = -12185.548396127379
Iteration 920: Loss = -12185.548410305795
1
Iteration 930: Loss = -12185.54839484256
Iteration 940: Loss = -12185.548389587237
Iteration 950: Loss = -12185.548401414502
1
Iteration 960: Loss = -12185.548361507012
Iteration 970: Loss = -12185.548397297154
1
Iteration 980: Loss = -12185.548400682745
2
Iteration 990: Loss = -12185.548464347812
3
Stopping early at iteration 990 due to no improvement.
pi: tensor([[0.7687, 0.2313],
        [0.7930, 0.2070]], dtype=torch.float64)
alpha: tensor([0.7744, 0.2256])
beta: tensor([[[0.1939, 0.1968],
         [0.3561, 0.1870]],

        [[0.5453, 0.1858],
         [0.2529, 0.3216]],

        [[0.2192, 0.1923],
         [0.7157, 0.9480]],

        [[0.4995, 0.1876],
         [0.6193, 0.8684]],

        [[0.6745, 0.1896],
         [0.7163, 0.9825]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21126.511714029977
Iteration 100: Loss = -12185.855652050675
Iteration 200: Loss = -12185.615950685791
Iteration 300: Loss = -12185.552466309637
Iteration 400: Loss = -12185.528293059995
Iteration 500: Loss = -12185.517403471846
Iteration 600: Loss = -12185.511489804176
Iteration 700: Loss = -12185.50778068549
Iteration 800: Loss = -12185.505044310583
Iteration 900: Loss = -12185.502944452059
Iteration 1000: Loss = -12185.501036922997
Iteration 1100: Loss = -12185.499281441324
Iteration 1200: Loss = -12185.49748516437
Iteration 1300: Loss = -12185.495645313718
Iteration 1400: Loss = -12185.493646989107
Iteration 1500: Loss = -12185.491333999576
Iteration 1600: Loss = -12185.488795278443
Iteration 1700: Loss = -12185.485835144993
Iteration 1800: Loss = -12185.482290572701
Iteration 1900: Loss = -12185.477976456325
Iteration 2000: Loss = -12185.47265918269
Iteration 2100: Loss = -12185.465870332508
Iteration 2200: Loss = -12185.456981082132
Iteration 2300: Loss = -12185.444901157183
Iteration 2400: Loss = -12185.427815215684
Iteration 2500: Loss = -12185.402700056622
Iteration 2600: Loss = -12185.36509507588
Iteration 2700: Loss = -12185.311629757534
Iteration 2800: Loss = -12185.247762747915
Iteration 2900: Loss = -12185.189744461892
Iteration 3000: Loss = -12185.14802265279
Iteration 3100: Loss = -12185.121052478384
Iteration 3200: Loss = -12185.103622005174
Iteration 3300: Loss = -12185.091911292651
Iteration 3400: Loss = -12185.083691951948
Iteration 3500: Loss = -12185.077688507758
Iteration 3600: Loss = -12185.07317436802
Iteration 3700: Loss = -12185.069671245921
Iteration 3800: Loss = -12185.066938550754
Iteration 3900: Loss = -12185.064770087998
Iteration 4000: Loss = -12185.063084980995
Iteration 4100: Loss = -12185.061692431986
Iteration 4200: Loss = -12185.06062943957
Iteration 4300: Loss = -12185.05970550808
Iteration 4400: Loss = -12185.0590364038
Iteration 4500: Loss = -12185.058478108363
Iteration 4600: Loss = -12185.057994730445
Iteration 4700: Loss = -12185.057602349083
Iteration 4800: Loss = -12185.057267860966
Iteration 4900: Loss = -12185.056955827038
Iteration 5000: Loss = -12185.056736077397
Iteration 5100: Loss = -12185.056508773041
Iteration 5200: Loss = -12185.056339357425
Iteration 5300: Loss = -12185.056157853833
Iteration 5400: Loss = -12185.056065226418
Iteration 5500: Loss = -12185.05591827251
Iteration 5600: Loss = -12185.055812042583
Iteration 5700: Loss = -12185.05571147762
Iteration 5800: Loss = -12185.055635909583
Iteration 5900: Loss = -12185.055557619513
Iteration 6000: Loss = -12185.055472845055
Iteration 6100: Loss = -12185.055392823784
Iteration 6200: Loss = -12185.055342892905
Iteration 6300: Loss = -12185.055370769232
1
Iteration 6400: Loss = -12185.05525005106
Iteration 6500: Loss = -12185.055421877525
1
Iteration 6600: Loss = -12185.05515564621
Iteration 6700: Loss = -12185.05510055727
Iteration 6800: Loss = -12185.055216857649
1
Iteration 6900: Loss = -12185.055034379628
Iteration 7000: Loss = -12185.055035865778
1
Iteration 7100: Loss = -12185.055373213005
2
Iteration 7200: Loss = -12185.055002786226
Iteration 7300: Loss = -12185.054973845348
Iteration 7400: Loss = -12185.055017965742
1
Iteration 7500: Loss = -12185.054903782038
Iteration 7600: Loss = -12185.054871490136
Iteration 7700: Loss = -12185.055243353556
1
Iteration 7800: Loss = -12185.054879671765
2
Iteration 7900: Loss = -12185.054926235845
3
Iteration 8000: Loss = -12185.05546763524
4
Iteration 8100: Loss = -12185.05480672677
Iteration 8200: Loss = -12185.055450138596
1
Iteration 8300: Loss = -12185.05616682021
2
Iteration 8400: Loss = -12185.075674931044
3
Iteration 8500: Loss = -12185.055270256758
4
Iteration 8600: Loss = -12185.055653254503
5
Stopping early at iteration 8600 due to no improvement.
pi: tensor([[0.9799, 0.0201],
        [0.9671, 0.0329]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0178, 0.9822], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1931, 0.1986],
         [0.6422, 0.1990]],

        [[0.6096, 0.1810],
         [0.5933, 0.5283]],

        [[0.7103, 0.2040],
         [0.6793, 0.5291]],

        [[0.5364, 0.1861],
         [0.6811, 0.7212]],

        [[0.6414, 0.2257],
         [0.6264, 0.6401]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0008340614720394737
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20535.086459253947
Iteration 100: Loss = -12186.011828924193
Iteration 200: Loss = -12185.587747620566
Iteration 300: Loss = -12185.54634011937
Iteration 400: Loss = -12185.529770502475
Iteration 500: Loss = -12185.51954496257
Iteration 600: Loss = -12185.51222890471
Iteration 700: Loss = -12185.506303953907
Iteration 800: Loss = -12185.501097839478
Iteration 900: Loss = -12185.49631696072
Iteration 1000: Loss = -12185.491673518049
Iteration 1100: Loss = -12185.487096867411
Iteration 1200: Loss = -12185.482495960456
Iteration 1300: Loss = -12185.477814332178
Iteration 1400: Loss = -12185.472923841335
Iteration 1500: Loss = -12185.46764819011
Iteration 1600: Loss = -12185.461836861099
Iteration 1700: Loss = -12185.455393708611
Iteration 1800: Loss = -12185.447965408403
Iteration 1900: Loss = -12185.439294855863
Iteration 2000: Loss = -12185.42887229417
Iteration 2100: Loss = -12185.416139369792
Iteration 2200: Loss = -12185.400217735263
Iteration 2300: Loss = -12185.37995193855
Iteration 2400: Loss = -12185.354117016386
Iteration 2500: Loss = -12185.321415370146
Iteration 2600: Loss = -12185.282306826004
Iteration 2700: Loss = -12185.240335398941
Iteration 2800: Loss = -12185.20099368436
Iteration 2900: Loss = -12185.168154741188
Iteration 3000: Loss = -12185.14278717988
Iteration 3100: Loss = -12185.123949040524
Iteration 3200: Loss = -12185.110013536509
Iteration 3300: Loss = -12185.099628096614
Iteration 3400: Loss = -12185.091751670185
Iteration 3500: Loss = -12185.085608042584
Iteration 3600: Loss = -12185.080732175998
Iteration 3700: Loss = -12185.076800573204
Iteration 3800: Loss = -12185.07364380079
Iteration 3900: Loss = -12185.07089389316
Iteration 4000: Loss = -12185.068649544308
Iteration 4100: Loss = -12185.066751036136
Iteration 4200: Loss = -12185.065129220839
Iteration 4300: Loss = -12185.063805133741
Iteration 4400: Loss = -12185.062625612409
Iteration 4500: Loss = -12185.061612660087
Iteration 4600: Loss = -12185.06077812325
Iteration 4700: Loss = -12185.060024782686
Iteration 4800: Loss = -12185.059369846109
Iteration 4900: Loss = -12185.058841087506
Iteration 5000: Loss = -12185.05833370547
Iteration 5100: Loss = -12185.05790035036
Iteration 5200: Loss = -12185.057601653965
Iteration 5300: Loss = -12185.057265212023
Iteration 5400: Loss = -12185.057026946004
Iteration 5500: Loss = -12185.056765822763
Iteration 5600: Loss = -12185.0565958019
Iteration 5700: Loss = -12185.056378260555
Iteration 5800: Loss = -12185.05621376735
Iteration 5900: Loss = -12185.056092295717
Iteration 6000: Loss = -12185.055971636182
Iteration 6100: Loss = -12185.055868968553
Iteration 6200: Loss = -12185.055746937216
Iteration 6300: Loss = -12185.05567068654
Iteration 6400: Loss = -12185.055601268046
Iteration 6500: Loss = -12185.055487006834
Iteration 6600: Loss = -12185.055446695764
Iteration 6700: Loss = -12185.055399031415
Iteration 6800: Loss = -12185.055337282474
Iteration 6900: Loss = -12185.055265044519
Iteration 7000: Loss = -12185.055327982473
1
Iteration 7100: Loss = -12185.05519490306
Iteration 7200: Loss = -12185.055173276061
Iteration 7300: Loss = -12185.055127218344
Iteration 7400: Loss = -12185.055076007839
Iteration 7500: Loss = -12185.055094735242
1
Iteration 7600: Loss = -12185.055082530973
2
Iteration 7700: Loss = -12185.08198812641
3
Iteration 7800: Loss = -12185.054984225326
Iteration 7900: Loss = -12185.054955470385
Iteration 8000: Loss = -12185.06411942035
1
Iteration 8100: Loss = -12185.054926563162
Iteration 8200: Loss = -12185.054891299085
Iteration 8300: Loss = -12185.076840810942
1
Iteration 8400: Loss = -12185.054846947738
Iteration 8500: Loss = -12185.055974332176
1
Iteration 8600: Loss = -12185.072440033731
2
Iteration 8700: Loss = -12185.054903744462
3
Iteration 8800: Loss = -12185.054987064223
4
Iteration 8900: Loss = -12185.056201415895
5
Stopping early at iteration 8900 due to no improvement.
pi: tensor([[0.9798, 0.0202],
        [0.9668, 0.0332]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0200, 0.9800], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1930, 0.1985],
         [0.6182, 0.1989]],

        [[0.7152, 0.1811],
         [0.6938, 0.5482]],

        [[0.5415, 0.2040],
         [0.6902, 0.6085]],

        [[0.6546, 0.1861],
         [0.5259, 0.6941]],

        [[0.7140, 0.2256],
         [0.6232, 0.6594]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0008340614720394737
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22055.21346093289
Iteration 100: Loss = -12186.726345748562
Iteration 200: Loss = -12185.94805190891
Iteration 300: Loss = -12185.731325126333
Iteration 400: Loss = -12185.652715107091
Iteration 500: Loss = -12185.615812092385
Iteration 600: Loss = -12185.595206206957
Iteration 700: Loss = -12185.582317686001
Iteration 800: Loss = -12185.573337893173
Iteration 900: Loss = -12185.566605946822
Iteration 1000: Loss = -12185.56125732856
Iteration 1100: Loss = -12185.556751402288
Iteration 1200: Loss = -12185.552883233651
Iteration 1300: Loss = -12185.549453569072
Iteration 1400: Loss = -12185.546393302066
Iteration 1500: Loss = -12185.543546423669
Iteration 1600: Loss = -12185.540937368834
Iteration 1700: Loss = -12185.53843458439
Iteration 1800: Loss = -12185.536011037975
Iteration 1900: Loss = -12185.533672027957
Iteration 2000: Loss = -12185.531298132926
Iteration 2100: Loss = -12185.528900409323
Iteration 2200: Loss = -12185.526446592003
Iteration 2300: Loss = -12185.523818600985
Iteration 2400: Loss = -12185.520985663927
Iteration 2500: Loss = -12185.517807732196
Iteration 2600: Loss = -12185.514265231206
Iteration 2700: Loss = -12185.51015545988
Iteration 2800: Loss = -12185.505275326044
Iteration 2900: Loss = -12185.499448899496
Iteration 3000: Loss = -12185.492109193072
Iteration 3100: Loss = -12185.482929358435
Iteration 3200: Loss = -12185.470671081417
Iteration 3300: Loss = -12185.45346275214
Iteration 3400: Loss = -12185.427553377778
Iteration 3500: Loss = -12185.3867620892
Iteration 3600: Loss = -12185.326657041966
Iteration 3700: Loss = -12185.25461279226
Iteration 3800: Loss = -12185.203536630219
Iteration 3900: Loss = -12185.156179618003
Iteration 4000: Loss = -12185.132044532658
Iteration 4100: Loss = -12185.116345829669
Iteration 4200: Loss = -12185.105652607985
Iteration 4300: Loss = -12185.09787819527
Iteration 4400: Loss = -12185.09184543855
Iteration 4500: Loss = -12185.086969149468
Iteration 4600: Loss = -12185.082747624068
Iteration 4700: Loss = -12185.078973315674
Iteration 4800: Loss = -12185.07560129344
Iteration 4900: Loss = -12185.072474344288
Iteration 5000: Loss = -12185.06962952905
Iteration 5100: Loss = -12185.067090492541
Iteration 5200: Loss = -12185.064867322122
Iteration 5300: Loss = -12185.062906322833
Iteration 5400: Loss = -12185.061340650334
Iteration 5500: Loss = -12185.060093237133
Iteration 5600: Loss = -12185.059072997938
Iteration 5700: Loss = -12185.058277625336
Iteration 5800: Loss = -12185.057676879804
Iteration 5900: Loss = -12185.057240720582
Iteration 6000: Loss = -12185.05689653141
Iteration 6100: Loss = -12185.056602497247
Iteration 6200: Loss = -12185.056350415614
Iteration 6300: Loss = -12185.05617555818
Iteration 6400: Loss = -12185.056020997014
Iteration 6500: Loss = -12185.055893747036
Iteration 6600: Loss = -12185.055753057595
Iteration 6700: Loss = -12185.055669626377
Iteration 6800: Loss = -12185.055579576276
Iteration 6900: Loss = -12185.055485332885
Iteration 7000: Loss = -12185.055424848802
Iteration 7100: Loss = -12185.055532647893
1
Iteration 7200: Loss = -12185.055314958565
Iteration 7300: Loss = -12185.05553489917
1
Iteration 7400: Loss = -12185.055207353824
Iteration 7500: Loss = -12185.055155029817
Iteration 7600: Loss = -12185.055102183707
Iteration 7700: Loss = -12185.055186624213
1
Iteration 7800: Loss = -12185.057831534228
2
Iteration 7900: Loss = -12185.082434882828
3
Iteration 8000: Loss = -12185.105930554799
4
Iteration 8100: Loss = -12185.055023455829
Iteration 8200: Loss = -12185.055097402379
1
Iteration 8300: Loss = -12185.070368072893
2
Iteration 8400: Loss = -12185.054923175248
Iteration 8500: Loss = -12185.057393576002
1
Iteration 8600: Loss = -12185.221989167212
2
Iteration 8700: Loss = -12185.054872992428
Iteration 8800: Loss = -12185.143324721026
1
Iteration 8900: Loss = -12185.054857600031
Iteration 9000: Loss = -12185.06846794742
1
Iteration 9100: Loss = -12185.055940532218
2
Iteration 9200: Loss = -12185.060494411275
3
Iteration 9300: Loss = -12185.055461773829
4
Iteration 9400: Loss = -12185.055655424583
5
Stopping early at iteration 9400 due to no improvement.
pi: tensor([[0.0332, 0.9668],
        [0.0202, 0.9798]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9810, 0.0190], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1985, 0.1985],
         [0.5299, 0.1929]],

        [[0.5686, 0.1811],
         [0.7028, 0.6100]],

        [[0.5394, 0.2040],
         [0.6735, 0.5024]],

        [[0.7144, 0.1861],
         [0.5464, 0.5361]],

        [[0.5728, 0.2257],
         [0.5113, 0.6304]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0008340614720394737
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21833.419280012757
Iteration 100: Loss = -12186.658730561687
Iteration 200: Loss = -12185.84995222324
Iteration 300: Loss = -12185.658612356481
Iteration 400: Loss = -12185.59417873069
Iteration 500: Loss = -12185.566795943483
Iteration 600: Loss = -12185.55240277591
Iteration 700: Loss = -12185.54325908846
Iteration 800: Loss = -12185.536417903719
Iteration 900: Loss = -12185.530841817625
Iteration 1000: Loss = -12185.525928403094
Iteration 1100: Loss = -12185.521404866848
Iteration 1200: Loss = -12185.517194084605
Iteration 1300: Loss = -12185.513107875626
Iteration 1400: Loss = -12185.509073004176
Iteration 1500: Loss = -12185.505005856941
Iteration 1600: Loss = -12185.50082260877
Iteration 1700: Loss = -12185.496421123871
Iteration 1800: Loss = -12185.491722923587
Iteration 1900: Loss = -12185.486618898689
Iteration 2000: Loss = -12185.480817978952
Iteration 2100: Loss = -12185.474141563465
Iteration 2200: Loss = -12185.46628590778
Iteration 2300: Loss = -12185.456757778667
Iteration 2400: Loss = -12185.444873620896
Iteration 2500: Loss = -12185.429574526668
Iteration 2600: Loss = -12185.409518638071
Iteration 2700: Loss = -12185.383025244193
Iteration 2800: Loss = -12185.3485907866
Iteration 2900: Loss = -12185.306619928497
Iteration 3000: Loss = -12185.26181006338
Iteration 3100: Loss = -12185.220734005306
Iteration 3200: Loss = -12185.18727407827
Iteration 3300: Loss = -12185.161584058606
Iteration 3400: Loss = -12185.142424650314
Iteration 3500: Loss = -12185.12826047936
Iteration 3600: Loss = -12185.12154225098
Iteration 3700: Loss = -12185.109645469232
Iteration 3800: Loss = -12185.103397865356
Iteration 3900: Loss = -12185.098384121984
Iteration 4000: Loss = -12185.094275701807
Iteration 4100: Loss = -12185.090820360905
Iteration 4200: Loss = -12185.087846348031
Iteration 4300: Loss = -12185.08526986561
Iteration 4400: Loss = -12185.083000596618
Iteration 4500: Loss = -12185.080883512152
Iteration 4600: Loss = -12185.078976919542
Iteration 4700: Loss = -12185.07720003362
Iteration 4800: Loss = -12185.075489776304
Iteration 4900: Loss = -12185.073794029639
Iteration 5000: Loss = -12185.072073905549
Iteration 5100: Loss = -12185.070288071029
Iteration 5200: Loss = -12185.068230695317
Iteration 5300: Loss = -12185.065988334005
Iteration 5400: Loss = -12185.063535251453
Iteration 5500: Loss = -12185.061503804176
Iteration 5600: Loss = -12185.060091779766
Iteration 5700: Loss = -12185.059168643204
Iteration 5800: Loss = -12185.05854970819
Iteration 5900: Loss = -12185.058005760344
Iteration 6000: Loss = -12185.057530882426
Iteration 6100: Loss = -12185.057119559182
Iteration 6200: Loss = -12185.056815948174
Iteration 6300: Loss = -12185.0565661066
Iteration 6400: Loss = -12185.056350276052
Iteration 6500: Loss = -12185.056096487846
Iteration 6600: Loss = -12185.055991778769
Iteration 6700: Loss = -12185.05586893897
Iteration 6800: Loss = -12185.055730278247
Iteration 6900: Loss = -12185.055582318753
Iteration 7000: Loss = -12185.05556068231
Iteration 7100: Loss = -12185.056644877795
1
Iteration 7200: Loss = -12185.056500363333
2
Iteration 7300: Loss = -12185.055330346226
Iteration 7400: Loss = -12185.055435451044
1
Iteration 7500: Loss = -12185.05577829886
2
Iteration 7600: Loss = -12185.056289062648
3
Iteration 7700: Loss = -12185.15500009837
4
Iteration 7800: Loss = -12185.055126499858
Iteration 7900: Loss = -12185.115874198524
1
Iteration 8000: Loss = -12185.055030390142
Iteration 8100: Loss = -12185.056538697376
1
Iteration 8200: Loss = -12185.05498275959
Iteration 8300: Loss = -12185.054930047632
Iteration 8400: Loss = -12185.064207994274
1
Iteration 8500: Loss = -12185.076596210978
2
Iteration 8600: Loss = -12185.054914658247
Iteration 8700: Loss = -12185.056847180622
1
Iteration 8800: Loss = -12185.054846652443
Iteration 8900: Loss = -12185.055517614432
1
Iteration 9000: Loss = -12185.054851407322
2
Iteration 9100: Loss = -12185.055603248617
3
Iteration 9200: Loss = -12185.05666518554
4
Iteration 9300: Loss = -12185.05536963453
5
Stopping early at iteration 9300 due to no improvement.
pi: tensor([[0.9798, 0.0202],
        [0.9666, 0.0334]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0202, 0.9798], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1932, 0.1985],
         [0.5177, 0.1989]],

        [[0.5664, 0.1811],
         [0.6465, 0.6552]],

        [[0.5666, 0.2040],
         [0.6446, 0.6096]],

        [[0.5686, 0.1862],
         [0.6320, 0.6565]],

        [[0.6817, 0.2256],
         [0.5619, 0.5529]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0008340614720394737
Average Adjusted Rand Index: 0.0
11752.668410444872
[-0.0008340614720394737, -0.0008340614720394737, -0.0008340614720394737, -0.0008340614720394737] [0.0, 0.0, 0.0, 0.0] [12185.055653254503, 12185.056201415895, 12185.055655424583, 12185.05536963453]
-----------------------------------------------------------------------------------------
This iteration is 2
True Objective function: Loss = -11957.535052382842
Iteration 0: Loss = -12419.862969939224
Iteration 10: Loss = -12385.902982790438
Iteration 20: Loss = -12385.811218271636
Iteration 30: Loss = -12385.636008866355
Iteration 40: Loss = -12384.713448952272
Iteration 50: Loss = -12384.6966318783
Iteration 60: Loss = -12384.69624805492
Iteration 70: Loss = -12384.696202882711
Iteration 80: Loss = -12384.696053571735
Iteration 90: Loss = -12384.695900141054
Iteration 100: Loss = -12384.695786005119
Iteration 110: Loss = -12384.695689704951
Iteration 120: Loss = -12384.6955861202
Iteration 130: Loss = -12384.69547629942
Iteration 140: Loss = -12384.695369040852
Iteration 150: Loss = -12384.695305861216
Iteration 160: Loss = -12384.69522172551
Iteration 170: Loss = -12384.695163325703
Iteration 180: Loss = -12384.695094424414
Iteration 190: Loss = -12384.695070695263
Iteration 200: Loss = -12384.695010576803
Iteration 210: Loss = -12384.69496477033
Iteration 220: Loss = -12384.694960053586
Iteration 230: Loss = -12384.6949662392
1
Iteration 240: Loss = -12384.694930400217
Iteration 250: Loss = -12384.694915217302
Iteration 260: Loss = -12384.694918175737
1
Iteration 270: Loss = -12384.694930022817
2
Iteration 280: Loss = -12384.694863086952
Iteration 290: Loss = -12384.694895320281
1
Iteration 300: Loss = -12384.69492807775
2
Iteration 310: Loss = -12384.694899398923
3
Stopping early at iteration 310 due to no improvement.
pi: tensor([[0.9628, 0.0372],
        [0.9815, 0.0185]], dtype=torch.float64)
alpha: tensor([0.9636, 0.0364])
beta: tensor([[[0.1979, 0.1169],
         [0.9065, 0.2210]],

        [[0.0214, 0.1930],
         [0.5606, 0.0697]],

        [[0.2543, 0.1999],
         [0.5819, 0.2630]],

        [[0.6951, 0.2326],
         [0.4598, 0.7858]],

        [[0.8547, 0.2518],
         [0.8895, 0.2833]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.00776683106263838
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00016110136691517576
Average Adjusted Rand Index: 0.001553366212527676
Iteration 0: Loss = -12336.389111253968
Iteration 10: Loss = -11950.055098324923
Iteration 20: Loss = -11950.055064780929
Iteration 30: Loss = -11950.055064780927
Iteration 40: Loss = -11950.055064780927
1
Iteration 50: Loss = -11950.055064780927
2
Iteration 60: Loss = -11950.055064780927
3
Stopping early at iteration 60 due to no improvement.
pi: tensor([[0.7774, 0.2226],
        [0.2493, 0.7507]], dtype=torch.float64)
alpha: tensor([0.4978, 0.5022])
beta: tensor([[[0.2911, 0.1064],
         [0.6840, 0.2881]],

        [[0.3755, 0.1044],
         [0.4805, 0.1273]],

        [[0.9524, 0.1036],
         [0.4276, 0.1594]],

        [[0.6061, 0.1074],
         [0.6899, 0.7226]],

        [[0.5513, 0.1069],
         [0.5743, 0.5877]]], dtype=torch.float64)
time is 0
tensor([0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 4
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9760961002009502
Average Adjusted Rand Index: 0.975998244592595
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21762.415988636087
Iteration 100: Loss = -12387.148402782133
Iteration 200: Loss = -12386.573604734791
Iteration 300: Loss = -12386.42442626144
Iteration 400: Loss = -12386.28998538533
Iteration 500: Loss = -12385.963427024426
Iteration 600: Loss = -12385.412274188782
Iteration 700: Loss = -12385.161031132948
Iteration 800: Loss = -12385.02916171057
Iteration 900: Loss = -12384.934133406106
Iteration 1000: Loss = -12384.850847800644
Iteration 1100: Loss = -12384.769836545103
Iteration 1200: Loss = -12384.684963830074
Iteration 1300: Loss = -12384.585101619306
Iteration 1400: Loss = -12384.454334219005
Iteration 1500: Loss = -12384.328717690549
Iteration 1600: Loss = -12384.243129432632
Iteration 1700: Loss = -12384.188307312088
Iteration 1800: Loss = -12384.153723116015
Iteration 1900: Loss = -12384.13392623522
Iteration 2000: Loss = -12384.121395431775
Iteration 2100: Loss = -12384.113003120405
Iteration 2200: Loss = -12384.107527031394
Iteration 2300: Loss = -12384.103809260292
Iteration 2400: Loss = -12384.10077504708
Iteration 2500: Loss = -12384.098421958974
Iteration 2600: Loss = -12384.096388330081
Iteration 2700: Loss = -12384.094582486534
Iteration 2800: Loss = -12384.093191422342
Iteration 2900: Loss = -12384.092193397517
Iteration 3000: Loss = -12384.093031076272
1
Iteration 3100: Loss = -12384.090623110063
Iteration 3200: Loss = -12384.089926809645
Iteration 3300: Loss = -12384.089206442948
Iteration 3400: Loss = -12384.0883870462
Iteration 3500: Loss = -12384.08757590491
Iteration 3600: Loss = -12384.086686285154
Iteration 3700: Loss = -12384.085899956057
Iteration 3800: Loss = -12384.088441499847
1
Iteration 3900: Loss = -12384.084955570985
Iteration 4000: Loss = -12384.084726506811
Iteration 4100: Loss = -12384.084719604385
Iteration 4200: Loss = -12384.084506479398
Iteration 4300: Loss = -12384.084383886264
Iteration 4400: Loss = -12384.084344344043
Iteration 4500: Loss = -12384.084284183125
Iteration 4600: Loss = -12384.084400168531
1
Iteration 4700: Loss = -12384.084131110127
Iteration 4800: Loss = -12384.084134593786
1
Iteration 4900: Loss = -12384.084103489018
Iteration 5000: Loss = -12384.084016444107
Iteration 5100: Loss = -12384.08398051457
Iteration 5200: Loss = -12384.084000303297
1
Iteration 5300: Loss = -12384.083975341291
Iteration 5400: Loss = -12384.083910089665
Iteration 5500: Loss = -12384.083949014095
1
Iteration 5600: Loss = -12384.083869550723
Iteration 5700: Loss = -12384.083834855295
Iteration 5800: Loss = -12384.083816362989
Iteration 5900: Loss = -12384.083806932993
Iteration 6000: Loss = -12384.08483158388
1
Iteration 6100: Loss = -12384.083725802873
Iteration 6200: Loss = -12384.083666385954
Iteration 6300: Loss = -12384.08367936957
1
Iteration 6400: Loss = -12384.083670161042
2
Iteration 6500: Loss = -12384.083618204426
Iteration 6600: Loss = -12384.083812422337
1
Iteration 6700: Loss = -12384.083664493119
2
Iteration 6800: Loss = -12384.0836658663
3
Iteration 6900: Loss = -12384.083652891006
4
Iteration 7000: Loss = -12384.08370074077
5
Stopping early at iteration 7000 due to no improvement.
pi: tensor([[0.9973, 0.0027],
        [0.5135, 0.4865]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9303, 0.0697], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2023, 0.1288],
         [0.5973, 0.1331]],

        [[0.5957, 0.1730],
         [0.6744, 0.6895]],

        [[0.6579, 0.1950],
         [0.6091, 0.6449]],

        [[0.5951, 0.2108],
         [0.7266, 0.6527]],

        [[0.6427, 0.2584],
         [0.6645, 0.5191]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.007493599095377873
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0001304368371122338
Average Adjusted Rand Index: 0.0014987198190755746
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23892.91216476647
Iteration 100: Loss = -12386.972561875036
Iteration 200: Loss = -12386.389928319146
Iteration 300: Loss = -12386.275314128694
Iteration 400: Loss = -12386.205385469742
Iteration 500: Loss = -12386.140376310259
Iteration 600: Loss = -12386.07022846757
Iteration 700: Loss = -12385.989987013372
Iteration 800: Loss = -12385.89749568629
Iteration 900: Loss = -12385.797491173626
Iteration 1000: Loss = -12385.70282079526
Iteration 1100: Loss = -12385.622057758112
Iteration 1200: Loss = -12385.546877347353
Iteration 1300: Loss = -12385.455237608794
Iteration 1400: Loss = -12385.236950381883
Iteration 1500: Loss = -12384.677915259226
Iteration 1600: Loss = -12384.496124492593
Iteration 1700: Loss = -12384.429989809856
Iteration 1800: Loss = -12384.402153397328
Iteration 1900: Loss = -12384.38924290724
Iteration 2000: Loss = -12384.382814478493
Iteration 2100: Loss = -12384.379319514986
Iteration 2200: Loss = -12384.377180972431
Iteration 2300: Loss = -12384.375767103602
Iteration 2400: Loss = -12384.374786760936
Iteration 2500: Loss = -12384.374023306886
Iteration 2600: Loss = -12384.373481280605
Iteration 2700: Loss = -12384.372986187354
Iteration 2800: Loss = -12384.372600702009
Iteration 2900: Loss = -12384.37226285643
Iteration 3000: Loss = -12384.3719795392
Iteration 3100: Loss = -12384.371689352274
Iteration 3200: Loss = -12384.371413262174
Iteration 3300: Loss = -12384.371229330545
Iteration 3400: Loss = -12384.37098471716
Iteration 3500: Loss = -12384.370794818067
Iteration 3600: Loss = -12384.370612835839
Iteration 3700: Loss = -12384.370409735817
Iteration 3800: Loss = -12384.370141933017
Iteration 3900: Loss = -12384.370003410557
Iteration 4000: Loss = -12384.369807627347
Iteration 4100: Loss = -12384.36963419667
Iteration 4200: Loss = -12384.369473378938
Iteration 4300: Loss = -12384.369260580586
Iteration 4400: Loss = -12384.369103820234
Iteration 4500: Loss = -12384.368918769753
Iteration 4600: Loss = -12384.368774708877
Iteration 4700: Loss = -12384.368596326785
Iteration 4800: Loss = -12384.368410749352
Iteration 4900: Loss = -12384.368251596163
Iteration 5000: Loss = -12384.368067009333
Iteration 5100: Loss = -12384.36789895107
Iteration 5200: Loss = -12384.367762167058
Iteration 5300: Loss = -12384.36757449969
Iteration 5400: Loss = -12384.367420320134
Iteration 5500: Loss = -12384.367289429336
Iteration 5600: Loss = -12384.36715571853
Iteration 5700: Loss = -12384.366959951234
Iteration 5800: Loss = -12384.372681492398
1
Iteration 5900: Loss = -12384.366697400901
Iteration 6000: Loss = -12384.36660171502
Iteration 6100: Loss = -12384.366460063156
Iteration 6200: Loss = -12384.366321248694
Iteration 6300: Loss = -12384.371802014157
1
Iteration 6400: Loss = -12384.366078175515
Iteration 6500: Loss = -12384.365960037681
Iteration 6600: Loss = -12384.368843786853
1
Iteration 6700: Loss = -12384.365762915737
Iteration 6800: Loss = -12384.365653804856
Iteration 6900: Loss = -12384.36560470357
Iteration 7000: Loss = -12384.365443688741
Iteration 7100: Loss = -12384.365351819599
Iteration 7200: Loss = -12384.365303924367
Iteration 7300: Loss = -12384.365169433022
Iteration 7400: Loss = -12384.365121481758
Iteration 7500: Loss = -12384.366621377654
1
Iteration 7600: Loss = -12384.365975665967
2
Iteration 7700: Loss = -12384.364903756985
Iteration 7800: Loss = -12384.404124263017
1
Iteration 7900: Loss = -12384.364820262854
Iteration 8000: Loss = -12384.36487102896
1
Iteration 8100: Loss = -12384.635321099602
2
Iteration 8200: Loss = -12384.364548348836
Iteration 8300: Loss = -12384.462023151602
1
Iteration 8400: Loss = -12384.364464727403
Iteration 8500: Loss = -12384.364422147595
Iteration 8600: Loss = -12384.364845409313
1
Iteration 8700: Loss = -12384.364297683453
Iteration 8800: Loss = -12384.364259132388
Iteration 8900: Loss = -12384.364381898751
1
Iteration 9000: Loss = -12384.364218771589
Iteration 9100: Loss = -12384.36412539182
Iteration 9200: Loss = -12384.364137994306
1
Iteration 9300: Loss = -12384.364030021203
Iteration 9400: Loss = -12384.36402808841
Iteration 9500: Loss = -12384.36414916696
1
Iteration 9600: Loss = -12384.36396120708
Iteration 9700: Loss = -12384.363950122844
Iteration 9800: Loss = -12384.36493451292
1
Iteration 9900: Loss = -12384.363886229987
Iteration 10000: Loss = -12384.363877384594
Iteration 10100: Loss = -12384.405455544285
1
Iteration 10200: Loss = -12384.363824713024
Iteration 10300: Loss = -12384.36381707359
Iteration 10400: Loss = -12384.395759347266
1
Iteration 10500: Loss = -12384.363776544438
Iteration 10600: Loss = -12384.363725690739
Iteration 10700: Loss = -12384.795201354622
1
Iteration 10800: Loss = -12384.3637297434
2
Iteration 10900: Loss = -12384.363678815156
Iteration 11000: Loss = -12384.95275903632
1
Iteration 11100: Loss = -12384.363686151735
2
Iteration 11200: Loss = -12384.363625376483
Iteration 11300: Loss = -12384.363646239319
1
Iteration 11400: Loss = -12384.36395286019
2
Iteration 11500: Loss = -12384.363595293225
Iteration 11600: Loss = -12384.363591743693
Iteration 11700: Loss = -12384.36362307039
1
Iteration 11800: Loss = -12384.363558379328
Iteration 11900: Loss = -12384.364228021577
1
Iteration 12000: Loss = -12384.36609027537
2
Iteration 12100: Loss = -12384.369270562624
3
Iteration 12200: Loss = -12384.379803540143
4
Iteration 12300: Loss = -12384.363544080183
Iteration 12400: Loss = -12384.375762016276
1
Iteration 12500: Loss = -12384.363522758653
Iteration 12600: Loss = -12384.36447432851
1
Iteration 12700: Loss = -12384.363501365524
Iteration 12800: Loss = -12384.363560690015
1
Iteration 12900: Loss = -12384.36352610392
2
Iteration 13000: Loss = -12384.363876096884
3
Iteration 13100: Loss = -12384.363483821766
Iteration 13200: Loss = -12384.363550572287
1
Iteration 13300: Loss = -12384.363482722134
Iteration 13400: Loss = -12384.364077354934
1
Iteration 13500: Loss = -12384.363492047334
2
Iteration 13600: Loss = -12384.363567957436
3
Iteration 13700: Loss = -12384.363424245415
Iteration 13800: Loss = -12384.363487380768
1
Iteration 13900: Loss = -12384.363437211025
2
Iteration 14000: Loss = -12384.38143537925
3
Iteration 14100: Loss = -12384.363463163816
4
Iteration 14200: Loss = -12384.367671040713
5
Stopping early at iteration 14200 due to no improvement.
pi: tensor([[2.5015e-04, 9.9975e-01],
        [3.9830e-02, 9.6017e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0354, 0.9646], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2330, 0.1154],
         [0.6748, 0.1996]],

        [[0.6188, 0.1966],
         [0.6516, 0.5942]],

        [[0.6074, 0.2028],
         [0.6727, 0.5970]],

        [[0.5480, 0.2372],
         [0.6986, 0.6161]],

        [[0.7197, 0.2538],
         [0.5923, 0.6820]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.00776683106263838
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00016110136691517576
Average Adjusted Rand Index: 0.001553366212527676
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20568.35269962048
Iteration 100: Loss = -12386.718114692838
Iteration 200: Loss = -12386.324857179638
Iteration 300: Loss = -12386.214582791026
Iteration 400: Loss = -12386.09724618823
Iteration 500: Loss = -12385.976504662241
Iteration 600: Loss = -12385.854939693982
Iteration 700: Loss = -12385.714281911876
Iteration 800: Loss = -12385.576578538825
Iteration 900: Loss = -12385.488547309862
Iteration 1000: Loss = -12385.435920634724
Iteration 1100: Loss = -12385.395167221199
Iteration 1200: Loss = -12385.348740847789
Iteration 1300: Loss = -12385.246350100202
Iteration 1400: Loss = -12384.817512217525
Iteration 1500: Loss = -12384.514331784492
Iteration 1600: Loss = -12384.436266595068
Iteration 1700: Loss = -12384.408142714738
Iteration 1800: Loss = -12384.395511678982
Iteration 1900: Loss = -12384.388934180468
Iteration 2000: Loss = -12384.385079157124
Iteration 2100: Loss = -12384.382679334805
Iteration 2200: Loss = -12384.381038129057
Iteration 2300: Loss = -12384.379880277675
Iteration 2400: Loss = -12384.378955983693
Iteration 2500: Loss = -12384.378301709334
Iteration 2600: Loss = -12384.377721256098
Iteration 2700: Loss = -12384.377227598829
Iteration 2800: Loss = -12384.376803086456
Iteration 2900: Loss = -12384.376382054723
Iteration 3000: Loss = -12384.376003400688
Iteration 3100: Loss = -12384.375666901333
Iteration 3200: Loss = -12384.3753153471
Iteration 3300: Loss = -12384.375019690568
Iteration 3400: Loss = -12384.374709045564
Iteration 3500: Loss = -12384.37438151958
Iteration 3600: Loss = -12384.374100811277
Iteration 3700: Loss = -12384.373759367223
Iteration 3800: Loss = -12384.373432759035
Iteration 3900: Loss = -12384.373133542635
Iteration 4000: Loss = -12384.372777839197
Iteration 4100: Loss = -12384.372486267406
Iteration 4200: Loss = -12384.372164998078
Iteration 4300: Loss = -12384.371854448898
Iteration 4400: Loss = -12384.371517728585
Iteration 4500: Loss = -12384.371258767405
Iteration 4600: Loss = -12384.370900581862
Iteration 4700: Loss = -12384.370635188372
Iteration 4800: Loss = -12384.370257525687
Iteration 4900: Loss = -12384.369999126104
Iteration 5000: Loss = -12384.369730763583
Iteration 5100: Loss = -12384.369408153143
Iteration 5200: Loss = -12384.369357654561
Iteration 5300: Loss = -12384.368869070446
Iteration 5400: Loss = -12384.368611156677
Iteration 5500: Loss = -12384.368379699335
Iteration 5600: Loss = -12384.36813052195
Iteration 5700: Loss = -12384.368032335651
Iteration 5800: Loss = -12384.367628576705
Iteration 5900: Loss = -12384.36752001048
Iteration 6000: Loss = -12384.367242801143
Iteration 6100: Loss = -12384.367038967777
Iteration 6200: Loss = -12384.366876555821
Iteration 6300: Loss = -12384.366690807024
Iteration 6400: Loss = -12384.367388292914
1
Iteration 6500: Loss = -12384.366347712348
Iteration 6600: Loss = -12384.36618353784
Iteration 6700: Loss = -12384.366061925915
Iteration 6800: Loss = -12384.365924369797
Iteration 6900: Loss = -12384.36934604841
1
Iteration 7000: Loss = -12384.366999048352
2
Iteration 7100: Loss = -12384.365584397134
Iteration 7200: Loss = -12384.366158550374
1
Iteration 7300: Loss = -12384.365482778865
Iteration 7400: Loss = -12384.365241398395
Iteration 7500: Loss = -12384.365738837165
1
Iteration 7600: Loss = -12384.365071368265
Iteration 7700: Loss = -12384.364971437737
Iteration 7800: Loss = -12384.379216594887
1
Iteration 7900: Loss = -12384.364806033058
Iteration 8000: Loss = -12384.364730766094
Iteration 8100: Loss = -12384.364619489415
Iteration 8200: Loss = -12384.36561919742
1
Iteration 8300: Loss = -12384.364484014439
Iteration 8400: Loss = -12384.364411991972
Iteration 8500: Loss = -12385.092145830858
1
Iteration 8600: Loss = -12384.364356644557
Iteration 8700: Loss = -12384.364319575616
Iteration 8800: Loss = -12384.364230872809
Iteration 8900: Loss = -12384.365036160036
1
Iteration 9000: Loss = -12384.364171641848
Iteration 9100: Loss = -12384.364101595698
Iteration 9200: Loss = -12384.3643393472
1
Iteration 9300: Loss = -12384.364044047503
Iteration 9400: Loss = -12384.36399998751
Iteration 9500: Loss = -12384.363956959209
Iteration 9600: Loss = -12384.363926593824
Iteration 9700: Loss = -12384.363919426798
Iteration 9800: Loss = -12384.369878211979
1
Iteration 9900: Loss = -12384.363888529968
Iteration 10000: Loss = -12384.363854750163
Iteration 10100: Loss = -12384.411318480059
1
Iteration 10200: Loss = -12384.363779380088
Iteration 10300: Loss = -12384.363783696472
1
Iteration 10400: Loss = -12384.423638540418
2
Iteration 10500: Loss = -12384.363759670709
Iteration 10600: Loss = -12384.363709749292
Iteration 10700: Loss = -12384.36371336707
1
Iteration 10800: Loss = -12384.363940017058
2
Iteration 10900: Loss = -12384.363684199612
Iteration 11000: Loss = -12384.363637707353
Iteration 11100: Loss = -12384.363759417873
1
Iteration 11200: Loss = -12384.363607057445
Iteration 11300: Loss = -12384.36359112362
Iteration 11400: Loss = -12384.363705887446
1
Iteration 11500: Loss = -12384.363587786318
Iteration 11600: Loss = -12384.363643419734
1
Iteration 11700: Loss = -12384.364375090381
2
Iteration 11800: Loss = -12384.36353677572
Iteration 11900: Loss = -12384.373880114154
1
Iteration 12000: Loss = -12384.363528535543
Iteration 12100: Loss = -12384.367901820244
1
Iteration 12200: Loss = -12384.363507840173
Iteration 12300: Loss = -12384.363502093824
Iteration 12400: Loss = -12384.36443976362
1
Iteration 12500: Loss = -12384.363789921008
2
Iteration 12600: Loss = -12384.363521580026
3
Iteration 12700: Loss = -12384.382865954718
4
Iteration 12800: Loss = -12384.363520652732
5
Stopping early at iteration 12800 due to no improvement.
pi: tensor([[4.6835e-04, 9.9953e-01],
        [3.9857e-02, 9.6014e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0355, 0.9645], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2329, 0.1154],
         [0.6196, 0.1998]],

        [[0.5595, 0.1963],
         [0.7061, 0.6102]],

        [[0.5344, 0.2025],
         [0.7122, 0.5633]],

        [[0.5551, 0.2367],
         [0.5288, 0.6269]],

        [[0.5054, 0.2535],
         [0.5563, 0.5190]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.00776683106263838
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00016110136691517576
Average Adjusted Rand Index: 0.001553366212527676
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21545.098049999975
Iteration 100: Loss = -12386.817699558784
Iteration 200: Loss = -12386.104093277261
Iteration 300: Loss = -12385.822312102811
Iteration 400: Loss = -12385.528880887829
Iteration 500: Loss = -12385.014901935447
Iteration 600: Loss = -12384.658919108162
Iteration 700: Loss = -12384.502183515006
Iteration 800: Loss = -12384.427636632106
Iteration 900: Loss = -12384.3888646705
Iteration 1000: Loss = -12384.365853999238
Iteration 1100: Loss = -12384.348364437725
Iteration 1200: Loss = -12384.331217659997
Iteration 1300: Loss = -12384.311752741118
Iteration 1400: Loss = -12384.288732436935
Iteration 1500: Loss = -12384.26214338445
Iteration 1600: Loss = -12384.233209449008
Iteration 1700: Loss = -12384.204228781287
Iteration 1800: Loss = -12384.177537924275
Iteration 1900: Loss = -12384.15474351241
Iteration 2000: Loss = -12384.13628568558
Iteration 2100: Loss = -12384.121887632327
Iteration 2200: Loss = -12384.111229645807
Iteration 2300: Loss = -12384.10344655746
Iteration 2400: Loss = -12384.097930205407
Iteration 2500: Loss = -12384.09396864786
Iteration 2600: Loss = -12384.091212052605
Iteration 2700: Loss = -12384.089244673194
Iteration 2800: Loss = -12384.087783830248
Iteration 2900: Loss = -12384.086747922305
Iteration 3000: Loss = -12384.086039539781
Iteration 3100: Loss = -12384.085467962319
Iteration 3200: Loss = -12384.085009204742
Iteration 3300: Loss = -12384.08470973986
Iteration 3400: Loss = -12384.08449968464
Iteration 3500: Loss = -12384.08430943272
Iteration 3600: Loss = -12384.08415754908
Iteration 3700: Loss = -12384.084035716409
Iteration 3800: Loss = -12384.083942957486
Iteration 3900: Loss = -12384.08387286669
Iteration 4000: Loss = -12384.083826664912
Iteration 4100: Loss = -12384.083778225124
Iteration 4200: Loss = -12384.083748149298
Iteration 4300: Loss = -12384.08371283777
Iteration 4400: Loss = -12384.083704625991
Iteration 4500: Loss = -12384.0837256833
1
Iteration 4600: Loss = -12384.083659015232
Iteration 4700: Loss = -12384.083677166727
1
Iteration 4800: Loss = -12384.083647335128
Iteration 4900: Loss = -12384.083668304589
1
Iteration 5000: Loss = -12384.083650106018
2
Iteration 5100: Loss = -12384.083653040612
3
Iteration 5200: Loss = -12384.083634145767
Iteration 5300: Loss = -12384.083617754142
Iteration 5400: Loss = -12384.083632451
1
Iteration 5500: Loss = -12384.083620115432
2
Iteration 5600: Loss = -12384.08364305887
3
Iteration 5700: Loss = -12384.083825887488
4
Iteration 5800: Loss = -12384.083653336758
5
Stopping early at iteration 5800 due to no improvement.
pi: tensor([[0.4840, 0.5160],
        [0.0028, 0.9972]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0698, 0.9302], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1331, 0.1288],
         [0.5854, 0.2023]],

        [[0.5458, 0.1729],
         [0.6422, 0.7038]],

        [[0.6926, 0.1951],
         [0.7207, 0.6005]],

        [[0.6831, 0.2102],
         [0.5163, 0.6009]],

        [[0.5239, 0.2591],
         [0.5229, 0.6491]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.007493599095377873
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0001304368371122338
Average Adjusted Rand Index: 0.0014987198190755746
11957.535052382842
[0.0001304368371122338, 0.00016110136691517576, 0.00016110136691517576, 0.0001304368371122338] [0.0014987198190755746, 0.001553366212527676, 0.001553366212527676, 0.0014987198190755746] [12384.08370074077, 12384.367671040713, 12384.363520652732, 12384.083653336758]
-----------------------------------------------------------------------------------------
This iteration is 3
True Objective function: Loss = -11787.88974980866
Iteration 0: Loss = -12335.197568158073
Iteration 10: Loss = -12335.19756815821
1
Iteration 20: Loss = -12335.197568158821
2
Iteration 30: Loss = -12335.19756816708
3
Stopping early at iteration 30 due to no improvement.
pi: tensor([[1.0000e+00, 4.3269e-13],
        [1.0000e+00, 3.4097e-12]], dtype=torch.float64)
alpha: tensor([1.0000e+00, 4.0576e-13])
beta: tensor([[[0.1966, 0.1447],
         [0.8474, 0.1130]],

        [[0.1173, 0.2521],
         [0.4483, 0.6834]],

        [[0.6688, 0.1898],
         [0.5247, 0.8922]],

        [[0.5570, 0.0922],
         [0.7075, 0.4720]],

        [[0.5934, 0.2806],
         [0.4562, 0.7556]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 41
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -12279.223301442082
Iteration 10: Loss = -11777.706695350862
Iteration 20: Loss = -11777.691943881542
Iteration 30: Loss = -11777.691907107146
Iteration 40: Loss = -11777.691903838475
Iteration 50: Loss = -11777.691903838475
1
Iteration 60: Loss = -11777.691903838475
2
Iteration 70: Loss = -11777.691903838475
3
Stopping early at iteration 70 due to no improvement.
pi: tensor([[0.2214, 0.7786],
        [0.7948, 0.2052]], dtype=torch.float64)
alpha: tensor([0.4928, 0.5072])
beta: tensor([[[0.2988, 0.0888],
         [0.8820, 0.2875]],

        [[0.0098, 0.1068],
         [0.0514, 0.2935]],

        [[0.6278, 0.0978],
         [0.7659, 0.4062]],

        [[0.1064, 0.0924],
         [0.9852, 0.2951]],

        [[0.1402, 0.0989],
         [0.3627, 0.2844]]], dtype=torch.float64)
time is 0
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599725554098923
time is 4
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.039714258646855106
Average Adjusted Rand Index: 0.9919945110819786
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21219.905907950164
Iteration 100: Loss = -12335.189604122046
Iteration 200: Loss = -12334.82341390978
Iteration 300: Loss = -12334.647723153614
Iteration 400: Loss = -12334.393265172004
Iteration 500: Loss = -12333.98988466336
Iteration 600: Loss = -12333.702323111334
Iteration 700: Loss = -12333.519349092197
Iteration 800: Loss = -12333.353004288068
Iteration 900: Loss = -12333.185939528983
Iteration 1000: Loss = -12333.036797840663
Iteration 1100: Loss = -12332.913570097762
Iteration 1200: Loss = -12332.791453052754
Iteration 1300: Loss = -12330.088832123538
Iteration 1400: Loss = -11985.284399299504
Iteration 1500: Loss = -11882.848699652313
Iteration 1600: Loss = -11855.360824664673
Iteration 1700: Loss = -11834.935253391684
Iteration 1800: Loss = -11819.566930149453
Iteration 1900: Loss = -11806.221693095727
Iteration 2000: Loss = -11806.202590636109
Iteration 2100: Loss = -11806.18716110896
Iteration 2200: Loss = -11799.670155644184
Iteration 2300: Loss = -11799.652155080756
Iteration 2400: Loss = -11799.64350462248
Iteration 2500: Loss = -11786.316686957283
Iteration 2600: Loss = -11786.295147490811
Iteration 2700: Loss = -11780.074386696475
Iteration 2800: Loss = -11780.069621367209
Iteration 2900: Loss = -11780.06607524598
Iteration 3000: Loss = -11780.063937140121
Iteration 3100: Loss = -11780.061920618546
Iteration 3200: Loss = -11780.061156633275
Iteration 3300: Loss = -11780.057794406886
Iteration 3400: Loss = -11780.055871242106
Iteration 3500: Loss = -11780.054594978463
Iteration 3600: Loss = -11780.05356071729
Iteration 3700: Loss = -11780.054568160542
1
Iteration 3800: Loss = -11780.051518466787
Iteration 3900: Loss = -11780.05060111439
Iteration 4000: Loss = -11779.681673232883
Iteration 4100: Loss = -11778.401744743693
Iteration 4200: Loss = -11778.39594506704
Iteration 4300: Loss = -11778.391083074685
Iteration 4400: Loss = -11778.386025953378
Iteration 4500: Loss = -11778.231764369202
Iteration 4600: Loss = -11778.229887092773
Iteration 4700: Loss = -11778.229192862347
Iteration 4800: Loss = -11778.228820939337
Iteration 4900: Loss = -11778.227975111517
Iteration 5000: Loss = -11778.239865774009
1
Iteration 5100: Loss = -11778.227379721346
Iteration 5200: Loss = -11778.230577909046
1
Iteration 5300: Loss = -11778.22322109784
Iteration 5400: Loss = -11778.220325363107
Iteration 5500: Loss = -11778.219709839996
Iteration 5600: Loss = -11778.219520088975
Iteration 5700: Loss = -11778.224184639548
1
Iteration 5800: Loss = -11778.219170750215
Iteration 5900: Loss = -11778.222184816337
1
Iteration 6000: Loss = -11778.2196089171
2
Iteration 6100: Loss = -11778.219123501212
Iteration 6200: Loss = -11778.218330938984
Iteration 6300: Loss = -11778.218417421458
1
Iteration 6400: Loss = -11778.220558202542
2
Iteration 6500: Loss = -11778.218910344427
3
Iteration 6600: Loss = -11778.217353099113
Iteration 6700: Loss = -11778.217308820502
Iteration 6800: Loss = -11778.217270259065
Iteration 6900: Loss = -11778.218663111222
1
Iteration 7000: Loss = -11778.21695252853
Iteration 7100: Loss = -11778.216837575545
Iteration 7200: Loss = -11778.218032816307
1
Iteration 7300: Loss = -11778.21668347093
Iteration 7400: Loss = -11778.21669474581
1
Iteration 7500: Loss = -11778.223150395337
2
Iteration 7600: Loss = -11778.216439867361
Iteration 7700: Loss = -11778.221985108472
1
Iteration 7800: Loss = -11778.220912257246
2
Iteration 7900: Loss = -11778.216249464664
Iteration 8000: Loss = -11778.227968367026
1
Iteration 8100: Loss = -11778.21781663556
2
Iteration 8200: Loss = -11778.216163028275
Iteration 8300: Loss = -11778.216194671573
1
Iteration 8400: Loss = -11778.228699317748
2
Iteration 8500: Loss = -11778.216108637202
Iteration 8600: Loss = -11778.215910404135
Iteration 8700: Loss = -11778.139202735583
Iteration 8800: Loss = -11778.11205507788
Iteration 8900: Loss = -11778.178151363041
1
Iteration 9000: Loss = -11778.111635045247
Iteration 9100: Loss = -11778.109856190495
Iteration 9200: Loss = -11778.110189119843
1
Iteration 9300: Loss = -11778.110657135738
2
Iteration 9400: Loss = -11778.112814913091
3
Iteration 9500: Loss = -11778.120114256286
4
Iteration 9600: Loss = -11778.112674877762
5
Stopping early at iteration 9600 due to no improvement.
pi: tensor([[0.1937, 0.8063],
        [0.7833, 0.2167]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5385, 0.4615], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2934, 0.0888],
         [0.7274, 0.3041]],

        [[0.5344, 0.1071],
         [0.7170, 0.5460]],

        [[0.7154, 0.0994],
         [0.6723, 0.6219]],

        [[0.6658, 0.0920],
         [0.6364, 0.7250]],

        [[0.6373, 0.0989],
         [0.5558, 0.5147]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208047711084835
time is 3
tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599725554098923
time is 4
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.03650790882013631
Average Adjusted Rand Index: 0.976155465303675
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23982.718394717773
Iteration 100: Loss = -12335.435983001491
Iteration 200: Loss = -12334.914951828416
Iteration 300: Loss = -12334.697784604448
Iteration 400: Loss = -12334.435346266351
Iteration 500: Loss = -12333.95520126286
Iteration 600: Loss = -12333.653185951804
Iteration 700: Loss = -12331.423491713102
Iteration 800: Loss = -12330.613712959457
Iteration 900: Loss = -12330.139580008901
Iteration 1000: Loss = -12329.287970230398
Iteration 1100: Loss = -12328.529215721706
Iteration 1200: Loss = -12327.982761054463
Iteration 1300: Loss = -12327.444600748491
Iteration 1400: Loss = -12319.397440170995
Iteration 1500: Loss = -11843.059786099695
Iteration 1600: Loss = -11822.813150590711
Iteration 1700: Loss = -11811.95156131581
Iteration 1800: Loss = -11811.903510901739
Iteration 1900: Loss = -11811.851288543217
Iteration 2000: Loss = -11802.226808864978
Iteration 2100: Loss = -11802.213875944517
Iteration 2200: Loss = -11784.275191136867
Iteration 2300: Loss = -11774.917748360893
Iteration 2400: Loss = -11774.910929641246
Iteration 2500: Loss = -11774.90553640069
Iteration 2600: Loss = -11774.879776878446
Iteration 2700: Loss = -11774.847746527767
Iteration 2800: Loss = -11774.84526579774
Iteration 2900: Loss = -11774.843100867278
Iteration 3000: Loss = -11774.841219469834
Iteration 3100: Loss = -11774.839600922773
Iteration 3200: Loss = -11774.83808185628
Iteration 3300: Loss = -11774.836614759362
Iteration 3400: Loss = -11774.835496630252
Iteration 3500: Loss = -11774.833718924216
Iteration 3600: Loss = -11774.8322119543
Iteration 3700: Loss = -11774.83096764121
Iteration 3800: Loss = -11774.830225910693
Iteration 3900: Loss = -11774.829461334244
Iteration 4000: Loss = -11774.829030124742
Iteration 4100: Loss = -11774.82840090229
Iteration 4200: Loss = -11774.828776338218
1
Iteration 4300: Loss = -11774.82782228732
Iteration 4400: Loss = -11774.82714506026
Iteration 4500: Loss = -11774.826849697558
Iteration 4600: Loss = -11774.83529901126
1
Iteration 4700: Loss = -11774.826058878934
Iteration 4800: Loss = -11774.825603044315
Iteration 4900: Loss = -11774.825945459443
1
Iteration 5000: Loss = -11774.82621957953
2
Iteration 5100: Loss = -11774.823870961081
Iteration 5200: Loss = -11774.823234525515
Iteration 5300: Loss = -11774.823912650954
1
Iteration 5400: Loss = -11774.821688037067
Iteration 5500: Loss = -11774.82654243941
1
Iteration 5600: Loss = -11774.820360736085
Iteration 5700: Loss = -11774.818664719265
Iteration 5800: Loss = -11774.816810188302
Iteration 5900: Loss = -11774.81623386369
Iteration 6000: Loss = -11774.816857262844
1
Iteration 6100: Loss = -11774.817550191192
2
Iteration 6200: Loss = -11774.826329081046
3
Iteration 6300: Loss = -11774.814538244917
Iteration 6400: Loss = -11774.814493590437
Iteration 6500: Loss = -11774.833075197013
1
Iteration 6600: Loss = -11774.81424897658
Iteration 6700: Loss = -11774.817830718772
1
Iteration 6800: Loss = -11774.813910436054
Iteration 6900: Loss = -11774.814411502177
1
Iteration 7000: Loss = -11774.79049071224
Iteration 7100: Loss = -11774.79049999315
1
Iteration 7200: Loss = -11774.790725432138
2
Iteration 7300: Loss = -11774.79059771007
3
Iteration 7400: Loss = -11774.797125493278
4
Iteration 7500: Loss = -11774.79155274423
5
Stopping early at iteration 7500 due to no improvement.
pi: tensor([[0.8203, 0.1797],
        [0.2478, 0.7522]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5407, 0.4593], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2959, 0.0888],
         [0.5920, 0.3035]],

        [[0.7267, 0.1067],
         [0.5925, 0.5279]],

        [[0.5915, 0.0979],
         [0.5616, 0.6003]],

        [[0.6064, 0.0918],
         [0.6754, 0.5870]],

        [[0.6385, 0.0989],
         [0.5867, 0.6186]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599725554098923
time is 4
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840299677396377
Average Adjusted Rand Index: 0.9839943230151149
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21682.966292268004
Iteration 100: Loss = -12335.974879868718
Iteration 200: Loss = -12335.101054982999
Iteration 300: Loss = -12334.896090407914
Iteration 400: Loss = -12334.811391624035
Iteration 500: Loss = -12334.751205299885
Iteration 600: Loss = -12334.681589309648
Iteration 700: Loss = -12334.557419565921
Iteration 800: Loss = -12334.263664971399
Iteration 900: Loss = -12333.862650497338
Iteration 1000: Loss = -12333.603433308195
Iteration 1100: Loss = -12333.41563179539
Iteration 1200: Loss = -12333.237849223819
Iteration 1300: Loss = -12333.056382189045
Iteration 1400: Loss = -12332.880382153793
Iteration 1500: Loss = -12332.684223218592
Iteration 1600: Loss = -12327.025695366752
Iteration 1700: Loss = -12032.986121673339
Iteration 1800: Loss = -11901.347869978958
Iteration 1900: Loss = -11877.700507268512
Iteration 2000: Loss = -11854.963611454774
Iteration 2100: Loss = -11843.933563790124
Iteration 2200: Loss = -11822.09222439589
Iteration 2300: Loss = -11806.260195055698
Iteration 2400: Loss = -11806.230370743022
Iteration 2500: Loss = -11806.209141905914
Iteration 2600: Loss = -11806.175587120722
Iteration 2700: Loss = -11799.673580909739
Iteration 2800: Loss = -11799.66424281688
Iteration 2900: Loss = -11799.65646068798
Iteration 3000: Loss = -11799.649358460678
Iteration 3100: Loss = -11799.642204526823
Iteration 3200: Loss = -11799.633963898106
Iteration 3300: Loss = -11786.325417784568
Iteration 3400: Loss = -11786.289051280934
Iteration 3500: Loss = -11786.257349718388
Iteration 3600: Loss = -11780.06125516917
Iteration 3700: Loss = -11780.055916731873
Iteration 3800: Loss = -11780.050447691969
Iteration 3900: Loss = -11780.047178378658
Iteration 4000: Loss = -11780.042613761716
Iteration 4100: Loss = -11780.041618385001
Iteration 4200: Loss = -11780.045677091855
1
Iteration 4300: Loss = -11780.038089618125
Iteration 4400: Loss = -11780.04273458993
1
Iteration 4500: Loss = -11780.035681577494
Iteration 4600: Loss = -11780.034439230234
Iteration 4700: Loss = -11780.036444658283
1
Iteration 4800: Loss = -11780.031811782595
Iteration 4900: Loss = -11780.04553555325
1
Iteration 5000: Loss = -11780.031848371933
2
Iteration 5100: Loss = -11778.371089301558
Iteration 5200: Loss = -11778.371276244572
1
Iteration 5300: Loss = -11778.114670431947
Iteration 5400: Loss = -11778.11251159146
Iteration 5500: Loss = -11778.112003607319
Iteration 5600: Loss = -11778.114595852592
1
Iteration 5700: Loss = -11778.11116315825
Iteration 5800: Loss = -11778.115184927976
1
Iteration 5900: Loss = -11778.113853303017
2
Iteration 6000: Loss = -11778.110148524065
Iteration 6100: Loss = -11778.109957415241
Iteration 6200: Loss = -11778.109500743898
Iteration 6300: Loss = -11778.111245941214
1
Iteration 6400: Loss = -11778.108956066932
Iteration 6500: Loss = -11778.115584001078
1
Iteration 6600: Loss = -11778.112919351814
2
Iteration 6700: Loss = -11778.108986633875
3
Iteration 6800: Loss = -11778.107990830162
Iteration 6900: Loss = -11778.108107549242
1
Iteration 7000: Loss = -11778.107557294757
Iteration 7100: Loss = -11778.107441835704
Iteration 7200: Loss = -11778.10871487274
1
Iteration 7300: Loss = -11778.110212057461
2
Iteration 7400: Loss = -11778.109187104488
3
Iteration 7500: Loss = -11778.106853372068
Iteration 7600: Loss = -11778.106286794417
Iteration 7700: Loss = -11778.118987113388
1
Iteration 7800: Loss = -11778.104517445869
Iteration 7900: Loss = -11778.104468860742
Iteration 8000: Loss = -11778.104189860875
Iteration 8100: Loss = -11775.46390411014
Iteration 8200: Loss = -11775.465455828908
1
Iteration 8300: Loss = -11775.463716040878
Iteration 8400: Loss = -11775.462436051395
Iteration 8500: Loss = -11775.462426454835
Iteration 8600: Loss = -11775.464782529449
1
Iteration 8700: Loss = -11775.46225871581
Iteration 8800: Loss = -11775.461338062736
Iteration 8900: Loss = -11775.46034580654
Iteration 9000: Loss = -11775.460058313676
Iteration 9100: Loss = -11775.485187501454
1
Iteration 9200: Loss = -11775.459920587498
Iteration 9300: Loss = -11775.462022104279
1
Iteration 9400: Loss = -11775.47760383939
2
Iteration 9500: Loss = -11775.463348625162
3
Iteration 9600: Loss = -11775.46002446735
4
Iteration 9700: Loss = -11775.46541570846
5
Stopping early at iteration 9700 due to no improvement.
pi: tensor([[0.2185, 0.7815],
        [0.7990, 0.2010]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4647, 0.5353], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3048, 0.0891],
         [0.5496, 0.2932]],

        [[0.5638, 0.1068],
         [0.6708, 0.5843]],

        [[0.6049, 0.0978],
         [0.6420, 0.5403]],

        [[0.6185, 0.0921],
         [0.6635, 0.5047]],

        [[0.5053, 0.0989],
         [0.5846, 0.7029]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599725554098923
time is 4
tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.03809391150750206
Average Adjusted Rand Index: 0.9839943230151149
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22212.310824307577
Iteration 100: Loss = -12335.498450116293
Iteration 200: Loss = -12335.097678690696
Iteration 300: Loss = -12334.97646173134
Iteration 400: Loss = -12334.900137287303
Iteration 500: Loss = -12334.832041697702
Iteration 600: Loss = -12334.76206279704
Iteration 700: Loss = -12334.68747701087
Iteration 800: Loss = -12334.60815111794
Iteration 900: Loss = -12334.528888042449
Iteration 1000: Loss = -12334.457449529647
Iteration 1100: Loss = -12334.395876884848
Iteration 1200: Loss = -12334.34064341061
Iteration 1300: Loss = -12334.287602245806
Iteration 1400: Loss = -12334.233939823975
Iteration 1500: Loss = -12334.177221087622
Iteration 1600: Loss = -12334.114877416258
Iteration 1700: Loss = -12334.043637005647
Iteration 1800: Loss = -12333.960404607755
Iteration 1900: Loss = -12333.861872625102
Iteration 2000: Loss = -12333.746165305916
Iteration 2100: Loss = -12333.622369992116
Iteration 2200: Loss = -12333.503810592027
Iteration 2300: Loss = -12333.400745619834
Iteration 2400: Loss = -12333.32150164522
Iteration 2500: Loss = -12333.264178479214
Iteration 2600: Loss = -12333.223221476197
Iteration 2700: Loss = -12333.195355215415
Iteration 2800: Loss = -12333.176900409917
Iteration 2900: Loss = -12333.164183213323
Iteration 3000: Loss = -12333.155290032419
Iteration 3100: Loss = -12333.148571391212
Iteration 3200: Loss = -12333.14333752058
Iteration 3300: Loss = -12333.13916320104
Iteration 3400: Loss = -12333.13583400958
Iteration 3500: Loss = -12333.133017007862
Iteration 3600: Loss = -12333.130519709348
Iteration 3700: Loss = -12333.128163524312
Iteration 3800: Loss = -12333.125817522852
Iteration 3900: Loss = -12333.1234648255
Iteration 4000: Loss = -12333.121066581936
Iteration 4100: Loss = -12333.118516271606
Iteration 4200: Loss = -12333.12647251229
1
Iteration 4300: Loss = -12333.112753968282
Iteration 4400: Loss = -12333.109248477826
Iteration 4500: Loss = -12333.10525936091
Iteration 4600: Loss = -12333.100342422313
Iteration 4700: Loss = -12333.122803887336
1
Iteration 4800: Loss = -12333.086958167294
Iteration 4900: Loss = -12333.07869070875
Iteration 5000: Loss = -12333.067982549654
Iteration 5100: Loss = -12333.058666278943
Iteration 5200: Loss = -12333.054201753845
Iteration 5300: Loss = -12333.050252242916
Iteration 5400: Loss = -12333.049540739465
Iteration 5500: Loss = -12333.049556959062
1
Iteration 5600: Loss = -12333.049574757673
2
Iteration 5700: Loss = -12333.049580662082
3
Iteration 5800: Loss = -12333.049317335312
Iteration 5900: Loss = -12333.049371938163
1
Iteration 6000: Loss = -12333.0505536368
2
Iteration 6100: Loss = -12333.049275548437
Iteration 6200: Loss = -12333.049988717172
1
Iteration 6300: Loss = -12333.049223541811
Iteration 6400: Loss = -12333.04994924698
1
Iteration 6500: Loss = -12333.049239923683
2
Iteration 6600: Loss = -12333.049237943784
3
Iteration 6700: Loss = -12333.049184672833
Iteration 6800: Loss = -12333.049214673974
1
Iteration 6900: Loss = -12333.049131790664
Iteration 7000: Loss = -12333.04928124459
1
Iteration 7100: Loss = -12333.04913725517
2
Iteration 7200: Loss = -12333.049108811258
Iteration 7300: Loss = -12333.049156099985
1
Iteration 7400: Loss = -12333.050142358838
2
Iteration 7500: Loss = -12333.05136592888
3
Iteration 7600: Loss = -12333.049489469566
4
Iteration 7700: Loss = -12333.052629543236
5
Stopping early at iteration 7700 due to no improvement.
pi: tensor([[0.0088, 0.9912],
        [0.4766, 0.5234]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1234, 0.8766], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2260, 0.2058],
         [0.6381, 0.1883]],

        [[0.6896, 0.2108],
         [0.6564, 0.5646]],

        [[0.5768, 0.2024],
         [0.6451, 0.7308]],

        [[0.5568, 0.1994],
         [0.6684, 0.6138]],

        [[0.5002, 0.2057],
         [0.5104, 0.6487]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.006002224857279073
time is 2
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.0009975514204148314
time is 3
tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.025586367749811115
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: -0.024360446076493753
Global Adjusted Rand Index: -0.008798060745719432
Average Adjusted Rand Index: -0.008988428077888124
11787.88974980866
[0.03650790882013631, 0.9840299677396377, 0.03809391150750206, -0.008798060745719432] [0.976155465303675, 0.9839943230151149, 0.9839943230151149, -0.008988428077888124] [11778.112674877762, 11774.79155274423, 11775.46541570846, 12333.052629543236]
-----------------------------------------------------------------------------------------
This iteration is 4
True Objective function: Loss = -12114.140964075712
Iteration 0: Loss = -12556.607005983702
Iteration 10: Loss = -12556.607005973343
Iteration 20: Loss = -12556.607005973343
1
Iteration 30: Loss = -12556.607005973343
2
Iteration 40: Loss = -12556.607005973343
3
Stopping early at iteration 40 due to no improvement.
pi: tensor([[7.7235e-32, 1.0000e+00],
        [3.8482e-17, 1.0000e+00]], dtype=torch.float64)
alpha: tensor([3.7014e-17, 1.0000e+00])
beta: tensor([[[0.3000, 0.2296],
         [0.1420, 0.2030]],

        [[0.8660, 0.3120],
         [0.1663, 0.1359]],

        [[0.8797, 0.2878],
         [0.0751, 0.9523]],

        [[0.7711, 0.2200],
         [0.6322, 0.2515]],

        [[0.2796, 0.2669],
         [0.1621, 0.7050]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -12572.834405104095
Iteration 10: Loss = -12556.378151395771
Iteration 20: Loss = -12556.066339890189
Iteration 30: Loss = -12555.887309758284
Iteration 40: Loss = -12555.82682337595
Iteration 50: Loss = -12555.807432691126
Iteration 60: Loss = -12555.800520525734
Iteration 70: Loss = -12555.796979290624
Iteration 80: Loss = -12555.783628689507
Iteration 90: Loss = -12555.740643881663
Iteration 100: Loss = -12555.721003184062
Iteration 110: Loss = -12555.716994163478
Iteration 120: Loss = -12555.715635102384
Iteration 130: Loss = -12555.714951004058
Iteration 140: Loss = -12555.714634502927
Iteration 150: Loss = -12555.714464902805
Iteration 160: Loss = -12555.714349767652
Iteration 170: Loss = -12555.714297328304
Iteration 180: Loss = -12555.714272393712
Iteration 190: Loss = -12555.71428239161
1
Iteration 200: Loss = -12555.714266076311
Iteration 210: Loss = -12555.714247675802
Iteration 220: Loss = -12555.714270212315
1
Iteration 230: Loss = -12555.714255988034
2
Iteration 240: Loss = -12555.714283625479
3
Stopping early at iteration 240 due to no improvement.
pi: tensor([[0.0521, 0.9479],
        [0.0296, 0.9704]], dtype=torch.float64)
alpha: tensor([0.0304, 0.9696])
beta: tensor([[[0.3062, 0.2374],
         [0.2098, 0.1996]],

        [[0.8316, 0.2913],
         [0.4781, 0.3729]],

        [[0.6856, 0.2661],
         [0.3441, 0.1531]],

        [[0.1258, 0.2331],
         [0.7761, 0.6999]],

        [[0.9258, 0.2538],
         [0.6248, 0.4121]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: -0.004284949116229245
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0008570565949531054
Average Adjusted Rand Index: -0.0008569898232458489
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21766.789867463925
Iteration 100: Loss = -12557.079978118712
Iteration 200: Loss = -12556.485816037271
Iteration 300: Loss = -12556.352897423727
Iteration 400: Loss = -12556.290507532312
Iteration 500: Loss = -12556.245823519364
Iteration 600: Loss = -12556.20526970979
Iteration 700: Loss = -12556.164192613393
Iteration 800: Loss = -12556.120712813818
Iteration 900: Loss = -12556.074734654874
Iteration 1000: Loss = -12556.026694117483
Iteration 1100: Loss = -12555.975773239801
Iteration 1200: Loss = -12555.918959856881
Iteration 1300: Loss = -12555.856705885306
Iteration 1400: Loss = -12555.801862267166
Iteration 1500: Loss = -12555.763884031898
Iteration 1600: Loss = -12555.737935533105
Iteration 1700: Loss = -12555.719217579346
Iteration 1800: Loss = -12555.705119349204
Iteration 1900: Loss = -12555.693959679116
Iteration 2000: Loss = -12555.684811942107
Iteration 2100: Loss = -12555.676970819035
Iteration 2200: Loss = -12555.670013433404
Iteration 2300: Loss = -12555.663596070803
Iteration 2400: Loss = -12555.657451021058
Iteration 2500: Loss = -12555.651399344886
Iteration 2600: Loss = -12555.645295942779
Iteration 2700: Loss = -12555.638850964944
Iteration 2800: Loss = -12555.631731988875
Iteration 2900: Loss = -12555.6230684903
Iteration 3000: Loss = -12555.61066807126
Iteration 3100: Loss = -12555.584752990408
Iteration 3200: Loss = -12555.501102266086
Iteration 3300: Loss = -12555.435748363561
Iteration 3400: Loss = -12555.420991563655
Iteration 3500: Loss = -12555.406793439326
Iteration 3600: Loss = -12555.387426107245
Iteration 3700: Loss = -12555.359958572439
Iteration 3800: Loss = -12555.321082824601
Iteration 3900: Loss = -12555.269007241299
Iteration 4000: Loss = -12555.199946671577
Iteration 4100: Loss = -12555.106694634173
Iteration 4200: Loss = -12554.954572081648
Iteration 4300: Loss = -12242.142063472158
Iteration 4400: Loss = -12099.340373302985
Iteration 4500: Loss = -12099.188032373899
Iteration 4600: Loss = -12099.121607097875
Iteration 4700: Loss = -12098.887169145597
Iteration 4800: Loss = -12098.841660209016
Iteration 4900: Loss = -12098.825096374883
Iteration 5000: Loss = -12098.812494753454
Iteration 5100: Loss = -12098.798981855547
Iteration 5200: Loss = -12098.76634491605
Iteration 5300: Loss = -12098.762277689046
Iteration 5400: Loss = -12098.761708397997
Iteration 5500: Loss = -12098.757967412128
Iteration 5600: Loss = -12098.759173183616
1
Iteration 5700: Loss = -12098.75207822467
Iteration 5800: Loss = -12098.75542877939
1
Iteration 5900: Loss = -12098.757734603143
2
Iteration 6000: Loss = -12098.747244620174
Iteration 6100: Loss = -12098.746814081689
Iteration 6200: Loss = -12098.75544478198
1
Iteration 6300: Loss = -12098.744504493645
Iteration 6400: Loss = -12098.744185410851
Iteration 6500: Loss = -12098.743000512291
Iteration 6600: Loss = -12098.7440230633
1
Iteration 6700: Loss = -12098.735582181725
Iteration 6800: Loss = -12098.73473602017
Iteration 6900: Loss = -12098.733390906591
Iteration 7000: Loss = -12098.731808826484
Iteration 7100: Loss = -12098.731357651333
Iteration 7200: Loss = -12098.727314977054
Iteration 7300: Loss = -12098.720940969264
Iteration 7400: Loss = -12098.71213503062
Iteration 7500: Loss = -12098.712631118367
1
Iteration 7600: Loss = -12098.716974907617
2
Iteration 7700: Loss = -12098.711329656846
Iteration 7800: Loss = -12098.711133944289
Iteration 7900: Loss = -12098.715412318466
1
Iteration 8000: Loss = -12098.711698428911
2
Iteration 8100: Loss = -12098.710394025244
Iteration 8200: Loss = -12098.716582732044
1
Iteration 8300: Loss = -12098.709368081263
Iteration 8400: Loss = -12098.708985497678
Iteration 8500: Loss = -12098.708570507542
Iteration 8600: Loss = -12098.708231630633
Iteration 8700: Loss = -12098.707996213554
Iteration 8800: Loss = -12098.71258936921
1
Iteration 8900: Loss = -12098.704127405095
Iteration 9000: Loss = -12098.70422833453
1
Iteration 9100: Loss = -12098.710137272825
2
Iteration 9200: Loss = -12098.70388858188
Iteration 9300: Loss = -12098.703941622927
1
Iteration 9400: Loss = -12098.716289912227
2
Iteration 9500: Loss = -12098.70382966966
Iteration 9600: Loss = -12098.708556882853
1
Iteration 9700: Loss = -12098.708355152105
2
Iteration 9800: Loss = -12098.710127428758
3
Iteration 9900: Loss = -12098.701840921376
Iteration 10000: Loss = -12098.701519155169
Iteration 10100: Loss = -12098.777363117291
1
Iteration 10200: Loss = -12098.7028912317
2
Iteration 10300: Loss = -12098.724827341608
3
Iteration 10400: Loss = -12098.705284910775
4
Iteration 10500: Loss = -12098.718927610682
5
Stopping early at iteration 10500 due to no improvement.
pi: tensor([[0.6493, 0.3507],
        [0.2667, 0.7333]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4824, 0.5176], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3023, 0.1062],
         [0.5205, 0.3029]],

        [[0.7129, 0.0962],
         [0.5995, 0.6533]],

        [[0.6278, 0.1059],
         [0.5777, 0.5836]],

        [[0.6507, 0.1115],
         [0.6409, 0.6861]],

        [[0.6024, 0.1109],
         [0.6550, 0.5755]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
time is 3
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9207702484198148
time is 4
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
Global Adjusted Rand Index: 0.9603158667108215
Average Adjusted Rand Index: 0.9601503500467091
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23753.600786867584
Iteration 100: Loss = -12556.97376351284
Iteration 200: Loss = -12556.439746314547
Iteration 300: Loss = -12556.354535051165
Iteration 400: Loss = -12556.29673790271
Iteration 500: Loss = -12556.249046422634
Iteration 600: Loss = -12556.201401604367
Iteration 700: Loss = -12556.143061170209
Iteration 800: Loss = -12556.065086682423
Iteration 900: Loss = -12555.976090064138
Iteration 1000: Loss = -12555.884056734838
Iteration 1100: Loss = -12555.806638144222
Iteration 1200: Loss = -12555.764840169693
Iteration 1300: Loss = -12555.737391473667
Iteration 1400: Loss = -12555.716341736292
Iteration 1500: Loss = -12555.699837144515
Iteration 1600: Loss = -12555.686944952175
Iteration 1700: Loss = -12555.676764014679
Iteration 1800: Loss = -12555.66827644697
Iteration 1900: Loss = -12555.660617087893
Iteration 2000: Loss = -12555.652916011582
Iteration 2100: Loss = -12555.644218486941
Iteration 2200: Loss = -12555.632159740593
Iteration 2300: Loss = -12555.606835269264
Iteration 2400: Loss = -12555.51447725345
Iteration 2500: Loss = -12555.432470013091
Iteration 2600: Loss = -12555.421641511346
Iteration 2700: Loss = -12555.414382039455
Iteration 2800: Loss = -12555.402241881113
Iteration 2900: Loss = -12555.378097911816
Iteration 3000: Loss = -12555.332676562904
Iteration 3100: Loss = -12555.259119336391
Iteration 3200: Loss = -12555.14776971811
Iteration 3300: Loss = -12554.960091243136
Iteration 3400: Loss = -12123.880100059956
Iteration 3500: Loss = -12099.298183899562
Iteration 3600: Loss = -12099.109286589617
Iteration 3700: Loss = -12098.854786246906
Iteration 3800: Loss = -12098.846084253448
Iteration 3900: Loss = -12098.837247570296
Iteration 4000: Loss = -12098.819944669676
Iteration 4100: Loss = -12098.811381971154
Iteration 4200: Loss = -12098.810561336766
Iteration 4300: Loss = -12098.805601167633
Iteration 4400: Loss = -12098.779177544722
Iteration 4500: Loss = -12098.774397071858
Iteration 4600: Loss = -12098.775485378252
1
Iteration 4700: Loss = -12098.779641514815
2
Iteration 4800: Loss = -12098.768176910115
Iteration 4900: Loss = -12098.765011831061
Iteration 5000: Loss = -12098.754164617665
Iteration 5100: Loss = -12098.746357861277
Iteration 5200: Loss = -12098.746052976407
Iteration 5300: Loss = -12098.744724972885
Iteration 5400: Loss = -12098.743394299634
Iteration 5500: Loss = -12098.741142593271
Iteration 5600: Loss = -12098.739860885991
Iteration 5700: Loss = -12098.739989553933
1
Iteration 5800: Loss = -12098.74386612199
2
Iteration 5900: Loss = -12098.741594873729
3
Iteration 6000: Loss = -12098.738640535505
Iteration 6100: Loss = -12098.738007947799
Iteration 6200: Loss = -12098.731220048523
Iteration 6300: Loss = -12098.73084910066
Iteration 6400: Loss = -12098.732413334856
1
Iteration 6500: Loss = -12098.719632441906
Iteration 6600: Loss = -12098.71916373693
Iteration 6700: Loss = -12098.718639760924
Iteration 6800: Loss = -12098.721236438463
1
Iteration 6900: Loss = -12098.713178086708
Iteration 7000: Loss = -12098.708913086379
Iteration 7100: Loss = -12098.706712408726
Iteration 7200: Loss = -12098.71312259325
1
Iteration 7300: Loss = -12098.70602597372
Iteration 7400: Loss = -12098.70899485012
1
Iteration 7500: Loss = -12098.705396756843
Iteration 7600: Loss = -12098.71161277315
1
Iteration 7700: Loss = -12098.70505301536
Iteration 7800: Loss = -12098.704820378338
Iteration 7900: Loss = -12098.70446875554
Iteration 8000: Loss = -12098.703875310312
Iteration 8100: Loss = -12098.73832520794
1
Iteration 8200: Loss = -12098.703805603247
Iteration 8300: Loss = -12098.70369368171
Iteration 8400: Loss = -12098.70364369378
Iteration 8500: Loss = -12098.70354159734
Iteration 8600: Loss = -12098.703564902213
1
Iteration 8700: Loss = -12098.70407438193
2
Iteration 8800: Loss = -12098.70317749577
Iteration 8900: Loss = -12098.702860498472
Iteration 9000: Loss = -12098.702734988721
Iteration 9100: Loss = -12098.702697240118
Iteration 9200: Loss = -12098.702620145616
Iteration 9300: Loss = -12098.703674217195
1
Iteration 9400: Loss = -12098.702882781176
2
Iteration 9500: Loss = -12098.702590265473
Iteration 9600: Loss = -12098.702568222829
Iteration 9700: Loss = -12098.702723161214
1
Iteration 9800: Loss = -12098.722253067011
2
Iteration 9900: Loss = -12098.70445526405
3
Iteration 10000: Loss = -12098.702455533654
Iteration 10100: Loss = -12098.71082950537
1
Iteration 10200: Loss = -12098.744582525485
2
Iteration 10300: Loss = -12098.705593563742
3
Iteration 10400: Loss = -12098.712080237377
4
Iteration 10500: Loss = -12098.70196856658
Iteration 10600: Loss = -12098.70339602137
1
Iteration 10700: Loss = -12098.716301823111
2
Iteration 10800: Loss = -12098.711848540015
3
Iteration 10900: Loss = -12098.740255718149
4
Iteration 11000: Loss = -12098.70028726582
Iteration 11100: Loss = -12098.7066578537
1
Iteration 11200: Loss = -12098.700622112816
2
Iteration 11300: Loss = -12098.700717317202
3
Iteration 11400: Loss = -12098.702459653005
4
Iteration 11500: Loss = -12098.700258418325
Iteration 11600: Loss = -12098.712033694035
1
Iteration 11700: Loss = -12098.743209590917
2
Iteration 11800: Loss = -12098.700310955232
3
Iteration 11900: Loss = -12098.701941985672
4
Iteration 12000: Loss = -12098.702279542385
5
Stopping early at iteration 12000 due to no improvement.
pi: tensor([[0.6505, 0.3495],
        [0.2624, 0.7376]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4836, 0.5164], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3021, 0.1061],
         [0.6988, 0.3031]],

        [[0.7065, 0.0969],
         [0.5377, 0.6325]],

        [[0.6478, 0.1059],
         [0.5230, 0.6620]],

        [[0.6542, 0.1116],
         [0.7074, 0.5816]],

        [[0.6484, 0.1109],
         [0.6512, 0.5388]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
time is 3
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9207702484198148
time is 4
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
Global Adjusted Rand Index: 0.9603158667108215
Average Adjusted Rand Index: 0.9601503500467091
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23510.922918078846
Iteration 100: Loss = -12557.074103901277
Iteration 200: Loss = -12556.459635356385
Iteration 300: Loss = -12556.331946177974
Iteration 400: Loss = -12556.280415176705
Iteration 500: Loss = -12556.243514372703
Iteration 600: Loss = -12556.19986045763
Iteration 700: Loss = -12556.121214605217
Iteration 800: Loss = -12555.932016209865
Iteration 900: Loss = -12555.655827327842
Iteration 1000: Loss = -12555.518925251015
Iteration 1100: Loss = -12555.447361836845
Iteration 1200: Loss = -12555.402716217635
Iteration 1300: Loss = -12555.373498704272
Iteration 1400: Loss = -12555.35481296374
Iteration 1500: Loss = -12555.342611156017
Iteration 1600: Loss = -12555.334208502974
Iteration 1700: Loss = -12555.328249159596
Iteration 1800: Loss = -12555.32404868293
Iteration 1900: Loss = -12555.321009976315
Iteration 2000: Loss = -12555.318814597937
Iteration 2100: Loss = -12555.317255521008
Iteration 2200: Loss = -12555.316012475025
Iteration 2300: Loss = -12555.315080849849
Iteration 2400: Loss = -12555.314364473927
Iteration 2500: Loss = -12555.31378366627
Iteration 2600: Loss = -12555.31335938489
Iteration 2700: Loss = -12555.313034973566
Iteration 2800: Loss = -12555.312759573477
Iteration 2900: Loss = -12555.312508355613
Iteration 3000: Loss = -12555.31234889332
Iteration 3100: Loss = -12555.312234884957
Iteration 3200: Loss = -12555.312091148657
Iteration 3300: Loss = -12555.312021663447
Iteration 3400: Loss = -12555.311933604373
Iteration 3500: Loss = -12555.311856282655
Iteration 3600: Loss = -12555.311785390171
Iteration 3700: Loss = -12555.311746619936
Iteration 3800: Loss = -12555.311704256466
Iteration 3900: Loss = -12555.311699067035
Iteration 4000: Loss = -12555.311616020963
Iteration 4100: Loss = -12555.311586139136
Iteration 4200: Loss = -12555.3116015848
1
Iteration 4300: Loss = -12555.31158576261
Iteration 4400: Loss = -12555.311565965561
Iteration 4500: Loss = -12555.311536316794
Iteration 4600: Loss = -12555.311484261887
Iteration 4700: Loss = -12555.311468027236
Iteration 4800: Loss = -12555.3114820913
1
Iteration 4900: Loss = -12555.311449094665
Iteration 5000: Loss = -12555.311456549707
1
Iteration 5100: Loss = -12555.311460246028
2
Iteration 5200: Loss = -12555.311454265784
3
Iteration 5300: Loss = -12555.311412318433
Iteration 5400: Loss = -12555.311443598166
1
Iteration 5500: Loss = -12555.311421433851
2
Iteration 5600: Loss = -12555.311385691357
Iteration 5700: Loss = -12555.311372992246
Iteration 5800: Loss = -12555.311634894519
1
Iteration 5900: Loss = -12555.311406746605
2
Iteration 6000: Loss = -12555.311411407081
3
Iteration 6100: Loss = -12555.311393532807
4
Iteration 6200: Loss = -12555.311372125941
Iteration 6300: Loss = -12555.31153346704
1
Iteration 6400: Loss = -12555.311371396523
Iteration 6500: Loss = -12555.311438804289
1
Iteration 6600: Loss = -12555.311365300427
Iteration 6700: Loss = -12555.311551291643
1
Iteration 6800: Loss = -12555.31520923606
2
Iteration 6900: Loss = -12555.311379266768
3
Iteration 7000: Loss = -12555.311399277654
4
Iteration 7100: Loss = -12555.311744186845
5
Stopping early at iteration 7100 due to no improvement.
pi: tensor([[0.2023, 0.7977],
        [0.0277, 0.9723]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0264, 0.9736], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3333, 0.2396],
         [0.5569, 0.2016]],

        [[0.7148, 0.2879],
         [0.6210, 0.5546]],

        [[0.7051, 0.2677],
         [0.6938, 0.5945]],

        [[0.7230, 0.2337],
         [0.6425, 0.6898]],

        [[0.5852, 0.2552],
         [0.5920, 0.5352]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: -0.004284949116229245
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0008570565949531054
Average Adjusted Rand Index: -0.0008569898232458489
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23270.982935634212
Iteration 100: Loss = -12435.573303859337
Iteration 200: Loss = -12100.453036670717
Iteration 300: Loss = -12099.245607964518
Iteration 400: Loss = -12099.02467727814
Iteration 500: Loss = -12098.929552941894
Iteration 600: Loss = -12098.877270935056
Iteration 700: Loss = -12098.845412622848
Iteration 800: Loss = -12098.823646011766
Iteration 900: Loss = -12098.807893773583
Iteration 1000: Loss = -12098.79609047615
Iteration 1100: Loss = -12098.7873587311
Iteration 1200: Loss = -12098.780491730102
Iteration 1300: Loss = -12098.774923230507
Iteration 1400: Loss = -12098.770278531485
Iteration 1500: Loss = -12098.765946742848
Iteration 1600: Loss = -12098.762136551137
Iteration 1700: Loss = -12098.759477197595
Iteration 1800: Loss = -12098.757294691786
Iteration 1900: Loss = -12098.755459028145
Iteration 2000: Loss = -12098.75385790043
Iteration 2100: Loss = -12098.752477859536
Iteration 2200: Loss = -12098.751266051593
Iteration 2300: Loss = -12098.75017841527
Iteration 2400: Loss = -12098.749267934676
Iteration 2500: Loss = -12098.748401487308
Iteration 2600: Loss = -12098.747857491726
Iteration 2700: Loss = -12098.746993934079
Iteration 2800: Loss = -12098.746347944196
Iteration 2900: Loss = -12098.745822563791
Iteration 3000: Loss = -12098.745256575603
Iteration 3100: Loss = -12098.744771585176
Iteration 3200: Loss = -12098.746814212875
1
Iteration 3300: Loss = -12098.747204900794
2
Iteration 3400: Loss = -12098.743623055005
Iteration 3500: Loss = -12098.743186256832
Iteration 3600: Loss = -12098.74641451617
1
Iteration 3700: Loss = -12098.74196390769
Iteration 3800: Loss = -12098.73432411431
Iteration 3900: Loss = -12098.730667247013
Iteration 4000: Loss = -12098.728249509206
Iteration 4100: Loss = -12098.752600681386
1
Iteration 4200: Loss = -12098.723299010471
Iteration 4300: Loss = -12098.724791967645
1
Iteration 4400: Loss = -12098.722832197856
Iteration 4500: Loss = -12098.725916411264
1
Iteration 4600: Loss = -12098.72245334815
Iteration 4700: Loss = -12098.725415071547
1
Iteration 4800: Loss = -12098.722063768242
Iteration 4900: Loss = -12098.72172696694
Iteration 5000: Loss = -12098.718751574594
Iteration 5100: Loss = -12098.722761597837
1
Iteration 5200: Loss = -12098.71782907662
Iteration 5300: Loss = -12098.72326517164
1
Iteration 5400: Loss = -12098.716610468873
Iteration 5500: Loss = -12098.713587762571
Iteration 5600: Loss = -12098.717846594267
1
Iteration 5700: Loss = -12098.713368713245
Iteration 5800: Loss = -12098.714576252574
1
Iteration 5900: Loss = -12098.713399461669
2
Iteration 6000: Loss = -12098.714208273288
3
Iteration 6100: Loss = -12098.712820842673
Iteration 6200: Loss = -12098.712573263623
Iteration 6300: Loss = -12098.712619276033
1
Iteration 6400: Loss = -12098.713832521178
2
Iteration 6500: Loss = -12098.712734508548
3
Iteration 6600: Loss = -12098.71756863876
4
Iteration 6700: Loss = -12098.708419275177
Iteration 6800: Loss = -12098.712309520248
1
Iteration 6900: Loss = -12098.708104812136
Iteration 7000: Loss = -12098.708166733944
1
Iteration 7100: Loss = -12098.708072749696
Iteration 7200: Loss = -12098.735582282012
1
Iteration 7300: Loss = -12098.708634925228
2
Iteration 7400: Loss = -12098.708031566106
Iteration 7500: Loss = -12098.881143512035
1
Iteration 7600: Loss = -12098.707937565441
Iteration 7700: Loss = -12098.707911595297
Iteration 7800: Loss = -12098.708043489632
1
Iteration 7900: Loss = -12098.707862477962
Iteration 8000: Loss = -12098.707975586152
1
Iteration 8100: Loss = -12098.707888140507
2
Iteration 8200: Loss = -12098.707815975207
Iteration 8300: Loss = -12098.707780146127
Iteration 8400: Loss = -12098.708224311064
1
Iteration 8500: Loss = -12098.707749533678
Iteration 8600: Loss = -12098.707777535798
1
Iteration 8700: Loss = -12098.744184436307
2
Iteration 8800: Loss = -12098.70701586594
Iteration 8900: Loss = -12098.707081903172
1
Iteration 9000: Loss = -12098.71495163119
2
Iteration 9100: Loss = -12098.70695666801
Iteration 9200: Loss = -12098.707544276573
1
Iteration 9300: Loss = -12098.707931503228
2
Iteration 9400: Loss = -12098.742598974259
3
Iteration 9500: Loss = -12098.706990734501
4
Iteration 9600: Loss = -12098.804490065431
5
Stopping early at iteration 9600 due to no improvement.
pi: tensor([[0.6539, 0.3461],
        [0.2644, 0.7356]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4881, 0.5119], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3017, 0.1070],
         [0.6118, 0.3048]],

        [[0.7295, 0.0967],
         [0.7204, 0.7009]],

        [[0.5806, 0.1059],
         [0.6241, 0.5919]],

        [[0.5588, 0.1118],
         [0.5866, 0.6871]],

        [[0.7060, 0.1107],
         [0.5230, 0.7288]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
time is 3
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9207702484198148
time is 4
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
Global Adjusted Rand Index: 0.9603158667108215
Average Adjusted Rand Index: 0.9601503500467091
12114.140964075712
[0.9603158667108215, 0.9603158667108215, -0.0008570565949531054, 0.9603158667108215] [0.9601503500467091, 0.9601503500467091, -0.0008569898232458489, 0.9601503500467091] [12098.718927610682, 12098.702279542385, 12555.311744186845, 12098.804490065431]
-----------------------------------------------------------------------------------------
This iteration is 5
True Objective function: Loss = -11823.971288486695
Iteration 0: Loss = -12319.83182691631
Iteration 10: Loss = -12319.83182691631
1
Iteration 20: Loss = -12319.83182691631
2
Iteration 30: Loss = -12319.83182691631
3
Stopping early at iteration 30 due to no improvement.
pi: tensor([[9.8898e-27, 1.0000e+00],
        [4.4377e-22, 1.0000e+00]], dtype=torch.float64)
alpha: tensor([4.3920e-22, 1.0000e+00])
beta: tensor([[[0.2017, 0.2072],
         [0.1576, 0.1961]],

        [[0.4175, 0.2172],
         [0.0835, 0.1231]],

        [[0.9952, 0.1632],
         [0.6205, 0.0967]],

        [[0.4927, 0.1815],
         [0.5067, 0.2419]],

        [[0.3674, 0.2263],
         [0.2246, 0.4254]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -12325.813319648752
Iteration 10: Loss = -12319.801340718392
Iteration 20: Loss = -12319.776545176835
Iteration 30: Loss = -12319.75164416555
Iteration 40: Loss = -12319.719622540404
Iteration 50: Loss = -12319.680676547086
Iteration 60: Loss = -12319.636524279782
Iteration 70: Loss = -12319.58962867134
Iteration 80: Loss = -12319.54272671503
Iteration 90: Loss = -12319.497875822317
Iteration 100: Loss = -12319.45652538109
Iteration 110: Loss = -12319.419198994729
Iteration 120: Loss = -12319.385910539118
Iteration 130: Loss = -12319.356541335968
Iteration 140: Loss = -12319.330619139042
Iteration 150: Loss = -12319.307710728199
Iteration 160: Loss = -12319.287502048877
Iteration 170: Loss = -12319.269540052488
Iteration 180: Loss = -12319.253536814145
Iteration 190: Loss = -12319.239231430418
Iteration 200: Loss = -12319.22636859318
Iteration 210: Loss = -12319.214816948925
Iteration 220: Loss = -12319.20433226485
Iteration 230: Loss = -12319.194780210131
Iteration 240: Loss = -12319.186050084914
Iteration 250: Loss = -12319.17807550231
Iteration 260: Loss = -12319.170736326685
Iteration 270: Loss = -12319.163965229553
Iteration 280: Loss = -12319.15774597
Iteration 290: Loss = -12319.151949459489
Iteration 300: Loss = -12319.146582330484
Iteration 310: Loss = -12319.14156681631
Iteration 320: Loss = -12319.136847462081
Iteration 330: Loss = -12319.132476583725
Iteration 340: Loss = -12319.128383924488
Iteration 350: Loss = -12319.124489716094
Iteration 360: Loss = -12319.120877379053
Iteration 370: Loss = -12319.117412091933
Iteration 380: Loss = -12319.114175837003
Iteration 390: Loss = -12319.111056702104
Iteration 400: Loss = -12319.1081446782
Iteration 410: Loss = -12319.10539848428
Iteration 420: Loss = -12319.102779161258
Iteration 430: Loss = -12319.100250837126
Iteration 440: Loss = -12319.09786712444
Iteration 450: Loss = -12319.095620181417
Iteration 460: Loss = -12319.09341976589
Iteration 470: Loss = -12319.091300843731
Iteration 480: Loss = -12319.08931674019
Iteration 490: Loss = -12319.087381369289
Iteration 500: Loss = -12319.085561553467
Iteration 510: Loss = -12319.083745891145
Iteration 520: Loss = -12319.08205473579
Iteration 530: Loss = -12319.080451141737
Iteration 540: Loss = -12319.07887289376
Iteration 550: Loss = -12319.077331626235
Iteration 560: Loss = -12319.075881154102
Iteration 570: Loss = -12319.074436646059
Iteration 580: Loss = -12319.073091200722
Iteration 590: Loss = -12319.071749468401
Iteration 600: Loss = -12319.070481495717
Iteration 610: Loss = -12319.06921139316
Iteration 620: Loss = -12319.068047742161
Iteration 630: Loss = -12319.066904066241
Iteration 640: Loss = -12319.065740501008
Iteration 650: Loss = -12319.064602149792
Iteration 660: Loss = -12319.063571932174
Iteration 670: Loss = -12319.062548341857
Iteration 680: Loss = -12319.061506215181
Iteration 690: Loss = -12319.060536751744
Iteration 700: Loss = -12319.059614773578
Iteration 710: Loss = -12319.058664888635
Iteration 720: Loss = -12319.057746255405
Iteration 730: Loss = -12319.056834754747
Iteration 740: Loss = -12319.055987956292
Iteration 750: Loss = -12319.055142601028
Iteration 760: Loss = -12319.054311223226
Iteration 770: Loss = -12319.053532430295
Iteration 780: Loss = -12319.052763960171
Iteration 790: Loss = -12319.051971062427
Iteration 800: Loss = -12319.051254507178
Iteration 810: Loss = -12319.05049747464
Iteration 820: Loss = -12319.049742188523
Iteration 830: Loss = -12319.049063377824
Iteration 840: Loss = -12319.048391616487
Iteration 850: Loss = -12319.047728717305
Iteration 860: Loss = -12319.047040154657
Iteration 870: Loss = -12319.046382336843
Iteration 880: Loss = -12319.045730708023
Iteration 890: Loss = -12319.045120624045
Iteration 900: Loss = -12319.044495287322
Iteration 910: Loss = -12319.043904306489
Iteration 920: Loss = -12319.043288339044
Iteration 930: Loss = -12319.042738792523
Iteration 940: Loss = -12319.042127137245
Iteration 950: Loss = -12319.041568170383
Iteration 960: Loss = -12319.041060566957
Iteration 970: Loss = -12319.040488771087
Iteration 980: Loss = -12319.039954400174
Iteration 990: Loss = -12319.039421015515
Iteration 1000: Loss = -12319.038924143755
Iteration 1010: Loss = -12319.0384054687
Iteration 1020: Loss = -12319.037924109745
Iteration 1030: Loss = -12319.037377404333
Iteration 1040: Loss = -12319.036913106507
Iteration 1050: Loss = -12319.036424951224
Iteration 1060: Loss = -12319.035956556236
Iteration 1070: Loss = -12319.035464692965
Iteration 1080: Loss = -12319.034997566314
Iteration 1090: Loss = -12319.034524034829
Iteration 1100: Loss = -12319.034120864928
Iteration 1110: Loss = -12319.033672615074
Iteration 1120: Loss = -12319.033226111704
Iteration 1130: Loss = -12319.032775568621
Iteration 1140: Loss = -12319.032362782382
Iteration 1150: Loss = -12319.031952510335
Iteration 1160: Loss = -12319.031549780317
Iteration 1170: Loss = -12319.031086433164
Iteration 1180: Loss = -12319.030685004283
Iteration 1190: Loss = -12319.030322169014
Iteration 1200: Loss = -12319.02993311895
Iteration 1210: Loss = -12319.02952885753
Iteration 1220: Loss = -12319.029150624361
Iteration 1230: Loss = -12319.028710908513
Iteration 1240: Loss = -12319.028360178327
Iteration 1250: Loss = -12319.027974174549
Iteration 1260: Loss = -12319.027583121042
Iteration 1270: Loss = -12319.027243662082
Iteration 1280: Loss = -12319.026887609361
Iteration 1290: Loss = -12319.026490056725
Iteration 1300: Loss = -12319.02619528385
Iteration 1310: Loss = -12319.02579270121
Iteration 1320: Loss = -12319.025456878975
Iteration 1330: Loss = -12319.025101132565
Iteration 1340: Loss = -12319.024760803748
Iteration 1350: Loss = -12319.024495885189
Iteration 1360: Loss = -12319.024084609599
Iteration 1370: Loss = -12319.023735338638
Iteration 1380: Loss = -12319.023442536443
Iteration 1390: Loss = -12319.02312488338
Iteration 1400: Loss = -12319.022789158093
Iteration 1410: Loss = -12319.022455337306
Iteration 1420: Loss = -12319.022115642472
Iteration 1430: Loss = -12319.021843398705
Iteration 1440: Loss = -12319.021528285257
Iteration 1450: Loss = -12319.021196864696
Iteration 1460: Loss = -12319.020905912592
Iteration 1470: Loss = -12319.020594696409
Iteration 1480: Loss = -12319.020317068813
Iteration 1490: Loss = -12319.019978236067
Iteration 1500: Loss = -12319.019669508021
Iteration 1510: Loss = -12319.019398191609
Iteration 1520: Loss = -12319.019097206852
Iteration 1530: Loss = -12319.018789571297
Iteration 1540: Loss = -12319.018513358258
Iteration 1550: Loss = -12319.018199946084
Iteration 1560: Loss = -12319.017955357911
Iteration 1570: Loss = -12319.017690933071
Iteration 1580: Loss = -12319.01740234152
Iteration 1590: Loss = -12319.017129383297
Iteration 1600: Loss = -12319.016862831566
Iteration 1610: Loss = -12319.01655963727
Iteration 1620: Loss = -12319.016349682755
Iteration 1630: Loss = -12319.016072620572
Iteration 1640: Loss = -12319.01579064986
Iteration 1650: Loss = -12319.015537749898
Iteration 1660: Loss = -12319.015297785603
Iteration 1670: Loss = -12319.014992365732
Iteration 1680: Loss = -12319.014745160752
Iteration 1690: Loss = -12319.014507601227
Iteration 1700: Loss = -12319.014257946343
Iteration 1710: Loss = -12319.014022389125
Iteration 1720: Loss = -12319.013779744082
Iteration 1730: Loss = -12319.01349804933
Iteration 1740: Loss = -12319.013299757456
Iteration 1750: Loss = -12319.01297891345
Iteration 1760: Loss = -12319.012777648191
Iteration 1770: Loss = -12319.012532257895
Iteration 1780: Loss = -12319.012290930954
Iteration 1790: Loss = -12319.012009219512
Iteration 1800: Loss = -12319.011877101466
Iteration 1810: Loss = -12319.011604988249
Iteration 1820: Loss = -12319.011365308745
Iteration 1830: Loss = -12319.011119100041
Iteration 1840: Loss = -12319.01094928749
Iteration 1850: Loss = -12319.010720723656
Iteration 1860: Loss = -12319.010478533579
Iteration 1870: Loss = -12319.010264323344
Iteration 1880: Loss = -12319.010001769193
Iteration 1890: Loss = -12319.00981370589
Iteration 1900: Loss = -12319.009545697256
Iteration 1910: Loss = -12319.00935822499
Iteration 1920: Loss = -12319.009163980798
Iteration 1930: Loss = -12319.008985264678
Iteration 1940: Loss = -12319.00875128806
Iteration 1950: Loss = -12319.008559014092
Iteration 1960: Loss = -12319.008314891256
Iteration 1970: Loss = -12319.008108274267
Iteration 1980: Loss = -12319.00791473816
Iteration 1990: Loss = -12319.00771175705
Iteration 2000: Loss = -12319.007531508645
Iteration 2010: Loss = -12319.007346587527
Iteration 2020: Loss = -12319.00712683897
Iteration 2030: Loss = -12319.006957369216
Iteration 2040: Loss = -12319.006771210714
Iteration 2050: Loss = -12319.006566626957
Iteration 2060: Loss = -12319.00641164427
Iteration 2070: Loss = -12319.00618264874
Iteration 2080: Loss = -12319.006004655683
Iteration 2090: Loss = -12319.00578238642
Iteration 2100: Loss = -12319.00565841907
Iteration 2110: Loss = -12319.005456280378
Iteration 2120: Loss = -12319.005236652178
Iteration 2130: Loss = -12319.005076125904
Iteration 2140: Loss = -12319.004870720355
Iteration 2150: Loss = -12319.004726337116
Iteration 2160: Loss = -12319.004538666668
Iteration 2170: Loss = -12319.004370834118
Iteration 2180: Loss = -12319.004202608396
Iteration 2190: Loss = -12319.004051301548
Iteration 2200: Loss = -12319.00389731935
Iteration 2210: Loss = -12319.00368440885
Iteration 2220: Loss = -12319.003547021062
Iteration 2230: Loss = -12319.003362148202
Iteration 2240: Loss = -12319.003217353807
Iteration 2250: Loss = -12319.003035964142
Iteration 2260: Loss = -12319.002912936914
Iteration 2270: Loss = -12319.00267782787
Iteration 2280: Loss = -12319.002574756447
Iteration 2290: Loss = -12319.002394182273
Iteration 2300: Loss = -12319.002271146705
Iteration 2310: Loss = -12319.002075254524
Iteration 2320: Loss = -12319.001921581908
Iteration 2330: Loss = -12319.001787978486
Iteration 2340: Loss = -12319.00163220207
Iteration 2350: Loss = -12319.001528327757
Iteration 2360: Loss = -12319.001324278306
Iteration 2370: Loss = -12319.001226456652
Iteration 2380: Loss = -12319.001039319995
Iteration 2390: Loss = -12319.00090600313
Iteration 2400: Loss = -12319.00078492733
Iteration 2410: Loss = -12319.000633765709
Iteration 2420: Loss = -12319.000499268903
Iteration 2430: Loss = -12319.000391285183
Iteration 2440: Loss = -12319.000214246264
Iteration 2450: Loss = -12319.000108960456
Iteration 2460: Loss = -12318.999938708439
Iteration 2470: Loss = -12318.999771531231
Iteration 2480: Loss = -12318.999691951334
Iteration 2490: Loss = -12318.999528211894
Iteration 2500: Loss = -12318.999415973673
Iteration 2510: Loss = -12318.999287192251
Iteration 2520: Loss = -12318.999151779637
Iteration 2530: Loss = -12318.999060098247
Iteration 2540: Loss = -12318.998926832335
Iteration 2550: Loss = -12318.998834287593
Iteration 2560: Loss = -12318.99865270976
Iteration 2570: Loss = -12318.998556650391
Iteration 2580: Loss = -12318.99843569455
Iteration 2590: Loss = -12318.998310933295
Iteration 2600: Loss = -12318.998186851284
Iteration 2610: Loss = -12318.998072168186
Iteration 2620: Loss = -12318.997940314333
Iteration 2630: Loss = -12318.997821279287
Iteration 2640: Loss = -12318.997713267325
Iteration 2650: Loss = -12318.997579860561
Iteration 2660: Loss = -12318.997500868198
Iteration 2670: Loss = -12318.997375270135
Iteration 2680: Loss = -12318.99727820223
Iteration 2690: Loss = -12318.997195327775
Iteration 2700: Loss = -12318.997102385632
Iteration 2710: Loss = -12318.996959879429
Iteration 2720: Loss = -12318.996828841562
Iteration 2730: Loss = -12318.996727286169
Iteration 2740: Loss = -12318.996614833162
Iteration 2750: Loss = -12318.996582089107
Iteration 2760: Loss = -12318.996441508138
Iteration 2770: Loss = -12318.996352600878
Iteration 2780: Loss = -12318.996276308275
Iteration 2790: Loss = -12318.996172661748
Iteration 2800: Loss = -12318.996034310883
Iteration 2810: Loss = -12318.995942936806
Iteration 2820: Loss = -12318.99586261904
Iteration 2830: Loss = -12318.995769117515
Iteration 2840: Loss = -12318.995650018023
Iteration 2850: Loss = -12318.995554089495
Iteration 2860: Loss = -12318.99548062513
Iteration 2870: Loss = -12318.99543243909
Iteration 2880: Loss = -12318.995296850391
Iteration 2890: Loss = -12318.995170154154
Iteration 2900: Loss = -12318.995094613487
Iteration 2910: Loss = -12318.995017150277
Iteration 2920: Loss = -12318.99494755502
Iteration 2930: Loss = -12318.994887686098
Iteration 2940: Loss = -12318.99473844537
Iteration 2950: Loss = -12318.994685283325
Iteration 2960: Loss = -12318.994625204723
Iteration 2970: Loss = -12318.994515154594
Iteration 2980: Loss = -12318.994422521408
Iteration 2990: Loss = -12318.994362016923
Iteration 3000: Loss = -12318.994263391964
Iteration 3010: Loss = -12318.99425323772
Iteration 3020: Loss = -12318.994097143152
Iteration 3030: Loss = -12318.994025413269
Iteration 3040: Loss = -12318.993986553014
Iteration 3050: Loss = -12318.993873366571
Iteration 3060: Loss = -12318.993822728215
Iteration 3070: Loss = -12318.993757516208
Iteration 3080: Loss = -12318.993692723363
Iteration 3090: Loss = -12318.993592738207
Iteration 3100: Loss = -12318.993503748772
Iteration 3110: Loss = -12318.993486647223
Iteration 3120: Loss = -12318.99339513646
Iteration 3130: Loss = -12318.99331249337
Iteration 3140: Loss = -12318.993262817661
Iteration 3150: Loss = -12318.993209164491
Iteration 3160: Loss = -12318.99312591226
Iteration 3170: Loss = -12318.993056659752
Iteration 3180: Loss = -12318.992982377098
Iteration 3190: Loss = -12318.992871487619
Iteration 3200: Loss = -12318.992866141694
Iteration 3210: Loss = -12318.992784262442
Iteration 3220: Loss = -12318.992710975544
Iteration 3230: Loss = -12318.99265275772
Iteration 3240: Loss = -12318.99260535102
Iteration 3250: Loss = -12318.992490630058
Iteration 3260: Loss = -12318.992490471575
Iteration 3270: Loss = -12318.992449315683
Iteration 3280: Loss = -12318.992405318422
Iteration 3290: Loss = -12318.992324108534
Iteration 3300: Loss = -12318.992239406538
Iteration 3310: Loss = -12318.992139288506
Iteration 3320: Loss = -12318.992138750558
Iteration 3330: Loss = -12318.992039820583
Iteration 3340: Loss = -12318.991999962851
Iteration 3350: Loss = -12318.99191718681
Iteration 3360: Loss = -12318.991941074622
1
Iteration 3370: Loss = -12318.991846063182
Iteration 3380: Loss = -12318.991782745692
Iteration 3390: Loss = -12318.991744014149
Iteration 3400: Loss = -12318.991708620753
Iteration 3410: Loss = -12318.991620739775
Iteration 3420: Loss = -12318.991552018853
Iteration 3430: Loss = -12318.991558056696
1
Iteration 3440: Loss = -12318.99146257466
Iteration 3450: Loss = -12318.991385862324
Iteration 3460: Loss = -12318.991367487464
Iteration 3470: Loss = -12318.991323148884
Iteration 3480: Loss = -12318.99128306557
Iteration 3490: Loss = -12318.991227239747
Iteration 3500: Loss = -12318.991165818828
Iteration 3510: Loss = -12318.991148137045
Iteration 3520: Loss = -12318.991111994095
Iteration 3530: Loss = -12318.991013131936
Iteration 3540: Loss = -12318.990987463496
Iteration 3550: Loss = -12318.990949198707
Iteration 3560: Loss = -12318.990910390592
Iteration 3570: Loss = -12318.990847364494
Iteration 3580: Loss = -12318.990845359956
Iteration 3590: Loss = -12318.990771781575
Iteration 3600: Loss = -12318.990755654242
Iteration 3610: Loss = -12318.99070915499
Iteration 3620: Loss = -12318.990614328406
Iteration 3630: Loss = -12318.990625391829
1
Iteration 3640: Loss = -12318.990548539266
Iteration 3650: Loss = -12318.990508913019
Iteration 3660: Loss = -12318.990473812759
Iteration 3670: Loss = -12318.990461736072
Iteration 3680: Loss = -12318.990364271216
Iteration 3690: Loss = -12318.990361374692
Iteration 3700: Loss = -12318.990317175283
Iteration 3710: Loss = -12318.990251571395
Iteration 3720: Loss = -12318.990280470449
1
Iteration 3730: Loss = -12318.990181651594
Iteration 3740: Loss = -12318.990182836149
1
Iteration 3750: Loss = -12318.990113622614
Iteration 3760: Loss = -12318.990075269172
Iteration 3770: Loss = -12318.990044216906
Iteration 3780: Loss = -12318.990013266923
Iteration 3790: Loss = -12318.989945746827
Iteration 3800: Loss = -12318.989959661985
1
Iteration 3810: Loss = -12318.98991024444
Iteration 3820: Loss = -12318.989856914171
Iteration 3830: Loss = -12318.9898347261
Iteration 3840: Loss = -12318.98979746397
Iteration 3850: Loss = -12318.989802456368
1
Iteration 3860: Loss = -12318.9897159118
Iteration 3870: Loss = -12318.98968287932
Iteration 3880: Loss = -12318.989659936406
Iteration 3890: Loss = -12318.989625181981
Iteration 3900: Loss = -12318.989574558089
Iteration 3910: Loss = -12318.989572194692
Iteration 3920: Loss = -12318.989548715774
Iteration 3930: Loss = -12318.98950694461
Iteration 3940: Loss = -12318.989467548334
Iteration 3950: Loss = -12318.989421183656
Iteration 3960: Loss = -12318.989466030218
1
Iteration 3970: Loss = -12318.989395692923
Iteration 3980: Loss = -12318.989350737795
Iteration 3990: Loss = -12318.989349974332
Iteration 4000: Loss = -12318.989263367343
Iteration 4010: Loss = -12318.989242952706
Iteration 4020: Loss = -12318.989256924628
1
Iteration 4030: Loss = -12318.989258815769
2
Iteration 4040: Loss = -12318.989190565999
Iteration 4050: Loss = -12318.989187063826
Iteration 4060: Loss = -12318.989093363689
Iteration 4070: Loss = -12318.989136130014
1
Iteration 4080: Loss = -12318.98906823404
Iteration 4090: Loss = -12318.98907467402
1
Iteration 4100: Loss = -12318.989010900716
Iteration 4110: Loss = -12318.988994863483
Iteration 4120: Loss = -12318.988961040324
Iteration 4130: Loss = -12318.988911801061
Iteration 4140: Loss = -12318.988897743238
Iteration 4150: Loss = -12318.988916119348
1
Iteration 4160: Loss = -12318.988876652389
Iteration 4170: Loss = -12318.988826090192
Iteration 4180: Loss = -12318.988785034186
Iteration 4190: Loss = -12318.98876425543
Iteration 4200: Loss = -12318.988717662996
Iteration 4210: Loss = -12318.988754728663
1
Iteration 4220: Loss = -12318.988728757331
2
Iteration 4230: Loss = -12318.988679643842
Iteration 4240: Loss = -12318.988646231486
Iteration 4250: Loss = -12318.988632227938
Iteration 4260: Loss = -12318.98861744448
Iteration 4270: Loss = -12318.98857860671
Iteration 4280: Loss = -12318.988594109525
1
Iteration 4290: Loss = -12318.988587625327
2
Iteration 4300: Loss = -12318.988557964449
Iteration 4310: Loss = -12318.988500370766
Iteration 4320: Loss = -12318.98848256553
Iteration 4330: Loss = -12318.988464253749
Iteration 4340: Loss = -12318.988454891018
Iteration 4350: Loss = -12318.988370312502
Iteration 4360: Loss = -12318.988389506503
1
Iteration 4370: Loss = -12318.98842090023
2
Iteration 4380: Loss = -12318.98837380905
3
Stopping early at iteration 4380 due to no improvement.
pi: tensor([[1.2406e-01, 8.7594e-01],
        [1.0000e+00, 2.0872e-12]], dtype=torch.float64)
alpha: tensor([0.5331, 0.4669])
beta: tensor([[[0.1961, 0.1970],
         [0.0313, 0.1961]],

        [[0.0846, 0.2008],
         [0.7431, 0.1231]],

        [[0.3462, 0.1900],
         [0.7982, 0.8682]],

        [[0.9823, 0.1916],
         [0.2440, 0.2493]],

        [[0.9685, 0.2012],
         [0.8780, 0.1052]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22729.03403124054
Iteration 100: Loss = -12320.749772906975
Iteration 200: Loss = -12319.820507798488
Iteration 300: Loss = -12319.658982880044
Iteration 400: Loss = -12319.588931938064
Iteration 500: Loss = -12319.550213835819
Iteration 600: Loss = -12319.525176743227
Iteration 700: Loss = -12319.507431542379
Iteration 800: Loss = -12319.494036175722
Iteration 900: Loss = -12319.48329177485
Iteration 1000: Loss = -12319.474339439295
Iteration 1100: Loss = -12319.46656872957
Iteration 1200: Loss = -12319.459569443121
Iteration 1300: Loss = -12319.452924792022
Iteration 1400: Loss = -12319.446464427734
Iteration 1500: Loss = -12319.439871951683
Iteration 1600: Loss = -12319.432903169216
Iteration 1700: Loss = -12319.42529655485
Iteration 1800: Loss = -12319.416672482033
Iteration 1900: Loss = -12319.406676576917
Iteration 2000: Loss = -12319.394702115791
Iteration 2100: Loss = -12319.379896637065
Iteration 2200: Loss = -12319.36107789809
Iteration 2300: Loss = -12319.33635075956
Iteration 2400: Loss = -12319.3027828638
Iteration 2500: Loss = -12319.256225820098
Iteration 2600: Loss = -12319.191597484818
Iteration 2700: Loss = -12319.106895285056
Iteration 2800: Loss = -12319.010392027827
Iteration 2900: Loss = -12318.918030543204
Iteration 3000: Loss = -12318.923052262555
1
Iteration 3100: Loss = -12318.794251086687
Iteration 3200: Loss = -12318.760051545678
Iteration 3300: Loss = -12318.738770739572
Iteration 3400: Loss = -12318.723427721057
Iteration 3500: Loss = -12318.712802404301
Iteration 3600: Loss = -12318.705068570212
Iteration 3700: Loss = -12318.698923387732
Iteration 3800: Loss = -12318.693995743066
Iteration 3900: Loss = -12318.689988304943
Iteration 4000: Loss = -12318.686867865228
Iteration 4100: Loss = -12318.684242256613
Iteration 4200: Loss = -12318.68234871154
Iteration 4300: Loss = -12318.68091615374
Iteration 4400: Loss = -12318.67995432853
Iteration 4500: Loss = -12318.679354972755
Iteration 4600: Loss = -12318.678812906483
Iteration 4700: Loss = -12318.678544635539
Iteration 4800: Loss = -12318.678392034097
Iteration 4900: Loss = -12318.678020889965
Iteration 5000: Loss = -12318.677849763179
Iteration 5100: Loss = -12318.677618678617
Iteration 5200: Loss = -12318.67740597104
Iteration 5300: Loss = -12318.67716119585
Iteration 5400: Loss = -12318.676916523847
Iteration 5500: Loss = -12318.680824331368
1
Iteration 5600: Loss = -12318.67657249118
Iteration 5700: Loss = -12318.676100441966
Iteration 5800: Loss = -12318.675758400861
Iteration 5900: Loss = -12318.675451484589
Iteration 6000: Loss = -12318.674994673094
Iteration 6100: Loss = -12318.674592979713
Iteration 6200: Loss = -12318.675239134753
1
Iteration 6300: Loss = -12318.673704564173
Iteration 6400: Loss = -12318.686461135629
1
Iteration 6500: Loss = -12318.672701340638
Iteration 6600: Loss = -12318.672254908652
Iteration 6700: Loss = -12318.671756912998
Iteration 6800: Loss = -12318.671469917383
Iteration 6900: Loss = -12318.70711771613
1
Iteration 7000: Loss = -12318.671106405343
Iteration 7100: Loss = -12318.670106091007
Iteration 7200: Loss = -12318.672148666943
1
Iteration 7300: Loss = -12318.66939885961
Iteration 7400: Loss = -12318.676177189649
1
Iteration 7500: Loss = -12318.668920034288
Iteration 7600: Loss = -12318.670003257766
1
Iteration 7700: Loss = -12318.72399906507
2
Iteration 7800: Loss = -12318.668541740859
Iteration 7900: Loss = -12318.668498804675
Iteration 8000: Loss = -12318.668434592602
Iteration 8100: Loss = -12318.671932346571
1
Iteration 8200: Loss = -12318.670255833542
2
Iteration 8300: Loss = -12318.670519618141
3
Iteration 8400: Loss = -12318.676908737049
4
Iteration 8500: Loss = -12318.759116404837
5
Stopping early at iteration 8500 due to no improvement.
pi: tensor([[0.2811, 0.7189],
        [0.8443, 0.1557]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8699, 0.1301], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1968, 0.2014],
         [0.6725, 0.2010]],

        [[0.5480, 0.2021],
         [0.5353, 0.6580]],

        [[0.7015, 0.1930],
         [0.7075, 0.7093]],

        [[0.5020, 0.1917],
         [0.6258, 0.5380]],

        [[0.5501, 0.2037],
         [0.5595, 0.6773]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0019679364230320478
Average Adjusted Rand Index: -0.0005926784880256398
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21211.13929716607
Iteration 100: Loss = -12319.83146095179
Iteration 200: Loss = -12319.545155570517
Iteration 300: Loss = -12319.488728039389
Iteration 400: Loss = -12319.462998444711
Iteration 500: Loss = -12319.448250330162
Iteration 600: Loss = -12319.437894377179
Iteration 700: Loss = -12319.4291918584
Iteration 800: Loss = -12319.421037258733
Iteration 900: Loss = -12319.412710303213
Iteration 1000: Loss = -12319.403756625848
Iteration 1100: Loss = -12319.393631816387
Iteration 1200: Loss = -12319.38172444411
Iteration 1300: Loss = -12319.367256566054
Iteration 1400: Loss = -12319.34926700816
Iteration 1500: Loss = -12319.326103713302
Iteration 1600: Loss = -12319.295497321116
Iteration 1700: Loss = -12319.25414096346
Iteration 1800: Loss = -12319.197865300517
Iteration 1900: Loss = -12319.123680289806
Iteration 2000: Loss = -12319.035498369052
Iteration 2100: Loss = -12318.946494888327
Iteration 2200: Loss = -12318.870018598443
Iteration 2300: Loss = -12318.81262784523
Iteration 2400: Loss = -12318.77331670697
Iteration 2500: Loss = -12318.74851715816
Iteration 2600: Loss = -12318.730595738767
Iteration 2700: Loss = -12318.718099662296
Iteration 2800: Loss = -12318.709950511291
Iteration 2900: Loss = -12318.702068825032
Iteration 3000: Loss = -12318.696445271322
Iteration 3100: Loss = -12318.692080734332
Iteration 3200: Loss = -12318.688310871397
Iteration 3300: Loss = -12318.685423776993
Iteration 3400: Loss = -12318.68441903012
Iteration 3500: Loss = -12318.68157198181
Iteration 3600: Loss = -12318.68047733054
Iteration 3700: Loss = -12318.679752420921
Iteration 3800: Loss = -12318.679144020954
Iteration 3900: Loss = -12318.678827330466
Iteration 4000: Loss = -12318.698014313817
1
Iteration 4100: Loss = -12318.67848031706
Iteration 4200: Loss = -12318.678291626973
Iteration 4300: Loss = -12318.67826343697
Iteration 4400: Loss = -12318.678028452898
Iteration 4500: Loss = -12318.677946939457
Iteration 4600: Loss = -12318.678280590666
1
Iteration 4700: Loss = -12318.677647656932
Iteration 4800: Loss = -12318.681634697552
1
Iteration 4900: Loss = -12318.677289212204
Iteration 5000: Loss = -12318.677131256436
Iteration 5100: Loss = -12318.679994460193
1
Iteration 5200: Loss = -12318.676696112107
Iteration 5300: Loss = -12318.67649245071
Iteration 5400: Loss = -12318.67645740458
Iteration 5500: Loss = -12318.676031913858
Iteration 5600: Loss = -12318.67591059346
Iteration 5700: Loss = -12318.675453829057
Iteration 5800: Loss = -12318.678842378818
1
Iteration 5900: Loss = -12318.674932930411
Iteration 6000: Loss = -12318.67456720113
Iteration 6100: Loss = -12318.6742441206
Iteration 6200: Loss = -12318.673847840713
Iteration 6300: Loss = -12318.677533664813
1
Iteration 6400: Loss = -12318.676784822825
2
Iteration 6500: Loss = -12318.672738603042
Iteration 6600: Loss = -12318.672444598536
Iteration 6700: Loss = -12318.673347302883
1
Iteration 6800: Loss = -12318.67148178429
Iteration 6900: Loss = -12318.674311834413
1
Iteration 7000: Loss = -12318.67070648107
Iteration 7100: Loss = -12318.671202374697
1
Iteration 7200: Loss = -12318.670517001345
Iteration 7300: Loss = -12318.67998073656
1
Iteration 7400: Loss = -12318.672921699861
2
Iteration 7500: Loss = -12318.669534955387
Iteration 7600: Loss = -12318.669076299324
Iteration 7700: Loss = -12318.671902545377
1
Iteration 7800: Loss = -12318.672395104346
2
Iteration 7900: Loss = -12318.670905503152
3
Iteration 8000: Loss = -12318.668940483507
Iteration 8100: Loss = -12318.671155655093
1
Iteration 8200: Loss = -12318.676417825995
2
Iteration 8300: Loss = -12318.669203683221
3
Iteration 8400: Loss = -12318.668365311843
Iteration 8500: Loss = -12318.679025239524
1
Iteration 8600: Loss = -12318.678334211076
2
Iteration 8700: Loss = -12318.73902976686
3
Iteration 8800: Loss = -12318.66959096838
4
Iteration 8900: Loss = -12318.668346767607
Iteration 9000: Loss = -12318.670296767274
1
Iteration 9100: Loss = -12318.672120828534
2
Iteration 9200: Loss = -12318.76240350957
3
Iteration 9300: Loss = -12318.671113012779
4
Iteration 9400: Loss = -12318.668305326151
Iteration 9500: Loss = -12318.670691643854
1
Iteration 9600: Loss = -12318.668187532967
Iteration 9700: Loss = -12318.668349111855
1
Iteration 9800: Loss = -12318.790795280762
2
Iteration 9900: Loss = -12318.695823846054
3
Iteration 10000: Loss = -12318.835127675904
4
Iteration 10100: Loss = -12318.668503718784
5
Stopping early at iteration 10100 due to no improvement.
pi: tensor([[0.2622, 0.7378],
        [0.8264, 0.1736]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8595, 0.1405], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1971, 0.2012],
         [0.5427, 0.1996]],

        [[0.5528, 0.2029],
         [0.5174, 0.6015]],

        [[0.5156, 0.1918],
         [0.7120, 0.6812]],

        [[0.5243, 0.1934],
         [0.5080, 0.6968]],

        [[0.6109, 0.2034],
         [0.7166, 0.5029]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0018866294423625228
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19288.71519570983
Iteration 100: Loss = -12320.147058350658
Iteration 200: Loss = -12319.66039772799
Iteration 300: Loss = -12319.569498308902
Iteration 400: Loss = -12319.51332779486
Iteration 500: Loss = -12319.472397115645
Iteration 600: Loss = -12319.441176946882
Iteration 700: Loss = -12319.416251690416
Iteration 800: Loss = -12319.395342802594
Iteration 900: Loss = -12319.376564084332
Iteration 1000: Loss = -12319.358457613384
Iteration 1100: Loss = -12319.339855828906
Iteration 1200: Loss = -12319.31953177628
Iteration 1300: Loss = -12319.296434653896
Iteration 1400: Loss = -12319.269343227226
Iteration 1500: Loss = -12319.23702781011
Iteration 1600: Loss = -12319.19830181053
Iteration 1700: Loss = -12319.1523755729
Iteration 1800: Loss = -12319.099580018292
Iteration 1900: Loss = -12319.041837050541
Iteration 2000: Loss = -12318.9824402052
Iteration 2100: Loss = -12318.925126070513
Iteration 2200: Loss = -12318.873692779413
Iteration 2300: Loss = -12318.830604091947
Iteration 2400: Loss = -12318.796434298945
Iteration 2500: Loss = -12318.776261497407
Iteration 2600: Loss = -12318.75132537543
Iteration 2700: Loss = -12318.736539466041
Iteration 2800: Loss = -12318.725618748129
Iteration 2900: Loss = -12318.716852865293
Iteration 3000: Loss = -12318.710128464745
Iteration 3100: Loss = -12318.704444580457
Iteration 3200: Loss = -12318.699860286852
Iteration 3300: Loss = -12318.69654731763
Iteration 3400: Loss = -12318.692531032402
Iteration 3500: Loss = -12318.6895507492
Iteration 3600: Loss = -12318.686927439845
Iteration 3700: Loss = -12318.691219414468
1
Iteration 3800: Loss = -12318.683216455458
Iteration 3900: Loss = -12318.680688935954
Iteration 4000: Loss = -12318.690255777616
1
Iteration 4100: Loss = -12318.677933982737
Iteration 4200: Loss = -12318.676938548679
Iteration 4300: Loss = -12318.688887099644
1
Iteration 4400: Loss = -12318.675709111829
Iteration 4500: Loss = -12318.675174425975
Iteration 4600: Loss = -12318.675357646795
1
Iteration 4700: Loss = -12318.674617280923
Iteration 4800: Loss = -12318.674307015677
Iteration 4900: Loss = -12318.67420206686
Iteration 5000: Loss = -12318.673917772207
Iteration 5100: Loss = -12318.675599861104
1
Iteration 5200: Loss = -12318.673475307089
Iteration 5300: Loss = -12318.67360127242
1
Iteration 5400: Loss = -12318.673153004132
Iteration 5500: Loss = -12318.672714509814
Iteration 5600: Loss = -12318.68558895379
1
Iteration 5700: Loss = -12318.67218662524
Iteration 5800: Loss = -12318.67192393805
Iteration 5900: Loss = -12318.67542144374
1
Iteration 6000: Loss = -12318.671327909193
Iteration 6100: Loss = -12318.67169893894
1
Iteration 6200: Loss = -12318.670753835662
Iteration 6300: Loss = -12318.670501693086
Iteration 6400: Loss = -12318.670334501638
Iteration 6500: Loss = -12318.671875975406
1
Iteration 6600: Loss = -12318.669702495601
Iteration 6700: Loss = -12318.670342080623
1
Iteration 6800: Loss = -12318.669317168124
Iteration 6900: Loss = -12318.726730383594
1
Iteration 7000: Loss = -12318.66894395855
Iteration 7100: Loss = -12318.668820882996
Iteration 7200: Loss = -12318.668715656864
Iteration 7300: Loss = -12318.668584164008
Iteration 7400: Loss = -12318.669327800299
1
Iteration 7500: Loss = -12318.668415954697
Iteration 7600: Loss = -12318.66841881652
1
Iteration 7700: Loss = -12318.673187450893
2
Iteration 7800: Loss = -12318.668838692804
3
Iteration 7900: Loss = -12318.668476827586
4
Iteration 8000: Loss = -12318.66847949107
5
Stopping early at iteration 8000 due to no improvement.
pi: tensor([[0.1612, 0.8388],
        [0.7186, 0.2814]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1280, 0.8720], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1995, 0.2014],
         [0.5941, 0.1971]],

        [[0.5250, 0.2029],
         [0.5134, 0.6780]],

        [[0.6676, 0.1917],
         [0.5332, 0.6880]],

        [[0.6704, 0.1934],
         [0.5990, 0.6934]],

        [[0.6260, 0.2034],
         [0.5666, 0.7175]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.0032454170932134756
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.001970293435614751
Average Adjusted Rand Index: -0.0006490834186426951
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21667.756835931566
Iteration 100: Loss = -12320.896966724064
Iteration 200: Loss = -12319.972982733525
Iteration 300: Loss = -12319.72002539576
Iteration 400: Loss = -12319.629517757632
Iteration 500: Loss = -12319.587745443483
Iteration 600: Loss = -12319.562842045334
Iteration 700: Loss = -12319.545186925716
Iteration 800: Loss = -12319.531337332435
Iteration 900: Loss = -12319.5198261631
Iteration 1000: Loss = -12319.50990723627
Iteration 1100: Loss = -12319.50108218674
Iteration 1200: Loss = -12319.493146888875
Iteration 1300: Loss = -12319.485699929519
Iteration 1400: Loss = -12319.478725285218
Iteration 1500: Loss = -12319.472050385035
Iteration 1600: Loss = -12319.465576654136
Iteration 1700: Loss = -12319.45906297733
Iteration 1800: Loss = -12319.452561661948
Iteration 1900: Loss = -12319.445794412912
Iteration 2000: Loss = -12319.438697086525
Iteration 2100: Loss = -12319.431098148423
Iteration 2200: Loss = -12319.422717526028
Iteration 2300: Loss = -12319.413348201779
Iteration 2400: Loss = -12319.402721237833
Iteration 2500: Loss = -12319.390304755927
Iteration 2600: Loss = -12319.375599661433
Iteration 2700: Loss = -12319.357820135003
Iteration 2800: Loss = -12319.33586945475
Iteration 2900: Loss = -12319.30839405251
Iteration 3000: Loss = -12319.273742738007
Iteration 3100: Loss = -12319.230023396563
Iteration 3200: Loss = -12319.17586012462
Iteration 3300: Loss = -12319.111448514586
Iteration 3400: Loss = -12319.039363358546
Iteration 3500: Loss = -12318.965490489647
Iteration 3600: Loss = -12318.897888133954
Iteration 3700: Loss = -12318.844116194501
Iteration 3800: Loss = -12318.801926343824
Iteration 3900: Loss = -12318.772223019647
Iteration 4000: Loss = -12318.750134635864
Iteration 4100: Loss = -12318.734624902416
Iteration 4200: Loss = -12318.722665928275
Iteration 4300: Loss = -12318.713628352876
Iteration 4400: Loss = -12318.706374190242
Iteration 4500: Loss = -12318.700427290858
Iteration 4600: Loss = -12318.701676115867
1
Iteration 4700: Loss = -12318.691372492576
Iteration 4800: Loss = -12318.68796595668
Iteration 4900: Loss = -12318.685957274532
Iteration 5000: Loss = -12318.682790354704
Iteration 5100: Loss = -12318.68104232208
Iteration 5200: Loss = -12318.679753709239
Iteration 5300: Loss = -12318.679110175475
Iteration 5400: Loss = -12318.678256369658
Iteration 5500: Loss = -12318.677595484887
Iteration 5600: Loss = -12318.6772525999
Iteration 5700: Loss = -12318.677023175314
Iteration 5800: Loss = -12318.678174772436
1
Iteration 5900: Loss = -12318.676348576824
Iteration 6000: Loss = -12318.676087047508
Iteration 6100: Loss = -12318.676161914069
1
Iteration 6200: Loss = -12318.675984609677
Iteration 6300: Loss = -12318.676587588077
1
Iteration 6400: Loss = -12318.674870695815
Iteration 6500: Loss = -12318.674509236964
Iteration 6600: Loss = -12318.687778091102
1
Iteration 6700: Loss = -12318.673795545741
Iteration 6800: Loss = -12318.674232348965
1
Iteration 6900: Loss = -12318.673033955412
Iteration 7000: Loss = -12318.672611743938
Iteration 7100: Loss = -12318.677549905784
1
Iteration 7200: Loss = -12318.671904626528
Iteration 7300: Loss = -12318.671795607494
Iteration 7400: Loss = -12318.671155560518
Iteration 7500: Loss = -12318.670942921093
Iteration 7600: Loss = -12318.671183534567
1
Iteration 7700: Loss = -12318.67027126849
Iteration 7800: Loss = -12318.672221218083
1
Iteration 7900: Loss = -12318.673513911017
2
Iteration 8000: Loss = -12318.670787290614
3
Iteration 8100: Loss = -12318.669916780178
Iteration 8200: Loss = -12318.680299342685
1
Iteration 8300: Loss = -12318.715041310248
2
Iteration 8400: Loss = -12318.680727848654
3
Iteration 8500: Loss = -12318.6699415433
4
Iteration 8600: Loss = -12318.668719353624
Iteration 8700: Loss = -12318.668851002927
1
Iteration 8800: Loss = -12318.671734634585
2
Iteration 8900: Loss = -12318.669853449477
3
Iteration 9000: Loss = -12318.66888198119
4
Iteration 9100: Loss = -12318.677089441817
5
Stopping early at iteration 9100 due to no improvement.
pi: tensor([[0.1502, 0.8498],
        [0.7188, 0.2812]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1335, 0.8665], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1996, 0.2013],
         [0.5776, 0.1971]],

        [[0.6016, 0.2028],
         [0.6285, 0.5394]],

        [[0.6124, 0.1917],
         [0.6686, 0.6905]],

        [[0.5569, 0.1946],
         [0.6582, 0.7077]],

        [[0.6382, 0.2035],
         [0.6480, 0.6610]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.0005624593535232805
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.001891343090701653
Average Adjusted Rand Index: -0.0001124918707046561
11823.971288486695
[-0.0019679364230320478, -0.0018866294423625228, -0.001970293435614751, -0.001891343090701653] [-0.0005926784880256398, 0.0, -0.0006490834186426951, -0.0001124918707046561] [12318.759116404837, 12318.668503718784, 12318.66847949107, 12318.677089441817]
-----------------------------------------------------------------------------------------
This iteration is 6
True Objective function: Loss = -11804.432205462863
Iteration 0: Loss = -12479.672623058654
Iteration 10: Loss = -12286.841395440744
Iteration 20: Loss = -12286.73048964613
Iteration 30: Loss = -12286.667255487539
Iteration 40: Loss = -12286.62609777426
Iteration 50: Loss = -12286.58463191339
Iteration 60: Loss = -12286.521301703799
Iteration 70: Loss = -12286.40748074681
Iteration 80: Loss = -12286.201468873323
Iteration 90: Loss = -12285.883134000987
Iteration 100: Loss = -12285.542457622274
Iteration 110: Loss = -12285.288584667856
Iteration 120: Loss = -12285.100151983559
Iteration 130: Loss = -12284.945201761304
Iteration 140: Loss = -12284.834561091462
Iteration 150: Loss = -12284.791859069328
Iteration 160: Loss = -12284.821614373925
1
Iteration 170: Loss = -12284.896359222015
2
Iteration 180: Loss = -12284.978609862741
3
Stopping early at iteration 180 due to no improvement.
pi: tensor([[0.9499, 0.0501],
        [0.9636, 0.0364]], dtype=torch.float64)
alpha: tensor([0.9483, 0.0517])
beta: tensor([[[0.1905, 0.2500],
         [0.1404, 0.3112]],

        [[0.7374, 0.2433],
         [0.0713, 0.6078]],

        [[0.1522, 0.2181],
         [0.1167, 0.1752]],

        [[0.4451, 0.2597],
         [0.4341, 0.0384]],

        [[0.9746, 0.1931],
         [0.8956, 0.6398]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 43
Adjusted Rand Index: -0.005513710431743218
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0008735738497905159
Average Adjusted Rand Index: -0.0007692965357627092
Iteration 0: Loss = -12391.427914596585
Iteration 10: Loss = -11825.663038247258
Iteration 20: Loss = -11825.490851979897
Iteration 30: Loss = -11825.490851907834
Iteration 40: Loss = -11825.490851907834
1
Iteration 50: Loss = -11825.490851907834
2
Iteration 60: Loss = -11825.490851907834
3
Stopping early at iteration 60 due to no improvement.
pi: tensor([[0.5949, 0.4051],
        [0.3707, 0.6293]], dtype=torch.float64)
alpha: tensor([0.4625, 0.5375])
beta: tensor([[[0.2793, 0.0988],
         [0.2384, 0.2977]],

        [[0.6098, 0.1018],
         [0.1707, 0.1172]],

        [[0.5288, 0.1032],
         [0.0977, 0.9356]],

        [[0.4833, 0.0926],
         [0.7536, 0.5480]],

        [[0.4197, 0.0967],
         [0.3296, 0.7035]]], dtype=torch.float64)
time is 0
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599857335115991
time is 2
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.03807031739393574
Average Adjusted Rand Index: 0.9679949210890684
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21328.838656366486
Iteration 100: Loss = -12288.106963687202
Iteration 200: Loss = -12287.141542722151
Iteration 300: Loss = -12286.753931337254
Iteration 400: Loss = -12286.067883187556
Iteration 500: Loss = -12285.71915636857
Iteration 600: Loss = -12285.478271241162
Iteration 700: Loss = -12285.26114549807
Iteration 800: Loss = -12284.989500510575
Iteration 900: Loss = -12239.266803104343
Iteration 1000: Loss = -12209.213254828492
Iteration 1100: Loss = -12208.878064351631
Iteration 1200: Loss = -12208.245416592386
Iteration 1300: Loss = -12208.101991881169
Iteration 1400: Loss = -12208.025152023323
Iteration 1500: Loss = -12207.97040867899
Iteration 1600: Loss = -12207.930217134788
Iteration 1700: Loss = -12207.896639375565
Iteration 1800: Loss = -12207.866011543285
Iteration 1900: Loss = -12207.834997012436
Iteration 2000: Loss = -12207.791066458807
Iteration 2100: Loss = -12196.096803454702
Iteration 2200: Loss = -11860.8381111743
Iteration 2300: Loss = -11810.406026846713
Iteration 2400: Loss = -11803.73805386109
Iteration 2500: Loss = -11803.601706068474
Iteration 2600: Loss = -11797.26104189185
Iteration 2700: Loss = -11797.238610090279
Iteration 2800: Loss = -11797.222832595497
Iteration 2900: Loss = -11797.21086065658
Iteration 3000: Loss = -11797.201190693542
Iteration 3100: Loss = -11797.193034455475
Iteration 3200: Loss = -11797.185803026865
Iteration 3300: Loss = -11797.17759643838
Iteration 3400: Loss = -11797.149065478246
Iteration 3500: Loss = -11797.140620829507
Iteration 3600: Loss = -11797.131484056608
Iteration 3700: Loss = -11797.128175231417
Iteration 3800: Loss = -11797.133715990094
1
Iteration 3900: Loss = -11797.120868199136
Iteration 4000: Loss = -11797.113954646444
Iteration 4100: Loss = -11797.114498480127
1
Iteration 4200: Loss = -11797.10700466652
Iteration 4300: Loss = -11797.105736279778
Iteration 4400: Loss = -11797.104243852142
Iteration 4500: Loss = -11797.103038005107
Iteration 4600: Loss = -11797.108919937562
1
Iteration 4700: Loss = -11797.10116527901
Iteration 4800: Loss = -11797.100291763074
Iteration 4900: Loss = -11797.100539108076
1
Iteration 5000: Loss = -11797.098621407798
Iteration 5100: Loss = -11797.100211027864
1
Iteration 5200: Loss = -11797.09716952787
Iteration 5300: Loss = -11797.096350348185
Iteration 5400: Loss = -11797.09797479816
1
Iteration 5500: Loss = -11797.095404034048
Iteration 5600: Loss = -11797.094835426049
Iteration 5700: Loss = -11797.094673441668
Iteration 5800: Loss = -11797.09385773327
Iteration 5900: Loss = -11797.0934958989
Iteration 6000: Loss = -11797.0933382197
Iteration 6100: Loss = -11797.092987101987
Iteration 6200: Loss = -11797.09224362889
Iteration 6300: Loss = -11797.091524853839
Iteration 6400: Loss = -11797.08888977057
Iteration 6500: Loss = -11797.088598167946
Iteration 6600: Loss = -11797.095602339632
1
Iteration 6700: Loss = -11797.088102499187
Iteration 6800: Loss = -11797.08786657237
Iteration 6900: Loss = -11797.095529316617
1
Iteration 7000: Loss = -11797.089387382499
2
Iteration 7100: Loss = -11797.086779344241
Iteration 7200: Loss = -11797.08584357735
Iteration 7300: Loss = -11797.085683531737
Iteration 7400: Loss = -11797.072161716584
Iteration 7500: Loss = -11797.105675815728
1
Iteration 7600: Loss = -11797.070649543472
Iteration 7700: Loss = -11797.070331342495
Iteration 7800: Loss = -11797.07453936544
1
Iteration 7900: Loss = -11797.071626756506
2
Iteration 8000: Loss = -11797.069610825862
Iteration 8100: Loss = -11797.070704915315
1
Iteration 8200: Loss = -11797.067249452259
Iteration 8300: Loss = -11797.06420283057
Iteration 8400: Loss = -11797.063627745663
Iteration 8500: Loss = -11797.065643452772
1
Iteration 8600: Loss = -11797.063474494618
Iteration 8700: Loss = -11797.062866611535
Iteration 8800: Loss = -11797.063287180346
1
Iteration 8900: Loss = -11797.064152062854
2
Iteration 9000: Loss = -11797.062448008966
Iteration 9100: Loss = -11797.06777965097
1
Iteration 9200: Loss = -11797.104623457655
2
Iteration 9300: Loss = -11797.062993516925
3
Iteration 9400: Loss = -11797.115294200528
4
Iteration 9500: Loss = -11797.060043753414
Iteration 9600: Loss = -11797.061090450414
1
Iteration 9700: Loss = -11797.060731960295
2
Iteration 9800: Loss = -11797.059961488068
Iteration 9900: Loss = -11797.085376844763
1
Iteration 10000: Loss = -11797.059827477868
Iteration 10100: Loss = -11797.236072461414
1
Iteration 10200: Loss = -11797.060985610122
2
Iteration 10300: Loss = -11797.062443138237
3
Iteration 10400: Loss = -11797.064492338102
4
Iteration 10500: Loss = -11797.067107367764
5
Stopping early at iteration 10500 due to no improvement.
pi: tensor([[0.7342, 0.2658],
        [0.2654, 0.7346]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5764, 0.4236], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2973, 0.0985],
         [0.5463, 0.2928]],

        [[0.7240, 0.1016],
         [0.6087, 0.6391]],

        [[0.5739, 0.1044],
         [0.7068, 0.6583]],

        [[0.7254, 0.0931],
         [0.5886, 0.5680]],

        [[0.7036, 0.0966],
         [0.7302, 0.6133]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
time is 2
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 3
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.960320507679542
Average Adjusted Rand Index: 0.9601559633673616
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21854.104319330316
Iteration 100: Loss = -12288.06937994488
Iteration 200: Loss = -12287.507999738487
Iteration 300: Loss = -12287.36555861813
Iteration 400: Loss = -12287.236623551311
Iteration 500: Loss = -12286.841854332552
Iteration 600: Loss = -12285.652547030651
Iteration 700: Loss = -12283.643047734391
Iteration 800: Loss = -12208.180070653574
Iteration 900: Loss = -12207.91937404303
Iteration 1000: Loss = -12207.811439229929
Iteration 1100: Loss = -12196.060494511621
Iteration 1200: Loss = -11813.723051606403
Iteration 1300: Loss = -11798.75663460624
Iteration 1400: Loss = -11798.52525248223
Iteration 1500: Loss = -11798.486749796206
Iteration 1600: Loss = -11798.463137160486
Iteration 1700: Loss = -11798.44709409976
Iteration 1800: Loss = -11798.435320024935
Iteration 1900: Loss = -11798.426393220325
Iteration 2000: Loss = -11798.419593015311
Iteration 2100: Loss = -11798.414052691534
Iteration 2200: Loss = -11798.409434706224
Iteration 2300: Loss = -11798.405255560117
Iteration 2400: Loss = -11798.397690799346
Iteration 2500: Loss = -11797.162050105302
Iteration 2600: Loss = -11797.151274184847
Iteration 2700: Loss = -11797.1483064107
Iteration 2800: Loss = -11797.144084307069
Iteration 2900: Loss = -11797.142197805393
Iteration 3000: Loss = -11797.140391017634
Iteration 3100: Loss = -11797.1383102748
Iteration 3200: Loss = -11797.134836653808
Iteration 3300: Loss = -11797.108185257091
Iteration 3400: Loss = -11797.105283620787
Iteration 3500: Loss = -11797.104389694938
Iteration 3600: Loss = -11797.103468497437
Iteration 3700: Loss = -11797.104245256385
1
Iteration 3800: Loss = -11797.103808013824
2
Iteration 3900: Loss = -11797.101132348871
Iteration 4000: Loss = -11797.100047461203
Iteration 4100: Loss = -11797.097931973887
Iteration 4200: Loss = -11797.097500510738
Iteration 4300: Loss = -11797.10226599187
1
Iteration 4400: Loss = -11797.096518506996
Iteration 4500: Loss = -11797.096149825325
Iteration 4600: Loss = -11797.096272288247
1
Iteration 4700: Loss = -11797.09994402435
2
Iteration 4800: Loss = -11797.094869273504
Iteration 4900: Loss = -11797.092993230926
Iteration 5000: Loss = -11797.08269068771
Iteration 5100: Loss = -11797.08792904046
1
Iteration 5200: Loss = -11797.076074612778
Iteration 5300: Loss = -11797.076177878353
1
Iteration 5400: Loss = -11797.070582795011
Iteration 5500: Loss = -11797.062894319612
Iteration 5600: Loss = -11797.06337495526
1
Iteration 5700: Loss = -11797.061877471617
Iteration 5800: Loss = -11797.061143506642
Iteration 5900: Loss = -11797.066498293487
1
Iteration 6000: Loss = -11797.06059112277
Iteration 6100: Loss = -11797.061079328689
1
Iteration 6200: Loss = -11797.06203288564
2
Iteration 6300: Loss = -11797.062491196171
3
Iteration 6400: Loss = -11797.0614765222
4
Iteration 6500: Loss = -11797.05852800541
Iteration 6600: Loss = -11797.05932647332
1
Iteration 6700: Loss = -11797.05838565567
Iteration 6800: Loss = -11797.05876091624
1
Iteration 6900: Loss = -11797.058204237139
Iteration 7000: Loss = -11797.058415210968
1
Iteration 7100: Loss = -11797.058073050248
Iteration 7200: Loss = -11797.059011283925
1
Iteration 7300: Loss = -11797.068046569044
2
Iteration 7400: Loss = -11797.057972561828
Iteration 7500: Loss = -11797.058186584445
1
Iteration 7600: Loss = -11797.057865709614
Iteration 7700: Loss = -11797.057829263991
Iteration 7800: Loss = -11797.060255460608
1
Iteration 7900: Loss = -11797.110174480184
2
Iteration 8000: Loss = -11797.0576188765
Iteration 8100: Loss = -11797.057547192371
Iteration 8200: Loss = -11797.084271212652
1
Iteration 8300: Loss = -11797.059251179748
2
Iteration 8400: Loss = -11797.06020033612
3
Iteration 8500: Loss = -11797.069332985364
4
Iteration 8600: Loss = -11797.059022877209
5
Stopping early at iteration 8600 due to no improvement.
pi: tensor([[0.7334, 0.2666],
        [0.2670, 0.7330]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5799, 0.4201], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2972, 0.0986],
         [0.7043, 0.2929]],

        [[0.6982, 0.1015],
         [0.6597, 0.5866]],

        [[0.7195, 0.1044],
         [0.5980, 0.7294]],

        [[0.5221, 0.0927],
         [0.7033, 0.6568]],

        [[0.7264, 0.0966],
         [0.6979, 0.6232]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
time is 2
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 3
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.960320507679542
Average Adjusted Rand Index: 0.9601559633673616
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21025.91737769293
Iteration 100: Loss = -12288.08135212965
Iteration 200: Loss = -12287.545789342092
Iteration 300: Loss = -12287.316485166415
Iteration 400: Loss = -12286.970812848233
Iteration 500: Loss = -12286.18978697433
Iteration 600: Loss = -12285.887273139902
Iteration 700: Loss = -12285.563756400785
Iteration 800: Loss = -12284.690547301498
Iteration 900: Loss = -12209.920710024833
Iteration 1000: Loss = -12208.244322473844
Iteration 1100: Loss = -12208.048509079337
Iteration 1200: Loss = -12207.936317298449
Iteration 1300: Loss = -12207.853865509536
Iteration 1400: Loss = -12207.436386186126
Iteration 1500: Loss = -12069.35556703733
Iteration 1600: Loss = -11834.915730704786
Iteration 1700: Loss = -11810.505108807749
Iteration 1800: Loss = -11810.274503802471
Iteration 1900: Loss = -11803.775869819581
Iteration 2000: Loss = -11803.72567219457
Iteration 2100: Loss = -11803.694989933005
Iteration 2200: Loss = -11803.671328979746
Iteration 2300: Loss = -11797.244051940095
Iteration 2400: Loss = -11797.211585166599
Iteration 2500: Loss = -11797.186701518856
Iteration 2600: Loss = -11797.180353673353
Iteration 2700: Loss = -11797.169456985854
Iteration 2800: Loss = -11797.16226209808
Iteration 2900: Loss = -11797.153051141453
Iteration 3000: Loss = -11797.142137349556
Iteration 3100: Loss = -11797.137905945367
Iteration 3200: Loss = -11797.134861583276
Iteration 3300: Loss = -11797.131251661522
Iteration 3400: Loss = -11797.128481067215
Iteration 3500: Loss = -11797.126298585285
Iteration 3600: Loss = -11797.123617596837
Iteration 3700: Loss = -11797.121523854858
Iteration 3800: Loss = -11797.119570089819
Iteration 3900: Loss = -11797.124334524582
1
Iteration 4000: Loss = -11797.119121674637
Iteration 4100: Loss = -11797.114881449692
Iteration 4200: Loss = -11797.130107195835
1
Iteration 4300: Loss = -11797.113031097273
Iteration 4400: Loss = -11797.111746735709
Iteration 4500: Loss = -11797.110570836738
Iteration 4600: Loss = -11797.10975160355
Iteration 4700: Loss = -11797.123256759447
1
Iteration 4800: Loss = -11797.108050482679
Iteration 4900: Loss = -11797.108388393039
1
Iteration 5000: Loss = -11797.106635207414
Iteration 5100: Loss = -11797.10619996359
Iteration 5200: Loss = -11797.105156193273
Iteration 5300: Loss = -11797.110929046332
1
Iteration 5400: Loss = -11797.103801696343
Iteration 5500: Loss = -11797.10289301438
Iteration 5600: Loss = -11797.10395715987
1
Iteration 5700: Loss = -11797.098645814174
Iteration 5800: Loss = -11797.087697362374
Iteration 5900: Loss = -11797.086112183559
Iteration 6000: Loss = -11797.081768210135
Iteration 6100: Loss = -11797.074406409947
Iteration 6200: Loss = -11797.073812712277
Iteration 6300: Loss = -11797.072616732405
Iteration 6400: Loss = -11797.072022283144
Iteration 6500: Loss = -11797.07217285925
1
Iteration 6600: Loss = -11797.070508222303
Iteration 6700: Loss = -11797.07093362701
1
Iteration 6800: Loss = -11797.069299439301
Iteration 6900: Loss = -11797.097569140888
1
Iteration 7000: Loss = -11797.069049369238
Iteration 7100: Loss = -11797.069123763436
1
Iteration 7200: Loss = -11797.073747855753
2
Iteration 7300: Loss = -11797.068484712516
Iteration 7400: Loss = -11797.090861396871
1
Iteration 7500: Loss = -11797.068152946455
Iteration 7600: Loss = -11797.090136652476
1
Iteration 7700: Loss = -11797.067893020867
Iteration 7800: Loss = -11797.068002994118
1
Iteration 7900: Loss = -11797.177739179026
2
Iteration 8000: Loss = -11797.068794547742
3
Iteration 8100: Loss = -11797.068577533542
4
Iteration 8200: Loss = -11797.072032958598
5
Stopping early at iteration 8200 due to no improvement.
pi: tensor([[0.7329, 0.2671],
        [0.2660, 0.7340]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5805, 0.4195], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2979, 0.0986],
         [0.6932, 0.2929]],

        [[0.5272, 0.1015],
         [0.6906, 0.5519]],

        [[0.6148, 0.1044],
         [0.7263, 0.6634]],

        [[0.5892, 0.0927],
         [0.5531, 0.6664]],

        [[0.6596, 0.0966],
         [0.6404, 0.6060]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599857335115991
time is 2
tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 2
Adjusted Rand Index: 0.9208012930401136
time is 3
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.960320507679542
Average Adjusted Rand Index: 0.9601559633673616
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21582.571260034932
Iteration 100: Loss = -12287.723295542603
Iteration 200: Loss = -12287.314023780062
Iteration 300: Loss = -12287.05680590162
Iteration 400: Loss = -12286.584945782182
Iteration 500: Loss = -12285.719677215719
Iteration 600: Loss = -12284.708107620474
Iteration 700: Loss = -12283.664890253762
Iteration 800: Loss = -12281.906568654978
Iteration 900: Loss = -12226.555603277162
Iteration 1000: Loss = -12210.047764634472
Iteration 1100: Loss = -12208.174812010871
Iteration 1200: Loss = -12208.0139606235
Iteration 1300: Loss = -12207.921981980706
Iteration 1400: Loss = -12207.83054061013
Iteration 1500: Loss = -12199.99389446541
Iteration 1600: Loss = -11867.630162961803
Iteration 1700: Loss = -11804.527946788668
Iteration 1800: Loss = -11797.549287368378
Iteration 1900: Loss = -11797.418419566702
Iteration 2000: Loss = -11797.346514717938
Iteration 2100: Loss = -11797.298118635039
Iteration 2200: Loss = -11797.260610204483
Iteration 2300: Loss = -11797.230661046402
Iteration 2400: Loss = -11797.206751717546
Iteration 2500: Loss = -11797.188404028868
Iteration 2600: Loss = -11797.175176427936
Iteration 2700: Loss = -11797.190808691485
1
Iteration 2800: Loss = -11797.154998751656
Iteration 2900: Loss = -11797.146584388924
Iteration 3000: Loss = -11797.13955303387
Iteration 3100: Loss = -11797.133952606096
Iteration 3200: Loss = -11797.128813850046
Iteration 3300: Loss = -11797.12352565742
Iteration 3400: Loss = -11797.118555690584
Iteration 3500: Loss = -11797.116224176323
Iteration 3600: Loss = -11797.113903899548
Iteration 3700: Loss = -11797.10919182999
Iteration 3800: Loss = -11797.10665567142
Iteration 3900: Loss = -11797.104097281852
Iteration 4000: Loss = -11797.101006568271
Iteration 4100: Loss = -11796.973486353463
Iteration 4200: Loss = -11794.715246730077
Iteration 4300: Loss = -11794.708580379993
Iteration 4400: Loss = -11794.70691431749
Iteration 4500: Loss = -11794.706325606958
Iteration 4600: Loss = -11794.703872915645
Iteration 4700: Loss = -11794.711856415697
1
Iteration 4800: Loss = -11794.700401245958
Iteration 4900: Loss = -11794.691405532627
Iteration 5000: Loss = -11794.666544566799
Iteration 5100: Loss = -11794.661245656349
Iteration 5200: Loss = -11794.621925357595
Iteration 5300: Loss = -11794.617726971652
Iteration 5400: Loss = -11794.610576430967
Iteration 5500: Loss = -11794.609293800595
Iteration 5600: Loss = -11794.607318029848
Iteration 5700: Loss = -11794.61314973801
1
Iteration 5800: Loss = -11794.605947662536
Iteration 5900: Loss = -11794.605606544197
Iteration 6000: Loss = -11794.604903984182
Iteration 6100: Loss = -11794.604858073602
Iteration 6200: Loss = -11794.603941067062
Iteration 6300: Loss = -11794.609702504946
1
Iteration 6400: Loss = -11794.603220072218
Iteration 6500: Loss = -11794.602864042869
Iteration 6600: Loss = -11794.602760051703
Iteration 6700: Loss = -11794.60216852835
Iteration 6800: Loss = -11794.602163199417
Iteration 6900: Loss = -11794.600927610923
Iteration 7000: Loss = -11794.59996131002
Iteration 7100: Loss = -11794.599956153175
Iteration 7200: Loss = -11794.602947190779
1
Iteration 7300: Loss = -11794.59946238137
Iteration 7400: Loss = -11794.601412083428
1
Iteration 7500: Loss = -11794.624975757284
2
Iteration 7600: Loss = -11794.599468359891
3
Iteration 7700: Loss = -11794.598449395635
Iteration 7800: Loss = -11794.597733515688
Iteration 7900: Loss = -11794.597470360064
Iteration 8000: Loss = -11794.600161953085
1
Iteration 8100: Loss = -11794.59705530527
Iteration 8200: Loss = -11794.597078405066
1
Iteration 8300: Loss = -11794.611089317
2
Iteration 8400: Loss = -11794.652084206891
3
Iteration 8500: Loss = -11794.59843866411
4
Iteration 8600: Loss = -11794.598410517905
5
Stopping early at iteration 8600 due to no improvement.
pi: tensor([[0.7302, 0.2698],
        [0.2733, 0.7267]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4208, 0.5792], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2913, 0.0986],
         [0.5818, 0.2991]],

        [[0.6516, 0.1015],
         [0.7124, 0.6256]],

        [[0.6345, 0.1033],
         [0.5819, 0.6548]],

        [[0.5288, 0.0927],
         [0.6062, 0.6812]],

        [[0.6768, 0.0966],
         [0.6578, 0.6743]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599857335115991
time is 2
tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 4
tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599960816486464
Global Adjusted Rand Index: 0.9760961487628163
Average Adjusted Rand Index: 0.9759957047593391
11804.432205462863
[0.960320507679542, 0.960320507679542, 0.960320507679542, 0.9760961487628163] [0.9601559633673616, 0.9601559633673616, 0.9601559633673616, 0.9759957047593391] [11797.067107367764, 11797.059022877209, 11797.072032958598, 11794.598410517905]
-----------------------------------------------------------------------------------------
This iteration is 7
True Objective function: Loss = -11857.920732998846
Iteration 0: Loss = -12376.948133492817
Iteration 10: Loss = -12376.94814105744
1
Iteration 20: Loss = -12376.948156108674
2
Iteration 30: Loss = -12376.948218769288
3
Stopping early at iteration 30 due to no improvement.
pi: tensor([[1.0000e+00, 4.7848e-09],
        [1.0000e+00, 2.9001e-17]], dtype=torch.float64)
alpha: tensor([1.0000e+00, 4.6338e-09])
beta: tensor([[[0.1978, 0.1835],
         [0.3277, 0.2187]],

        [[0.6816, 0.2758],
         [0.1375, 0.9946]],

        [[0.3374, 0.1334],
         [0.0466, 0.0068]],

        [[0.3752, 0.1786],
         [0.4658, 0.8745]],

        [[0.3022, 0.2008],
         [0.1033, 0.7693]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -12376.948135060762
Iteration 10: Loss = -12376.948158596486
1
Iteration 20: Loss = -12376.948183992385
2
Iteration 30: Loss = -12376.948276342091
3
Stopping early at iteration 30 due to no improvement.
pi: tensor([[1.0000e+00, 1.2736e-08],
        [1.0000e+00, 5.3841e-09]], dtype=torch.float64)
alpha: tensor([1.0000e+00, 1.6756e-08])
beta: tensor([[[0.1978, 0.3131],
         [0.1043, 0.4077]],

        [[0.6653, 0.2758],
         [0.6793, 0.9590]],

        [[0.9920, 0.1334],
         [0.1672, 0.9233]],

        [[0.3892, 0.1786],
         [0.8598, 0.4312]],

        [[0.3190, 0.2008],
         [0.9761, 0.8136]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22938.736597299616
Iteration 100: Loss = -12377.510402263051
Iteration 200: Loss = -12376.892471949262
Iteration 300: Loss = -12376.723364035335
Iteration 400: Loss = -12376.659678667358
Iteration 500: Loss = -12376.623368497607
Iteration 600: Loss = -12376.594681037428
Iteration 700: Loss = -12376.568333549765
Iteration 800: Loss = -12376.54270484339
Iteration 900: Loss = -12376.517406712685
Iteration 1000: Loss = -12376.492185795885
Iteration 1100: Loss = -12376.46667452264
Iteration 1200: Loss = -12376.440594857128
Iteration 1300: Loss = -12376.413836998065
Iteration 1400: Loss = -12376.386522646335
Iteration 1500: Loss = -12376.358713472615
Iteration 1600: Loss = -12376.330178052425
Iteration 1700: Loss = -12376.300164312874
Iteration 1800: Loss = -12376.26763303319
Iteration 1900: Loss = -12376.231194427017
Iteration 2000: Loss = -12376.188800753716
Iteration 2100: Loss = -12376.137789177643
Iteration 2200: Loss = -12376.076740841148
Iteration 2300: Loss = -12376.012252696282
Iteration 2400: Loss = -12375.96037361794
Iteration 2500: Loss = -12375.92859340885
Iteration 2600: Loss = -12375.91157715661
Iteration 2700: Loss = -12375.902304878691
Iteration 2800: Loss = -12375.896814007772
Iteration 2900: Loss = -12375.893199256516
Iteration 3000: Loss = -12375.890826989375
Iteration 3100: Loss = -12375.88917935057
Iteration 3200: Loss = -12375.887943177882
Iteration 3300: Loss = -12375.88704367428
Iteration 3400: Loss = -12375.886303118861
Iteration 3500: Loss = -12375.885724756052
Iteration 3600: Loss = -12375.885166442738
Iteration 3700: Loss = -12375.884624327764
Iteration 3800: Loss = -12375.884105619367
Iteration 3900: Loss = -12375.883587398292
Iteration 4000: Loss = -12375.883128007483
Iteration 4100: Loss = -12375.88259652221
Iteration 4200: Loss = -12375.882056397599
Iteration 4300: Loss = -12375.88154753149
Iteration 4400: Loss = -12375.88099485947
Iteration 4500: Loss = -12375.88047733691
Iteration 4600: Loss = -12375.879951208402
Iteration 4700: Loss = -12375.879384130918
Iteration 4800: Loss = -12375.87882079691
Iteration 4900: Loss = -12375.878237728002
Iteration 5000: Loss = -12375.8776671789
Iteration 5100: Loss = -12375.877061293117
Iteration 5200: Loss = -12375.876428335843
Iteration 5300: Loss = -12375.875795089074
Iteration 5400: Loss = -12375.875151346016
Iteration 5500: Loss = -12375.874789827782
Iteration 5600: Loss = -12375.873769315922
Iteration 5700: Loss = -12375.872981325634
Iteration 5800: Loss = -12375.872120305901
Iteration 5900: Loss = -12375.871212149857
Iteration 6000: Loss = -12375.87013500095
Iteration 6100: Loss = -12375.868845853563
Iteration 6200: Loss = -12375.867222283812
Iteration 6300: Loss = -12375.865098263243
Iteration 6400: Loss = -12375.863437956406
Iteration 6500: Loss = -12375.858930373306
Iteration 6600: Loss = -12375.855436636084
Iteration 6700: Loss = -12375.853460242208
Iteration 6800: Loss = -12375.85097181579
Iteration 6900: Loss = -12375.85018606477
Iteration 7000: Loss = -12375.849634939055
Iteration 7100: Loss = -12375.850600804279
1
Iteration 7200: Loss = -12375.849350721257
Iteration 7300: Loss = -12375.854621292458
1
Iteration 7400: Loss = -12375.851666788807
2
Iteration 7500: Loss = -12375.849457913419
3
Iteration 7600: Loss = -12375.8494036425
4
Iteration 7700: Loss = -12375.84969522371
5
Stopping early at iteration 7700 due to no improvement.
pi: tensor([[0.0180, 0.9820],
        [0.0477, 0.9523]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2154, 0.7846], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1912, 0.1958],
         [0.6954, 0.2006]],

        [[0.6598, 0.2370],
         [0.7168, 0.6189]],

        [[0.6669, 0.1553],
         [0.6372, 0.6864]],

        [[0.5123, 0.1852],
         [0.6737, 0.5369]],

        [[0.6455, 0.1989],
         [0.5873, 0.6793]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 60
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23101.328233877477
Iteration 100: Loss = -12036.082454369453
Iteration 200: Loss = -12033.017860445827
Iteration 300: Loss = -11887.933695973661
Iteration 400: Loss = -11846.099594450912
Iteration 500: Loss = -11845.940818606905
Iteration 600: Loss = -11845.874967153448
Iteration 700: Loss = -11845.837149636529
Iteration 800: Loss = -11845.812407481475
Iteration 900: Loss = -11845.795537952414
Iteration 1000: Loss = -11845.783457948359
Iteration 1100: Loss = -11845.774315754366
Iteration 1200: Loss = -11845.767150256383
Iteration 1300: Loss = -11845.761354799919
Iteration 1400: Loss = -11845.75595247943
Iteration 1500: Loss = -11845.751983269862
Iteration 1600: Loss = -11845.748858655492
Iteration 1700: Loss = -11845.746218628943
Iteration 1800: Loss = -11845.74400205041
Iteration 1900: Loss = -11845.74205317092
Iteration 2000: Loss = -11845.740407828798
Iteration 2100: Loss = -11845.738997018469
Iteration 2200: Loss = -11845.737707234808
Iteration 2300: Loss = -11845.736590829578
Iteration 2400: Loss = -11845.735597358747
Iteration 2500: Loss = -11845.73473517162
Iteration 2600: Loss = -11845.733926809631
Iteration 2700: Loss = -11845.733197364369
Iteration 2800: Loss = -11845.733104052462
Iteration 2900: Loss = -11845.732054707038
Iteration 3000: Loss = -11845.731519987661
Iteration 3100: Loss = -11845.731025368405
Iteration 3200: Loss = -11845.731655313162
1
Iteration 3300: Loss = -11845.730242094645
Iteration 3400: Loss = -11845.731901620571
1
Iteration 3500: Loss = -11845.729538377203
Iteration 3600: Loss = -11845.729273633482
Iteration 3700: Loss = -11845.729984118881
1
Iteration 3800: Loss = -11845.728700798865
Iteration 3900: Loss = -11845.728917823315
1
Iteration 4000: Loss = -11845.72827595012
Iteration 4100: Loss = -11845.728524579139
1
Iteration 4200: Loss = -11845.735560966441
2
Iteration 4300: Loss = -11845.73135103174
3
Iteration 4400: Loss = -11845.737606521552
4
Iteration 4500: Loss = -11845.727714249466
Iteration 4600: Loss = -11845.727430533507
Iteration 4700: Loss = -11845.727732268926
1
Iteration 4800: Loss = -11845.726984437744
Iteration 4900: Loss = -11845.72707731712
1
Iteration 5000: Loss = -11845.726765337777
Iteration 5100: Loss = -11845.726731418534
Iteration 5200: Loss = -11845.726577580486
Iteration 5300: Loss = -11845.72744180093
1
Iteration 5400: Loss = -11845.726364679196
Iteration 5500: Loss = -11845.726561838412
1
Iteration 5600: Loss = -11845.732696639763
2
Iteration 5700: Loss = -11845.728460402355
3
Iteration 5800: Loss = -11845.72703182969
4
Iteration 5900: Loss = -11845.726111220263
Iteration 6000: Loss = -11845.72663643529
1
Iteration 6100: Loss = -11845.725968851908
Iteration 6200: Loss = -11845.725910112144
Iteration 6300: Loss = -11845.725856576566
Iteration 6400: Loss = -11845.725941821169
1
Iteration 6500: Loss = -11845.72863200979
2
Iteration 6600: Loss = -11845.737829379605
3
Iteration 6700: Loss = -11845.725762421249
Iteration 6800: Loss = -11845.725730042484
Iteration 6900: Loss = -11845.72569215201
Iteration 7000: Loss = -11845.726277637914
1
Iteration 7100: Loss = -11845.726661064631
2
Iteration 7200: Loss = -11845.729125809534
3
Iteration 7300: Loss = -11845.726626615065
4
Iteration 7400: Loss = -11845.725498608543
Iteration 7500: Loss = -11845.72582750733
1
Iteration 7600: Loss = -11845.725495548197
Iteration 7700: Loss = -11845.725496351472
1
Iteration 7800: Loss = -11845.725473359847
Iteration 7900: Loss = -11845.725463881618
Iteration 8000: Loss = -11845.725528381266
1
Iteration 8100: Loss = -11845.726940253951
2
Iteration 8200: Loss = -11845.746792267288
3
Iteration 8300: Loss = -11845.725355718143
Iteration 8400: Loss = -11845.725481088002
1
Iteration 8500: Loss = -11845.725992285403
2
Iteration 8600: Loss = -11845.73039220529
3
Iteration 8700: Loss = -11845.7630127267
4
Iteration 8800: Loss = -11845.738666224779
5
Stopping early at iteration 8800 due to no improvement.
pi: tensor([[0.7712, 0.2288],
        [0.2623, 0.7377]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5576, 0.4424], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2953, 0.0957],
         [0.5004, 0.3046]],

        [[0.6734, 0.1159],
         [0.7161, 0.6786]],

        [[0.5853, 0.0970],
         [0.6861, 0.6834]],

        [[0.7033, 0.0944],
         [0.5486, 0.6934]],

        [[0.7248, 0.0910],
         [0.6824, 0.6415]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599725554098923
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9919994334721989
Average Adjusted Rand Index: 0.9919945110819786
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21850.593856900825
Iteration 100: Loss = -12377.784662773229
Iteration 200: Loss = -12377.046035839216
Iteration 300: Loss = -12376.83366228926
Iteration 400: Loss = -12376.736558423232
Iteration 500: Loss = -12376.67381907085
Iteration 600: Loss = -12376.622365736888
Iteration 700: Loss = -12376.575723271806
Iteration 800: Loss = -12376.532302604723
Iteration 900: Loss = -12376.491475720444
Iteration 1000: Loss = -12376.451911264048
Iteration 1100: Loss = -12376.41109232175
Iteration 1200: Loss = -12376.363744049604
Iteration 1300: Loss = -12376.280948690903
Iteration 1400: Loss = -12375.59869325476
Iteration 1500: Loss = -12375.380461606288
Iteration 1600: Loss = -12375.274518007205
Iteration 1700: Loss = -12375.208500576064
Iteration 1800: Loss = -12375.161559937418
Iteration 1900: Loss = -12375.122324176238
Iteration 2000: Loss = -12375.084292617144
Iteration 2100: Loss = -12375.04267057361
Iteration 2200: Loss = -12374.992153169242
Iteration 2300: Loss = -12374.92504049334
Iteration 2400: Loss = -12374.828724310468
Iteration 2500: Loss = -12374.693050120764
Iteration 2600: Loss = -12374.539801821287
Iteration 2700: Loss = -12374.404119420018
Iteration 2800: Loss = -12374.308598938871
Iteration 2900: Loss = -12374.242700488006
Iteration 3000: Loss = -12374.194494803749
Iteration 3100: Loss = -12374.158323538752
Iteration 3200: Loss = -12374.130435829617
Iteration 3300: Loss = -12374.10830941376
Iteration 3400: Loss = -12374.09037727386
Iteration 3500: Loss = -12374.075984894613
Iteration 3600: Loss = -12374.06456854888
Iteration 3700: Loss = -12374.055250754594
Iteration 3800: Loss = -12374.047686782525
Iteration 3900: Loss = -12374.041194003834
Iteration 4000: Loss = -12374.035809974457
Iteration 4100: Loss = -12374.031375718057
Iteration 4200: Loss = -12374.02716948967
Iteration 4300: Loss = -12374.023685778588
Iteration 4400: Loss = -12374.02161762529
Iteration 4500: Loss = -12374.017910958028
Iteration 4600: Loss = -12374.015522901193
Iteration 4700: Loss = -12374.013350414842
Iteration 4800: Loss = -12374.011433198946
Iteration 4900: Loss = -12374.009766965564
Iteration 5000: Loss = -12374.008179923292
Iteration 5100: Loss = -12374.00678975518
Iteration 5200: Loss = -12374.005493564167
Iteration 5300: Loss = -12374.004280216732
Iteration 5400: Loss = -12374.003256397857
Iteration 5500: Loss = -12374.002243601606
Iteration 5600: Loss = -12374.001339598812
Iteration 5700: Loss = -12374.000481406301
Iteration 5800: Loss = -12373.999975855253
Iteration 5900: Loss = -12373.998999352676
Iteration 6000: Loss = -12373.998344313883
Iteration 6100: Loss = -12373.997724130133
Iteration 6200: Loss = -12373.997179431413
Iteration 6300: Loss = -12373.998476321727
1
Iteration 6400: Loss = -12373.996144216575
Iteration 6500: Loss = -12373.995677762385
Iteration 6600: Loss = -12373.995252648223
Iteration 6700: Loss = -12373.994853496053
Iteration 6800: Loss = -12373.99472590988
Iteration 6900: Loss = -12373.994210796316
Iteration 7000: Loss = -12374.267546606074
1
Iteration 7100: Loss = -12373.993464238816
Iteration 7200: Loss = -12374.151396714831
1
Iteration 7300: Loss = -12373.992907665306
Iteration 7400: Loss = -12373.992659897827
Iteration 7500: Loss = -12373.992420573539
Iteration 7600: Loss = -12373.992206916497
Iteration 7700: Loss = -12373.991993713362
Iteration 7800: Loss = -12374.009945243593
1
Iteration 7900: Loss = -12373.99159841611
Iteration 8000: Loss = -12373.99141856
Iteration 8100: Loss = -12374.033203049088
1
Iteration 8200: Loss = -12373.991068872621
Iteration 8300: Loss = -12373.990930216996
Iteration 8400: Loss = -12373.990779801687
Iteration 8500: Loss = -12373.991032063712
1
Iteration 8600: Loss = -12373.990509809555
Iteration 8700: Loss = -12373.99036047908
Iteration 8800: Loss = -12374.005035311848
1
Iteration 8900: Loss = -12373.990171303358
Iteration 9000: Loss = -12373.99008158567
Iteration 9100: Loss = -12373.994113735333
1
Iteration 9200: Loss = -12373.989925214122
Iteration 9300: Loss = -12373.989787287277
Iteration 9400: Loss = -12373.989704216445
Iteration 9500: Loss = -12373.989777076673
1
Iteration 9600: Loss = -12373.989532524667
Iteration 9700: Loss = -12373.989509694526
Iteration 9800: Loss = -12374.011405264422
1
Iteration 9900: Loss = -12373.9893428487
Iteration 10000: Loss = -12373.989338177313
Iteration 10100: Loss = -12374.14589265424
1
Iteration 10200: Loss = -12373.98916209265
Iteration 10300: Loss = -12373.989145084912
Iteration 10400: Loss = -12373.98911763054
Iteration 10500: Loss = -12373.989226148036
1
Iteration 10600: Loss = -12373.989034066743
Iteration 10700: Loss = -12373.988947211388
Iteration 10800: Loss = -12373.995454279242
1
Iteration 10900: Loss = -12373.988900402892
Iteration 11000: Loss = -12373.988843046332
Iteration 11100: Loss = -12374.018921357088
1
Iteration 11200: Loss = -12373.988802711823
Iteration 11300: Loss = -12373.98871858761
Iteration 11400: Loss = -12373.9958267601
1
Iteration 11500: Loss = -12373.98871025277
Iteration 11600: Loss = -12373.988676155603
Iteration 11700: Loss = -12373.988640656129
Iteration 11800: Loss = -12373.991326123954
1
Iteration 11900: Loss = -12373.988613707255
Iteration 12000: Loss = -12373.988591380004
Iteration 12100: Loss = -12374.010944598733
1
Iteration 12200: Loss = -12373.988519298504
Iteration 12300: Loss = -12373.988523962691
1
Iteration 12400: Loss = -12373.998662301467
2
Iteration 12500: Loss = -12373.988507078706
Iteration 12600: Loss = -12373.988491009655
Iteration 12700: Loss = -12373.98846969833
Iteration 12800: Loss = -12373.98861165693
1
Iteration 12900: Loss = -12373.988414885831
Iteration 13000: Loss = -12373.988393494443
Iteration 13100: Loss = -12373.990857361432
1
Iteration 13200: Loss = -12373.988401894889
2
Iteration 13300: Loss = -12373.988328485439
Iteration 13400: Loss = -12373.992771003603
1
Iteration 13500: Loss = -12373.988385501703
2
Iteration 13600: Loss = -12373.988361792563
3
Iteration 13700: Loss = -12373.995597699099
4
Iteration 13800: Loss = -12373.988345226753
5
Stopping early at iteration 13800 due to no improvement.
pi: tensor([[1.0000e+00, 1.6278e-06],
        [4.6207e-01, 5.3793e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9595, 0.0405], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1976, 0.2922],
         [0.6571, 0.6352]],

        [[0.6497, 0.2346],
         [0.7121, 0.6988]],

        [[0.5969, 0.2132],
         [0.6707, 0.5778]],

        [[0.5507, 0.2225],
         [0.5032, 0.5468]],

        [[0.5965, 0.2185],
         [0.6131, 0.6984]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.012285862605987194
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: -0.010213452814961178
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0040390666275071764
Average Adjusted Rand Index: -0.004499863084189674
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22667.640933511346
Iteration 100: Loss = -12377.49801357926
Iteration 200: Loss = -12376.936978964148
Iteration 300: Loss = -12376.75052190476
Iteration 400: Loss = -12376.673329658523
Iteration 500: Loss = -12376.63405204366
Iteration 600: Loss = -12376.608102640388
Iteration 700: Loss = -12376.584776138156
Iteration 800: Loss = -12376.551131068676
Iteration 900: Loss = -12376.403980050532
Iteration 1000: Loss = -12375.711299837018
Iteration 1100: Loss = -12375.486607660132
Iteration 1200: Loss = -12375.29815196963
Iteration 1300: Loss = -12375.115616316294
Iteration 1400: Loss = -12374.930732048755
Iteration 1500: Loss = -12374.745398617819
Iteration 1600: Loss = -12374.577490094745
Iteration 1700: Loss = -12374.447342342863
Iteration 1800: Loss = -12374.350931826553
Iteration 1900: Loss = -12374.280212397707
Iteration 2000: Loss = -12374.227196751934
Iteration 2100: Loss = -12374.186358983838
Iteration 2200: Loss = -12374.154168936842
Iteration 2300: Loss = -12374.128065354578
Iteration 2400: Loss = -12374.106689631812
Iteration 2500: Loss = -12374.089490861912
Iteration 2600: Loss = -12374.075599270409
Iteration 2700: Loss = -12374.064132668873
Iteration 2800: Loss = -12374.054354163613
Iteration 2900: Loss = -12374.04588306744
Iteration 3000: Loss = -12374.038788177506
Iteration 3100: Loss = -12374.033175756786
Iteration 3200: Loss = -12374.028506446286
Iteration 3300: Loss = -12374.024551138531
Iteration 3400: Loss = -12374.021143055119
Iteration 3500: Loss = -12374.018244501322
Iteration 3600: Loss = -12374.01571235725
Iteration 3700: Loss = -12374.013418149356
Iteration 3800: Loss = -12374.011505472572
Iteration 3900: Loss = -12374.009730168471
Iteration 4000: Loss = -12374.008103191723
Iteration 4100: Loss = -12374.00694827906
Iteration 4200: Loss = -12374.005348212151
Iteration 4300: Loss = -12374.004217953083
Iteration 4400: Loss = -12374.00311580955
Iteration 4500: Loss = -12374.002094521364
Iteration 4600: Loss = -12374.001247102084
Iteration 4700: Loss = -12374.000426200095
Iteration 4800: Loss = -12373.99965907928
Iteration 4900: Loss = -12373.999878574365
1
Iteration 5000: Loss = -12373.998262136682
Iteration 5100: Loss = -12373.997658547849
Iteration 5200: Loss = -12373.997083466902
Iteration 5300: Loss = -12373.996542381603
Iteration 5400: Loss = -12373.996085662684
Iteration 5500: Loss = -12373.995631105121
Iteration 5600: Loss = -12373.995172416058
Iteration 5700: Loss = -12373.994756940097
Iteration 5800: Loss = -12373.994382854673
Iteration 5900: Loss = -12373.994043843644
Iteration 6000: Loss = -12373.993718160616
Iteration 6100: Loss = -12373.993397797614
Iteration 6200: Loss = -12373.99311325652
Iteration 6300: Loss = -12373.992819543228
Iteration 6400: Loss = -12373.992679581337
Iteration 6500: Loss = -12373.992361329538
Iteration 6600: Loss = -12373.992154183583
Iteration 6700: Loss = -12373.991895455556
Iteration 6800: Loss = -12373.991674826115
Iteration 6900: Loss = -12373.991535508048
Iteration 7000: Loss = -12373.991327984919
Iteration 7100: Loss = -12373.991160474588
Iteration 7200: Loss = -12373.991184485947
1
Iteration 7300: Loss = -12373.99082867305
Iteration 7400: Loss = -12373.991649051284
1
Iteration 7500: Loss = -12373.99178663295
2
Iteration 7600: Loss = -12373.990494964086
Iteration 7700: Loss = -12373.990362161043
Iteration 7800: Loss = -12373.990564870579
1
Iteration 7900: Loss = -12373.990065659724
Iteration 8000: Loss = -12373.990084137735
1
Iteration 8100: Loss = -12374.002219153204
2
Iteration 8200: Loss = -12373.98987236879
Iteration 8300: Loss = -12373.989788693098
Iteration 8400: Loss = -12374.366944223395
1
Iteration 8500: Loss = -12373.98962329167
Iteration 8600: Loss = -12373.989559778624
Iteration 8700: Loss = -12374.005299617244
1
Iteration 8800: Loss = -12373.989410120108
Iteration 8900: Loss = -12373.989321494977
Iteration 9000: Loss = -12373.989279499212
Iteration 9100: Loss = -12373.99069534063
1
Iteration 9200: Loss = -12373.98913356808
Iteration 9300: Loss = -12373.989119051006
Iteration 9400: Loss = -12374.091060124252
1
Iteration 9500: Loss = -12373.989019804916
Iteration 9600: Loss = -12373.989022032483
1
Iteration 9700: Loss = -12373.989053913065
2
Iteration 9800: Loss = -12373.988928302539
Iteration 9900: Loss = -12373.988835015853
Iteration 10000: Loss = -12373.988897582747
1
Iteration 10100: Loss = -12373.988819808104
Iteration 10200: Loss = -12373.988769279735
Iteration 10300: Loss = -12373.988768260302
Iteration 10400: Loss = -12373.988817375859
1
Iteration 10500: Loss = -12373.988644830544
Iteration 10600: Loss = -12373.988656090596
1
Iteration 10700: Loss = -12373.98865690771
2
Iteration 10800: Loss = -12373.988613742245
Iteration 10900: Loss = -12373.988578032322
Iteration 11000: Loss = -12373.997570184729
1
Iteration 11100: Loss = -12373.98856806131
Iteration 11200: Loss = -12373.988504032362
Iteration 11300: Loss = -12373.991917129202
1
Iteration 11400: Loss = -12373.98850225513
Iteration 11500: Loss = -12373.98848924007
Iteration 11600: Loss = -12373.988472553838
Iteration 11700: Loss = -12373.988714544588
1
Iteration 11800: Loss = -12373.988449814224
Iteration 11900: Loss = -12373.988460981696
1
Iteration 12000: Loss = -12373.98889930094
2
Iteration 12100: Loss = -12373.988421459673
Iteration 12200: Loss = -12373.988379676463
Iteration 12300: Loss = -12373.993119666067
1
Iteration 12400: Loss = -12373.988382425092
2
Iteration 12500: Loss = -12373.988385322802
3
Iteration 12600: Loss = -12374.006872879623
4
Iteration 12700: Loss = -12373.988346746179
Iteration 12800: Loss = -12373.988351366374
1
Iteration 12900: Loss = -12373.988373076532
2
Iteration 13000: Loss = -12373.988452977012
3
Iteration 13100: Loss = -12373.988300303627
Iteration 13200: Loss = -12373.988311325924
1
Iteration 13300: Loss = -12373.98828958919
Iteration 13400: Loss = -12373.988265136191
Iteration 13500: Loss = -12373.98829646968
1
Iteration 13600: Loss = -12373.995717360262
2
Iteration 13700: Loss = -12373.988270579028
3
Iteration 13800: Loss = -12373.988283098346
4
Iteration 13900: Loss = -12374.419602323516
5
Stopping early at iteration 13900 due to no improvement.
pi: tensor([[1.0000e+00, 8.8144e-07],
        [4.5361e-01, 5.4639e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9583, 0.0417], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1949, 0.2887],
         [0.7055, 0.6350]],

        [[0.6335, 0.2319],
         [0.7150, 0.5962]],

        [[0.5931, 0.2131],
         [0.6740, 0.7017]],

        [[0.6941, 0.2226],
         [0.6454, 0.6520]],

        [[0.5944, 0.2185],
         [0.6316, 0.6255]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.012285862605987194
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: -0.010213452814961178
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0040390666275071764
Average Adjusted Rand Index: -0.004499863084189674
11857.920732998846
[0.0, 0.9919994334721989, -0.0040390666275071764, -0.0040390666275071764] [0.0, 0.9919945110819786, -0.004499863084189674, -0.004499863084189674] [12375.84969522371, 11845.738666224779, 12373.988345226753, 12374.419602323516]
-----------------------------------------------------------------------------------------
This iteration is 8
True Objective function: Loss = -11754.579933210178
Iteration 0: Loss = -12487.906230511828
Iteration 10: Loss = -12292.659947137145
Iteration 20: Loss = -12292.573198532244
Iteration 30: Loss = -12292.550304082648
Iteration 40: Loss = -12292.54300341232
Iteration 50: Loss = -12292.54005544742
Iteration 60: Loss = -12292.53841960973
Iteration 70: Loss = -12292.537240334574
Iteration 80: Loss = -12292.536169600296
Iteration 90: Loss = -12292.535234907375
Iteration 100: Loss = -12292.534364997116
Iteration 110: Loss = -12292.533525220248
Iteration 120: Loss = -12292.53275486246
Iteration 130: Loss = -12292.531972812254
Iteration 140: Loss = -12292.531181042108
Iteration 150: Loss = -12292.53049993687
Iteration 160: Loss = -12292.529789772721
Iteration 170: Loss = -12292.52915901766
Iteration 180: Loss = -12292.528496136378
Iteration 190: Loss = -12292.527951659524
Iteration 200: Loss = -12292.527360746888
Iteration 210: Loss = -12292.526785545568
Iteration 220: Loss = -12292.526299174551
Iteration 230: Loss = -12292.525777488034
Iteration 240: Loss = -12292.525312689948
Iteration 250: Loss = -12292.524865353616
Iteration 260: Loss = -12292.524389025197
Iteration 270: Loss = -12292.524003785078
Iteration 280: Loss = -12292.523614119844
Iteration 290: Loss = -12292.523283149307
Iteration 300: Loss = -12292.522948397762
Iteration 310: Loss = -12292.522620876023
Iteration 320: Loss = -12292.522335065434
Iteration 330: Loss = -12292.52204518886
Iteration 340: Loss = -12292.521828461924
Iteration 350: Loss = -12292.521553361223
Iteration 360: Loss = -12292.521365559573
Iteration 370: Loss = -12292.521160858443
Iteration 380: Loss = -12292.520988460108
Iteration 390: Loss = -12292.520864159806
Iteration 400: Loss = -12292.520689318526
Iteration 410: Loss = -12292.52052776973
Iteration 420: Loss = -12292.520455844737
Iteration 430: Loss = -12292.520346269426
Iteration 440: Loss = -12292.52024404017
Iteration 450: Loss = -12292.520258912058
1
Iteration 460: Loss = -12292.520143313086
Iteration 470: Loss = -12292.520115835954
Iteration 480: Loss = -12292.520123225742
1
Iteration 490: Loss = -12292.520100869982
Iteration 500: Loss = -12292.520039952115
Iteration 510: Loss = -12292.520049168801
1
Iteration 520: Loss = -12292.520104361867
2
Iteration 530: Loss = -12292.520091454095
3
Stopping early at iteration 530 due to no improvement.
pi: tensor([[0.7430, 0.2570],
        [0.6641, 0.3359]], dtype=torch.float64)
alpha: tensor([0.7212, 0.2788])
beta: tensor([[[0.1932, 0.1938],
         [0.5346, 0.2009]],

        [[0.0726, 0.1979],
         [0.8983, 0.3740]],

        [[0.0131, 0.2066],
         [0.0367, 0.2390]],

        [[0.6319, 0.1910],
         [0.3901, 0.0772]],

        [[0.7847, 0.1960],
         [0.8140, 0.0138]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -12343.285168098391
Iteration 10: Loss = -12291.445050495931
Iteration 20: Loss = -12291.335731620695
Iteration 30: Loss = -12291.311294404417
Iteration 40: Loss = -12291.305303989173
Iteration 50: Loss = -12291.303645252918
Iteration 60: Loss = -12291.303105569968
Iteration 70: Loss = -12291.30289046517
Iteration 80: Loss = -12291.302857565523
Iteration 90: Loss = -12291.302768221505
Iteration 100: Loss = -12291.302809208326
1
Iteration 110: Loss = -12291.302811886508
2
Iteration 120: Loss = -12291.302777050936
3
Stopping early at iteration 120 due to no improvement.
pi: tensor([[0.0250, 0.9750],
        [0.0232, 0.9768]], dtype=torch.float64)
alpha: tensor([0.0232, 0.9768])
beta: tensor([[[0.2398, 0.1931],
         [0.9826, 0.1938]],

        [[0.3129, 0.2118],
         [0.7317, 0.8866]],

        [[0.8215, 0.2471],
         [0.4125, 0.8718]],

        [[0.1645, 0.1823],
         [0.6239, 0.9947]],

        [[0.0679, 0.3158],
         [0.1129, 0.8598]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: -0.004284949116229245
Global Adjusted Rand Index: -0.0003371649239211524
Average Adjusted Rand Index: -0.0008569898232458489
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22000.152551458672
Iteration 100: Loss = -12294.02153543594
Iteration 200: Loss = -12293.140642054337
Iteration 300: Loss = -12292.96301881569
Iteration 400: Loss = -12292.904493216209
Iteration 500: Loss = -12292.87135926962
Iteration 600: Loss = -12292.841904726289
Iteration 700: Loss = -12292.801521412957
Iteration 800: Loss = -12292.622415053127
Iteration 900: Loss = -12291.500507252436
Iteration 1000: Loss = -12291.213330675699
Iteration 1100: Loss = -12291.114520372505
Iteration 1200: Loss = -12291.06302998122
Iteration 1300: Loss = -12291.03142242899
Iteration 1400: Loss = -12291.010632308058
Iteration 1500: Loss = -12290.996510338156
Iteration 1600: Loss = -12290.986683814774
Iteration 1700: Loss = -12290.979654691779
Iteration 1800: Loss = -12290.974523943973
Iteration 1900: Loss = -12290.970758401054
Iteration 2000: Loss = -12290.967819436693
Iteration 2100: Loss = -12290.96553780045
Iteration 2200: Loss = -12290.963771101311
Iteration 2300: Loss = -12290.962343830204
Iteration 2400: Loss = -12290.961230405133
Iteration 2500: Loss = -12290.960227070465
Iteration 2600: Loss = -12290.959413838844
Iteration 2700: Loss = -12290.958657183164
Iteration 2800: Loss = -12290.957976622632
Iteration 2900: Loss = -12290.957424039676
Iteration 3000: Loss = -12290.956847594383
Iteration 3100: Loss = -12290.956367143091
Iteration 3200: Loss = -12290.955885522644
Iteration 3300: Loss = -12290.955474134382
Iteration 3400: Loss = -12290.955045613613
Iteration 3500: Loss = -12290.954726711525
Iteration 3600: Loss = -12290.954313626815
Iteration 3700: Loss = -12290.954017757984
Iteration 3800: Loss = -12290.953682575233
Iteration 3900: Loss = -12290.953458069664
Iteration 4000: Loss = -12290.953114505079
Iteration 4100: Loss = -12290.952918005341
Iteration 4200: Loss = -12290.952653047158
Iteration 4300: Loss = -12290.952424008832
Iteration 4400: Loss = -12290.952230897301
Iteration 4500: Loss = -12290.952009759432
Iteration 4600: Loss = -12290.951793675613
Iteration 4700: Loss = -12290.951618397234
Iteration 4800: Loss = -12290.951429225293
Iteration 4900: Loss = -12290.951294091301
Iteration 5000: Loss = -12290.951125172778
Iteration 5100: Loss = -12290.951001743899
Iteration 5200: Loss = -12290.950882671463
Iteration 5300: Loss = -12290.950754618158
Iteration 5400: Loss = -12290.95065509786
Iteration 5500: Loss = -12290.950493721597
Iteration 5600: Loss = -12290.95043951737
Iteration 5700: Loss = -12290.950297578283
Iteration 5800: Loss = -12290.950238840349
Iteration 5900: Loss = -12290.950160082855
Iteration 6000: Loss = -12290.950042860266
Iteration 6100: Loss = -12290.950005730803
Iteration 6200: Loss = -12290.949968107792
Iteration 6300: Loss = -12290.949859057571
Iteration 6400: Loss = -12290.949814892381
Iteration 6500: Loss = -12290.949737586261
Iteration 6600: Loss = -12290.949883879775
1
Iteration 6700: Loss = -12290.949655261053
Iteration 6800: Loss = -12290.950625038888
1
Iteration 6900: Loss = -12290.94991593491
2
Iteration 7000: Loss = -12290.949921729836
3
Iteration 7100: Loss = -12290.949636291814
Iteration 7200: Loss = -12290.963564444384
1
Iteration 7300: Loss = -12290.949486343916
Iteration 7400: Loss = -12290.995759811572
1
Iteration 7500: Loss = -12290.949403723473
Iteration 7600: Loss = -12290.94939521757
Iteration 7700: Loss = -12290.950057368682
1
Iteration 7800: Loss = -12290.94935662055
Iteration 7900: Loss = -12290.949331610373
Iteration 8000: Loss = -12290.953504578223
1
Iteration 8100: Loss = -12290.949284713453
Iteration 8200: Loss = -12290.949302426852
1
Iteration 8300: Loss = -12291.38222865572
2
Iteration 8400: Loss = -12290.949310681253
3
Iteration 8500: Loss = -12290.949273005283
Iteration 8600: Loss = -12290.94926568701
Iteration 8700: Loss = -12290.949372656813
1
Iteration 8800: Loss = -12290.949218316717
Iteration 8900: Loss = -12290.949255120931
1
Iteration 9000: Loss = -12290.953205270576
2
Iteration 9100: Loss = -12290.949250713504
3
Iteration 9200: Loss = -12290.949218814261
4
Iteration 9300: Loss = -12290.950484538707
5
Stopping early at iteration 9300 due to no improvement.
pi: tensor([[0.9779, 0.0221],
        [0.9621, 0.0379]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9982, 0.0018], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1965, 0.1810],
         [0.6783, 0.2805]],

        [[0.5655, 0.2172],
         [0.7017, 0.5165]],

        [[0.6249, 0.2504],
         [0.6785, 0.6839]],

        [[0.5597, 0.1858],
         [0.6384, 0.6178]],

        [[0.5999, 0.3156],
         [0.6173, 0.6010]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: -0.004284949116229245
Global Adjusted Rand Index: -0.0003371649239211524
Average Adjusted Rand Index: -0.0008569898232458489
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21378.184270500245
Iteration 100: Loss = -12293.884492896403
Iteration 200: Loss = -12293.168124250698
Iteration 300: Loss = -12292.966494361235
Iteration 400: Loss = -12292.840887740978
Iteration 500: Loss = -12292.748233906235
Iteration 600: Loss = -12292.678678689617
Iteration 700: Loss = -12292.61734594017
Iteration 800: Loss = -12292.501899934872
Iteration 900: Loss = -12291.59053588913
Iteration 1000: Loss = -12291.15416037485
Iteration 1100: Loss = -12291.0536161279
Iteration 1200: Loss = -12291.017339603475
Iteration 1300: Loss = -12290.999597893364
Iteration 1400: Loss = -12290.98911492086
Iteration 1500: Loss = -12290.98207320786
Iteration 1600: Loss = -12290.976966954877
Iteration 1700: Loss = -12290.97303245495
Iteration 1800: Loss = -12290.969922792921
Iteration 1900: Loss = -12290.96734556324
Iteration 2000: Loss = -12290.965178926517
Iteration 2100: Loss = -12290.963291665314
Iteration 2200: Loss = -12290.96170601759
Iteration 2300: Loss = -12290.960370024748
Iteration 2400: Loss = -12290.959153264272
Iteration 2500: Loss = -12290.958076415323
Iteration 2600: Loss = -12290.957156796012
Iteration 2700: Loss = -12290.956317208012
Iteration 2800: Loss = -12290.955586301088
Iteration 2900: Loss = -12290.954983752472
Iteration 3000: Loss = -12290.954371055628
Iteration 3100: Loss = -12290.953882861017
Iteration 3200: Loss = -12290.953393699232
Iteration 3300: Loss = -12290.9529872063
Iteration 3400: Loss = -12290.952589791663
Iteration 3500: Loss = -12290.952254116
Iteration 3600: Loss = -12290.951954268898
Iteration 3700: Loss = -12290.951740590657
Iteration 3800: Loss = -12290.951487162627
Iteration 3900: Loss = -12290.951245984988
Iteration 4000: Loss = -12290.951058943505
Iteration 4100: Loss = -12290.9508819683
Iteration 4200: Loss = -12290.950671481549
Iteration 4300: Loss = -12290.950542773193
Iteration 4400: Loss = -12290.950404679063
Iteration 4500: Loss = -12290.950290711839
Iteration 4600: Loss = -12290.95020040865
Iteration 4700: Loss = -12290.950098375983
Iteration 4800: Loss = -12290.950005801706
Iteration 4900: Loss = -12290.949901985536
Iteration 5000: Loss = -12290.949840222587
Iteration 5100: Loss = -12290.949772820457
Iteration 5200: Loss = -12290.949729032618
Iteration 5300: Loss = -12290.949670287166
Iteration 5400: Loss = -12290.94963732773
Iteration 5500: Loss = -12290.949548770432
Iteration 5600: Loss = -12290.952749244563
1
Iteration 5700: Loss = -12290.949476584236
Iteration 5800: Loss = -12290.949453760884
Iteration 5900: Loss = -12290.949565849372
1
Iteration 6000: Loss = -12290.94940579055
Iteration 6100: Loss = -12290.951225069877
1
Iteration 6200: Loss = -12290.949348050422
Iteration 6300: Loss = -12290.949340912732
Iteration 6400: Loss = -12290.949373194397
1
Iteration 6500: Loss = -12290.94934776753
2
Iteration 6600: Loss = -12290.949281476896
Iteration 6700: Loss = -12290.949263784982
Iteration 6800: Loss = -12290.95025828312
1
Iteration 6900: Loss = -12290.949261261672
Iteration 7000: Loss = -12290.949252826738
Iteration 7100: Loss = -12290.949229338747
Iteration 7200: Loss = -12290.94927176116
1
Iteration 7300: Loss = -12290.949247899638
2
Iteration 7400: Loss = -12290.94979119176
3
Iteration 7500: Loss = -12290.949271756102
4
Iteration 7600: Loss = -12290.9495652104
5
Stopping early at iteration 7600 due to no improvement.
pi: tensor([[0.0379, 0.9621],
        [0.0222, 0.9778]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0018, 0.9982], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2806, 0.1810],
         [0.6867, 0.1958]],

        [[0.5061, 0.2173],
         [0.6098, 0.5026]],

        [[0.6926, 0.2504],
         [0.5831, 0.5777]],

        [[0.6577, 0.1858],
         [0.7249, 0.5165]],

        [[0.6860, 0.3156],
         [0.6788, 0.7050]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: -0.004284949116229245
Global Adjusted Rand Index: -0.0003371649239211524
Average Adjusted Rand Index: -0.0008569898232458489
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23360.00850919345
Iteration 100: Loss = -12293.675067777507
Iteration 200: Loss = -12292.965610307398
Iteration 300: Loss = -12292.801689854527
Iteration 400: Loss = -12292.706531764463
Iteration 500: Loss = -12292.572820494895
Iteration 600: Loss = -12291.6819712205
Iteration 700: Loss = -12291.142013461515
Iteration 800: Loss = -12291.042241785875
Iteration 900: Loss = -12291.00952270768
Iteration 1000: Loss = -12290.994229355972
Iteration 1100: Loss = -12290.985210220912
Iteration 1200: Loss = -12290.97914280961
Iteration 1300: Loss = -12290.97473011942
Iteration 1400: Loss = -12290.971245523884
Iteration 1500: Loss = -12290.968500595218
Iteration 1600: Loss = -12290.966219512273
Iteration 1700: Loss = -12290.964261699786
Iteration 1800: Loss = -12290.96264848588
Iteration 1900: Loss = -12290.961211819449
Iteration 2000: Loss = -12290.959946559848
Iteration 2100: Loss = -12290.958824698684
Iteration 2200: Loss = -12290.95784897066
Iteration 2300: Loss = -12290.956963408582
Iteration 2400: Loss = -12290.956203616151
Iteration 2500: Loss = -12290.955530857014
Iteration 2600: Loss = -12290.954889092623
Iteration 2700: Loss = -12290.954337626705
Iteration 2800: Loss = -12290.95382204009
Iteration 2900: Loss = -12290.953366884452
Iteration 3000: Loss = -12290.952964299857
Iteration 3100: Loss = -12290.952616432789
Iteration 3200: Loss = -12290.952250966784
Iteration 3300: Loss = -12290.951988899275
Iteration 3400: Loss = -12290.951721290063
Iteration 3500: Loss = -12290.951467550629
Iteration 3600: Loss = -12290.951239056973
Iteration 3700: Loss = -12290.951055847081
Iteration 3800: Loss = -12290.950864064487
Iteration 3900: Loss = -12290.950698342058
Iteration 4000: Loss = -12290.950528994963
Iteration 4100: Loss = -12290.950412022254
Iteration 4200: Loss = -12290.950275451185
Iteration 4300: Loss = -12290.950190805703
Iteration 4400: Loss = -12290.950096444698
Iteration 4500: Loss = -12290.94996566649
Iteration 4600: Loss = -12290.949924724237
Iteration 4700: Loss = -12290.949865457857
Iteration 4800: Loss = -12290.949779112614
Iteration 4900: Loss = -12290.949701516707
Iteration 5000: Loss = -12290.94968505208
Iteration 5100: Loss = -12290.949633851747
Iteration 5200: Loss = -12290.949553687193
Iteration 5300: Loss = -12290.949524005346
Iteration 5400: Loss = -12290.949467388806
Iteration 5500: Loss = -12290.949476413125
1
Iteration 5600: Loss = -12290.949436698784
Iteration 5700: Loss = -12290.9493891026
Iteration 5800: Loss = -12290.949360162436
Iteration 5900: Loss = -12290.9494352346
1
Iteration 6000: Loss = -12290.949345430556
Iteration 6100: Loss = -12290.949378789865
1
Iteration 6200: Loss = -12290.949329876277
Iteration 6300: Loss = -12290.949332016648
1
Iteration 6400: Loss = -12290.949351074254
2
Iteration 6500: Loss = -12290.949249589321
Iteration 6600: Loss = -12290.949260778796
1
Iteration 6700: Loss = -12290.949245300113
Iteration 6800: Loss = -12290.949252682314
1
Iteration 6900: Loss = -12290.949527881387
2
Iteration 7000: Loss = -12290.949248101359
3
Iteration 7100: Loss = -12290.949399508425
4
Iteration 7200: Loss = -12290.949254991787
5
Stopping early at iteration 7200 due to no improvement.
pi: tensor([[0.9778, 0.0222],
        [0.9621, 0.0379]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9981, 0.0019], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1957, 0.1812],
         [0.6325, 0.2805]],

        [[0.6687, 0.2173],
         [0.6890, 0.5426]],

        [[0.7303, 0.2504],
         [0.6128, 0.5790]],

        [[0.5176, 0.1858],
         [0.5396, 0.6762]],

        [[0.6718, 0.3156],
         [0.6384, 0.6632]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: -0.004284949116229245
Global Adjusted Rand Index: -0.0003371649239211524
Average Adjusted Rand Index: -0.0008569898232458489
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21203.64002660818
Iteration 100: Loss = -12292.90107164564
Iteration 200: Loss = -12292.13300254055
Iteration 300: Loss = -12291.742013541227
Iteration 400: Loss = -12291.468533824254
Iteration 500: Loss = -12291.291002450613
Iteration 600: Loss = -12291.171407803602
Iteration 700: Loss = -12291.096134052235
Iteration 800: Loss = -12291.051098751577
Iteration 900: Loss = -12291.024539196324
Iteration 1000: Loss = -12291.008175648672
Iteration 1100: Loss = -12290.997464798002
Iteration 1200: Loss = -12290.98985068364
Iteration 1300: Loss = -12290.984221173092
Iteration 1400: Loss = -12290.979842801016
Iteration 1500: Loss = -12290.976313247436
Iteration 1600: Loss = -12290.973463383143
Iteration 1700: Loss = -12290.971079349334
Iteration 1800: Loss = -12290.969014472874
Iteration 1900: Loss = -12290.96727966576
Iteration 2000: Loss = -12290.965715785756
Iteration 2100: Loss = -12290.964432502127
Iteration 2200: Loss = -12290.963180687393
Iteration 2300: Loss = -12290.96214153137
Iteration 2400: Loss = -12290.961187215331
Iteration 2500: Loss = -12290.960309134716
Iteration 2600: Loss = -12290.95950488538
Iteration 2700: Loss = -12290.958802670102
Iteration 2800: Loss = -12290.958139080347
Iteration 2900: Loss = -12290.957539691999
Iteration 3000: Loss = -12290.956980011579
Iteration 3100: Loss = -12290.956426539606
Iteration 3200: Loss = -12290.955949632962
Iteration 3300: Loss = -12290.955465104804
Iteration 3400: Loss = -12290.955057468278
Iteration 3500: Loss = -12290.954685506027
Iteration 3600: Loss = -12290.954330531296
Iteration 3700: Loss = -12290.95396104552
Iteration 3800: Loss = -12290.953636093367
Iteration 3900: Loss = -12290.953321941708
Iteration 4000: Loss = -12290.953043763453
Iteration 4100: Loss = -12290.952775501168
Iteration 4200: Loss = -12290.952494787234
Iteration 4300: Loss = -12290.952298006083
Iteration 4400: Loss = -12290.952073411661
Iteration 4500: Loss = -12290.951864434706
Iteration 4600: Loss = -12290.951679077774
Iteration 4700: Loss = -12290.951479611687
Iteration 4800: Loss = -12290.951352178956
Iteration 4900: Loss = -12290.951133888659
Iteration 5000: Loss = -12290.951027123283
Iteration 5100: Loss = -12290.950902005447
Iteration 5200: Loss = -12290.950719385763
Iteration 5300: Loss = -12290.950668510943
Iteration 5400: Loss = -12290.95053196169
Iteration 5500: Loss = -12290.950376857018
Iteration 5600: Loss = -12290.950306411089
Iteration 5700: Loss = -12290.950241498342
Iteration 5800: Loss = -12290.95013467968
Iteration 5900: Loss = -12290.95004790653
Iteration 6000: Loss = -12290.949982940863
Iteration 6100: Loss = -12290.949992288932
1
Iteration 6200: Loss = -12290.949865567447
Iteration 6300: Loss = -12290.94981651778
Iteration 6400: Loss = -12290.950936982214
1
Iteration 6500: Loss = -12290.949703601344
Iteration 6600: Loss = -12290.95614490505
1
Iteration 6700: Loss = -12290.951936343621
2
Iteration 6800: Loss = -12291.01605413896
3
Iteration 6900: Loss = -12290.949520101545
Iteration 7000: Loss = -12290.952142060642
1
Iteration 7100: Loss = -12290.949514738597
Iteration 7200: Loss = -12290.99015183955
1
Iteration 7300: Loss = -12290.949421228732
Iteration 7400: Loss = -12290.949427446052
1
Iteration 7500: Loss = -12290.965073078232
2
Iteration 7600: Loss = -12290.949379118281
Iteration 7700: Loss = -12290.949347557138
Iteration 7800: Loss = -12290.964081406297
1
Iteration 7900: Loss = -12290.949369841457
2
Iteration 8000: Loss = -12290.949302190673
Iteration 8100: Loss = -12290.94929175461
Iteration 8200: Loss = -12290.94936339604
1
Iteration 8300: Loss = -12290.94928599987
Iteration 8400: Loss = -12290.949260173587
Iteration 8500: Loss = -12290.9495944697
1
Iteration 8600: Loss = -12290.94923759911
Iteration 8700: Loss = -12290.949269530316
1
Iteration 8800: Loss = -12291.16538166694
2
Iteration 8900: Loss = -12290.949234066384
Iteration 9000: Loss = -12290.949209115652
Iteration 9100: Loss = -12290.94927433019
1
Iteration 9200: Loss = -12290.949212541123
2
Iteration 9300: Loss = -12290.949218999545
3
Iteration 9400: Loss = -12290.949192062242
Iteration 9500: Loss = -12290.990242972697
1
Iteration 9600: Loss = -12290.949229509646
2
Iteration 9700: Loss = -12290.9492023167
3
Iteration 9800: Loss = -12290.949539565463
4
Iteration 9900: Loss = -12290.949281175383
5
Stopping early at iteration 9900 due to no improvement.
pi: tensor([[0.9777, 0.0223],
        [0.9625, 0.0375]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9983, 0.0017], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1957, 0.1806],
         [0.6580, 0.2806]],

        [[0.5364, 0.2173],
         [0.5542, 0.6017]],

        [[0.6628, 0.2504],
         [0.6673, 0.6787]],

        [[0.6777, 0.1858],
         [0.5477, 0.5653]],

        [[0.5690, 0.3155],
         [0.7021, 0.5525]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: -0.004284949116229245
Global Adjusted Rand Index: -0.0003371649239211524
Average Adjusted Rand Index: -0.0008569898232458489
11754.579933210178
[-0.0003371649239211524, -0.0003371649239211524, -0.0003371649239211524, -0.0003371649239211524] [-0.0008569898232458489, -0.0008569898232458489, -0.0008569898232458489, -0.0008569898232458489] [12290.950484538707, 12290.9495652104, 12290.949254991787, 12290.949281175383]
-----------------------------------------------------------------------------------------
This iteration is 9
True Objective function: Loss = -11779.595599257842
Iteration 0: Loss = -12393.958264024819
Iteration 10: Loss = -12334.697499346881
Iteration 20: Loss = -12333.570677209738
Iteration 30: Loss = -12333.385755882342
Iteration 40: Loss = -12333.33181994734
Iteration 50: Loss = -12333.304187497131
Iteration 60: Loss = -12333.285626260717
Iteration 70: Loss = -12333.270797348212
Iteration 80: Loss = -12333.257652059972
Iteration 90: Loss = -12333.245026174998
Iteration 100: Loss = -12333.232642819145
Iteration 110: Loss = -12333.220742988819
Iteration 120: Loss = -12333.210247932713
Iteration 130: Loss = -12333.202236168898
Iteration 140: Loss = -12333.196890732597
Iteration 150: Loss = -12333.193720453855
Iteration 160: Loss = -12333.191969592013
Iteration 170: Loss = -12333.19095958438
Iteration 180: Loss = -12333.190419935518
Iteration 190: Loss = -12333.190051588688
Iteration 200: Loss = -12333.189926214483
Iteration 210: Loss = -12333.189786018012
Iteration 220: Loss = -12333.189715995453
Iteration 230: Loss = -12333.18966120338
Iteration 240: Loss = -12333.189643917704
Iteration 250: Loss = -12333.18961473513
Iteration 260: Loss = -12333.189632487458
1
Iteration 270: Loss = -12333.189634475213
2
Iteration 280: Loss = -12333.189622372247
3
Stopping early at iteration 280 due to no improvement.
pi: tensor([[9.6858e-01, 3.1420e-02],
        [1.0000e+00, 6.0539e-29]], dtype=torch.float64)
alpha: tensor([0.9688, 0.0312])
beta: tensor([[[0.1961, 0.2927],
         [0.3412, 0.1974]],

        [[0.9152, 0.1103],
         [0.1872, 0.2964]],

        [[0.0215, 0.2039],
         [0.8806, 0.4301]],

        [[0.2178, 0.2138],
         [0.8706, 0.6854]],

        [[0.0839, 0.1866],
         [0.9638, 0.9099]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 59
Adjusted Rand Index: -0.006658343736995423
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -5.091514160815462e-05
Average Adjusted Rand Index: -0.0016378823189826981
pi: tensor([[5.3911e-38, 1.0000e+00],
        [1.8163e-24, 1.0000e+00]], dtype=torch.float64)
alpha: tensor([1.4530e-24, 1.0000e+00])
beta: tensor([[[0.0000, 0.2700],
         [0.2079, 0.1966]],

        [[0.3622, 0.2549],
         [0.1261, 0.9860]],

        [[0.8190, 0.2900],
         [0.8763, 0.6507]],

        [[0.6343, 0.3098],
         [0.6151, 0.9740]],

        [[0.6759,    nan],
         [0.0111, 0.3979]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 40
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21037.551217127795
Iteration 100: Loss = -12337.060913328012
Iteration 200: Loss = -12336.524141730357
Iteration 300: Loss = -12336.402407423602
Iteration 400: Loss = -12336.352987469885
Iteration 500: Loss = -12336.314440927668
Iteration 600: Loss = -12336.229756489374
Iteration 700: Loss = -12334.341624771338
Iteration 800: Loss = -12333.3701574731
Iteration 900: Loss = -12331.490209648719
Iteration 1000: Loss = -12250.409083425166
Iteration 1100: Loss = -12249.985900811253
Iteration 1200: Loss = -12249.89283003963
Iteration 1300: Loss = -12249.833763949726
Iteration 1400: Loss = -12249.773398376463
Iteration 1500: Loss = -12249.65411789955
Iteration 1600: Loss = -12249.29881165659
Iteration 1700: Loss = -12248.96987113907
Iteration 1800: Loss = -12248.774611727269
Iteration 1900: Loss = -12248.582945784452
Iteration 2000: Loss = -12248.449023377305
Iteration 2100: Loss = -12248.38569560637
Iteration 2200: Loss = -12248.36132317356
Iteration 2300: Loss = -12248.352685413003
Iteration 2400: Loss = -12248.34823378201
Iteration 2500: Loss = -12248.345224155466
Iteration 2600: Loss = -12248.342952685858
Iteration 2700: Loss = -12248.341133404047
Iteration 2800: Loss = -12248.339740627693
Iteration 2900: Loss = -12248.338462794938
Iteration 3000: Loss = -12248.337437076201
Iteration 3100: Loss = -12248.336474485961
Iteration 3200: Loss = -12248.335741535204
Iteration 3300: Loss = -12248.33502751519
Iteration 3400: Loss = -12248.334379995367
Iteration 3500: Loss = -12248.334382133078
1
Iteration 3600: Loss = -12248.333381821987
Iteration 3700: Loss = -12248.332932865807
Iteration 3800: Loss = -12248.332530741445
Iteration 3900: Loss = -12248.332140575549
Iteration 4000: Loss = -12248.33179812654
Iteration 4100: Loss = -12248.331483401333
Iteration 4200: Loss = -12248.336070991225
1
Iteration 4300: Loss = -12248.330874710075
Iteration 4400: Loss = -12248.33055253581
Iteration 4500: Loss = -12248.330244141836
Iteration 4600: Loss = -12248.329942249473
Iteration 4700: Loss = -12248.329658612185
Iteration 4800: Loss = -12248.329395393364
Iteration 4900: Loss = -12248.330198383972
1
Iteration 5000: Loss = -12248.329130140004
Iteration 5100: Loss = -12248.329010573583
Iteration 5200: Loss = -12248.328920626485
Iteration 5300: Loss = -12248.328773418683
Iteration 5400: Loss = -12248.328658525183
Iteration 5500: Loss = -12248.328591992069
Iteration 5600: Loss = -12248.328589640085
Iteration 5700: Loss = -12248.328415663698
Iteration 5800: Loss = -12248.328360204545
Iteration 5900: Loss = -12248.333728567468
1
Iteration 6000: Loss = -12248.328210327072
Iteration 6100: Loss = -12248.328163520575
Iteration 6200: Loss = -12248.328082146867
Iteration 6300: Loss = -12248.328096188945
1
Iteration 6400: Loss = -12248.327989957672
Iteration 6500: Loss = -12248.32807412721
1
Iteration 6600: Loss = -12248.32788830045
Iteration 6700: Loss = -12248.328588473427
1
Iteration 6800: Loss = -12248.327871799409
Iteration 6900: Loss = -12248.327769379655
Iteration 7000: Loss = -12248.329132673913
1
Iteration 7100: Loss = -12248.327747620096
Iteration 7200: Loss = -12248.327896047715
1
Iteration 7300: Loss = -12248.33891820363
2
Iteration 7400: Loss = -12248.436967825015
3
Iteration 7500: Loss = -12248.327879070424
4
Iteration 7600: Loss = -12248.32763882624
Iteration 7700: Loss = -12248.331200959095
1
Iteration 7800: Loss = -12248.327559519896
Iteration 7900: Loss = -12248.32820011626
1
Iteration 8000: Loss = -12248.327513417924
Iteration 8100: Loss = -12248.331692263113
1
Iteration 8200: Loss = -12248.327490169715
Iteration 8300: Loss = -12248.327481654702
Iteration 8400: Loss = -12248.327519490445
1
Iteration 8500: Loss = -12248.327468144147
Iteration 8600: Loss = -12248.371872175012
1
Iteration 8700: Loss = -12248.327447756232
Iteration 8800: Loss = -12248.32740027292
Iteration 8900: Loss = -12248.383415313907
1
Iteration 9000: Loss = -12248.32739549579
Iteration 9100: Loss = -12248.327514658497
1
Iteration 9200: Loss = -12248.327437051787
2
Iteration 9300: Loss = -12248.32749544552
3
Iteration 9400: Loss = -12248.32734096825
Iteration 9500: Loss = -12248.327452480831
1
Iteration 9600: Loss = -12248.327311211198
Iteration 9700: Loss = -12248.34519661908
1
Iteration 9800: Loss = -12248.327320774097
2
Iteration 9900: Loss = -12248.327287994936
Iteration 10000: Loss = -12248.327502970815
1
Iteration 10100: Loss = -12248.327306014407
2
Iteration 10200: Loss = -12248.327325984596
3
Iteration 10300: Loss = -12248.327679402504
4
Iteration 10400: Loss = -12248.327316377758
5
Stopping early at iteration 10400 due to no improvement.
pi: tensor([[0.0100, 0.9900],
        [0.0246, 0.9754]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5952, 0.4048], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3143, 0.1003],
         [0.5765, 0.2015]],

        [[0.5620, 0.1037],
         [0.5498, 0.6340]],

        [[0.6203, 0.1971],
         [0.5509, 0.6552]],

        [[0.5071, 0.2138],
         [0.7118, 0.6022]],

        [[0.6817, 0.1660],
         [0.6889, 0.6187]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.06247592349782576
Average Adjusted Rand Index: 0.19984777489712913
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23041.830472434856
Iteration 100: Loss = -12336.925798114413
Iteration 200: Loss = -12336.438795101543
Iteration 300: Loss = -12336.223628918886
Iteration 400: Loss = -12333.12642791125
Iteration 500: Loss = -12327.545260747249
Iteration 600: Loss = -12250.152169699013
Iteration 700: Loss = -12249.94015228781
Iteration 800: Loss = -12249.871878876187
Iteration 900: Loss = -12249.824845901476
Iteration 1000: Loss = -12249.780761300459
Iteration 1100: Loss = -12249.713164656416
Iteration 1200: Loss = -12249.506896678948
Iteration 1300: Loss = -12249.108665138378
Iteration 1400: Loss = -12248.921861852803
Iteration 1500: Loss = -12248.76540105328
Iteration 1600: Loss = -12248.585784226945
Iteration 1700: Loss = -12248.472420928743
Iteration 1800: Loss = -12248.410960920073
Iteration 1900: Loss = -12248.372795846446
Iteration 2000: Loss = -12248.355067681649
Iteration 2100: Loss = -12248.342450795497
Iteration 2200: Loss = -12248.338334213966
Iteration 2300: Loss = -12248.336324151553
Iteration 2400: Loss = -12248.335064841753
Iteration 2500: Loss = -12248.334129544055
Iteration 2600: Loss = -12248.333412904656
Iteration 2700: Loss = -12248.332791869805
Iteration 2800: Loss = -12248.332279151151
Iteration 2900: Loss = -12248.331771760242
Iteration 3000: Loss = -12248.331320566276
Iteration 3100: Loss = -12248.335325856082
1
Iteration 3200: Loss = -12248.330518545323
Iteration 3300: Loss = -12248.330257906302
Iteration 3400: Loss = -12248.329987219076
Iteration 3500: Loss = -12248.32976237379
Iteration 3600: Loss = -12248.329576549137
Iteration 3700: Loss = -12248.329375189367
Iteration 3800: Loss = -12248.329467803205
1
Iteration 3900: Loss = -12248.329092777709
Iteration 4000: Loss = -12248.328943913588
Iteration 4100: Loss = -12248.329195910414
1
Iteration 4200: Loss = -12248.328695672011
Iteration 4300: Loss = -12248.328642652992
Iteration 4400: Loss = -12248.32854338256
Iteration 4500: Loss = -12248.329113272095
1
Iteration 4600: Loss = -12248.328374443037
Iteration 4700: Loss = -12248.328276875796
Iteration 4800: Loss = -12248.328205497584
Iteration 4900: Loss = -12248.328132786854
Iteration 5000: Loss = -12248.328084962126
Iteration 5100: Loss = -12248.328019616203
Iteration 5200: Loss = -12248.328006458138
Iteration 5300: Loss = -12248.327953338945
Iteration 5400: Loss = -12248.327896489252
Iteration 5500: Loss = -12248.327887920428
Iteration 5600: Loss = -12248.327816653404
Iteration 5700: Loss = -12248.3277935204
Iteration 5800: Loss = -12248.32775772201
Iteration 5900: Loss = -12248.32774569018
Iteration 6000: Loss = -12248.331199830865
1
Iteration 6100: Loss = -12248.32764401032
Iteration 6200: Loss = -12248.328026620271
1
Iteration 6300: Loss = -12248.327582773396
Iteration 6400: Loss = -12248.3275673557
Iteration 6500: Loss = -12248.327578287686
1
Iteration 6600: Loss = -12248.327533950393
Iteration 6700: Loss = -12248.32976126377
1
Iteration 6800: Loss = -12248.327521309315
Iteration 6900: Loss = -12248.327596923195
1
Iteration 7000: Loss = -12248.327469505906
Iteration 7100: Loss = -12248.327475671802
1
Iteration 7200: Loss = -12248.327447456435
Iteration 7300: Loss = -12248.327484150082
1
Iteration 7400: Loss = -12248.327442931302
Iteration 7500: Loss = -12248.32739057383
Iteration 7600: Loss = -12248.327571116613
1
Iteration 7700: Loss = -12248.327410224601
2
Iteration 7800: Loss = -12248.327411757664
3
Iteration 7900: Loss = -12248.327435518797
4
Iteration 8000: Loss = -12248.328451126154
5
Stopping early at iteration 8000 due to no improvement.
pi: tensor([[0.0101, 0.9899],
        [0.0248, 0.9752]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5953, 0.4047], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3143, 0.1003],
         [0.5049, 0.2016]],

        [[0.6597, 0.1037],
         [0.5030, 0.6804]],

        [[0.6422, 0.1971],
         [0.6450, 0.6316]],

        [[0.6296, 0.2138],
         [0.5502, 0.6421]],

        [[0.5751, 0.1660],
         [0.6750, 0.7039]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.06247592349782576
Average Adjusted Rand Index: 0.19984777489712913
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22599.759132621333
Iteration 100: Loss = -12337.158557469167
Iteration 200: Loss = -12335.054410934135
Iteration 300: Loss = -12332.628902526456
Iteration 400: Loss = -12330.647764124966
Iteration 500: Loss = -12258.083222863239
Iteration 600: Loss = -12250.410757229669
Iteration 700: Loss = -12250.131980062237
Iteration 800: Loss = -12250.011487895477
Iteration 900: Loss = -12249.938089875199
Iteration 1000: Loss = -12249.883613539088
Iteration 1100: Loss = -12249.834756965634
Iteration 1200: Loss = -12249.778778595044
Iteration 1300: Loss = -12249.684210127714
Iteration 1400: Loss = -12249.452428437948
Iteration 1500: Loss = -12249.09532708727
Iteration 1600: Loss = -12248.829125072878
Iteration 1700: Loss = -12248.616777655121
Iteration 1800: Loss = -12248.472732149234
Iteration 1900: Loss = -12248.410471389929
Iteration 2000: Loss = -12248.383579047442
Iteration 2100: Loss = -12248.369623905008
Iteration 2200: Loss = -12248.361196501935
Iteration 2300: Loss = -12248.355443206097
Iteration 2400: Loss = -12248.35120960536
Iteration 2500: Loss = -12248.347923989651
Iteration 2600: Loss = -12248.345312075475
Iteration 2700: Loss = -12248.343199951692
Iteration 2800: Loss = -12248.341471539889
Iteration 2900: Loss = -12248.339921634652
Iteration 3000: Loss = -12248.338656615553
Iteration 3100: Loss = -12248.337546485642
Iteration 3200: Loss = -12248.33660583346
Iteration 3300: Loss = -12248.335768245452
Iteration 3400: Loss = -12248.335039105114
Iteration 3500: Loss = -12248.334389555937
Iteration 3600: Loss = -12248.33379465499
Iteration 3700: Loss = -12248.33327474943
Iteration 3800: Loss = -12248.332753751181
Iteration 3900: Loss = -12248.33234385175
Iteration 4000: Loss = -12248.332003550102
Iteration 4100: Loss = -12248.331606731628
Iteration 4200: Loss = -12248.331310767557
Iteration 4300: Loss = -12248.331008093463
Iteration 4400: Loss = -12248.330891943604
Iteration 4500: Loss = -12248.330488604679
Iteration 4600: Loss = -12248.330233997009
Iteration 4700: Loss = -12248.330075687485
Iteration 4800: Loss = -12248.329882186936
Iteration 4900: Loss = -12248.329650253803
Iteration 5000: Loss = -12248.329511662265
Iteration 5100: Loss = -12248.329574179463
1
Iteration 5200: Loss = -12248.329234065253
Iteration 5300: Loss = -12248.329076542692
Iteration 5400: Loss = -12248.328962245689
Iteration 5500: Loss = -12248.328854731786
Iteration 5600: Loss = -12248.328749872742
Iteration 5700: Loss = -12248.328676019382
Iteration 5800: Loss = -12248.32860813849
Iteration 5900: Loss = -12248.32848160437
Iteration 6000: Loss = -12248.32839064709
Iteration 6100: Loss = -12248.328323533384
Iteration 6200: Loss = -12248.328252719086
Iteration 6300: Loss = -12248.328152334536
Iteration 6400: Loss = -12248.328118154026
Iteration 6500: Loss = -12248.328388409722
1
Iteration 6600: Loss = -12248.32800413721
Iteration 6700: Loss = -12248.327994703446
Iteration 6800: Loss = -12248.327912177867
Iteration 6900: Loss = -12248.327893129499
Iteration 7000: Loss = -12248.327887279718
Iteration 7100: Loss = -12248.327822068464
Iteration 7200: Loss = -12248.32785547929
1
Iteration 7300: Loss = -12248.327744089192
Iteration 7400: Loss = -12248.327971960052
1
Iteration 7500: Loss = -12248.327676035664
Iteration 7600: Loss = -12248.327714895771
1
Iteration 7700: Loss = -12248.327673257772
Iteration 7800: Loss = -12248.332653330528
1
Iteration 7900: Loss = -12248.330056680361
2
Iteration 8000: Loss = -12248.327612321647
Iteration 8100: Loss = -12248.327540541588
Iteration 8200: Loss = -12248.327720986865
1
Iteration 8300: Loss = -12248.327999490453
2
Iteration 8400: Loss = -12248.327477478451
Iteration 8500: Loss = -12248.327555849708
1
Iteration 8600: Loss = -12248.33166494942
2
Iteration 8700: Loss = -12248.327575974901
3
Iteration 8800: Loss = -12248.59568246629
4
Iteration 8900: Loss = -12248.327425293544
Iteration 9000: Loss = -12248.50915721225
1
Iteration 9100: Loss = -12248.327422987088
Iteration 9200: Loss = -12248.327392317877
Iteration 9300: Loss = -12248.327433172519
1
Iteration 9400: Loss = -12248.327383633321
Iteration 9500: Loss = -12248.327389638991
1
Iteration 9600: Loss = -12248.32754664276
2
Iteration 9700: Loss = -12248.366411237603
3
Iteration 9800: Loss = -12248.327394396956
4
Iteration 9900: Loss = -12248.32833138892
5
Stopping early at iteration 9900 due to no improvement.
pi: tensor([[0.9754, 0.0246],
        [0.9900, 0.0100]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4047, 0.5953], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2016, 0.1002],
         [0.6162, 0.3143]],

        [[0.5027, 0.1037],
         [0.6907, 0.5595]],

        [[0.5761, 0.1971],
         [0.7255, 0.5022]],

        [[0.5776, 0.2138],
         [0.6716, 0.5258]],

        [[0.5247, 0.1660],
         [0.5841, 0.5413]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.06247592349782576
Average Adjusted Rand Index: 0.19984777489712913
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21717.877939258306
Iteration 100: Loss = -12336.281700931882
Iteration 200: Loss = -12334.609402329972
Iteration 300: Loss = -12333.638660761973
Iteration 400: Loss = -12333.31138203915
Iteration 500: Loss = -12332.875861699558
Iteration 600: Loss = -12332.012768125096
Iteration 700: Loss = -12298.8779396675
Iteration 800: Loss = -12250.10066303247
Iteration 900: Loss = -12249.400510676172
Iteration 1000: Loss = -12249.041538995161
Iteration 1100: Loss = -12248.871388293806
Iteration 1200: Loss = -12248.760667749973
Iteration 1300: Loss = -12248.67838495228
Iteration 1400: Loss = -12248.612824770173
Iteration 1500: Loss = -12248.5606919696
Iteration 1600: Loss = -12248.519919811612
Iteration 1700: Loss = -12248.489524410023
Iteration 1800: Loss = -12248.466561336241
Iteration 1900: Loss = -12248.449149619899
Iteration 2000: Loss = -12248.433857453969
Iteration 2100: Loss = -12248.416330783382
Iteration 2200: Loss = -12248.395646791681
Iteration 2300: Loss = -12248.372622587873
Iteration 2400: Loss = -12248.355481281025
Iteration 2500: Loss = -12248.346525350677
Iteration 2600: Loss = -12248.341942516721
Iteration 2700: Loss = -12248.339084684923
Iteration 2800: Loss = -12248.336895037453
Iteration 2900: Loss = -12248.33535472333
Iteration 3000: Loss = -12248.334352228972
Iteration 3100: Loss = -12248.333572670455
Iteration 3200: Loss = -12248.332976435899
Iteration 3300: Loss = -12248.332475023799
Iteration 3400: Loss = -12248.332046610107
Iteration 3500: Loss = -12248.331681701518
Iteration 3600: Loss = -12248.331305815422
Iteration 3700: Loss = -12248.330977105279
Iteration 3800: Loss = -12248.330647371808
Iteration 3900: Loss = -12248.330414565942
Iteration 4000: Loss = -12248.330160215552
Iteration 4100: Loss = -12248.329963904449
Iteration 4200: Loss = -12248.329794794214
Iteration 4300: Loss = -12248.329585318765
Iteration 4400: Loss = -12248.329479361139
Iteration 4500: Loss = -12248.329292533841
Iteration 4600: Loss = -12248.329193742178
Iteration 4700: Loss = -12248.329036784473
Iteration 4800: Loss = -12248.32886706372
Iteration 4900: Loss = -12248.32877133048
Iteration 5000: Loss = -12248.328665383006
Iteration 5100: Loss = -12248.328723322536
1
Iteration 5200: Loss = -12248.328471036759
Iteration 5300: Loss = -12248.328396667246
Iteration 5400: Loss = -12248.328302952164
Iteration 5500: Loss = -12248.32827746562
Iteration 5600: Loss = -12248.32817089157
Iteration 5700: Loss = -12248.328129236306
Iteration 5800: Loss = -12248.328298697006
1
Iteration 5900: Loss = -12248.327981668963
Iteration 6000: Loss = -12248.327941550831
Iteration 6100: Loss = -12248.334677802131
1
Iteration 6200: Loss = -12248.328012907785
2
Iteration 6300: Loss = -12248.327819324562
Iteration 6400: Loss = -12248.327793798744
Iteration 6500: Loss = -12248.32775272594
Iteration 6600: Loss = -12248.327736730025
Iteration 6700: Loss = -12248.327665501036
Iteration 6800: Loss = -12248.327662966673
Iteration 6900: Loss = -12248.331033343586
1
Iteration 7000: Loss = -12248.327601047651
Iteration 7100: Loss = -12248.32766085144
1
Iteration 7200: Loss = -12248.327555866726
Iteration 7300: Loss = -12248.327813767879
1
Iteration 7400: Loss = -12248.327532379437
Iteration 7500: Loss = -12248.32750292673
Iteration 7600: Loss = -12248.328220830124
1
Iteration 7700: Loss = -12248.327535198177
2
Iteration 7800: Loss = -12248.328166919522
3
Iteration 7900: Loss = -12248.32763677535
4
Iteration 8000: Loss = -12248.348875303072
5
Stopping early at iteration 8000 due to no improvement.
pi: tensor([[0.9753, 0.0247],
        [0.9900, 0.0100]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4043, 0.5957], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2017, 0.1003],
         [0.5537, 0.3143]],

        [[0.6330, 0.1037],
         [0.7027, 0.7170]],

        [[0.7255, 0.1972],
         [0.7029, 0.5604]],

        [[0.6784, 0.2138],
         [0.5117, 0.6709]],

        [[0.6677, 0.1661],
         [0.5568, 0.6838]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.06247592349782576
Average Adjusted Rand Index: 0.19984777489712913
11779.595599257842
[0.06247592349782576, 0.06247592349782576, 0.06247592349782576, 0.06247592349782576] [0.19984777489712913, 0.19984777489712913, 0.19984777489712913, 0.19984777489712913] [12248.327316377758, 12248.328451126154, 12248.32833138892, 12248.348875303072]
-----------------------------------------------------------------------------------------
This iteration is 10
True Objective function: Loss = -11878.463619653518
Iteration 0: Loss = -12365.866067122433
Iteration 10: Loss = -12365.836788923403
Iteration 20: Loss = -12365.836555826416
Iteration 30: Loss = -12365.835539311402
Iteration 40: Loss = -12365.834740192677
Iteration 50: Loss = -12365.833807597715
Iteration 60: Loss = -12365.832613137607
Iteration 70: Loss = -12365.830981205645
Iteration 80: Loss = -12365.828813291313
Iteration 90: Loss = -12365.825746026689
Iteration 100: Loss = -12365.821740461495
Iteration 110: Loss = -12365.816343034543
Iteration 120: Loss = -12365.809385027984
Iteration 130: Loss = -12365.800519291366
Iteration 140: Loss = -12365.789518547166
Iteration 150: Loss = -12365.776318415374
Iteration 160: Loss = -12365.761066482137
Iteration 170: Loss = -12365.744009840131
Iteration 180: Loss = -12365.725613524433
Iteration 190: Loss = -12365.706285382246
Iteration 200: Loss = -12365.686140162094
Iteration 210: Loss = -12365.663150898119
Iteration 220: Loss = -12365.6241980332
Iteration 230: Loss = -12365.552411176435
Iteration 240: Loss = -12365.493902990118
Iteration 250: Loss = -12365.441829668374
Iteration 260: Loss = -12365.37946065947
Iteration 270: Loss = -12365.299207653048
Iteration 280: Loss = -12365.246444718461
Iteration 290: Loss = -12365.235346723362
Iteration 300: Loss = -12365.23509012293
Iteration 310: Loss = -12365.23640329352
1
Iteration 320: Loss = -12365.237676793713
2
Iteration 330: Loss = -12365.238797001013
3
Stopping early at iteration 330 due to no improvement.
pi: tensor([[7.6189e-17, 1.0000e+00],
        [2.2055e-02, 9.7795e-01]], dtype=torch.float64)
alpha: tensor([0.0219, 0.9781])
beta: tensor([[[0.1692, 0.1501],
         [0.6791, 0.1983]],

        [[0.7038, 0.1711],
         [0.1921, 0.3195]],

        [[0.5144, 0.2284],
         [0.7400, 0.1828]],

        [[0.9073, 0.2119],
         [0.1897, 0.8892]],

        [[0.5757, 0.1363],
         [0.9241, 0.9893]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -12456.860848126395
Iteration 10: Loss = -11861.588318014597
Iteration 20: Loss = -11861.569369370514
Iteration 30: Loss = -11861.569367052647
Iteration 40: Loss = -11861.569367052647
1
Iteration 50: Loss = -11861.569367052647
2
Iteration 60: Loss = -11861.569367052647
3
Stopping early at iteration 60 due to no improvement.
pi: tensor([[0.2834, 0.7166],
        [0.7970, 0.2030]], dtype=torch.float64)
alpha: tensor([0.5315, 0.4685])
beta: tensor([[[0.2888, 0.1011],
         [0.0571, 0.2976]],

        [[0.2115, 0.0974],
         [0.7349, 0.7453]],

        [[0.1029, 0.0965],
         [0.5324, 0.2102]],

        [[0.7742, 0.1041],
         [0.2390, 0.1188]],

        [[0.8921, 0.1049],
         [0.9364, 0.1819]]], dtype=torch.float64)
time is 0
tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.9599967086364498
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208065164923572
time is 4
tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 99
Adjusted Rand Index: 0.96
Global Adjusted Rand Index: 0.039699141508821735
Average Adjusted Rand Index: 0.9601598613554906
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22935.738026079645
Iteration 100: Loss = -12366.602839433486
Iteration 200: Loss = -12365.778309304424
Iteration 300: Loss = -12365.61397711396
Iteration 400: Loss = -12365.541980345859
Iteration 500: Loss = -12365.502034427433
Iteration 600: Loss = -12365.473623041898
Iteration 700: Loss = -12365.45003382542
Iteration 800: Loss = -12365.428751848236
Iteration 900: Loss = -12365.408607587207
Iteration 1000: Loss = -12365.38907021129
Iteration 1100: Loss = -12365.3702934327
Iteration 1200: Loss = -12365.352399768346
Iteration 1300: Loss = -12365.335505093926
Iteration 1400: Loss = -12365.319460982368
Iteration 1500: Loss = -12365.303916090616
Iteration 1600: Loss = -12365.288388525212
Iteration 1700: Loss = -12365.27261407282
Iteration 1800: Loss = -12365.256082468113
Iteration 1900: Loss = -12365.23862996834
Iteration 2000: Loss = -12365.21983134406
Iteration 2100: Loss = -12365.19934216908
Iteration 2200: Loss = -12365.176746889269
Iteration 2300: Loss = -12365.151401114525
Iteration 2400: Loss = -12365.122582325841
Iteration 2500: Loss = -12365.089529326588
Iteration 2600: Loss = -12365.053401740186
Iteration 2700: Loss = -12365.016444468609
Iteration 2800: Loss = -12364.98142223194
Iteration 2900: Loss = -12364.94798449892
Iteration 3000: Loss = -12364.90816896684
Iteration 3100: Loss = -12364.84822049193
Iteration 3200: Loss = -12364.769744650755
Iteration 3300: Loss = -12364.698743744517
Iteration 3400: Loss = -12364.64360471866
Iteration 3500: Loss = -12364.612171714563
Iteration 3600: Loss = -12364.597496408905
Iteration 3700: Loss = -12364.590958809815
Iteration 3800: Loss = -12364.596168045031
1
Iteration 3900: Loss = -12364.588170086412
Iteration 4000: Loss = -12364.58457443394
Iteration 4100: Loss = -12364.584100541837
Iteration 4200: Loss = -12364.583391665206
Iteration 4300: Loss = -12364.583219189122
Iteration 4400: Loss = -12364.58442527082
1
Iteration 4500: Loss = -12364.58264811484
Iteration 4600: Loss = -12364.582512898001
Iteration 4700: Loss = -12364.583760280155
1
Iteration 4800: Loss = -12364.582399975052
Iteration 4900: Loss = -12364.583553650034
1
Iteration 5000: Loss = -12364.581935064969
Iteration 5100: Loss = -12364.581909620014
Iteration 5200: Loss = -12364.60328770022
1
Iteration 5300: Loss = -12364.581717758292
Iteration 5400: Loss = -12364.58258093621
1
Iteration 5500: Loss = -12364.581520404083
Iteration 5600: Loss = -12364.581474579409
Iteration 5700: Loss = -12364.581286240582
Iteration 5800: Loss = -12364.581179129265
Iteration 5900: Loss = -12364.581040089737
Iteration 6000: Loss = -12364.580842573176
Iteration 6100: Loss = -12364.585982454159
1
Iteration 6200: Loss = -12364.58089856515
2
Iteration 6300: Loss = -12364.577594908584
Iteration 6400: Loss = -12364.57171531633
Iteration 6500: Loss = -12364.569214609617
Iteration 6600: Loss = -12364.56851053902
Iteration 6700: Loss = -12364.568994286692
1
Iteration 6800: Loss = -12364.584773795497
2
Iteration 6900: Loss = -12364.568717460834
3
Iteration 7000: Loss = -12364.568536663286
4
Iteration 7100: Loss = -12364.567803444535
Iteration 7200: Loss = -12364.568545441987
1
Iteration 7300: Loss = -12364.567676842244
Iteration 7400: Loss = -12364.595372902988
1
Iteration 7500: Loss = -12364.578431019623
2
Iteration 7600: Loss = -12364.567960052012
3
Iteration 7700: Loss = -12364.56841212581
4
Iteration 7800: Loss = -12364.567584122648
Iteration 7900: Loss = -12364.568138009108
1
Iteration 8000: Loss = -12364.57810793343
2
Iteration 8100: Loss = -12364.604862700444
3
Iteration 8200: Loss = -12364.604023526412
4
Iteration 8300: Loss = -12364.594329359432
5
Stopping early at iteration 8300 due to no improvement.
pi: tensor([[0.7057, 0.2943],
        [0.3451, 0.6549]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9851, 0.0149], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1930, 0.2291],
         [0.6527, 0.2129]],

        [[0.5763, 0.2000],
         [0.6789, 0.7221]],

        [[0.6392, 0.2065],
         [0.6013, 0.6606]],

        [[0.6550, 0.2036],
         [0.6957, 0.6530]],

        [[0.5177, 0.1989],
         [0.6697, 0.6255]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.009264024704065878
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.011396933836266209
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: -5.322894721296349e-05
Average Adjusted Rand Index: -0.0011538545537127934
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20854.068174804248
Iteration 100: Loss = -12366.095663327505
Iteration 200: Loss = -12365.65367267706
Iteration 300: Loss = -12365.568515367393
Iteration 400: Loss = -12365.511222808072
Iteration 500: Loss = -12365.46188728314
Iteration 600: Loss = -12365.41515732122
Iteration 700: Loss = -12365.369360670193
Iteration 800: Loss = -12365.32535707729
Iteration 900: Loss = -12365.285095406445
Iteration 1000: Loss = -12365.24873692914
Iteration 1100: Loss = -12365.214019948377
Iteration 1200: Loss = -12365.178892407701
Iteration 1300: Loss = -12365.142432209292
Iteration 1400: Loss = -12365.10515570779
Iteration 1500: Loss = -12365.068808728129
Iteration 1600: Loss = -12365.03542615808
Iteration 1700: Loss = -12365.006064416591
Iteration 1800: Loss = -12364.980605968767
Iteration 1900: Loss = -12364.958227962466
Iteration 2000: Loss = -12364.937925842132
Iteration 2100: Loss = -12364.918132293064
Iteration 2200: Loss = -12364.896457141149
Iteration 2300: Loss = -12364.869573328571
Iteration 2400: Loss = -12364.832646367664
Iteration 2500: Loss = -12364.781113670895
Iteration 2600: Loss = -12364.720079686813
Iteration 2700: Loss = -12364.669935994669
Iteration 2800: Loss = -12364.639001912972
Iteration 2900: Loss = -12364.618335352508
Iteration 3000: Loss = -12364.606248992504
Iteration 3100: Loss = -12364.602842941016
Iteration 3200: Loss = -12364.594069158567
Iteration 3300: Loss = -12364.590930717548
Iteration 3400: Loss = -12364.588823008151
Iteration 3500: Loss = -12364.589889017729
1
Iteration 3600: Loss = -12364.58624844171
Iteration 3700: Loss = -12364.585542712159
Iteration 3800: Loss = -12364.584885727261
Iteration 3900: Loss = -12364.607418127936
1
Iteration 4000: Loss = -12364.584018865498
Iteration 4100: Loss = -12364.584267171736
1
Iteration 4200: Loss = -12364.583349938983
Iteration 4300: Loss = -12364.583116468204
Iteration 4400: Loss = -12364.5827880431
Iteration 4500: Loss = -12364.584865501216
1
Iteration 4600: Loss = -12364.582233696055
Iteration 4700: Loss = -12364.582072851263
Iteration 4800: Loss = -12364.581688113429
Iteration 4900: Loss = -12364.587635963933
1
Iteration 5000: Loss = -12364.58393082148
2
Iteration 5100: Loss = -12364.582242594215
3
Iteration 5200: Loss = -12364.57954310791
Iteration 5300: Loss = -12364.578890535282
Iteration 5400: Loss = -12364.576271824644
Iteration 5500: Loss = -12364.580417323295
1
Iteration 5600: Loss = -12364.56868784447
Iteration 5700: Loss = -12364.56785465231
Iteration 5800: Loss = -12364.567652444479
Iteration 5900: Loss = -12364.567829905653
1
Iteration 6000: Loss = -12364.579889754203
2
Iteration 6100: Loss = -12364.567574859773
Iteration 6200: Loss = -12364.567483244435
Iteration 6300: Loss = -12364.56787898434
1
Iteration 6400: Loss = -12364.567515575382
2
Iteration 6500: Loss = -12364.572943114079
3
Iteration 6600: Loss = -12364.568055417447
4
Iteration 6700: Loss = -12364.567769827785
5
Stopping early at iteration 6700 due to no improvement.
pi: tensor([[0.6543, 0.3457],
        [0.2949, 0.7051]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0137, 0.9863], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2127, 0.2308],
         [0.5973, 0.1931]],

        [[0.6619, 0.1999],
         [0.7158, 0.6716]],

        [[0.6193, 0.2071],
         [0.7298, 0.6306]],

        [[0.5021, 0.2037],
         [0.6967, 0.6449]],

        [[0.5002, 0.1990],
         [0.7184, 0.5796]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.008763529051498655
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.009861251969013659
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: -0.00034252314899781426
Average Adjusted Rand Index: -0.000946817310775728
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21118.88959716021
Iteration 100: Loss = -12365.942190936848
Iteration 200: Loss = -12365.629745093065
Iteration 300: Loss = -12365.571721556444
Iteration 400: Loss = -12365.538956083805
Iteration 500: Loss = -12365.511463154364
Iteration 600: Loss = -12365.484668154057
Iteration 700: Loss = -12365.45708907833
Iteration 800: Loss = -12365.42844129024
Iteration 900: Loss = -12365.398591822988
Iteration 1000: Loss = -12365.36780150061
Iteration 1100: Loss = -12365.336387325773
Iteration 1200: Loss = -12365.304341936391
Iteration 1300: Loss = -12365.271604716345
Iteration 1400: Loss = -12365.238139674939
Iteration 1500: Loss = -12365.203869100158
Iteration 1600: Loss = -12365.169214250021
Iteration 1700: Loss = -12365.134946334558
Iteration 1800: Loss = -12365.101794603905
Iteration 1900: Loss = -12365.070103834625
Iteration 2000: Loss = -12365.040289279304
Iteration 2100: Loss = -12365.013004446788
Iteration 2200: Loss = -12364.98914913349
Iteration 2300: Loss = -12364.968836878945
Iteration 2400: Loss = -12364.95077512117
Iteration 2500: Loss = -12364.932755800128
Iteration 2600: Loss = -12364.910910388011
Iteration 2700: Loss = -12364.879109290927
Iteration 2800: Loss = -12364.829795667289
Iteration 2900: Loss = -12364.768627546833
Iteration 3000: Loss = -12364.712355280206
Iteration 3100: Loss = -12364.666818752998
Iteration 3200: Loss = -12364.634030902938
Iteration 3300: Loss = -12364.613637276654
Iteration 3400: Loss = -12364.604634018056
Iteration 3500: Loss = -12364.594984518513
Iteration 3600: Loss = -12364.604236067275
1
Iteration 3700: Loss = -12364.614465104869
2
Iteration 3800: Loss = -12364.58701208109
Iteration 3900: Loss = -12364.586642177095
Iteration 4000: Loss = -12364.58518433944
Iteration 4100: Loss = -12364.584569483843
Iteration 4200: Loss = -12364.584118098122
Iteration 4300: Loss = -12364.5837093948
Iteration 4400: Loss = -12364.595350660938
1
Iteration 4500: Loss = -12364.58302601909
Iteration 4600: Loss = -12364.584119512965
1
Iteration 4700: Loss = -12364.582741853605
Iteration 4800: Loss = -12364.582459060024
Iteration 4900: Loss = -12364.594278081177
1
Iteration 5000: Loss = -12364.582118207474
Iteration 5100: Loss = -12364.581917859197
Iteration 5200: Loss = -12364.586193951029
1
Iteration 5300: Loss = -12364.581473196697
Iteration 5400: Loss = -12364.583596163666
1
Iteration 5500: Loss = -12364.581002330255
Iteration 5600: Loss = -12364.583575593708
1
Iteration 5700: Loss = -12364.58169149608
2
Iteration 5800: Loss = -12364.579761693853
Iteration 5900: Loss = -12364.577926976326
Iteration 6000: Loss = -12364.575386368806
Iteration 6100: Loss = -12364.57112826669
Iteration 6200: Loss = -12364.568357008597
Iteration 6300: Loss = -12364.568172687726
Iteration 6400: Loss = -12364.571748777695
1
Iteration 6500: Loss = -12364.56782944636
Iteration 6600: Loss = -12364.5681363286
1
Iteration 6700: Loss = -12364.567715206687
Iteration 6800: Loss = -12364.568928300656
1
Iteration 6900: Loss = -12364.567719954652
2
Iteration 7000: Loss = -12364.56827011269
3
Iteration 7100: Loss = -12364.567736420353
4
Iteration 7200: Loss = -12364.568172191039
5
Stopping early at iteration 7200 due to no improvement.
pi: tensor([[0.7055, 0.2945],
        [0.3464, 0.6536]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9883, 0.0117], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1932, 0.2338],
         [0.7282, 0.2126]],

        [[0.6651, 0.1999],
         [0.7059, 0.6194]],

        [[0.6237, 0.2071],
         [0.6011, 0.6146]],

        [[0.6934, 0.2037],
         [0.6336, 0.7188]],

        [[0.6450, 0.1990],
         [0.5632, 0.6424]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.004586397878790981
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: -0.009861251969013659
time is 4
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: -0.0005072722812953891
Average Adjusted Rand Index: -0.001782243545317263
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20487.95338466353
Iteration 100: Loss = -12365.823475698042
Iteration 200: Loss = -12365.5149329283
Iteration 300: Loss = -12365.475135565033
Iteration 400: Loss = -12365.450912637509
Iteration 500: Loss = -12365.425638589302
Iteration 600: Loss = -12365.396221429763
Iteration 700: Loss = -12365.36196196369
Iteration 800: Loss = -12365.323394584697
Iteration 900: Loss = -12365.283017534939
Iteration 1000: Loss = -12365.243933459087
Iteration 1100: Loss = -12365.206479950293
Iteration 1200: Loss = -12365.169141695413
Iteration 1300: Loss = -12365.131548972528
Iteration 1400: Loss = -12365.094654500243
Iteration 1500: Loss = -12365.059696980128
Iteration 1600: Loss = -12365.027399225502
Iteration 1700: Loss = -12364.998251089628
Iteration 1800: Loss = -12364.972470570263
Iteration 1900: Loss = -12364.948696981788
Iteration 2000: Loss = -12364.923602593537
Iteration 2100: Loss = -12364.891336229462
Iteration 2200: Loss = -12364.84460891541
Iteration 2300: Loss = -12364.788601245536
Iteration 2400: Loss = -12364.736424730965
Iteration 2500: Loss = -12364.688432394049
Iteration 2600: Loss = -12364.648356746558
Iteration 2700: Loss = -12364.621636633552
Iteration 2800: Loss = -12364.605621791692
Iteration 2900: Loss = -12364.596988462677
Iteration 3000: Loss = -12364.592318683433
Iteration 3100: Loss = -12364.589238086835
Iteration 3200: Loss = -12364.587441103646
Iteration 3300: Loss = -12364.58627869184
Iteration 3400: Loss = -12364.585441830397
Iteration 3500: Loss = -12364.62156028275
1
Iteration 3600: Loss = -12364.584446711122
Iteration 3700: Loss = -12364.584117761391
Iteration 3800: Loss = -12364.584212007083
1
Iteration 3900: Loss = -12364.58352527965
Iteration 4000: Loss = -12364.5844065869
1
Iteration 4100: Loss = -12364.58318708221
Iteration 4200: Loss = -12364.582938565136
Iteration 4300: Loss = -12364.582867105677
Iteration 4400: Loss = -12364.583442103434
1
Iteration 4500: Loss = -12364.582424886503
Iteration 4600: Loss = -12364.58286995184
1
Iteration 4700: Loss = -12364.582526039021
2
Iteration 4800: Loss = -12364.581940312148
Iteration 4900: Loss = -12364.582322870381
1
Iteration 5000: Loss = -12364.592618447943
2
Iteration 5100: Loss = -12364.58127583772
Iteration 5200: Loss = -12364.600461512959
1
Iteration 5300: Loss = -12364.580450955455
Iteration 5400: Loss = -12364.580001842762
Iteration 5500: Loss = -12364.578515725085
Iteration 5600: Loss = -12364.575990656846
Iteration 5700: Loss = -12364.571944002908
Iteration 5800: Loss = -12364.57249715012
1
Iteration 5900: Loss = -12364.567833740695
Iteration 6000: Loss = -12364.568253099113
1
Iteration 6100: Loss = -12364.569240231625
2
Iteration 6200: Loss = -12364.567666228782
Iteration 6300: Loss = -12364.568168813872
1
Iteration 6400: Loss = -12364.567606453951
Iteration 6500: Loss = -12364.567701933942
1
Iteration 6600: Loss = -12364.567545408445
Iteration 6700: Loss = -12364.567763672409
1
Iteration 6800: Loss = -12364.581723401792
2
Iteration 6900: Loss = -12364.568829688373
3
Iteration 7000: Loss = -12364.56771662769
4
Iteration 7100: Loss = -12364.574756327387
5
Stopping early at iteration 7100 due to no improvement.
pi: tensor([[0.6512, 0.3488],
        [0.2958, 0.7042]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0132, 0.9868], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2127, 0.2315],
         [0.5061, 0.1932]],

        [[0.7145, 0.1999],
         [0.5788, 0.6182]],

        [[0.6670, 0.2071],
         [0.5161, 0.6963]],

        [[0.6616, 0.2038],
         [0.6257, 0.6794]],

        [[0.6251, 0.1990],
         [0.5608, 0.6869]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.004586397878790981
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: -0.009861251969013659
time is 4
tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: -0.0005072722812953891
Average Adjusted Rand Index: -0.001782243545317263
11878.463619653518
[-5.322894721296349e-05, -0.00034252314899781426, -0.0005072722812953891, -0.0005072722812953891] [-0.0011538545537127934, -0.000946817310775728, -0.001782243545317263, -0.001782243545317263] [12364.594329359432, 12364.567769827785, 12364.568172191039, 12364.574756327387]
-----------------------------------------------------------------------------------------
This iteration is 11
True Objective function: Loss = -11684.613177382842
pi: tensor([[9.7486e-40, 1.0000e+00],
        [9.6902e-27, 1.0000e+00]], dtype=torch.float64)
alpha: tensor([7.7522e-27, 1.0000e+00])
beta: tensor([[[   nan, 0.2500],
         [0.4374, 0.1917]],

        [[0.6019, 0.2271],
         [0.7411, 0.1783]],

        [[0.4324, 0.2300],
         [0.8920, 0.1797]],

        [[0.1583, 0.3071],
         [0.7045, 0.1367]],

        [[0.2098, 0.2400],
         [0.6599, 0.0962]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -12239.501013217405
Iteration 10: Loss = -12143.828770241817
Iteration 20: Loss = -12142.639272557279
Iteration 30: Loss = -12142.553065796452
Iteration 40: Loss = -12142.553199409063
1
Iteration 50: Loss = -12142.562987870806
2
Iteration 60: Loss = -12142.571776630082
3
Stopping early at iteration 60 due to no improvement.
pi: tensor([[0.4991, 0.5009],
        [0.4885, 0.5115]], dtype=torch.float64)
alpha: tensor([0.4924, 0.5076])
beta: tensor([[[0.2129, 0.1951],
         [0.4822, 0.2047]],

        [[0.3455, 0.1896],
         [0.4920, 0.4689]],

        [[0.9057, 0.1052],
         [0.7497, 0.8559]],

        [[0.4477, 0.1934],
         [0.9440, 0.2906]],

        [[0.7504, 0.1905],
         [0.9368, 0.4883]]], dtype=torch.float64)
time is 0
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.00640644956002399
time is 1
tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 60
Adjusted Rand Index: 0.03246235398977342
time is 2
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
time is 3
tensor([0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 31
Adjusted Rand Index: 0.13627871668003683
time is 4
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.0005370569280343716
Global Adjusted Rand Index: 0.03365108348944839
Average Adjusted Rand Index: 0.21908221937934624
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22902.127968199766
Iteration 100: Loss = -12165.586889400973
Iteration 200: Loss = -12164.698901001542
Iteration 300: Loss = -12164.433686614839
Iteration 400: Loss = -12164.317446901337
Iteration 500: Loss = -12164.257956617068
Iteration 600: Loss = -12164.221962455462
Iteration 700: Loss = -12164.196860626782
Iteration 800: Loss = -12164.177103341337
Iteration 900: Loss = -12164.160392637987
Iteration 1000: Loss = -12164.145681873468
Iteration 1100: Loss = -12164.133276979335
Iteration 1200: Loss = -12164.123868686993
Iteration 1300: Loss = -12164.11732448165
Iteration 1400: Loss = -12164.112487790397
Iteration 1500: Loss = -12164.108264942815
Iteration 1600: Loss = -12164.104093508819
Iteration 1700: Loss = -12164.099887422311
Iteration 1800: Loss = -12164.095400962442
Iteration 1900: Loss = -12164.090604078965
Iteration 2000: Loss = -12164.085368261545
Iteration 2100: Loss = -12164.079562335493
Iteration 2200: Loss = -12164.0731728408
Iteration 2300: Loss = -12164.066094889138
Iteration 2400: Loss = -12164.058207444123
Iteration 2500: Loss = -12164.04944861516
Iteration 2600: Loss = -12164.039900050306
Iteration 2700: Loss = -12164.029519453607
Iteration 2800: Loss = -12164.018557605508
Iteration 2900: Loss = -12164.007092682356
Iteration 3000: Loss = -12163.995643550807
Iteration 3100: Loss = -12163.984410379067
Iteration 3200: Loss = -12163.973842615913
Iteration 3300: Loss = -12163.96412413762
Iteration 3400: Loss = -12163.955345179891
Iteration 3500: Loss = -12163.947624342534
Iteration 3600: Loss = -12163.940738028681
Iteration 3700: Loss = -12163.934607921086
Iteration 3800: Loss = -12163.943191540779
1
Iteration 3900: Loss = -12163.924532943232
Iteration 4000: Loss = -12163.920142863564
Iteration 4100: Loss = -12163.916250136555
Iteration 4200: Loss = -12163.912608817549
Iteration 4300: Loss = -12163.909262459096
Iteration 4400: Loss = -12163.906504832234
Iteration 4500: Loss = -12163.903466144944
Iteration 4600: Loss = -12163.900863190975
Iteration 4700: Loss = -12163.898595943041
Iteration 4800: Loss = -12163.896507132362
Iteration 4900: Loss = -12163.898079493983
1
Iteration 5000: Loss = -12163.892899093284
Iteration 5100: Loss = -12163.891496904696
Iteration 5200: Loss = -12163.890240125145
Iteration 5300: Loss = -12163.889126007867
Iteration 5400: Loss = -12163.88823332311
Iteration 5500: Loss = -12163.887491301286
Iteration 5600: Loss = -12163.886792618787
Iteration 5700: Loss = -12163.886213427895
Iteration 5800: Loss = -12163.885773814854
Iteration 5900: Loss = -12163.88535219111
Iteration 6000: Loss = -12163.88496689178
Iteration 6100: Loss = -12163.884784164771
Iteration 6200: Loss = -12163.884430951966
Iteration 6300: Loss = -12163.884191056732
Iteration 6400: Loss = -12163.883987019344
Iteration 6500: Loss = -12163.883858248715
Iteration 6600: Loss = -12163.8837034645
Iteration 6700: Loss = -12163.88357009882
Iteration 6800: Loss = -12163.885761121475
1
Iteration 6900: Loss = -12163.88331905462
Iteration 7000: Loss = -12163.883192333557
Iteration 7100: Loss = -12163.883136328852
Iteration 7200: Loss = -12163.889089341963
1
Iteration 7300: Loss = -12163.882971918712
Iteration 7400: Loss = -12163.890904806942
1
Iteration 7500: Loss = -12163.88283555321
Iteration 7600: Loss = -12163.882784331669
Iteration 7700: Loss = -12163.882918080033
1
Iteration 7800: Loss = -12163.88273333196
Iteration 7900: Loss = -12163.883102423855
1
Iteration 8000: Loss = -12163.882588217492
Iteration 8100: Loss = -12163.8848658062
1
Iteration 8200: Loss = -12163.882701889188
2
Iteration 8300: Loss = -12163.883210588441
3
Iteration 8400: Loss = -12163.883003063793
4
Iteration 8500: Loss = -12163.883473161279
5
Stopping early at iteration 8500 due to no improvement.
pi: tensor([[0.9601, 0.0399],
        [0.9710, 0.0290]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1099, 0.8901], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1923, 0.1953],
         [0.5174, 0.1975]],

        [[0.7053, 0.1900],
         [0.7075, 0.7153]],

        [[0.5492, 0.1920],
         [0.5704, 0.5528]],

        [[0.5903, 0.2149],
         [0.7138, 0.6994]],

        [[0.6360, 0.1927],
         [0.5992, 0.5684]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0013199815872971228
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21121.54502285399
Iteration 100: Loss = -12165.320531153355
Iteration 200: Loss = -12164.594011012252
Iteration 300: Loss = -12164.39505186329
Iteration 400: Loss = -12164.311051345887
Iteration 500: Loss = -12164.263399923335
Iteration 600: Loss = -12164.229686543065
Iteration 700: Loss = -12164.20304410424
Iteration 800: Loss = -12164.18069677061
Iteration 900: Loss = -12164.161276982119
Iteration 1000: Loss = -12164.144394082812
Iteration 1100: Loss = -12164.130109975422
Iteration 1200: Loss = -12164.118838209728
Iteration 1300: Loss = -12164.110269007731
Iteration 1400: Loss = -12164.103464992011
Iteration 1500: Loss = -12164.097619926793
Iteration 1600: Loss = -12164.092175891796
Iteration 1700: Loss = -12164.08686195556
Iteration 1800: Loss = -12164.081454961879
Iteration 1900: Loss = -12164.075869067201
Iteration 2000: Loss = -12164.07003277056
Iteration 2100: Loss = -12164.063858928612
Iteration 2200: Loss = -12164.05734143098
Iteration 2300: Loss = -12164.050453639707
Iteration 2400: Loss = -12164.043206362692
Iteration 2500: Loss = -12164.035557474146
Iteration 2600: Loss = -12164.027617066706
Iteration 2700: Loss = -12164.01948434963
Iteration 2800: Loss = -12164.011158500974
Iteration 2900: Loss = -12164.00269748057
Iteration 3000: Loss = -12163.994239225014
Iteration 3100: Loss = -12163.985795624292
Iteration 3200: Loss = -12163.977581916717
Iteration 3300: Loss = -12163.969537995605
Iteration 3400: Loss = -12163.961786583484
Iteration 3500: Loss = -12163.954517014563
Iteration 3600: Loss = -12163.947633001406
Iteration 3700: Loss = -12163.941290105246
Iteration 3800: Loss = -12163.935501552982
Iteration 3900: Loss = -12163.930162751263
Iteration 4000: Loss = -12163.925273457464
Iteration 4100: Loss = -12163.926714531022
1
Iteration 4200: Loss = -12163.916981853034
Iteration 4300: Loss = -12163.913394087793
Iteration 4400: Loss = -12163.910300239915
Iteration 4500: Loss = -12163.90734584182
Iteration 4600: Loss = -12163.905370451139
Iteration 4700: Loss = -12163.90243870209
Iteration 4800: Loss = -12163.900329478894
Iteration 4900: Loss = -12163.900511738551
1
Iteration 5000: Loss = -12163.896803026963
Iteration 5100: Loss = -12163.895318443367
Iteration 5200: Loss = -12163.894151707058
Iteration 5300: Loss = -12163.89280385771
Iteration 5400: Loss = -12163.89173905723
Iteration 5500: Loss = -12163.891028761744
Iteration 5600: Loss = -12163.890047152845
Iteration 5700: Loss = -12163.88932649139
Iteration 5800: Loss = -12163.888662206917
Iteration 5900: Loss = -12163.888054448733
Iteration 6000: Loss = -12163.887530728469
Iteration 6100: Loss = -12163.887054912051
Iteration 6200: Loss = -12163.886653837633
Iteration 6300: Loss = -12163.886292434858
Iteration 6400: Loss = -12163.885898638828
Iteration 6500: Loss = -12163.88560504079
Iteration 6600: Loss = -12163.88538837494
Iteration 6700: Loss = -12163.885055607987
Iteration 6800: Loss = -12163.886880254242
1
Iteration 6900: Loss = -12163.884608116994
Iteration 7000: Loss = -12163.8844630083
Iteration 7100: Loss = -12163.884280233435
Iteration 7200: Loss = -12163.88408894806
Iteration 7300: Loss = -12163.884256005578
1
Iteration 7400: Loss = -12163.884600498744
2
Iteration 7500: Loss = -12163.883677692818
Iteration 7600: Loss = -12163.883625503693
Iteration 7700: Loss = -12163.88339720518
Iteration 7800: Loss = -12163.883495184939
1
Iteration 7900: Loss = -12163.883716696564
2
Iteration 8000: Loss = -12163.891718317404
3
Iteration 8100: Loss = -12163.883076695554
Iteration 8200: Loss = -12163.88386036757
1
Iteration 8300: Loss = -12163.882897717263
Iteration 8400: Loss = -12163.885593671162
1
Iteration 8500: Loss = -12163.882767029561
Iteration 8600: Loss = -12163.882641877242
Iteration 8700: Loss = -12163.883011871718
1
Iteration 8800: Loss = -12163.882564258756
Iteration 8900: Loss = -12163.899828888721
1
Iteration 9000: Loss = -12163.882454854142
Iteration 9100: Loss = -12163.895777954665
1
Iteration 9200: Loss = -12163.882411169978
Iteration 9300: Loss = -12163.882937096223
1
Iteration 9400: Loss = -12163.882251381869
Iteration 9500: Loss = -12163.883214359777
1
Iteration 9600: Loss = -12163.884917237621
2
Iteration 9700: Loss = -12163.884321260442
3
Iteration 9800: Loss = -12163.890446890246
4
Iteration 9900: Loss = -12163.893001973867
5
Stopping early at iteration 9900 due to no improvement.
pi: tensor([[0.9389, 0.0611],
        [0.9741, 0.0259]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1163, 0.8837], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1924, 0.1953],
         [0.6991, 0.1975]],

        [[0.5131, 0.1900],
         [0.5916, 0.5194]],

        [[0.6334, 0.1922],
         [0.7173, 0.5521]],

        [[0.6889, 0.2079],
         [0.5346, 0.6573]],

        [[0.7113, 0.1929],
         [0.7038, 0.6338]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0013199815872971228
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20023.09371791786
Iteration 100: Loss = -12164.636569168453
Iteration 200: Loss = -12164.275980793422
Iteration 300: Loss = -12164.211490917169
Iteration 400: Loss = -12164.182862891856
Iteration 500: Loss = -12164.163191241962
Iteration 600: Loss = -12164.145992703734
Iteration 700: Loss = -12164.12977350497
Iteration 800: Loss = -12164.114372650645
Iteration 900: Loss = -12164.100700169483
Iteration 1000: Loss = -12164.089729882942
Iteration 1100: Loss = -12164.081554265065
Iteration 1200: Loss = -12164.074863005499
Iteration 1300: Loss = -12164.068657180855
Iteration 1400: Loss = -12164.062332239357
Iteration 1500: Loss = -12164.055690422356
Iteration 1600: Loss = -12164.048544206365
Iteration 1700: Loss = -12164.040897943853
Iteration 1800: Loss = -12164.032895059345
Iteration 1900: Loss = -12164.02465862677
Iteration 2000: Loss = -12164.016270160264
Iteration 2100: Loss = -12164.007887669011
Iteration 2200: Loss = -12163.999457091764
Iteration 2300: Loss = -12163.991021793447
Iteration 2400: Loss = -12163.98258968156
Iteration 2500: Loss = -12163.974235432406
Iteration 2600: Loss = -12163.966078343723
Iteration 2700: Loss = -12163.958099755666
Iteration 2800: Loss = -12163.950488116416
Iteration 2900: Loss = -12163.94329105664
Iteration 3000: Loss = -12163.936636779637
Iteration 3100: Loss = -12163.93056126466
Iteration 3200: Loss = -12163.925028031039
Iteration 3300: Loss = -12163.92005170566
Iteration 3400: Loss = -12163.91555028594
Iteration 3500: Loss = -12163.91166379957
Iteration 3600: Loss = -12163.9082022274
Iteration 3700: Loss = -12163.905169041427
Iteration 3800: Loss = -12163.904190252926
Iteration 3900: Loss = -12163.900253927013
Iteration 4000: Loss = -12163.898257108074
Iteration 4100: Loss = -12163.89667809162
Iteration 4200: Loss = -12163.89494167667
Iteration 4300: Loss = -12163.901675001147
1
Iteration 4400: Loss = -12163.892442778331
Iteration 4500: Loss = -12163.891376676274
Iteration 4600: Loss = -12163.890707863706
Iteration 4700: Loss = -12163.88966010767
Iteration 4800: Loss = -12163.888977897384
Iteration 4900: Loss = -12163.888981263284
1
Iteration 5000: Loss = -12163.887754633193
Iteration 5100: Loss = -12163.887268229846
Iteration 5200: Loss = -12163.88952848346
1
Iteration 5300: Loss = -12163.886383010542
Iteration 5400: Loss = -12163.886025706135
Iteration 5500: Loss = -12163.886256264734
1
Iteration 5600: Loss = -12163.886003034126
Iteration 5700: Loss = -12163.885092104676
Iteration 5800: Loss = -12163.884848378435
Iteration 5900: Loss = -12163.884643297337
Iteration 6000: Loss = -12163.891940119853
1
Iteration 6100: Loss = -12163.884850888835
2
Iteration 6200: Loss = -12163.884063570098
Iteration 6300: Loss = -12163.885859210262
1
Iteration 6400: Loss = -12163.883744102062
Iteration 6500: Loss = -12163.885225636634
1
Iteration 6600: Loss = -12163.883511884464
Iteration 6700: Loss = -12163.884545964498
1
Iteration 6800: Loss = -12163.883299093695
Iteration 6900: Loss = -12163.88517221153
1
Iteration 7000: Loss = -12163.883081965383
Iteration 7100: Loss = -12163.882984245329
Iteration 7200: Loss = -12163.884880522133
1
Iteration 7300: Loss = -12163.882827618721
Iteration 7400: Loss = -12163.883078639405
1
Iteration 7500: Loss = -12163.882761196659
Iteration 7600: Loss = -12163.883482202575
1
Iteration 7700: Loss = -12163.950038444515
2
Iteration 7800: Loss = -12163.882479002039
Iteration 7900: Loss = -12163.886156544917
1
Iteration 8000: Loss = -12163.882271597533
Iteration 8100: Loss = -12163.885484210103
1
Iteration 8200: Loss = -12163.882138385774
Iteration 8300: Loss = -12163.88210080978
Iteration 8400: Loss = -12163.92079575568
1
Iteration 8500: Loss = -12163.883852649751
2
Iteration 8600: Loss = -12163.88220101155
3
Iteration 8700: Loss = -12163.897172262235
4
Iteration 8800: Loss = -12163.881952027774
Iteration 8900: Loss = -12163.907061002068
1
Iteration 9000: Loss = -12163.881666327705
Iteration 9100: Loss = -12163.884056531064
1
Iteration 9200: Loss = -12163.884716007706
2
Iteration 9300: Loss = -12163.881113820667
Iteration 9400: Loss = -12163.912795243548
1
Iteration 9500: Loss = -12163.88113599548
2
Iteration 9600: Loss = -12163.88105945603
Iteration 9700: Loss = -12163.88253397313
1
Iteration 9800: Loss = -12163.881223173952
2
Iteration 9900: Loss = -12163.892880503507
3
Iteration 10000: Loss = -12163.90327562636
4
Iteration 10100: Loss = -12163.880940417057
Iteration 10200: Loss = -12163.882965811907
1
Iteration 10300: Loss = -12163.880792947471
Iteration 10400: Loss = -12163.882837106506
1
Iteration 10500: Loss = -12163.884153077124
2
Iteration 10600: Loss = -12163.881171607474
3
Iteration 10700: Loss = -12163.880654390585
Iteration 10800: Loss = -12163.884583342682
1
Iteration 10900: Loss = -12163.889785657766
2
Iteration 11000: Loss = -12163.881129078156
3
Iteration 11100: Loss = -12163.885174684894
4
Iteration 11200: Loss = -12163.880669639022
5
Stopping early at iteration 11200 due to no improvement.
pi: tensor([[0.8992, 0.1008],
        [0.9809, 0.0191]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0874, 0.9126], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1923, 0.1952],
         [0.5460, 0.1974]],

        [[0.5861, 0.1900],
         [0.5696, 0.5844]],

        [[0.5090, 0.1924],
         [0.6674, 0.6279]],

        [[0.6693, 0.2030],
         [0.6180, 0.5755]],

        [[0.5544, 0.1932],
         [0.5990, 0.7252]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0013199815872971228
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23730.83914685246
Iteration 100: Loss = -12164.804879845207
Iteration 200: Loss = -12164.34088994069
Iteration 300: Loss = -12164.26214093303
Iteration 400: Loss = -12164.220043248006
Iteration 500: Loss = -12164.192949577518
Iteration 600: Loss = -12164.173070840361
Iteration 700: Loss = -12164.157025471197
Iteration 800: Loss = -12164.143380159601
Iteration 900: Loss = -12164.131784085534
Iteration 1000: Loss = -12164.122208009992
Iteration 1100: Loss = -12164.114376418338
Iteration 1200: Loss = -12164.107418810652
Iteration 1300: Loss = -12164.10076359306
Iteration 1400: Loss = -12164.094187064993
Iteration 1500: Loss = -12164.087664755409
Iteration 1600: Loss = -12164.081101921873
Iteration 1700: Loss = -12164.074675829073
Iteration 1800: Loss = -12164.068182776276
Iteration 1900: Loss = -12164.06172584243
Iteration 2000: Loss = -12164.055071147817
Iteration 2100: Loss = -12164.048146050916
Iteration 2200: Loss = -12164.040976952809
Iteration 2300: Loss = -12164.03355103427
Iteration 2400: Loss = -12164.026070166059
Iteration 2500: Loss = -12164.018635169916
Iteration 2600: Loss = -12164.01149123913
Iteration 2700: Loss = -12164.004807456475
Iteration 2800: Loss = -12163.998583079463
Iteration 2900: Loss = -12163.993030434205
Iteration 3000: Loss = -12163.988136791615
Iteration 3100: Loss = -12163.983907498281
Iteration 3200: Loss = -12163.980293567305
Iteration 3300: Loss = -12163.977139662516
Iteration 3400: Loss = -12163.974480435116
Iteration 3500: Loss = -12163.97237925842
Iteration 3600: Loss = -12163.970393779311
Iteration 3700: Loss = -12163.99804233292
1
Iteration 3800: Loss = -12163.967289642087
Iteration 3900: Loss = -12163.96599269829
Iteration 4000: Loss = -12163.979361691196
1
Iteration 4100: Loss = -12163.96387279689
Iteration 4200: Loss = -12163.962987250186
Iteration 4300: Loss = -12163.962967167576
Iteration 4400: Loss = -12163.961489801088
Iteration 4500: Loss = -12163.960821097771
Iteration 4600: Loss = -12163.96026849978
Iteration 4700: Loss = -12163.95970485536
Iteration 4800: Loss = -12163.959139021306
Iteration 4900: Loss = -12163.958682535434
Iteration 5000: Loss = -12163.958253511148
Iteration 5100: Loss = -12163.957799672675
Iteration 5200: Loss = -12163.957447787114
Iteration 5300: Loss = -12163.95714225539
Iteration 5400: Loss = -12163.956668204548
Iteration 5500: Loss = -12163.956361480581
Iteration 5600: Loss = -12163.955992792451
Iteration 5700: Loss = -12163.956566776247
1
Iteration 5800: Loss = -12163.955386659132
Iteration 5900: Loss = -12163.955192233881
Iteration 6000: Loss = -12163.954787101116
Iteration 6100: Loss = -12163.954580174239
Iteration 6200: Loss = -12163.954170809337
Iteration 6300: Loss = -12163.95379075438
Iteration 6400: Loss = -12163.953300612422
Iteration 6500: Loss = -12163.952625263193
Iteration 6600: Loss = -12163.95176870764
Iteration 6700: Loss = -12163.951484872125
Iteration 6800: Loss = -12163.94582350391
Iteration 6900: Loss = -12163.933670840504
Iteration 7000: Loss = -12163.897931770634
Iteration 7100: Loss = -12163.890176898289
Iteration 7200: Loss = -12163.889457972558
Iteration 7300: Loss = -12163.89018363383
1
Iteration 7400: Loss = -12163.88926668131
Iteration 7500: Loss = -12163.889909526448
1
Iteration 7600: Loss = -12163.8891029127
Iteration 7700: Loss = -12163.889928986824
1
Iteration 7800: Loss = -12163.889009666385
Iteration 7900: Loss = -12163.932167726882
1
Iteration 8000: Loss = -12163.88903173599
2
Iteration 8100: Loss = -12163.88888668166
Iteration 8200: Loss = -12163.88885178698
Iteration 8300: Loss = -12163.889023821383
1
Iteration 8400: Loss = -12163.892873873043
2
Iteration 8500: Loss = -12163.888748156838
Iteration 8600: Loss = -12163.88873059258
Iteration 8700: Loss = -12163.88870635866
Iteration 8800: Loss = -12164.015833941228
1
Iteration 8900: Loss = -12163.888614315852
Iteration 9000: Loss = -12163.88864511998
1
Iteration 9100: Loss = -12163.888643403527
2
Iteration 9200: Loss = -12163.888579959385
Iteration 9300: Loss = -12163.888707067714
1
Iteration 9400: Loss = -12163.88902897324
2
Iteration 9500: Loss = -12163.980362332673
3
Iteration 9600: Loss = -12163.888496406824
Iteration 9700: Loss = -12163.933651331763
1
Iteration 9800: Loss = -12163.88896308919
2
Iteration 9900: Loss = -12163.888516054145
3
Iteration 10000: Loss = -12163.888444488242
Iteration 10100: Loss = -12163.888755325172
1
Iteration 10200: Loss = -12163.89042013751
2
Iteration 10300: Loss = -12163.891809371851
3
Iteration 10400: Loss = -12163.895098547102
4
Iteration 10500: Loss = -12163.888412534003
Iteration 10600: Loss = -12163.913691831245
1
Iteration 10700: Loss = -12163.888405953105
Iteration 10800: Loss = -12163.888867787715
1
Iteration 10900: Loss = -12163.888436647847
2
Iteration 11000: Loss = -12163.888397213266
Iteration 11100: Loss = -12163.891772231145
1
Iteration 11200: Loss = -12163.894003115069
2
Iteration 11300: Loss = -12163.888748631385
3
Iteration 11400: Loss = -12163.888425584668
4
Iteration 11500: Loss = -12163.888739446136
5
Stopping early at iteration 11500 due to no improvement.
pi: tensor([[9.9996e-01, 3.7186e-05],
        [7.2336e-01, 2.7664e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0526, 0.9474], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1925, 0.1970],
         [0.7179, 0.1966]],

        [[0.6134, 0.1920],
         [0.6574, 0.5736]],

        [[0.5132, 0.1927],
         [0.5987, 0.6152]],

        [[0.5352, 0.2343],
         [0.6355, 0.6214]],

        [[0.6178, 0.1746],
         [0.6968, 0.5607]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0013199815872971228
Average Adjusted Rand Index: 0.0
11684.613177382842
[-0.0013199815872971228, -0.0013199815872971228, -0.0013199815872971228, -0.0013199815872971228] [0.0, 0.0, 0.0, 0.0] [12163.883473161279, 12163.893001973867, 12163.880669639022, 12163.888739446136]
-----------------------------------------------------------------------------------------
This iteration is 12
True Objective function: Loss = -11768.709672355853
pi: tensor([[1.0000e+00, 2.1493e-47],
        [1.0000e+00, 2.8462e-76]], dtype=torch.float64)
alpha: tensor([1., 0.])
beta: tensor([[[0.1936,    nan],
         [0.7172,    nan]],

        [[0.0950,    nan],
         [0.9577, 0.3112]],

        [[0.1045,    nan],
         [0.8498, 0.7718]],

        [[0.3671, 0.2383],
         [0.9858, 0.3801]],

        [[0.6729,    nan],
         [0.4661, 0.6617]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -12231.257445045954
Iteration 10: Loss = -12231.23315732097
Iteration 20: Loss = -12231.232764337483
Iteration 30: Loss = -12231.232279178043
Iteration 40: Loss = -12231.231189023909
Iteration 50: Loss = -12231.229798292801
Iteration 60: Loss = -12231.227315090882
Iteration 70: Loss = -12231.223210262664
Iteration 80: Loss = -12231.216787757961
Iteration 90: Loss = -12231.206855278302
Iteration 100: Loss = -12231.192358833896
Iteration 110: Loss = -12231.172644621201
Iteration 120: Loss = -12231.148156758067
Iteration 130: Loss = -12231.12069931273
Iteration 140: Loss = -12231.092838859742
Iteration 150: Loss = -12231.06716912961
Iteration 160: Loss = -12231.044881994312
Iteration 170: Loss = -12231.026098052525
Iteration 180: Loss = -12231.01019453201
Iteration 190: Loss = -12230.996517301015
Iteration 200: Loss = -12230.985022241422
Iteration 210: Loss = -12230.975980307136
Iteration 220: Loss = -12230.969287623882
Iteration 230: Loss = -12230.96465725765
Iteration 240: Loss = -12230.961554863623
Iteration 250: Loss = -12230.959606478582
Iteration 260: Loss = -12230.958348051561
Iteration 270: Loss = -12230.957709309338
Iteration 280: Loss = -12230.957454499008
Iteration 290: Loss = -12230.95751228667
1
Iteration 300: Loss = -12230.957786820703
2
Iteration 310: Loss = -12230.958286577841
3
Stopping early at iteration 310 due to no improvement.
pi: tensor([[0.0981, 0.9019],
        [0.0431, 0.9569]], dtype=torch.float64)
alpha: tensor([0.0454, 0.9546])
beta: tensor([[[0.2307, 0.1872],
         [0.9907, 0.1919]],

        [[0.8642, 0.2309],
         [0.5024, 0.6787]],

        [[0.3841, 0.2134],
         [0.7111, 0.8806]],

        [[0.5469, 0.2133],
         [0.3961, 0.0772]],

        [[0.6716, 0.2074],
         [0.3388, 0.4024]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22702.115409575435
Iteration 100: Loss = -12231.70969861446
Iteration 200: Loss = -12231.155706279025
Iteration 300: Loss = -12231.05474517087
Iteration 400: Loss = -12231.002533177314
Iteration 500: Loss = -12230.967671268856
Iteration 600: Loss = -12230.939705115861
Iteration 700: Loss = -12230.913221118046
Iteration 800: Loss = -12230.883100777046
Iteration 900: Loss = -12230.844585012826
Iteration 1000: Loss = -12230.7973404969
Iteration 1100: Loss = -12230.749954208966
Iteration 1200: Loss = -12230.690059632432
Iteration 1300: Loss = -12230.49518468435
Iteration 1400: Loss = -12230.096849147236
Iteration 1500: Loss = -12229.798890760716
Iteration 1600: Loss = -12229.598710346254
Iteration 1700: Loss = -12229.478289153063
Iteration 1800: Loss = -12229.400552111507
Iteration 1900: Loss = -12229.3443605032
Iteration 2000: Loss = -12229.304711177749
Iteration 2100: Loss = -12229.276855441572
Iteration 2200: Loss = -12229.256262199882
Iteration 2300: Loss = -12229.240697990168
Iteration 2400: Loss = -12229.228339504196
Iteration 2500: Loss = -12229.218119109053
Iteration 2600: Loss = -12229.209682158455
Iteration 2700: Loss = -12229.202661065874
Iteration 2800: Loss = -12229.196692279655
Iteration 2900: Loss = -12229.19146731093
Iteration 3000: Loss = -12229.18694950979
Iteration 3100: Loss = -12229.182945214323
Iteration 3200: Loss = -12229.17934846064
Iteration 3300: Loss = -12229.176155893809
Iteration 3400: Loss = -12229.173226864106
Iteration 3500: Loss = -12229.170542797012
Iteration 3600: Loss = -12229.167996691447
Iteration 3700: Loss = -12229.16564707
Iteration 3800: Loss = -12229.163487116262
Iteration 3900: Loss = -12229.161533932438
Iteration 4000: Loss = -12229.159732456248
Iteration 4100: Loss = -12229.158187824498
Iteration 4200: Loss = -12229.156749201326
Iteration 4300: Loss = -12229.155486316125
Iteration 4400: Loss = -12229.154327843542
Iteration 4500: Loss = -12229.153200436082
Iteration 4600: Loss = -12229.152281874187
Iteration 4700: Loss = -12229.151371801981
Iteration 4800: Loss = -12229.150510506988
Iteration 4900: Loss = -12229.149767627394
Iteration 5000: Loss = -12229.148999445997
Iteration 5100: Loss = -12229.148327039456
Iteration 5200: Loss = -12229.147668343372
Iteration 5300: Loss = -12229.147069141978
Iteration 5400: Loss = -12229.146484374392
Iteration 5500: Loss = -12229.14595042802
Iteration 5600: Loss = -12229.145481249561
Iteration 5700: Loss = -12229.14506747871
Iteration 5800: Loss = -12229.144621140082
Iteration 5900: Loss = -12229.1442465469
Iteration 6000: Loss = -12229.143890751226
Iteration 6100: Loss = -12229.1435497668
Iteration 6200: Loss = -12229.14323349521
Iteration 6300: Loss = -12229.142952825367
Iteration 6400: Loss = -12229.142708758818
Iteration 6500: Loss = -12229.142473660042
Iteration 6600: Loss = -12229.142168264209
Iteration 6700: Loss = -12229.14205553774
Iteration 6800: Loss = -12229.141787986213
Iteration 6900: Loss = -12229.141620996752
Iteration 7000: Loss = -12229.141422802586
Iteration 7100: Loss = -12229.141345459027
Iteration 7200: Loss = -12229.141069726254
Iteration 7300: Loss = -12229.140965738918
Iteration 7400: Loss = -12229.140733243918
Iteration 7500: Loss = -12229.14063041693
Iteration 7600: Loss = -12229.14293646351
1
Iteration 7700: Loss = -12229.15934370585
2
Iteration 7800: Loss = -12229.196588156177
3
Iteration 7900: Loss = -12229.140772235001
4
Iteration 8000: Loss = -12229.140001078513
Iteration 8100: Loss = -12229.140063888133
1
Iteration 8200: Loss = -12229.139824474307
Iteration 8300: Loss = -12229.139886773823
1
Iteration 8400: Loss = -12229.139620355641
Iteration 8500: Loss = -12229.188345636245
1
Iteration 8600: Loss = -12229.13949512988
Iteration 8700: Loss = -12229.139430920619
Iteration 8800: Loss = -12229.140414238425
1
Iteration 8900: Loss = -12229.139275872914
Iteration 9000: Loss = -12229.139216456018
Iteration 9100: Loss = -12229.139805607374
1
Iteration 9200: Loss = -12229.13914723598
Iteration 9300: Loss = -12229.13908770956
Iteration 9400: Loss = -12229.139270386306
1
Iteration 9500: Loss = -12229.13900845229
Iteration 9600: Loss = -12229.143673642238
1
Iteration 9700: Loss = -12229.138893474243
Iteration 9800: Loss = -12229.138883982108
Iteration 9900: Loss = -12229.204029678383
1
Iteration 10000: Loss = -12229.138869760744
Iteration 10100: Loss = -12229.138767107128
Iteration 10200: Loss = -12229.576638113755
1
Iteration 10300: Loss = -12229.13872690896
Iteration 10400: Loss = -12229.13872689041
Iteration 10500: Loss = -12229.145470275844
1
Iteration 10600: Loss = -12229.138639254415
Iteration 10700: Loss = -12229.138629184052
Iteration 10800: Loss = -12229.139164321758
1
Iteration 10900: Loss = -12229.138634177621
2
Iteration 11000: Loss = -12229.138563854778
Iteration 11100: Loss = -12229.13858724465
1
Iteration 11200: Loss = -12229.138556683225
Iteration 11300: Loss = -12229.142593340579
1
Iteration 11400: Loss = -12229.138515676143
Iteration 11500: Loss = -12229.144182028771
1
Iteration 11600: Loss = -12229.156548180663
2
Iteration 11700: Loss = -12229.138924468638
3
Iteration 11800: Loss = -12229.138537591727
4
Iteration 11900: Loss = -12229.141396691379
5
Stopping early at iteration 11900 due to no improvement.
pi: tensor([[9.9998e-01, 2.4869e-05],
        [3.6489e-05, 9.9996e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9464, 0.0536], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1919, 0.1996],
         [0.7283, 0.2337]],

        [[0.6577, 0.2533],
         [0.7039, 0.5672]],

        [[0.6264, 0.2380],
         [0.5794, 0.6465]],

        [[0.7169, 0.2270],
         [0.5380, 0.5603]],

        [[0.6848, 0.2123],
         [0.7081, 0.6995]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.005119313988496187
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: -0.009009494263563287
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.005119313988496187
Global Adjusted Rand Index: -0.002340545779860964
Average Adjusted Rand Index: -0.0038401053009900895
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21439.034292749355
Iteration 100: Loss = -12231.617322866285
Iteration 200: Loss = -12231.12414455848
Iteration 300: Loss = -12230.989964837847
Iteration 400: Loss = -12230.933030828091
Iteration 500: Loss = -12230.903588485013
Iteration 600: Loss = -12230.884536964086
Iteration 700: Loss = -12230.869189370846
Iteration 800: Loss = -12230.854894036836
Iteration 900: Loss = -12230.84027670949
Iteration 1000: Loss = -12230.825290104603
Iteration 1100: Loss = -12230.809881851614
Iteration 1200: Loss = -12230.794178298233
Iteration 1300: Loss = -12230.77808462832
Iteration 1400: Loss = -12230.761284106073
Iteration 1500: Loss = -12230.743412792159
Iteration 1600: Loss = -12230.723755777566
Iteration 1700: Loss = -12230.701231902744
Iteration 1800: Loss = -12230.673212971313
Iteration 1900: Loss = -12230.63190026137
Iteration 2000: Loss = -12230.542073414485
Iteration 2100: Loss = -12230.287595532816
Iteration 2200: Loss = -12229.966778585636
Iteration 2300: Loss = -12229.752311590486
Iteration 2400: Loss = -12229.617985034773
Iteration 2500: Loss = -12229.53452050371
Iteration 2600: Loss = -12229.478121639175
Iteration 2700: Loss = -12229.413851319072
Iteration 2800: Loss = -12229.370443479273
Iteration 2900: Loss = -12229.338178762964
Iteration 3000: Loss = -12229.318321749113
Iteration 3100: Loss = -12229.305446401731
Iteration 3200: Loss = -12229.295594363928
Iteration 3300: Loss = -12229.287196496278
Iteration 3400: Loss = -12229.278761364425
Iteration 3500: Loss = -12229.269304724578
Iteration 3600: Loss = -12229.259277901463
Iteration 3700: Loss = -12229.245331132142
Iteration 3800: Loss = -12229.233189391867
Iteration 3900: Loss = -12229.22306106938
Iteration 4000: Loss = -12229.213733681801
Iteration 4100: Loss = -12229.207300480595
Iteration 4200: Loss = -12229.20290805195
Iteration 4300: Loss = -12229.19959820239
Iteration 4400: Loss = -12229.196823259788
Iteration 4500: Loss = -12229.194503407198
Iteration 4600: Loss = -12229.192297305564
Iteration 4700: Loss = -12229.190171670693
Iteration 4800: Loss = -12229.18803287509
Iteration 4900: Loss = -12229.185539531367
Iteration 5000: Loss = -12229.182571066163
Iteration 5100: Loss = -12229.178656935555
Iteration 5200: Loss = -12229.175863175065
Iteration 5300: Loss = -12229.173732074534
Iteration 5400: Loss = -12229.171274692791
Iteration 5500: Loss = -12229.16830563005
Iteration 5600: Loss = -12229.166514581022
Iteration 5700: Loss = -12229.168080566726
1
Iteration 5800: Loss = -12229.163880326318
Iteration 5900: Loss = -12229.162483332613
Iteration 6000: Loss = -12229.160939665533
Iteration 6100: Loss = -12229.159714595113
Iteration 6200: Loss = -12229.158776208322
Iteration 6300: Loss = -12229.157642611182
Iteration 6400: Loss = -12229.156005105096
Iteration 6500: Loss = -12229.152683985165
Iteration 6600: Loss = -12229.151813188744
Iteration 6700: Loss = -12229.151177469359
Iteration 6800: Loss = -12229.151165452424
Iteration 6900: Loss = -12229.150221322381
Iteration 7000: Loss = -12229.149863825402
Iteration 7100: Loss = -12229.150455655976
1
Iteration 7200: Loss = -12229.15013941832
2
Iteration 7300: Loss = -12229.160428689516
3
Iteration 7400: Loss = -12229.190438867214
4
Iteration 7500: Loss = -12229.147495802305
Iteration 7600: Loss = -12229.146003302756
Iteration 7700: Loss = -12229.17529973205
1
Iteration 7800: Loss = -12229.14494685085
Iteration 7900: Loss = -12229.166647925635
1
Iteration 8000: Loss = -12229.144214210748
Iteration 8100: Loss = -12229.143673692026
Iteration 8200: Loss = -12229.14861346563
1
Iteration 8300: Loss = -12229.142663484252
Iteration 8400: Loss = -12229.142413370744
Iteration 8500: Loss = -12229.15712208553
1
Iteration 8600: Loss = -12229.14204144135
Iteration 8700: Loss = -12229.141797748933
Iteration 8800: Loss = -12229.141624688013
Iteration 8900: Loss = -12229.42459548872
1
Iteration 9000: Loss = -12229.141358244842
Iteration 9100: Loss = -12229.141242790141
Iteration 9200: Loss = -12229.14449014201
1
Iteration 9300: Loss = -12229.1410120408
Iteration 9400: Loss = -12229.140952740347
Iteration 9500: Loss = -12229.53813679373
1
Iteration 9600: Loss = -12229.140813451544
Iteration 9700: Loss = -12229.140700352378
Iteration 9800: Loss = -12229.155944114262
1
Iteration 9900: Loss = -12229.140637261255
Iteration 10000: Loss = -12229.140513083612
Iteration 10100: Loss = -12229.140484900849
Iteration 10200: Loss = -12229.140641722997
1
Iteration 10300: Loss = -12229.140345011927
Iteration 10400: Loss = -12229.140291217529
Iteration 10500: Loss = -12229.140623884961
1
Iteration 10600: Loss = -12229.140140938316
Iteration 10700: Loss = -12229.139999225297
Iteration 10800: Loss = -12229.142325900568
1
Iteration 10900: Loss = -12229.139497238666
Iteration 11000: Loss = -12229.139330407219
Iteration 11100: Loss = -12229.14026036025
1
Iteration 11200: Loss = -12229.139011195348
Iteration 11300: Loss = -12229.139043287361
1
Iteration 11400: Loss = -12229.13910862166
2
Iteration 11500: Loss = -12229.138929395496
Iteration 11600: Loss = -12229.159238350321
1
Iteration 11700: Loss = -12229.138935871277
2
Iteration 11800: Loss = -12229.138709914607
Iteration 11900: Loss = -12229.151609610553
1
Iteration 12000: Loss = -12229.138624506108
Iteration 12100: Loss = -12229.143378679
1
Iteration 12200: Loss = -12229.138519870286
Iteration 12300: Loss = -12229.138846391228
1
Iteration 12400: Loss = -12229.1385799211
2
Iteration 12500: Loss = -12229.146755027605
3
Iteration 12600: Loss = -12229.138518969876
Iteration 12700: Loss = -12229.139117143057
1
Iteration 12800: Loss = -12229.27792600466
2
Iteration 12900: Loss = -12229.13845897433
Iteration 13000: Loss = -12229.284458926964
1
Iteration 13100: Loss = -12229.138451172823
Iteration 13200: Loss = -12229.138465183
1
Iteration 13300: Loss = -12229.13857458827
2
Iteration 13400: Loss = -12229.141465168039
3
Iteration 13500: Loss = -12229.210528686845
4
Iteration 13600: Loss = -12229.14395172773
5
Stopping early at iteration 13600 due to no improvement.
pi: tensor([[9.9998e-01, 2.2533e-05],
        [1.9004e-05, 9.9998e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0535, 0.9465], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2337, 0.1981],
         [0.5660, 0.1920]],

        [[0.6565, 0.2544],
         [0.6494, 0.6055]],

        [[0.6379, 0.2377],
         [0.5996, 0.5270]],

        [[0.6168, 0.2267],
         [0.7294, 0.6543]],

        [[0.5666, 0.2138],
         [0.6626, 0.5069]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: -0.005119313988496187
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: -0.009009494263563287
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: -0.005119313988496187
Global Adjusted Rand Index: -0.002340545779860964
Average Adjusted Rand Index: -0.0038401053009900895
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22775.911592577766
Iteration 100: Loss = -12231.577819122744
Iteration 200: Loss = -12231.11985544962
Iteration 300: Loss = -12231.03376554335
Iteration 400: Loss = -12230.988159263985
Iteration 500: Loss = -12230.957711933554
Iteration 600: Loss = -12230.933101985906
Iteration 700: Loss = -12230.909428714443
Iteration 800: Loss = -12230.883267452327
Iteration 900: Loss = -12230.852413538309
Iteration 1000: Loss = -12230.817364143175
Iteration 1100: Loss = -12230.779639926615
Iteration 1200: Loss = -12230.730557274184
Iteration 1300: Loss = -12230.622574144327
Iteration 1400: Loss = -12230.331672877406
Iteration 1500: Loss = -12230.013160037146
Iteration 1600: Loss = -12229.776344908949
Iteration 1700: Loss = -12229.61135148371
Iteration 1800: Loss = -12229.505230787852
Iteration 1900: Loss = -12229.43685307091
Iteration 2000: Loss = -12229.388927628526
Iteration 2100: Loss = -12229.35027415121
Iteration 2200: Loss = -12229.318656756626
Iteration 2300: Loss = -12229.294114440341
Iteration 2400: Loss = -12229.273407698858
Iteration 2500: Loss = -12229.255824896494
Iteration 2600: Loss = -12229.242253742206
Iteration 2700: Loss = -12229.23130690412
Iteration 2800: Loss = -12229.222792830547
Iteration 2900: Loss = -12229.215908817225
Iteration 3000: Loss = -12229.210064666084
Iteration 3100: Loss = -12229.20466248275
Iteration 3200: Loss = -12229.19937216498
Iteration 3300: Loss = -12229.194361946604
Iteration 3400: Loss = -12229.190011543793
Iteration 3500: Loss = -12229.186285646108
Iteration 3600: Loss = -12229.182892154533
Iteration 3700: Loss = -12229.179664039728
Iteration 3800: Loss = -12229.176608586842
Iteration 3900: Loss = -12229.173733539295
Iteration 4000: Loss = -12229.171060440493
Iteration 4100: Loss = -12229.168688010543
Iteration 4200: Loss = -12229.166659326536
Iteration 4300: Loss = -12229.164807580755
Iteration 4400: Loss = -12229.16321983954
Iteration 4500: Loss = -12229.161701864843
Iteration 4600: Loss = -12229.160316388676
Iteration 4700: Loss = -12229.158958031056
Iteration 4800: Loss = -12229.157601449351
Iteration 4900: Loss = -12229.156224037875
Iteration 5000: Loss = -12229.154946667122
Iteration 5100: Loss = -12229.153772126514
Iteration 5200: Loss = -12229.152757026544
Iteration 5300: Loss = -12229.151872881706
Iteration 5400: Loss = -12229.150984408005
Iteration 5500: Loss = -12229.15011809476
Iteration 5600: Loss = -12229.14930029711
Iteration 5700: Loss = -12229.148472602234
Iteration 5800: Loss = -12229.147750541224
Iteration 5900: Loss = -12229.146956031049
Iteration 6000: Loss = -12229.146171162094
Iteration 6100: Loss = -12229.145370091192
Iteration 6200: Loss = -12229.144731382594
Iteration 6300: Loss = -12229.144229703954
Iteration 6400: Loss = -12229.14384997551
Iteration 6500: Loss = -12229.143536482035
Iteration 6600: Loss = -12229.143272976255
Iteration 6700: Loss = -12229.143261944011
Iteration 6800: Loss = -12229.142738030243
Iteration 6900: Loss = -12229.142494090729
Iteration 7000: Loss = -12229.142268166448
Iteration 7100: Loss = -12229.14218258343
Iteration 7200: Loss = -12229.141857967667
Iteration 7300: Loss = -12229.14167587596
Iteration 7400: Loss = -12229.141511263842
Iteration 7500: Loss = -12229.14132874736
Iteration 7600: Loss = -12229.152620814137
1
Iteration 7700: Loss = -12229.165104712783
2
Iteration 7800: Loss = -12229.142132566098
3
Iteration 7900: Loss = -12229.144007996041
4
Iteration 8000: Loss = -12229.155945675202
5
Stopping early at iteration 8000 due to no improvement.
pi: tensor([[9.9977e-01, 2.2646e-04],
        [3.3636e-04, 9.9966e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9471, 0.0529], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1925, 0.1996],
         [0.5008, 0.2341]],

        [[0.6964, 0.2539],
         [0.6925, 0.6669]],

        [[0.6348, 0.2381],
         [0.5846, 0.7158]],

        [[0.6329, 0.2269],
         [0.5079, 0.5506]],

        [[0.6487, 0.2124],
         [0.6890, 0.6034]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.005119313988496187
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: -0.009009494263563287
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.005119313988496187
Global Adjusted Rand Index: -0.002340545779860964
Average Adjusted Rand Index: -0.0038401053009900895
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21676.44327818771
Iteration 100: Loss = -12231.60544666435
Iteration 200: Loss = -12231.210866982765
Iteration 300: Loss = -12231.106992250878
Iteration 400: Loss = -12231.05140765933
Iteration 500: Loss = -12231.01297563524
Iteration 600: Loss = -12230.981605955885
Iteration 700: Loss = -12230.95361978696
Iteration 800: Loss = -12230.926639958785
Iteration 900: Loss = -12230.898893446947
Iteration 1000: Loss = -12230.868634304124
Iteration 1100: Loss = -12230.835250408518
Iteration 1200: Loss = -12230.799275912672
Iteration 1300: Loss = -12230.762545497735
Iteration 1400: Loss = -12230.726922752357
Iteration 1500: Loss = -12230.693569005454
Iteration 1600: Loss = -12230.6620593909
Iteration 1700: Loss = -12230.630426759748
Iteration 1800: Loss = -12230.594564832232
Iteration 1900: Loss = -12230.547784490132
Iteration 2000: Loss = -12230.478782138798
Iteration 2100: Loss = -12230.374074249008
Iteration 2200: Loss = -12230.233776306723
Iteration 2300: Loss = -12230.078647566015
Iteration 2400: Loss = -12229.931083436313
Iteration 2500: Loss = -12229.801181594788
Iteration 2600: Loss = -12229.692832826973
Iteration 2700: Loss = -12229.606959535351
Iteration 2800: Loss = -12229.540787090722
Iteration 2900: Loss = -12229.490292494911
Iteration 3000: Loss = -12229.450947152287
Iteration 3100: Loss = -12229.418980059043
Iteration 3200: Loss = -12229.391971324072
Iteration 3300: Loss = -12229.36794475265
Iteration 3400: Loss = -12229.34510787391
Iteration 3500: Loss = -12229.323231926095
Iteration 3600: Loss = -12229.305576578363
Iteration 3700: Loss = -12229.291251015184
Iteration 3800: Loss = -12229.277892845164
Iteration 3900: Loss = -12229.264187155115
Iteration 4000: Loss = -12229.251908034848
Iteration 4100: Loss = -12229.242365220245
Iteration 4200: Loss = -12229.234548475506
Iteration 4300: Loss = -12229.227986185202
Iteration 4400: Loss = -12229.22212773053
Iteration 4500: Loss = -12229.216857145237
Iteration 4600: Loss = -12229.212081124653
Iteration 4700: Loss = -12229.207645207125
Iteration 4800: Loss = -12229.203309539726
Iteration 4900: Loss = -12229.199149130269
Iteration 5000: Loss = -12229.195520454015
Iteration 5100: Loss = -12229.19235937533
Iteration 5200: Loss = -12229.189417904985
Iteration 5300: Loss = -12229.186562100665
Iteration 5400: Loss = -12229.183862464195
Iteration 5500: Loss = -12229.181150348128
Iteration 5600: Loss = -12229.178498649582
Iteration 5700: Loss = -12229.175943703445
Iteration 5800: Loss = -12229.173709912211
Iteration 5900: Loss = -12229.171680963718
Iteration 6000: Loss = -12229.169803756924
Iteration 6100: Loss = -12229.168024416582
Iteration 6200: Loss = -12229.166372196432
Iteration 6300: Loss = -12229.164819091617
Iteration 6400: Loss = -12229.163317569786
Iteration 6500: Loss = -12229.161910327919
Iteration 6600: Loss = -12229.160522364082
Iteration 6700: Loss = -12229.159165127921
Iteration 6800: Loss = -12229.157922628496
Iteration 6900: Loss = -12229.17754731768
1
Iteration 7000: Loss = -12229.155375223516
Iteration 7100: Loss = -12229.154371075989
Iteration 7200: Loss = -12229.156170785745
1
Iteration 7300: Loss = -12229.152668867
Iteration 7400: Loss = -12229.151860316337
Iteration 7500: Loss = -12229.151151714022
Iteration 7600: Loss = -12229.150207552086
Iteration 7700: Loss = -12229.14941691322
Iteration 7800: Loss = -12229.149027787256
Iteration 7900: Loss = -12229.147827290393
Iteration 8000: Loss = -12229.14698607046
Iteration 8100: Loss = -12229.15102429257
1
Iteration 8200: Loss = -12229.145472304972
Iteration 8300: Loss = -12229.14498821004
Iteration 8400: Loss = -12229.152993135012
1
Iteration 8500: Loss = -12229.14414208068
Iteration 8600: Loss = -12229.143793142519
Iteration 8700: Loss = -12229.151430373191
1
Iteration 8800: Loss = -12229.143232701506
Iteration 8900: Loss = -12229.142953300425
Iteration 9000: Loss = -12229.145868404301
1
Iteration 9100: Loss = -12229.142468919812
Iteration 9200: Loss = -12229.142250698405
Iteration 9300: Loss = -12229.142253045564
1
Iteration 9400: Loss = -12229.14187879243
Iteration 9500: Loss = -12229.141680581797
Iteration 9600: Loss = -12229.141766571853
1
Iteration 9700: Loss = -12229.14141977456
Iteration 9800: Loss = -12229.141244312485
Iteration 9900: Loss = -12229.141349949292
1
Iteration 10000: Loss = -12229.140994146785
Iteration 10100: Loss = -12229.140846297756
Iteration 10200: Loss = -12229.140757668816
Iteration 10300: Loss = -12229.140626386381
Iteration 10400: Loss = -12229.140509032757
Iteration 10500: Loss = -12229.140921363492
1
Iteration 10600: Loss = -12229.140328892749
Iteration 10700: Loss = -12229.140221984546
Iteration 10800: Loss = -12229.140716944032
1
Iteration 10900: Loss = -12229.139982813675
Iteration 11000: Loss = -12229.139826016904
Iteration 11100: Loss = -12229.16906661226
1
Iteration 11200: Loss = -12229.13966331101
Iteration 11300: Loss = -12229.139535260103
Iteration 11400: Loss = -12229.139601736493
1
Iteration 11500: Loss = -12229.169837393005
2
Iteration 11600: Loss = -12229.140150833297
3
Iteration 11700: Loss = -12229.141626792703
4
Iteration 11800: Loss = -12229.146281938281
5
Stopping early at iteration 11800 due to no improvement.
pi: tensor([[9.9990e-01, 9.5481e-05],
        [1.4770e-04, 9.9985e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9471, 0.0529], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1920, 0.1993],
         [0.7081, 0.2340]],

        [[0.7247, 0.2551],
         [0.6604, 0.5186]],

        [[0.5549, 0.2373],
         [0.7251, 0.6154]],

        [[0.6503, 0.2261],
         [0.5947, 0.6332]],

        [[0.6816, 0.2132],
         [0.5519, 0.6853]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.005119313988496187
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: -0.009009494263563287
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.005119313988496187
Global Adjusted Rand Index: -0.002340545779860964
Average Adjusted Rand Index: -0.0038401053009900895
11768.709672355853
[-0.002340545779860964, -0.002340545779860964, -0.002340545779860964, -0.002340545779860964] [-0.0038401053009900895, -0.0038401053009900895, -0.0038401053009900895, -0.0038401053009900895] [12229.141396691379, 12229.14395172773, 12229.155945675202, 12229.146281938281]
-----------------------------------------------------------------------------------------
This iteration is 13
True Objective function: Loss = -12110.896277888682
Iteration 0: Loss = -12640.230548790032
Iteration 10: Loss = -12619.892327249747
Iteration 20: Loss = -12619.821771347932
Iteration 30: Loss = -12619.803056451985
Iteration 40: Loss = -12619.793717520157
Iteration 50: Loss = -12619.785316479823
Iteration 60: Loss = -12619.776731616967
Iteration 70: Loss = -12619.767575548081
Iteration 80: Loss = -12619.758073292083
Iteration 90: Loss = -12619.748994166017
Iteration 100: Loss = -12619.741047653773
Iteration 110: Loss = -12619.734394899122
Iteration 120: Loss = -12619.728661507515
Iteration 130: Loss = -12619.723523042057
Iteration 140: Loss = -12619.718832854804
Iteration 150: Loss = -12619.714470592426
Iteration 160: Loss = -12619.71030463416
Iteration 170: Loss = -12619.706446502885
Iteration 180: Loss = -12619.70281852428
Iteration 190: Loss = -12619.699315751512
Iteration 200: Loss = -12619.696091904028
Iteration 210: Loss = -12619.692980580145
Iteration 220: Loss = -12619.690094063593
Iteration 230: Loss = -12619.687294963114
Iteration 240: Loss = -12619.684663834812
Iteration 250: Loss = -12619.68216555697
Iteration 260: Loss = -12619.679765803601
Iteration 270: Loss = -12619.677546060744
Iteration 280: Loss = -12619.675430322384
Iteration 290: Loss = -12619.673322422654
Iteration 300: Loss = -12619.671369027114
Iteration 310: Loss = -12619.669504482337
Iteration 320: Loss = -12619.667732865455
Iteration 330: Loss = -12619.666022530362
Iteration 340: Loss = -12619.664366578138
Iteration 350: Loss = -12619.662810141264
Iteration 360: Loss = -12619.661279284865
Iteration 370: Loss = -12619.65984493449
Iteration 380: Loss = -12619.658440962567
Iteration 390: Loss = -12619.657101602723
Iteration 400: Loss = -12619.65579790612
Iteration 410: Loss = -12619.65459811227
Iteration 420: Loss = -12619.653359950986
Iteration 430: Loss = -12619.652195580935
Iteration 440: Loss = -12619.651119329694
Iteration 450: Loss = -12619.650007045386
Iteration 460: Loss = -12619.648955548804
Iteration 470: Loss = -12619.647927557278
Iteration 480: Loss = -12619.646917653552
Iteration 490: Loss = -12619.64600633291
Iteration 500: Loss = -12619.645016629374
Iteration 510: Loss = -12619.64411726931
Iteration 520: Loss = -12619.64328937446
Iteration 530: Loss = -12619.642440633603
Iteration 540: Loss = -12619.641583984429
Iteration 550: Loss = -12619.640830188802
Iteration 560: Loss = -12619.640026363091
Iteration 570: Loss = -12619.63928394976
Iteration 580: Loss = -12619.638507488851
Iteration 590: Loss = -12619.637783977221
Iteration 600: Loss = -12619.63705634955
Iteration 610: Loss = -12619.636370820133
Iteration 620: Loss = -12619.635720626531
Iteration 630: Loss = -12619.635060840003
Iteration 640: Loss = -12619.634393017313
Iteration 650: Loss = -12619.633762006017
Iteration 660: Loss = -12619.633151065487
Iteration 670: Loss = -12619.632521249283
Iteration 680: Loss = -12619.631918035295
Iteration 690: Loss = -12619.631392707719
Iteration 700: Loss = -12619.630792245493
Iteration 710: Loss = -12619.630266806862
Iteration 720: Loss = -12619.629687709976
Iteration 730: Loss = -12619.629159301086
Iteration 740: Loss = -12619.628592261855
Iteration 750: Loss = -12619.628070593353
Iteration 760: Loss = -12619.627562186057
Iteration 770: Loss = -12619.627071199802
Iteration 780: Loss = -12619.626586634206
Iteration 790: Loss = -12619.626106638185
Iteration 800: Loss = -12619.625574242964
Iteration 810: Loss = -12619.625173820285
Iteration 820: Loss = -12619.624715472157
Iteration 830: Loss = -12619.624224198293
Iteration 840: Loss = -12619.623759956785
Iteration 850: Loss = -12619.623349463582
Iteration 860: Loss = -12619.622897209112
Iteration 870: Loss = -12619.622509495644
Iteration 880: Loss = -12619.622047795581
Iteration 890: Loss = -12619.621671349285
Iteration 900: Loss = -12619.621197071536
Iteration 910: Loss = -12619.620779673298
Iteration 920: Loss = -12619.620395430164
Iteration 930: Loss = -12619.62004224554
Iteration 940: Loss = -12619.619655922037
Iteration 950: Loss = -12619.619246268281
Iteration 960: Loss = -12619.61884827327
Iteration 970: Loss = -12619.61849717527
Iteration 980: Loss = -12619.61812790454
Iteration 990: Loss = -12619.617775492154
Iteration 1000: Loss = -12619.617369122327
Iteration 1010: Loss = -12619.617011887183
Iteration 1020: Loss = -12619.616688199552
Iteration 1030: Loss = -12619.616317015094
Iteration 1040: Loss = -12619.615955972253
Iteration 1050: Loss = -12619.615672914893
Iteration 1060: Loss = -12619.615348522851
Iteration 1070: Loss = -12619.615003319452
Iteration 1080: Loss = -12619.614639712885
Iteration 1090: Loss = -12619.614327686002
Iteration 1100: Loss = -12619.613972362838
Iteration 1110: Loss = -12619.613684492859
Iteration 1120: Loss = -12619.613445867226
Iteration 1130: Loss = -12619.613074722398
Iteration 1140: Loss = -12619.612754270518
Iteration 1150: Loss = -12619.612479542086
Iteration 1160: Loss = -12619.612178641937
Iteration 1170: Loss = -12619.611873322678
Iteration 1180: Loss = -12619.611585748822
Iteration 1190: Loss = -12619.6112742035
Iteration 1200: Loss = -12619.61099385754
Iteration 1210: Loss = -12619.610718418904
Iteration 1220: Loss = -12619.610441220724
Iteration 1230: Loss = -12619.610149641017
Iteration 1240: Loss = -12619.609907758912
Iteration 1250: Loss = -12619.609630989999
Iteration 1260: Loss = -12619.609316206515
Iteration 1270: Loss = -12619.609074362008
Iteration 1280: Loss = -12619.608813515162
Iteration 1290: Loss = -12619.608603932526
Iteration 1300: Loss = -12619.608269648748
Iteration 1310: Loss = -12619.608048374323
Iteration 1320: Loss = -12619.60780893716
Iteration 1330: Loss = -12619.607562642428
Iteration 1340: Loss = -12619.607234380646
Iteration 1350: Loss = -12619.607056087016
Iteration 1360: Loss = -12619.606782043362
Iteration 1370: Loss = -12619.606557820141
Iteration 1380: Loss = -12619.606322773156
Iteration 1390: Loss = -12619.60608642456
Iteration 1400: Loss = -12619.60586902396
Iteration 1410: Loss = -12619.6056134829
Iteration 1420: Loss = -12619.605417263854
Iteration 1430: Loss = -12619.6051406986
Iteration 1440: Loss = -12619.604965673994
Iteration 1450: Loss = -12619.604747718164
Iteration 1460: Loss = -12619.604511570771
Iteration 1470: Loss = -12619.604264057483
Iteration 1480: Loss = -12619.604075935
Iteration 1490: Loss = -12619.603887218724
Iteration 1500: Loss = -12619.603682975434
Iteration 1510: Loss = -12619.603471514765
Iteration 1520: Loss = -12619.603230863613
Iteration 1530: Loss = -12619.603041100336
Iteration 1540: Loss = -12619.60286260233
Iteration 1550: Loss = -12619.602614503989
Iteration 1560: Loss = -12619.602423527269
Iteration 1570: Loss = -12619.602257603157
Iteration 1580: Loss = -12619.60206897355
Iteration 1590: Loss = -12619.601828040268
Iteration 1600: Loss = -12619.601655217095
Iteration 1610: Loss = -12619.601498880009
Iteration 1620: Loss = -12619.601287496771
Iteration 1630: Loss = -12619.601077439556
Iteration 1640: Loss = -12619.600951812714
Iteration 1650: Loss = -12619.600731915465
Iteration 1660: Loss = -12619.600601005228
Iteration 1670: Loss = -12619.600429599604
Iteration 1680: Loss = -12619.600210077871
Iteration 1690: Loss = -12619.600060663997
Iteration 1700: Loss = -12619.599871975472
Iteration 1710: Loss = -12619.599748863631
Iteration 1720: Loss = -12619.599563509846
Iteration 1730: Loss = -12619.599421263203
Iteration 1740: Loss = -12619.599226596854
Iteration 1750: Loss = -12619.599062373454
Iteration 1760: Loss = -12619.598834256605
Iteration 1770: Loss = -12619.598691342779
Iteration 1780: Loss = -12619.598570390783
Iteration 1790: Loss = -12619.598414354165
Iteration 1800: Loss = -12619.598267416595
Iteration 1810: Loss = -12619.598103183638
Iteration 1820: Loss = -12619.597974280196
Iteration 1830: Loss = -12619.597843071546
Iteration 1840: Loss = -12619.597643678657
Iteration 1850: Loss = -12619.597503027224
Iteration 1860: Loss = -12619.597350322068
Iteration 1870: Loss = -12619.59717156621
Iteration 1880: Loss = -12619.597071832135
Iteration 1890: Loss = -12619.596957928134
Iteration 1900: Loss = -12619.596773102025
Iteration 1910: Loss = -12619.596648759185
Iteration 1920: Loss = -12619.59649892585
Iteration 1930: Loss = -12619.596381444218
Iteration 1940: Loss = -12619.596267879497
Iteration 1950: Loss = -12619.596141442275
Iteration 1960: Loss = -12619.595941930145
Iteration 1970: Loss = -12619.595871058495
Iteration 1980: Loss = -12619.59570444632
Iteration 1990: Loss = -12619.595615093698
Iteration 2000: Loss = -12619.595470623963
Iteration 2010: Loss = -12619.595348785802
Iteration 2020: Loss = -12619.59522491888
Iteration 2030: Loss = -12619.595081270525
Iteration 2040: Loss = -12619.594957297268
Iteration 2050: Loss = -12619.594855257208
Iteration 2060: Loss = -12619.59469665467
Iteration 2070: Loss = -12619.594631267826
Iteration 2080: Loss = -12619.59451079882
Iteration 2090: Loss = -12619.594362223299
Iteration 2100: Loss = -12619.594235252733
Iteration 2110: Loss = -12619.594135483154
Iteration 2120: Loss = -12619.593971835098
Iteration 2130: Loss = -12619.59390596851
Iteration 2140: Loss = -12619.593821278742
Iteration 2150: Loss = -12619.593675968945
Iteration 2160: Loss = -12619.593626587572
Iteration 2170: Loss = -12619.59345884522
Iteration 2180: Loss = -12619.59339695648
Iteration 2190: Loss = -12619.593275688547
Iteration 2200: Loss = -12619.593148755532
Iteration 2210: Loss = -12619.593023675678
Iteration 2220: Loss = -12619.592913583416
Iteration 2230: Loss = -12619.592795666857
Iteration 2240: Loss = -12619.592739007498
Iteration 2250: Loss = -12619.59270474265
Iteration 2260: Loss = -12619.592527438852
Iteration 2270: Loss = -12619.592440115202
Iteration 2280: Loss = -12619.592357661908
Iteration 2290: Loss = -12619.592238243551
Iteration 2300: Loss = -12619.59218410158
Iteration 2310: Loss = -12619.592059799386
Iteration 2320: Loss = -12619.591974214542
Iteration 2330: Loss = -12619.591911343488
Iteration 2340: Loss = -12619.591769358593
Iteration 2350: Loss = -12619.591732445677
Iteration 2360: Loss = -12619.59158212565
Iteration 2370: Loss = -12619.591517554923
Iteration 2380: Loss = -12619.591418181142
Iteration 2390: Loss = -12619.591351649851
Iteration 2400: Loss = -12619.591269515706
Iteration 2410: Loss = -12619.591157032151
Iteration 2420: Loss = -12619.591100769238
Iteration 2430: Loss = -12619.590971860523
Iteration 2440: Loss = -12619.590964527051
Iteration 2450: Loss = -12619.590866830005
Iteration 2460: Loss = -12619.590733108125
Iteration 2470: Loss = -12619.590664079664
Iteration 2480: Loss = -12619.590587401415
Iteration 2490: Loss = -12619.590535639762
Iteration 2500: Loss = -12619.590453411722
Iteration 2510: Loss = -12619.5903143024
Iteration 2520: Loss = -12619.590253598486
Iteration 2530: Loss = -12619.590179837682
Iteration 2540: Loss = -12619.59007459684
Iteration 2550: Loss = -12619.590033517548
Iteration 2560: Loss = -12619.58996832377
Iteration 2570: Loss = -12619.589881175018
Iteration 2580: Loss = -12619.589794552854
Iteration 2590: Loss = -12619.589714384249
Iteration 2600: Loss = -12619.589649299956
Iteration 2610: Loss = -12619.589606232701
Iteration 2620: Loss = -12619.589501604838
Iteration 2630: Loss = -12619.589453349856
Iteration 2640: Loss = -12619.589437724895
Iteration 2650: Loss = -12619.589325577652
Iteration 2660: Loss = -12619.589239942807
Iteration 2670: Loss = -12619.589185585752
Iteration 2680: Loss = -12619.589090958585
Iteration 2690: Loss = -12619.589017405564
Iteration 2700: Loss = -12619.58897194452
Iteration 2710: Loss = -12619.588859858473
Iteration 2720: Loss = -12619.588819679899
Iteration 2730: Loss = -12619.588734639703
Iteration 2740: Loss = -12619.588720506108
Iteration 2750: Loss = -12619.588642153365
Iteration 2760: Loss = -12619.588566740826
Iteration 2770: Loss = -12619.588480092885
Iteration 2780: Loss = -12619.588423039162
Iteration 2790: Loss = -12619.58839680298
Iteration 2800: Loss = -12619.58827330936
Iteration 2810: Loss = -12619.588282616582
1
Iteration 2820: Loss = -12619.588214295107
Iteration 2830: Loss = -12619.588113491762
Iteration 2840: Loss = -12619.588119934335
1
Iteration 2850: Loss = -12619.588009475263
Iteration 2860: Loss = -12619.587957537016
Iteration 2870: Loss = -12619.58789230507
Iteration 2880: Loss = -12619.587846368866
Iteration 2890: Loss = -12619.587786785314
Iteration 2900: Loss = -12619.587781139971
Iteration 2910: Loss = -12619.587696130855
Iteration 2920: Loss = -12619.587611083554
Iteration 2930: Loss = -12619.587553470621
Iteration 2940: Loss = -12619.587550383987
Iteration 2950: Loss = -12619.587488327943
Iteration 2960: Loss = -12619.587388297383
Iteration 2970: Loss = -12619.587398602045
1
Iteration 2980: Loss = -12619.587255654731
Iteration 2990: Loss = -12619.587324390322
1
Iteration 3000: Loss = -12619.587228427998
Iteration 3010: Loss = -12619.587110233495
Iteration 3020: Loss = -12619.58703838732
Iteration 3030: Loss = -12619.587072137756
1
Iteration 3040: Loss = -12619.587000644484
Iteration 3050: Loss = -12619.586914529846
Iteration 3060: Loss = -12619.586896632743
Iteration 3070: Loss = -12619.586845989154
Iteration 3080: Loss = -12619.586779147932
Iteration 3090: Loss = -12619.586711654734
Iteration 3100: Loss = -12619.58668314347
Iteration 3110: Loss = -12619.586663739296
Iteration 3120: Loss = -12619.586561905173
Iteration 3130: Loss = -12619.586540052547
Iteration 3140: Loss = -12619.586529268683
Iteration 3150: Loss = -12619.586437448788
Iteration 3160: Loss = -12619.586419485093
Iteration 3170: Loss = -12619.586373780285
Iteration 3180: Loss = -12619.586302890937
Iteration 3190: Loss = -12619.586261055316
Iteration 3200: Loss = -12619.586232950725
Iteration 3210: Loss = -12619.586225494293
Iteration 3220: Loss = -12619.58611649623
Iteration 3230: Loss = -12619.58611740018
1
Iteration 3240: Loss = -12619.586046172
Iteration 3250: Loss = -12619.586058666788
1
Iteration 3260: Loss = -12619.585952502159
Iteration 3270: Loss = -12619.585934751956
Iteration 3280: Loss = -12619.585906050896
Iteration 3290: Loss = -12619.585856614927
Iteration 3300: Loss = -12619.585806186282
Iteration 3310: Loss = -12619.585773284085
Iteration 3320: Loss = -12619.585716259275
Iteration 3330: Loss = -12619.585670306158
Iteration 3340: Loss = -12619.585636359016
Iteration 3350: Loss = -12619.58559301685
Iteration 3360: Loss = -12619.585551242824
Iteration 3370: Loss = -12619.585495819909
Iteration 3380: Loss = -12619.585491671685
Iteration 3390: Loss = -12619.585465083235
Iteration 3400: Loss = -12619.585361549596
Iteration 3410: Loss = -12619.585404549955
1
Iteration 3420: Loss = -12619.585333927917
Iteration 3430: Loss = -12619.58525921477
Iteration 3440: Loss = -12619.585264560279
1
Iteration 3450: Loss = -12619.585183189824
Iteration 3460: Loss = -12619.585212180222
1
Iteration 3470: Loss = -12619.585139565
Iteration 3480: Loss = -12619.585098674153
Iteration 3490: Loss = -12619.585083339085
Iteration 3500: Loss = -12619.585017814015
Iteration 3510: Loss = -12619.585031974118
1
Iteration 3520: Loss = -12619.584967286504
Iteration 3530: Loss = -12619.584932206057
Iteration 3540: Loss = -12619.584915757432
Iteration 3550: Loss = -12619.584838722412
Iteration 3560: Loss = -12619.584811346887
Iteration 3570: Loss = -12619.58482773974
1
Iteration 3580: Loss = -12619.584768024803
Iteration 3590: Loss = -12619.58472076307
Iteration 3600: Loss = -12619.584654532551
Iteration 3610: Loss = -12619.584654698685
1
Iteration 3620: Loss = -12619.58461010316
Iteration 3630: Loss = -12619.584622232382
1
Iteration 3640: Loss = -12619.584557152672
Iteration 3650: Loss = -12619.584506214564
Iteration 3660: Loss = -12619.58448530116
Iteration 3670: Loss = -12619.584441314326
Iteration 3680: Loss = -12619.584419762808
Iteration 3690: Loss = -12619.584400318567
Iteration 3700: Loss = -12619.584372367473
Iteration 3710: Loss = -12619.584340400716
Iteration 3720: Loss = -12619.584294159731
Iteration 3730: Loss = -12619.584260471993
Iteration 3740: Loss = -12619.584214771672
Iteration 3750: Loss = -12619.584189478746
Iteration 3760: Loss = -12619.584217329926
1
Iteration 3770: Loss = -12619.58415797881
Iteration 3780: Loss = -12619.584137464488
Iteration 3790: Loss = -12619.584085659435
Iteration 3800: Loss = -12619.584021802886
Iteration 3810: Loss = -12619.584045036565
1
Iteration 3820: Loss = -12619.584022669078
2
Iteration 3830: Loss = -12619.583939269549
Iteration 3840: Loss = -12619.58394653987
1
Iteration 3850: Loss = -12619.583912227987
Iteration 3860: Loss = -12619.583899127116
Iteration 3870: Loss = -12619.583877985804
Iteration 3880: Loss = -12619.583840152321
Iteration 3890: Loss = -12619.583806372568
Iteration 3900: Loss = -12619.583752484621
Iteration 3910: Loss = -12619.583781926514
1
Iteration 3920: Loss = -12619.583715073286
Iteration 3930: Loss = -12619.583677324723
Iteration 3940: Loss = -12619.583651103814
Iteration 3950: Loss = -12619.583667543662
1
Iteration 3960: Loss = -12619.583614591182
Iteration 3970: Loss = -12619.583612963052
Iteration 3980: Loss = -12619.583571736071
Iteration 3990: Loss = -12619.58358114739
1
Iteration 4000: Loss = -12619.583518347368
Iteration 4010: Loss = -12619.583456029852
Iteration 4020: Loss = -12619.583466945875
1
Iteration 4030: Loss = -12619.583494439461
2
Iteration 4040: Loss = -12619.583404787432
Iteration 4050: Loss = -12619.583386698396
Iteration 4060: Loss = -12619.583412423039
1
Iteration 4070: Loss = -12619.583341089081
Iteration 4080: Loss = -12619.58334485839
1
Iteration 4090: Loss = -12619.583287467998
Iteration 4100: Loss = -12619.583297585197
1
Iteration 4110: Loss = -12619.583271192138
Iteration 4120: Loss = -12619.583266652076
Iteration 4130: Loss = -12619.583231038887
Iteration 4140: Loss = -12619.583199841536
Iteration 4150: Loss = -12619.583127679787
Iteration 4160: Loss = -12619.583100148737
Iteration 4170: Loss = -12619.583088008689
Iteration 4180: Loss = -12619.583047092367
Iteration 4190: Loss = -12619.583111522936
1
Iteration 4200: Loss = -12619.583073908087
2
Iteration 4210: Loss = -12619.583023236324
Iteration 4220: Loss = -12619.583042586859
1
Iteration 4230: Loss = -12619.58299570439
Iteration 4240: Loss = -12619.582932672096
Iteration 4250: Loss = -12619.58295893382
1
Iteration 4260: Loss = -12619.58290551896
Iteration 4270: Loss = -12619.582950773633
1
Iteration 4280: Loss = -12619.582878143003
Iteration 4290: Loss = -12619.582908087219
1
Iteration 4300: Loss = -12619.582870036762
Iteration 4310: Loss = -12619.58282748254
Iteration 4320: Loss = -12619.582816172946
Iteration 4330: Loss = -12619.582752078077
Iteration 4340: Loss = -12619.582790536262
1
Iteration 4350: Loss = -12619.582742606053
Iteration 4360: Loss = -12619.58272544038
Iteration 4370: Loss = -12619.582699883234
Iteration 4380: Loss = -12619.582650982326
Iteration 4390: Loss = -12619.582680438505
1
Iteration 4400: Loss = -12619.582646734621
Iteration 4410: Loss = -12619.582625162884
Iteration 4420: Loss = -12619.58260284898
Iteration 4430: Loss = -12619.582567972542
Iteration 4440: Loss = -12619.58255450741
Iteration 4450: Loss = -12619.582479815279
Iteration 4460: Loss = -12619.582502926422
1
Iteration 4470: Loss = -12619.582471545009
Iteration 4480: Loss = -12619.582510321796
1
Iteration 4490: Loss = -12619.582466404494
Iteration 4500: Loss = -12619.582416259882
Iteration 4510: Loss = -12619.582470565563
1
Iteration 4520: Loss = -12619.582424249944
2
Iteration 4530: Loss = -12619.582395970623
Iteration 4540: Loss = -12619.582377034894
Iteration 4550: Loss = -12619.58235101822
Iteration 4560: Loss = -12619.582339898468
Iteration 4570: Loss = -12619.582322949707
Iteration 4580: Loss = -12619.582362913863
1
Iteration 4590: Loss = -12619.582282076713
Iteration 4600: Loss = -12619.582274908696
Iteration 4610: Loss = -12619.582260645542
Iteration 4620: Loss = -12619.582213816355
Iteration 4630: Loss = -12619.582217858866
1
Iteration 4640: Loss = -12619.582186697591
Iteration 4650: Loss = -12619.582143473537
Iteration 4660: Loss = -12619.58215337807
1
Iteration 4670: Loss = -12619.582153986667
2
Iteration 4680: Loss = -12619.5821713339
3
Stopping early at iteration 4680 due to no improvement.
pi: tensor([[0.3175, 0.6825],
        [0.4201, 0.5799]], dtype=torch.float64)
alpha: tensor([0.3810, 0.6190])
beta: tensor([[[0.2046, 0.2056],
         [0.5799, 0.2050]],

        [[0.2285, 0.2079],
         [0.0245, 0.5067]],

        [[0.9935, 0.2075],
         [0.7386, 0.0290]],

        [[0.0942, 0.1980],
         [0.8614, 0.7216]],

        [[0.5200, 0.2052],
         [0.0776, 0.2734]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -12620.024327310328
Iteration 10: Loss = -12620.02373310934
Iteration 20: Loss = -12620.023647966913
Iteration 30: Loss = -12620.023587416716
Iteration 40: Loss = -12620.023587471514
1
Iteration 50: Loss = -12620.023547199768
Iteration 60: Loss = -12620.023510008976
Iteration 70: Loss = -12620.023238399925
Iteration 80: Loss = -12620.022769449277
Iteration 90: Loss = -12620.021673232417
Iteration 100: Loss = -12620.019146039624
Iteration 110: Loss = -12620.014374172848
Iteration 120: Loss = -12620.005902283052
Iteration 130: Loss = -12619.991112835585
Iteration 140: Loss = -12619.967889371992
Iteration 150: Loss = -12619.937081113658
Iteration 160: Loss = -12619.903481389234
Iteration 170: Loss = -12619.873049708067
Iteration 180: Loss = -12619.848776230745
Iteration 190: Loss = -12619.830695827275
Iteration 200: Loss = -12619.817281772772
Iteration 210: Loss = -12619.806993448643
Iteration 220: Loss = -12619.7987893916
Iteration 230: Loss = -12619.791808368169
Iteration 240: Loss = -12619.785503622303
Iteration 250: Loss = -12619.779543698602
Iteration 260: Loss = -12619.77380347907
Iteration 270: Loss = -12619.767965126348
Iteration 280: Loss = -12619.76183859855
Iteration 290: Loss = -12619.755564360785
Iteration 300: Loss = -12619.748951891701
Iteration 310: Loss = -12619.74219949287
Iteration 320: Loss = -12619.735395171245
Iteration 330: Loss = -12619.728767880968
Iteration 340: Loss = -12619.722496080238
Iteration 350: Loss = -12619.716694630964
Iteration 360: Loss = -12619.71138568802
Iteration 370: Loss = -12619.706499696567
Iteration 380: Loss = -12619.702002340737
Iteration 390: Loss = -12619.697809636187
Iteration 400: Loss = -12619.693939581446
Iteration 410: Loss = -12619.690287597443
Iteration 420: Loss = -12619.686883689992
Iteration 430: Loss = -12619.68366706168
Iteration 440: Loss = -12619.680652250321
Iteration 450: Loss = -12619.67780981406
Iteration 460: Loss = -12619.67506583453
Iteration 470: Loss = -12619.672522467612
Iteration 480: Loss = -12619.67007909073
Iteration 490: Loss = -12619.667813937058
Iteration 500: Loss = -12619.665580660543
Iteration 510: Loss = -12619.663430288596
Iteration 520: Loss = -12619.661446673215
Iteration 530: Loss = -12619.659511407948
Iteration 540: Loss = -12619.657664434051
Iteration 550: Loss = -12619.655911172893
Iteration 560: Loss = -12619.654290691018
Iteration 570: Loss = -12619.652640173055
Iteration 580: Loss = -12619.651048794116
Iteration 590: Loss = -12619.64958847359
Iteration 600: Loss = -12619.648178957137
Iteration 610: Loss = -12619.646810183096
Iteration 620: Loss = -12619.6454289685
Iteration 630: Loss = -12619.644150488128
Iteration 640: Loss = -12619.64289272604
Iteration 650: Loss = -12619.641725509035
Iteration 660: Loss = -12619.64054279002
Iteration 670: Loss = -12619.639474490918
Iteration 680: Loss = -12619.638427432985
Iteration 690: Loss = -12619.637343874547
Iteration 700: Loss = -12619.636299203446
Iteration 710: Loss = -12619.635309955665
Iteration 720: Loss = -12619.634374380868
Iteration 730: Loss = -12619.63346740665
Iteration 740: Loss = -12619.632594431481
Iteration 750: Loss = -12619.631733909557
Iteration 760: Loss = -12619.630853133141
Iteration 770: Loss = -12619.630095521794
Iteration 780: Loss = -12619.62923117858
Iteration 790: Loss = -12619.628443495872
Iteration 800: Loss = -12619.627722932519
Iteration 810: Loss = -12619.626980520658
Iteration 820: Loss = -12619.626282532281
Iteration 830: Loss = -12619.625607898472
Iteration 840: Loss = -12619.624937547458
Iteration 850: Loss = -12619.624261870793
Iteration 860: Loss = -12619.62356694052
Iteration 870: Loss = -12619.622967936462
Iteration 880: Loss = -12619.622338929365
Iteration 890: Loss = -12619.621768830299
Iteration 900: Loss = -12619.621159967452
Iteration 910: Loss = -12619.620599836822
Iteration 920: Loss = -12619.62001874425
Iteration 930: Loss = -12619.619501171463
Iteration 940: Loss = -12619.61892686039
Iteration 950: Loss = -12619.618450747432
Iteration 960: Loss = -12619.617926474142
Iteration 970: Loss = -12619.617454899717
Iteration 980: Loss = -12619.616952711815
Iteration 990: Loss = -12619.616488469599
Iteration 1000: Loss = -12619.616009565183
Iteration 1010: Loss = -12619.61553237486
Iteration 1020: Loss = -12619.615121639827
Iteration 1030: Loss = -12619.614647323711
Iteration 1040: Loss = -12619.614244919443
Iteration 1050: Loss = -12619.613787666249
Iteration 1060: Loss = -12619.613345016016
Iteration 1070: Loss = -12619.61297931586
Iteration 1080: Loss = -12619.612557816055
Iteration 1090: Loss = -12619.61221612724
Iteration 1100: Loss = -12619.61178845396
Iteration 1110: Loss = -12619.611399350188
Iteration 1120: Loss = -12619.611051227064
Iteration 1130: Loss = -12619.610738797179
Iteration 1140: Loss = -12619.610356495958
Iteration 1150: Loss = -12619.610001977593
Iteration 1160: Loss = -12619.609635601975
Iteration 1170: Loss = -12619.60933623696
Iteration 1180: Loss = -12619.608984511748
Iteration 1190: Loss = -12619.608631506293
Iteration 1200: Loss = -12619.608306464717
Iteration 1210: Loss = -12619.608037663316
Iteration 1220: Loss = -12619.607711550623
Iteration 1230: Loss = -12619.60744185994
Iteration 1240: Loss = -12619.607128562999
Iteration 1250: Loss = -12619.606828301676
Iteration 1260: Loss = -12619.606549688233
Iteration 1270: Loss = -12619.606254701077
Iteration 1280: Loss = -12619.605947166252
Iteration 1290: Loss = -12619.60569027567
Iteration 1300: Loss = -12619.605401919882
Iteration 1310: Loss = -12619.605153097702
Iteration 1320: Loss = -12619.604911330764
Iteration 1330: Loss = -12619.60459320259
Iteration 1340: Loss = -12619.604421596143
Iteration 1350: Loss = -12619.6041072047
Iteration 1360: Loss = -12619.60387376679
Iteration 1370: Loss = -12619.60362482614
Iteration 1380: Loss = -12619.603379217686
Iteration 1390: Loss = -12619.60313397777
Iteration 1400: Loss = -12619.602973398765
Iteration 1410: Loss = -12619.602759913192
Iteration 1420: Loss = -12619.602479742149
Iteration 1430: Loss = -12619.602263438697
Iteration 1440: Loss = -12619.602047586366
Iteration 1450: Loss = -12619.601847547927
Iteration 1460: Loss = -12619.601638234959
Iteration 1470: Loss = -12619.601403705703
Iteration 1480: Loss = -12619.60119004815
Iteration 1490: Loss = -12619.60096038832
Iteration 1500: Loss = -12619.600741133965
Iteration 1510: Loss = -12619.600580623257
Iteration 1520: Loss = -12619.600405815327
Iteration 1530: Loss = -12619.600197467435
Iteration 1540: Loss = -12619.600026127517
Iteration 1550: Loss = -12619.599814507596
Iteration 1560: Loss = -12619.599658037832
Iteration 1570: Loss = -12619.599464561277
Iteration 1580: Loss = -12619.599264059503
Iteration 1590: Loss = -12619.599086124494
Iteration 1600: Loss = -12619.598893011773
Iteration 1610: Loss = -12619.598766754501
Iteration 1620: Loss = -12619.598586355365
Iteration 1630: Loss = -12619.59841024433
Iteration 1640: Loss = -12619.598219737365
Iteration 1650: Loss = -12619.598118111542
Iteration 1660: Loss = -12619.597856232798
Iteration 1670: Loss = -12619.597728241488
Iteration 1680: Loss = -12619.597634683747
Iteration 1690: Loss = -12619.597391685533
Iteration 1700: Loss = -12619.597284614858
Iteration 1710: Loss = -12619.597132839732
Iteration 1720: Loss = -12619.59696784183
Iteration 1730: Loss = -12619.5968352954
Iteration 1740: Loss = -12619.59665183999
Iteration 1750: Loss = -12619.596507713959
Iteration 1760: Loss = -12619.596360349096
Iteration 1770: Loss = -12619.596173443271
Iteration 1780: Loss = -12619.596112319094
Iteration 1790: Loss = -12619.595941508383
Iteration 1800: Loss = -12619.595811064637
Iteration 1810: Loss = -12619.595696160046
Iteration 1820: Loss = -12619.595552395922
Iteration 1830: Loss = -12619.595410206392
Iteration 1840: Loss = -12619.595265800373
Iteration 1850: Loss = -12619.595128922489
Iteration 1860: Loss = -12619.595032333558
Iteration 1870: Loss = -12619.594911398588
Iteration 1880: Loss = -12619.59471870805
Iteration 1890: Loss = -12619.594642805656
Iteration 1900: Loss = -12619.594517271284
Iteration 1910: Loss = -12619.594420922862
Iteration 1920: Loss = -12619.594289346647
Iteration 1930: Loss = -12619.59418523293
Iteration 1940: Loss = -12619.594057788046
Iteration 1950: Loss = -12619.59395534645
Iteration 1960: Loss = -12619.593810039752
Iteration 1970: Loss = -12619.593690957503
Iteration 1980: Loss = -12619.59355292439
Iteration 1990: Loss = -12619.593447217812
Iteration 2000: Loss = -12619.593333201066
Iteration 2010: Loss = -12619.5932694444
Iteration 2020: Loss = -12619.59313016137
Iteration 2030: Loss = -12619.5930281513
Iteration 2040: Loss = -12619.592876803641
Iteration 2050: Loss = -12619.592803868183
Iteration 2060: Loss = -12619.592702463218
Iteration 2070: Loss = -12619.592625415384
Iteration 2080: Loss = -12619.592492846155
Iteration 2090: Loss = -12619.592374196916
Iteration 2100: Loss = -12619.592273234737
Iteration 2110: Loss = -12619.592215334136
Iteration 2120: Loss = -12619.592137980277
Iteration 2130: Loss = -12619.592017205588
Iteration 2140: Loss = -12619.591914886358
Iteration 2150: Loss = -12619.591824986013
Iteration 2160: Loss = -12619.591696184902
Iteration 2170: Loss = -12619.591624498253
Iteration 2180: Loss = -12619.59155172144
Iteration 2190: Loss = -12619.59143516142
Iteration 2200: Loss = -12619.59140363486
Iteration 2210: Loss = -12619.591279431557
Iteration 2220: Loss = -12619.591159364127
Iteration 2230: Loss = -12619.59111021122
Iteration 2240: Loss = -12619.591035358102
Iteration 2250: Loss = -12619.59091542728
Iteration 2260: Loss = -12619.590838176186
Iteration 2270: Loss = -12619.590746338226
Iteration 2280: Loss = -12619.590664498883
Iteration 2290: Loss = -12619.590601526947
Iteration 2300: Loss = -12619.590501233375
Iteration 2310: Loss = -12619.59043282592
Iteration 2320: Loss = -12619.590367830584
Iteration 2330: Loss = -12619.590275864362
Iteration 2340: Loss = -12619.590173094257
Iteration 2350: Loss = -12619.59012428061
Iteration 2360: Loss = -12619.589979451259
Iteration 2370: Loss = -12619.589928956251
Iteration 2380: Loss = -12619.589855118782
Iteration 2390: Loss = -12619.589801363982
Iteration 2400: Loss = -12619.58970641766
Iteration 2410: Loss = -12619.589647060004
Iteration 2420: Loss = -12619.589570181683
Iteration 2430: Loss = -12619.589498766729
Iteration 2440: Loss = -12619.589414623762
Iteration 2450: Loss = -12619.589369889212
Iteration 2460: Loss = -12619.589276708994
Iteration 2470: Loss = -12619.589206301376
Iteration 2480: Loss = -12619.5891248724
Iteration 2490: Loss = -12619.589047268446
Iteration 2500: Loss = -12619.589026832778
Iteration 2510: Loss = -12619.588945201807
Iteration 2520: Loss = -12619.588911031347
Iteration 2530: Loss = -12619.588792537621
Iteration 2540: Loss = -12619.588744475406
Iteration 2550: Loss = -12619.588665161285
Iteration 2560: Loss = -12619.588616492041
Iteration 2570: Loss = -12619.588587618948
Iteration 2580: Loss = -12619.588512191462
Iteration 2590: Loss = -12619.588439418776
Iteration 2600: Loss = -12619.58839064834
Iteration 2610: Loss = -12619.588270910153
Iteration 2620: Loss = -12619.588190366076
Iteration 2630: Loss = -12619.588178511625
Iteration 2640: Loss = -12619.588106694253
Iteration 2650: Loss = -12619.5880333051
Iteration 2660: Loss = -12619.58799261528
Iteration 2670: Loss = -12619.58794764648
Iteration 2680: Loss = -12619.5878759943
Iteration 2690: Loss = -12619.58778326499
Iteration 2700: Loss = -12619.58777196482
Iteration 2710: Loss = -12619.587732111995
Iteration 2720: Loss = -12619.58764146444
Iteration 2730: Loss = -12619.587630500193
Iteration 2740: Loss = -12619.587497186316
Iteration 2750: Loss = -12619.587453754391
Iteration 2760: Loss = -12619.587449371602
Iteration 2770: Loss = -12619.587344070633
Iteration 2780: Loss = -12619.587307453432
Iteration 2790: Loss = -12619.58726189523
Iteration 2800: Loss = -12619.587210862243
Iteration 2810: Loss = -12619.587117620466
Iteration 2820: Loss = -12619.587063334697
Iteration 2830: Loss = -12619.5870601763
Iteration 2840: Loss = -12619.586961369501
Iteration 2850: Loss = -12619.58697284626
1
Iteration 2860: Loss = -12619.586895554166
Iteration 2870: Loss = -12619.586828916596
Iteration 2880: Loss = -12619.58679386588
Iteration 2890: Loss = -12619.586692490864
Iteration 2900: Loss = -12619.586694870435
1
Iteration 2910: Loss = -12619.586595180828
Iteration 2920: Loss = -12619.586599643757
1
Iteration 2930: Loss = -12619.586563252225
Iteration 2940: Loss = -12619.586553854431
Iteration 2950: Loss = -12619.586440542878
Iteration 2960: Loss = -12619.586405819507
Iteration 2970: Loss = -12619.586342799807
Iteration 2980: Loss = -12619.586295115032
Iteration 2990: Loss = -12619.586265322445
Iteration 3000: Loss = -12619.58617650472
Iteration 3010: Loss = -12619.586202596825
1
Iteration 3020: Loss = -12619.586171312667
Iteration 3030: Loss = -12619.586055294441
Iteration 3040: Loss = -12619.586039799382
Iteration 3050: Loss = -12619.586010398994
Iteration 3060: Loss = -12619.585942742302
Iteration 3070: Loss = -12619.58590550567
Iteration 3080: Loss = -12619.585889441236
Iteration 3090: Loss = -12619.585782676715
Iteration 3100: Loss = -12619.585763656953
Iteration 3110: Loss = -12619.585748815503
Iteration 3120: Loss = -12619.585707136262
Iteration 3130: Loss = -12619.585688887057
Iteration 3140: Loss = -12619.58561954279
Iteration 3150: Loss = -12619.58558158135
Iteration 3160: Loss = -12619.585587494246
1
Iteration 3170: Loss = -12619.58551672835
Iteration 3180: Loss = -12619.585447458594
Iteration 3190: Loss = -12619.585426986032
Iteration 3200: Loss = -12619.585368791473
Iteration 3210: Loss = -12619.585386368772
1
Iteration 3220: Loss = -12619.58533208403
Iteration 3230: Loss = -12619.585260285725
Iteration 3240: Loss = -12619.585261963119
1
Iteration 3250: Loss = -12619.58522911576
Iteration 3260: Loss = -12619.585162689813
Iteration 3270: Loss = -12619.585135603083
Iteration 3280: Loss = -12619.585087402891
Iteration 3290: Loss = -12619.58507505688
Iteration 3300: Loss = -12619.585020734032
Iteration 3310: Loss = -12619.58500573741
Iteration 3320: Loss = -12619.584896423126
Iteration 3330: Loss = -12619.584909365076
1
Iteration 3340: Loss = -12619.58487652923
Iteration 3350: Loss = -12619.58485103622
Iteration 3360: Loss = -12619.584788604188
Iteration 3370: Loss = -12619.584752954455
Iteration 3380: Loss = -12619.584723867083
Iteration 3390: Loss = -12619.584737505129
1
Iteration 3400: Loss = -12619.584664999646
Iteration 3410: Loss = -12619.584625910182
Iteration 3420: Loss = -12619.584622101755
Iteration 3430: Loss = -12619.5845200554
Iteration 3440: Loss = -12619.584571755604
1
Iteration 3450: Loss = -12619.584464371128
Iteration 3460: Loss = -12619.584452121906
Iteration 3470: Loss = -12619.584494528568
1
Iteration 3480: Loss = -12619.584409548415
Iteration 3490: Loss = -12619.584343542796
Iteration 3500: Loss = -12619.584401921009
1
Iteration 3510: Loss = -12619.584292644337
Iteration 3520: Loss = -12619.584281632246
Iteration 3530: Loss = -12619.58424239729
Iteration 3540: Loss = -12619.584277734617
1
Iteration 3550: Loss = -12619.584147836717
Iteration 3560: Loss = -12619.584117593406
Iteration 3570: Loss = -12619.58409461543
Iteration 3580: Loss = -12619.584087050585
Iteration 3590: Loss = -12619.584099805492
1
Iteration 3600: Loss = -12619.58405658867
Iteration 3610: Loss = -12619.58403658708
Iteration 3620: Loss = -12619.58397017327
Iteration 3630: Loss = -12619.58396198853
Iteration 3640: Loss = -12619.583954685528
Iteration 3650: Loss = -12619.583882874796
Iteration 3660: Loss = -12619.58384818805
Iteration 3670: Loss = -12619.583850273906
1
Iteration 3680: Loss = -12619.583801635954
Iteration 3690: Loss = -12619.583810162829
1
Iteration 3700: Loss = -12619.583758258967
Iteration 3710: Loss = -12619.58370130491
Iteration 3720: Loss = -12619.583668097914
Iteration 3730: Loss = -12619.583606513772
Iteration 3740: Loss = -12619.58363121793
1
Iteration 3750: Loss = -12619.583613485818
2
Iteration 3760: Loss = -12619.583633512422
3
Stopping early at iteration 3760 due to no improvement.
pi: tensor([[0.3421, 0.6579],
        [0.3553, 0.6447]], dtype=torch.float64)
alpha: tensor([0.3507, 0.6493])
beta: tensor([[[0.2045, 0.2056],
         [0.9974, 0.2051]],

        [[0.7238, 0.2080],
         [0.2163, 0.0568]],

        [[0.4633, 0.2075],
         [0.4094, 0.8307]],

        [[0.6218, 0.1978],
         [0.9759, 0.4742]],

        [[0.0870, 0.2051],
         [0.7388, 0.4352]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23193.902116306057
Iteration 100: Loss = -12620.438060495007
Iteration 200: Loss = -12619.918364639107
Iteration 300: Loss = -12619.820877470105
Iteration 400: Loss = -12619.775703391751
Iteration 500: Loss = -12619.750277292667
Iteration 600: Loss = -12619.73403772309
Iteration 700: Loss = -12619.722781424798
Iteration 800: Loss = -12619.714378860455
Iteration 900: Loss = -12619.70786973263
Iteration 1000: Loss = -12619.702451955522
Iteration 1100: Loss = -12619.69783279127
Iteration 1200: Loss = -12619.693624478501
Iteration 1300: Loss = -12619.689568721236
Iteration 1400: Loss = -12619.685530625811
Iteration 1500: Loss = -12619.681186248305
Iteration 1600: Loss = -12619.6763571258
Iteration 1700: Loss = -12619.670514098045
Iteration 1800: Loss = -12619.6632140236
Iteration 1900: Loss = -12619.653460057634
Iteration 2000: Loss = -12619.639666591347
Iteration 2100: Loss = -12619.619655411425
Iteration 2200: Loss = -12619.591780868226
Iteration 2300: Loss = -12619.559293687056
Iteration 2400: Loss = -12619.527118624997
Iteration 2500: Loss = -12619.478253547126
Iteration 2600: Loss = -12619.15744916319
Iteration 2700: Loss = -12618.614984829203
Iteration 2800: Loss = -12618.40809570863
Iteration 2900: Loss = -12618.290642563044
Iteration 3000: Loss = -12618.208559296274
Iteration 3100: Loss = -12618.148048941508
Iteration 3200: Loss = -12618.103195177428
Iteration 3300: Loss = -12618.06972393415
Iteration 3400: Loss = -12618.04288444033
Iteration 3500: Loss = -12618.019877071307
Iteration 3600: Loss = -12617.999343814652
Iteration 3700: Loss = -12617.980503876108
Iteration 3800: Loss = -12617.961464050682
Iteration 3900: Loss = -12617.940579744327
Iteration 4000: Loss = -12617.917629161677
Iteration 4100: Loss = -12617.892697290981
Iteration 4200: Loss = -12617.864950924002
Iteration 4300: Loss = -12617.834927179612
Iteration 4400: Loss = -12617.803796282082
Iteration 4500: Loss = -12617.77318897954
Iteration 4600: Loss = -12617.744476360826
Iteration 4700: Loss = -12617.718511963614
Iteration 4800: Loss = -12617.695591799044
Iteration 4900: Loss = -12617.675652487207
Iteration 5000: Loss = -12617.658477550216
Iteration 5100: Loss = -12617.643662097425
Iteration 5200: Loss = -12617.630940140101
Iteration 5300: Loss = -12617.619852082822
Iteration 5400: Loss = -12617.610340004132
Iteration 5500: Loss = -12617.601900244088
Iteration 5600: Loss = -12617.594584550541
Iteration 5700: Loss = -12617.588109466104
Iteration 5800: Loss = -12617.582430428778
Iteration 5900: Loss = -12617.57732027889
Iteration 6000: Loss = -12617.572856744593
Iteration 6100: Loss = -12617.56873150463
Iteration 6200: Loss = -12617.565066570376
Iteration 6300: Loss = -12617.56175688638
Iteration 6400: Loss = -12617.558737975283
Iteration 6500: Loss = -12617.556035255446
Iteration 6600: Loss = -12617.553554830158
Iteration 6700: Loss = -12617.551294587227
Iteration 6800: Loss = -12617.549130011872
Iteration 6900: Loss = -12617.547327549226
Iteration 7000: Loss = -12617.544845876084
Iteration 7100: Loss = -12617.54317271418
Iteration 7200: Loss = -12617.541661907075
Iteration 7300: Loss = -12617.540296021025
Iteration 7400: Loss = -12617.539018692498
Iteration 7500: Loss = -12617.537824640462
Iteration 7600: Loss = -12617.536701925128
Iteration 7700: Loss = -12617.53571140172
Iteration 7800: Loss = -12617.534801478414
Iteration 7900: Loss = -12617.533870341587
Iteration 8000: Loss = -12617.533010113877
Iteration 8100: Loss = -12617.533046263528
1
Iteration 8200: Loss = -12617.531531772254
Iteration 8300: Loss = -12617.53082914597
Iteration 8400: Loss = -12617.530878922395
1
Iteration 8500: Loss = -12617.52958851525
Iteration 8600: Loss = -12617.529058969429
Iteration 8700: Loss = -12617.56572358152
1
Iteration 8800: Loss = -12617.52804722088
Iteration 8900: Loss = -12617.52757333184
Iteration 9000: Loss = -12617.556752113529
1
Iteration 9100: Loss = -12617.526732405962
Iteration 9200: Loss = -12617.526344204069
Iteration 9300: Loss = -12617.593914160867
1
Iteration 9400: Loss = -12617.52565116376
Iteration 9500: Loss = -12617.525354883424
Iteration 9600: Loss = -12617.541914660636
1
Iteration 9700: Loss = -12617.524743374071
Iteration 9800: Loss = -12617.68978950229
1
Iteration 9900: Loss = -12617.524250194878
Iteration 10000: Loss = -12617.524006835745
Iteration 10100: Loss = -12617.525849577774
1
Iteration 10200: Loss = -12617.523577358772
Iteration 10300: Loss = -12617.523423489472
Iteration 10400: Loss = -12617.526414479205
1
Iteration 10500: Loss = -12617.523026817697
Iteration 10600: Loss = -12617.52290320711
Iteration 10700: Loss = -12617.52316569668
1
Iteration 10800: Loss = -12617.522644125473
Iteration 10900: Loss = -12617.522448332174
Iteration 11000: Loss = -12617.52234426853
Iteration 11100: Loss = -12617.530250125998
1
Iteration 11200: Loss = -12617.522098852865
Iteration 11300: Loss = -12617.521997493413
Iteration 11400: Loss = -12617.521928467007
Iteration 11500: Loss = -12617.525305677074
1
Iteration 11600: Loss = -12617.521766501912
Iteration 11700: Loss = -12617.521647914617
Iteration 11800: Loss = -12617.521593536561
Iteration 11900: Loss = -12617.521514298285
Iteration 12000: Loss = -12617.521473620518
Iteration 12100: Loss = -12617.782120268977
1
Iteration 12200: Loss = -12617.521317116842
Iteration 12300: Loss = -12617.521261197837
Iteration 12400: Loss = -12617.535182821157
1
Iteration 12500: Loss = -12617.521225649592
Iteration 12600: Loss = -12617.521102149822
Iteration 12700: Loss = -12617.521092510473
Iteration 12800: Loss = -12617.52104560209
Iteration 12900: Loss = -12617.521043581837
Iteration 13000: Loss = -12617.520979475707
Iteration 13100: Loss = -12617.8242482242
1
Iteration 13200: Loss = -12617.520940939648
Iteration 13300: Loss = -12617.520922016674
Iteration 13400: Loss = -12617.653180909072
1
Iteration 13500: Loss = -12617.520884789554
Iteration 13600: Loss = -12617.520879334295
Iteration 13700: Loss = -12617.52095717307
1
Iteration 13800: Loss = -12617.520872354415
Iteration 13900: Loss = -12617.64923581012
1
Iteration 14000: Loss = -12617.520981389
2
Iteration 14100: Loss = -12617.57213451869
3
Iteration 14200: Loss = -12617.523916710401
4
Iteration 14300: Loss = -12617.520778473237
Iteration 14400: Loss = -12617.52089719438
1
Iteration 14500: Loss = -12617.520728711987
Iteration 14600: Loss = -12617.523721973104
1
Iteration 14700: Loss = -12617.520703752883
Iteration 14800: Loss = -12617.521983945884
1
Iteration 14900: Loss = -12617.520724213804
2
Iteration 15000: Loss = -12617.529089959708
3
Iteration 15100: Loss = -12617.520681949181
Iteration 15200: Loss = -12617.541434109684
1
Iteration 15300: Loss = -12617.520688272878
2
Iteration 15400: Loss = -12617.552386046356
3
Iteration 15500: Loss = -12617.520708075333
4
Iteration 15600: Loss = -12617.520708333515
5
Stopping early at iteration 15600 due to no improvement.
pi: tensor([[1.0000e+00, 4.4047e-06],
        [3.3118e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0188, 0.9812], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[2.2568e-04, 1.8471e-01],
         [6.7309e-01, 2.0705e-01]],

        [[5.1788e-01, 2.1504e-01],
         [5.0928e-01, 5.2802e-01]],

        [[5.6155e-01, 2.5060e-01],
         [5.8872e-01, 5.1807e-01]],

        [[7.2148e-01, 1.0884e-01],
         [5.5875e-01, 5.7873e-01]],

        [[5.1928e-01, 2.6820e-01],
         [5.3963e-01, 7.1439e-01]]], dtype=torch.float64,
       grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.009696969696969697
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.00776683106263838
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.002442591574126975
Global Adjusted Rand Index: -5.537904234981907e-05
Average Adjusted Rand Index: 0.0036705439988840654
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19689.24076143035
Iteration 100: Loss = -12620.070225173044
Iteration 200: Loss = -12619.737885782175
Iteration 300: Loss = -12619.669805868423
Iteration 400: Loss = -12619.631692233957
Iteration 500: Loss = -12619.60427482235
Iteration 600: Loss = -12619.58090664208
Iteration 700: Loss = -12619.558600758994
Iteration 800: Loss = -12619.534649956773
Iteration 900: Loss = -12619.504537940013
Iteration 1000: Loss = -12619.463686737347
Iteration 1100: Loss = -12619.426908123702
Iteration 1200: Loss = -12619.393929361611
Iteration 1300: Loss = -12619.315257964057
Iteration 1400: Loss = -12619.056856134292
Iteration 1500: Loss = -12618.673270379179
Iteration 1600: Loss = -12618.399946841299
Iteration 1700: Loss = -12618.254042185052
Iteration 1800: Loss = -12618.17289258555
Iteration 1900: Loss = -12618.121590398821
Iteration 2000: Loss = -12618.085754678787
Iteration 2100: Loss = -12618.058897855633
Iteration 2200: Loss = -12618.03765789808
Iteration 2300: Loss = -12618.019997866084
Iteration 2400: Loss = -12618.004659846847
Iteration 2500: Loss = -12617.990926576022
Iteration 2600: Loss = -12617.978021862427
Iteration 2700: Loss = -12617.965484186374
Iteration 2800: Loss = -12617.95286598939
Iteration 2900: Loss = -12617.939627005559
Iteration 3000: Loss = -12617.925506938522
Iteration 3100: Loss = -12617.909938217155
Iteration 3200: Loss = -12617.892750560675
Iteration 3300: Loss = -12617.87375591386
Iteration 3400: Loss = -12617.85299171767
Iteration 3500: Loss = -12617.830740991309
Iteration 3600: Loss = -12617.807549274026
Iteration 3700: Loss = -12617.784128521726
Iteration 3800: Loss = -12617.761171788698
Iteration 3900: Loss = -12617.739166407648
Iteration 4000: Loss = -12617.718620697568
Iteration 4100: Loss = -12617.699800443483
Iteration 4200: Loss = -12617.682773257799
Iteration 4300: Loss = -12617.667368535145
Iteration 4400: Loss = -12617.653700981813
Iteration 4500: Loss = -12617.641499399004
Iteration 4600: Loss = -12617.63058693469
Iteration 4700: Loss = -12617.62095405122
Iteration 4800: Loss = -12617.61228657205
Iteration 4900: Loss = -12617.604586770127
Iteration 5000: Loss = -12617.597679630635
Iteration 5100: Loss = -12617.591509545839
Iteration 5200: Loss = -12617.585934497763
Iteration 5300: Loss = -12617.580899129925
Iteration 5400: Loss = -12617.57632725545
Iteration 5500: Loss = -12617.57223660338
Iteration 5600: Loss = -12617.56841654817
Iteration 5700: Loss = -12617.56506158719
Iteration 5800: Loss = -12617.561861065424
Iteration 5900: Loss = -12617.55900170229
Iteration 6000: Loss = -12617.556402224342
Iteration 6100: Loss = -12617.553949812178
Iteration 6200: Loss = -12617.551677710027
Iteration 6300: Loss = -12617.549579496064
Iteration 6400: Loss = -12617.54774603576
Iteration 6500: Loss = -12617.545985912182
Iteration 6600: Loss = -12617.544484320984
Iteration 6700: Loss = -12617.542811837695
Iteration 6800: Loss = -12617.541422379443
Iteration 6900: Loss = -12617.540110384442
Iteration 7000: Loss = -12617.538840718571
Iteration 7100: Loss = -12617.547775016612
1
Iteration 7200: Loss = -12617.536650206293
Iteration 7300: Loss = -12617.535657472014
Iteration 7400: Loss = -12617.556783478007
1
Iteration 7500: Loss = -12617.533818786673
Iteration 7600: Loss = -12617.533034294645
Iteration 7700: Loss = -12617.810600178718
1
Iteration 7800: Loss = -12617.531555560403
Iteration 7900: Loss = -12617.530851953943
Iteration 8000: Loss = -12617.530248609506
Iteration 8100: Loss = -12617.530120249041
Iteration 8200: Loss = -12617.529071397406
Iteration 8300: Loss = -12617.528564965121
Iteration 8400: Loss = -12617.56197781246
1
Iteration 8500: Loss = -12617.527618173612
Iteration 8600: Loss = -12617.527156576283
Iteration 8700: Loss = -12617.52672330938
Iteration 8800: Loss = -12617.773133766708
1
Iteration 8900: Loss = -12617.526027404432
Iteration 9000: Loss = -12617.525701261138
Iteration 9100: Loss = -12617.525316526799
Iteration 9200: Loss = -12617.530100698026
1
Iteration 9300: Loss = -12617.52471815256
Iteration 9400: Loss = -12617.524452492444
Iteration 9500: Loss = -12617.524209993291
Iteration 9600: Loss = -12617.524284577874
1
Iteration 9700: Loss = -12617.52370928681
Iteration 9800: Loss = -12617.523491176644
Iteration 9900: Loss = -12617.523493764334
1
Iteration 10000: Loss = -12617.523129298479
Iteration 10100: Loss = -12617.522954073866
Iteration 10200: Loss = -12617.522769267716
Iteration 10300: Loss = -12617.773792885497
1
Iteration 10400: Loss = -12617.522455761535
Iteration 10500: Loss = -12617.52226787427
Iteration 10600: Loss = -12617.522193256995
Iteration 10700: Loss = -12617.523011236406
1
Iteration 10800: Loss = -12617.521956251525
Iteration 10900: Loss = -12617.521891604243
Iteration 11000: Loss = -12617.808553116973
1
Iteration 11100: Loss = -12617.52167876804
Iteration 11200: Loss = -12617.521609049347
Iteration 11300: Loss = -12617.521476290554
Iteration 11400: Loss = -12617.52162192436
1
Iteration 11500: Loss = -12617.521340410507
Iteration 11600: Loss = -12617.52127768594
Iteration 11700: Loss = -12617.546043153347
1
Iteration 11800: Loss = -12617.52119314722
Iteration 11900: Loss = -12617.521068267155
Iteration 12000: Loss = -12617.521048411647
Iteration 12100: Loss = -12617.521139507173
1
Iteration 12200: Loss = -12617.520925820658
Iteration 12300: Loss = -12617.520926225368
1
Iteration 12400: Loss = -12617.531716459975
2
Iteration 12500: Loss = -12617.520837745653
Iteration 12600: Loss = -12617.521744514921
1
Iteration 12700: Loss = -12617.52079251567
Iteration 12800: Loss = -12617.548307220268
1
Iteration 12900: Loss = -12617.52083452342
2
Iteration 13000: Loss = -12617.849093582046
3
Iteration 13100: Loss = -12617.5210979521
4
Iteration 13200: Loss = -12617.521590199976
5
Stopping early at iteration 13200 due to no improvement.
pi: tensor([[1.0000e+00, 7.8879e-08],
        [1.0940e-05, 9.9999e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9813, 0.0187], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[2.0702e-01, 1.8469e-01],
         [5.8881e-01, 4.0732e-04]],

        [[6.7888e-01, 2.1410e-01],
         [5.9251e-01, 5.6534e-01]],

        [[5.9306e-01, 2.5059e-01],
         [5.6069e-01, 7.2621e-01]],

        [[6.0240e-01, 1.0885e-01],
         [6.5251e-01, 6.0435e-01]],

        [[5.5475e-01, 2.6903e-01],
         [7.0236e-01, 6.5780e-01]]], dtype=torch.float64,
       grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.009696969696969697
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.00776683106263838
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.002442591574126975
Global Adjusted Rand Index: -5.537904234981907e-05
Average Adjusted Rand Index: 0.0036705439988840654
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22790.467074259544
Iteration 100: Loss = -12620.927933041452
Iteration 200: Loss = -12620.010884484316
Iteration 300: Loss = -12619.847330944976
Iteration 400: Loss = -12619.778276024028
Iteration 500: Loss = -12619.737415598049
Iteration 600: Loss = -12619.708802349543
Iteration 700: Loss = -12619.686240338835
Iteration 800: Loss = -12619.666826250514
Iteration 900: Loss = -12619.648550812612
Iteration 1000: Loss = -12619.630005939845
Iteration 1100: Loss = -12619.609769334447
Iteration 1200: Loss = -12619.586053384715
Iteration 1300: Loss = -12619.55624567762
Iteration 1400: Loss = -12619.51543744535
Iteration 1500: Loss = -12619.45127313133
Iteration 1600: Loss = -12619.325687747241
Iteration 1700: Loss = -12619.118109763407
Iteration 1800: Loss = -12618.917455669423
Iteration 1900: Loss = -12618.751687022717
Iteration 2000: Loss = -12618.624927138557
Iteration 2100: Loss = -12618.526741174539
Iteration 2200: Loss = -12618.448079653052
Iteration 2300: Loss = -12618.382551722465
Iteration 2400: Loss = -12618.32572637261
Iteration 2500: Loss = -12618.275400427032
Iteration 2600: Loss = -12618.230965956516
Iteration 2700: Loss = -12618.192358314202
Iteration 2800: Loss = -12618.158956357904
Iteration 2900: Loss = -12618.130299059598
Iteration 3000: Loss = -12618.105481212597
Iteration 3100: Loss = -12618.083738310059
Iteration 3200: Loss = -12618.064379097055
Iteration 3300: Loss = -12618.04692175398
Iteration 3400: Loss = -12618.030763994775
Iteration 3500: Loss = -12618.01546850234
Iteration 3600: Loss = -12618.000308557208
Iteration 3700: Loss = -12617.985011982837
Iteration 3800: Loss = -12617.9689961897
Iteration 3900: Loss = -12617.951634552226
Iteration 4000: Loss = -12617.932481718815
Iteration 4100: Loss = -12617.91116546716
Iteration 4200: Loss = -12617.887547010334
Iteration 4300: Loss = -12617.861919429526
Iteration 4400: Loss = -12617.834942274012
Iteration 4500: Loss = -12617.807565162693
Iteration 4600: Loss = -12617.780764969237
Iteration 4700: Loss = -12617.75532896536
Iteration 4800: Loss = -12617.7318781782
Iteration 4900: Loss = -12617.71059515823
Iteration 5000: Loss = -12617.69158363618
Iteration 5100: Loss = -12617.674801591926
Iteration 5200: Loss = -12617.659869463032
Iteration 5300: Loss = -12617.646767747838
Iteration 5400: Loss = -12617.63510039894
Iteration 5500: Loss = -12617.624876460422
Iteration 5600: Loss = -12617.615776768718
Iteration 5700: Loss = -12617.607607645205
Iteration 5800: Loss = -12617.600200952536
Iteration 5900: Loss = -12617.593595870658
Iteration 6000: Loss = -12617.587788846055
Iteration 6100: Loss = -12617.582525193633
Iteration 6200: Loss = -12617.577822816911
Iteration 6300: Loss = -12617.573564647795
Iteration 6400: Loss = -12617.569674534288
Iteration 6500: Loss = -12617.566187664537
Iteration 6600: Loss = -12617.562900051358
Iteration 6700: Loss = -12617.559995899022
Iteration 6800: Loss = -12617.557274694595
Iteration 6900: Loss = -12617.554812805805
Iteration 7000: Loss = -12617.55250377407
Iteration 7100: Loss = -12617.550365773326
Iteration 7200: Loss = -12617.54845477764
Iteration 7300: Loss = -12617.546622366734
Iteration 7400: Loss = -12617.544943925039
Iteration 7500: Loss = -12617.543388141099
Iteration 7600: Loss = -12617.542083966917
Iteration 7700: Loss = -12617.540628531753
Iteration 7800: Loss = -12617.54169150083
1
Iteration 7900: Loss = -12617.53823760023
Iteration 8000: Loss = -12617.537129944005
Iteration 8100: Loss = -12618.176428018163
1
Iteration 8200: Loss = -12617.535153025761
Iteration 8300: Loss = -12617.534225304445
Iteration 8400: Loss = -12617.53340884612
Iteration 8500: Loss = -12617.5339711703
1
Iteration 8600: Loss = -12617.531902632516
Iteration 8700: Loss = -12617.531268203642
Iteration 8800: Loss = -12617.530595379987
Iteration 8900: Loss = -12617.529989990637
Iteration 9000: Loss = -12617.529410329702
Iteration 9100: Loss = -12617.528893079994
Iteration 9200: Loss = -12617.917716450138
1
Iteration 9300: Loss = -12617.527946926648
Iteration 9400: Loss = -12617.52745779333
Iteration 9500: Loss = -12617.527060426293
Iteration 9600: Loss = -12617.526697486104
Iteration 9700: Loss = -12617.527040045936
1
Iteration 9800: Loss = -12617.525952828464
Iteration 9900: Loss = -12617.525626660796
Iteration 10000: Loss = -12617.916367457034
1
Iteration 10100: Loss = -12617.525001370006
Iteration 10200: Loss = -12617.524767123412
Iteration 10300: Loss = -12617.52446808463
Iteration 10400: Loss = -12617.53754985918
1
Iteration 10500: Loss = -12617.524018084792
Iteration 10600: Loss = -12617.523839009491
Iteration 10700: Loss = -12617.523579884253
Iteration 10800: Loss = -12617.523431558378
Iteration 10900: Loss = -12617.523211897911
Iteration 11000: Loss = -12617.5230787529
Iteration 11100: Loss = -12617.52292910288
Iteration 11200: Loss = -12617.523571039479
1
Iteration 11300: Loss = -12617.522610110173
Iteration 11400: Loss = -12617.522491322256
Iteration 11500: Loss = -12617.52941399694
1
Iteration 11600: Loss = -12617.522239716778
Iteration 11700: Loss = -12617.522119277299
Iteration 11800: Loss = -12617.522033060553
Iteration 11900: Loss = -12617.596300620056
1
Iteration 12000: Loss = -12617.521810203663
Iteration 12100: Loss = -12617.521725370998
Iteration 12200: Loss = -12617.521682882474
Iteration 12300: Loss = -12617.52161870456
Iteration 12400: Loss = -12617.521491549336
Iteration 12500: Loss = -12617.521419508279
Iteration 12600: Loss = -12617.532779708265
1
Iteration 12700: Loss = -12617.5213240971
Iteration 12800: Loss = -12617.521254626
Iteration 12900: Loss = -12617.52619515838
1
Iteration 13000: Loss = -12617.521174687272
Iteration 13100: Loss = -12617.525851327046
1
Iteration 13200: Loss = -12617.52111285379
Iteration 13300: Loss = -12617.534018302473
1
Iteration 13400: Loss = -12617.52110004756
Iteration 13500: Loss = -12617.522683309186
1
Iteration 13600: Loss = -12617.521339148747
2
Iteration 13700: Loss = -12617.520993581455
Iteration 13800: Loss = -12617.520980143878
Iteration 13900: Loss = -12617.522230102328
1
Iteration 14000: Loss = -12617.52090684127
Iteration 14100: Loss = -12617.555409296769
1
Iteration 14200: Loss = -12617.520833020537
Iteration 14300: Loss = -12617.5497305454
1
Iteration 14400: Loss = -12617.520730235596
Iteration 14500: Loss = -12617.60182335236
1
Iteration 14600: Loss = -12617.520676102082
Iteration 14700: Loss = -12617.525084333985
1
Iteration 14800: Loss = -12617.520722015715
2
Iteration 14900: Loss = -12617.521920416137
3
Iteration 15000: Loss = -12617.52093955401
4
Iteration 15100: Loss = -12617.523941655922
5
Stopping early at iteration 15100 due to no improvement.
pi: tensor([[9.9998e-01, 1.5647e-05],
        [9.2139e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0190, 0.9810], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[2.6824e-04, 1.8489e-01],
         [6.6137e-01, 2.0704e-01]],

        [[7.1277e-01, 2.1426e-01],
         [5.2303e-01, 5.5735e-01]],

        [[5.4111e-01, 2.5035e-01],
         [6.3512e-01, 6.2015e-01]],

        [[6.7025e-01, 1.0902e-01],
         [6.1558e-01, 6.7128e-01]],

        [[6.6691e-01, 2.6647e-01],
         [7.1706e-01, 6.6751e-01]]], dtype=torch.float64,
       grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.009696969696969697
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.00776683106263838
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.002442591574126975
Global Adjusted Rand Index: -5.537904234981907e-05
Average Adjusted Rand Index: 0.0036705439988840654
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22514.286781561346
Iteration 100: Loss = -12620.812230231597
Iteration 200: Loss = -12620.077638722325
Iteration 300: Loss = -12619.915063780698
Iteration 400: Loss = -12619.847426166085
Iteration 500: Loss = -12619.810240228664
Iteration 600: Loss = -12619.785192895637
Iteration 700: Loss = -12619.76599711041
Iteration 800: Loss = -12619.750238849934
Iteration 900: Loss = -12619.736752784172
Iteration 1000: Loss = -12619.724829092598
Iteration 1100: Loss = -12619.714173051776
Iteration 1200: Loss = -12619.704197619007
Iteration 1300: Loss = -12619.694879593328
Iteration 1400: Loss = -12619.685818589944
Iteration 1500: Loss = -12619.676853408222
Iteration 1600: Loss = -12619.66765555627
Iteration 1700: Loss = -12619.657925165224
Iteration 1800: Loss = -12619.647332624058
Iteration 1900: Loss = -12619.635261667947
Iteration 2000: Loss = -12619.621299695824
Iteration 2100: Loss = -12619.604838676238
Iteration 2200: Loss = -12619.58545147135
Iteration 2300: Loss = -12619.563154146248
Iteration 2400: Loss = -12619.53836283742
Iteration 2500: Loss = -12619.51245352283
Iteration 2600: Loss = -12619.486892615632
Iteration 2700: Loss = -12619.462041656574
Iteration 2800: Loss = -12619.435676327363
Iteration 2900: Loss = -12619.401352918598
Iteration 3000: Loss = -12619.342866529954
Iteration 3100: Loss = -12619.224254363415
Iteration 3200: Loss = -12619.025553311738
Iteration 3300: Loss = -12618.8240642168
Iteration 3400: Loss = -12618.667826115226
Iteration 3500: Loss = -12618.54044978387
Iteration 3600: Loss = -12618.434943461272
Iteration 3700: Loss = -12618.349905424035
Iteration 3800: Loss = -12618.275486278204
Iteration 3900: Loss = -12618.211013248661
Iteration 4000: Loss = -12618.15982410948
Iteration 4100: Loss = -12618.120392661764
Iteration 4200: Loss = -12618.08844914885
Iteration 4300: Loss = -12618.061167405664
Iteration 4400: Loss = -12618.036390222993
Iteration 4500: Loss = -12618.012439723829
Iteration 4600: Loss = -12617.987678717807
Iteration 4700: Loss = -12617.960655422628
Iteration 4800: Loss = -12617.930267757418
Iteration 4900: Loss = -12617.896154347658
Iteration 5000: Loss = -12617.859346238432
Iteration 5100: Loss = -12617.821880789199
Iteration 5200: Loss = -12617.786032936956
Iteration 5300: Loss = -12617.753426727104
Iteration 5400: Loss = -12617.724754017743
Iteration 5500: Loss = -12617.700053349326
Iteration 5600: Loss = -12617.679024401124
Iteration 5700: Loss = -12617.661118625912
Iteration 5800: Loss = -12617.645830626561
Iteration 5900: Loss = -12617.632747639036
Iteration 6000: Loss = -12617.621462145305
Iteration 6100: Loss = -12617.611720281702
Iteration 6200: Loss = -12617.603186841241
Iteration 6300: Loss = -12617.595745360115
Iteration 6400: Loss = -12617.589211037439
Iteration 6500: Loss = -12617.58335581891
Iteration 6600: Loss = -12617.578205999735
Iteration 6700: Loss = -12617.573567802545
Iteration 6800: Loss = -12617.569380569063
Iteration 6900: Loss = -12617.587555952485
1
Iteration 7000: Loss = -12617.562247859576
Iteration 7100: Loss = -12617.55923692099
Iteration 7200: Loss = -12617.556387603337
Iteration 7300: Loss = -12617.582012140596
1
Iteration 7400: Loss = -12617.551520820289
Iteration 7500: Loss = -12617.549403758618
Iteration 7600: Loss = -12617.549684047111
1
Iteration 7700: Loss = -12617.545618044556
Iteration 7800: Loss = -12617.544083784514
Iteration 7900: Loss = -12617.542500651953
Iteration 8000: Loss = -12617.540942540934
Iteration 8100: Loss = -12617.53984443372
Iteration 8200: Loss = -12617.538396967257
Iteration 8300: Loss = -12617.537299032765
Iteration 8400: Loss = -12617.536226092358
Iteration 8500: Loss = -12617.535224850388
Iteration 8600: Loss = -12617.550217866754
1
Iteration 8700: Loss = -12617.53344962969
Iteration 8800: Loss = -12617.532637700257
Iteration 8900: Loss = -12617.63827943211
1
Iteration 9000: Loss = -12617.531223142678
Iteration 9100: Loss = -12617.53052631714
Iteration 9200: Loss = -12617.529948544017
Iteration 9300: Loss = -12617.529354077702
Iteration 9400: Loss = -12617.52885194192
Iteration 9500: Loss = -12617.528275721546
Iteration 9600: Loss = -12617.558274569872
1
Iteration 9700: Loss = -12617.527411617817
Iteration 9800: Loss = -12617.526997771633
Iteration 9900: Loss = -12617.526570380483
Iteration 10000: Loss = -12617.526388827791
Iteration 10100: Loss = -12617.525853718334
Iteration 10200: Loss = -12617.525553948359
Iteration 10300: Loss = -12617.53029413219
1
Iteration 10400: Loss = -12617.524990468115
Iteration 10500: Loss = -12617.524708163759
Iteration 10600: Loss = -12617.52442365602
Iteration 10700: Loss = -12617.524273541667
Iteration 10800: Loss = -12617.52390474453
Iteration 10900: Loss = -12617.52371825284
Iteration 11000: Loss = -12618.22238282215
1
Iteration 11100: Loss = -12617.52332995633
Iteration 11200: Loss = -12617.523122901752
Iteration 11300: Loss = -12617.522946945464
Iteration 11400: Loss = -12617.522829049936
Iteration 11500: Loss = -12617.522621838292
Iteration 11600: Loss = -12617.52245566556
Iteration 11700: Loss = -12617.777734255678
1
Iteration 11800: Loss = -12617.522186450922
Iteration 11900: Loss = -12617.52196828608
Iteration 12000: Loss = -12617.521880495284
Iteration 12100: Loss = -12617.522928317152
1
Iteration 12200: Loss = -12617.52171998221
Iteration 12300: Loss = -12617.521572986741
Iteration 12400: Loss = -12617.895175841397
1
Iteration 12500: Loss = -12617.521492242966
Iteration 12600: Loss = -12617.521404785093
Iteration 12700: Loss = -12617.521304587133
Iteration 12800: Loss = -12617.52121754319
Iteration 12900: Loss = -12617.521367493087
1
Iteration 13000: Loss = -12617.52114351015
Iteration 13100: Loss = -12617.521116162561
Iteration 13200: Loss = -12617.528577148512
1
Iteration 13300: Loss = -12617.520983028475
Iteration 13400: Loss = -12617.521727400841
1
Iteration 13500: Loss = -12617.520935303739
Iteration 13600: Loss = -12617.52087044822
Iteration 13700: Loss = -12617.521099391495
1
Iteration 13800: Loss = -12617.521121164309
2
Iteration 13900: Loss = -12617.520826709339
Iteration 14000: Loss = -12617.521188392526
1
Iteration 14100: Loss = -12617.52089621531
2
Iteration 14200: Loss = -12617.520707300495
Iteration 14300: Loss = -12617.520772174206
1
Iteration 14400: Loss = -12617.52064683871
Iteration 14500: Loss = -12617.52159160954
1
Iteration 14600: Loss = -12617.52063847562
Iteration 14700: Loss = -12617.528525937572
1
Iteration 14800: Loss = -12617.520880227356
2
Iteration 14900: Loss = -12617.586222661223
3
Iteration 15000: Loss = -12617.520580335104
Iteration 15100: Loss = -12617.521115343368
1
Iteration 15200: Loss = -12617.575707450791
2
Iteration 15300: Loss = -12617.52068086444
3
Iteration 15400: Loss = -12617.520538428727
Iteration 15500: Loss = -12617.521175002148
1
Iteration 15600: Loss = -12617.521141150206
2
Iteration 15700: Loss = -12617.520557696693
3
Iteration 15800: Loss = -12617.520653008085
4
Iteration 15900: Loss = -12617.52068331531
5
Stopping early at iteration 15900 due to no improvement.
pi: tensor([[1.0000e+00, 9.2553e-08],
        [1.7491e-05, 9.9998e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9812, 0.0188], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[2.0705e-01, 1.8472e-01],
         [6.6597e-01, 2.2977e-04]],

        [[7.1769e-01, 2.1465e-01],
         [6.4141e-01, 7.1120e-01]],

        [[6.2416e-01, 2.5067e-01],
         [6.4304e-01, 5.4980e-01]],

        [[6.4440e-01, 1.0880e-01],
         [5.4968e-01, 6.6421e-01]],

        [[5.6056e-01, 2.6834e-01],
         [6.4345e-01, 5.6997e-01]]], dtype=torch.float64,
       grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.009696969696969697
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.00776683106263838
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.002442591574126975
Global Adjusted Rand Index: -5.537904234981907e-05
Average Adjusted Rand Index: 0.0036705439988840654
12110.896277888682
[-5.537904234981907e-05, -5.537904234981907e-05, -5.537904234981907e-05, -5.537904234981907e-05] [0.0036705439988840654, 0.0036705439988840654, 0.0036705439988840654, 0.0036705439988840654] [12617.520708333515, 12617.521590199976, 12617.523941655922, 12617.52068331531]
-----------------------------------------------------------------------------------------
This iteration is 14
True Objective function: Loss = -11932.820331775833
Iteration 0: Loss = -12554.96115375394
Iteration 10: Loss = -12432.634574165691
Iteration 20: Loss = -12432.633446050117
Iteration 30: Loss = -12432.633150425781
Iteration 40: Loss = -12432.632968929598
Iteration 50: Loss = -12432.632701161687
Iteration 60: Loss = -12432.632510779096
Iteration 70: Loss = -12432.632300769432
Iteration 80: Loss = -12432.632184153104
Iteration 90: Loss = -12432.631989598041
Iteration 100: Loss = -12432.631835396494
Iteration 110: Loss = -12432.631768073048
Iteration 120: Loss = -12432.631598691698
Iteration 130: Loss = -12432.631464320511
Iteration 140: Loss = -12432.631359935382
Iteration 150: Loss = -12432.631297716278
Iteration 160: Loss = -12432.631186304794
Iteration 170: Loss = -12432.631115183707
Iteration 180: Loss = -12432.631032075411
Iteration 190: Loss = -12432.631005628602
Iteration 200: Loss = -12432.630909031637
Iteration 210: Loss = -12432.63086069778
Iteration 220: Loss = -12432.630782868764
Iteration 230: Loss = -12432.630786932474
1
Iteration 240: Loss = -12432.630770469872
Iteration 250: Loss = -12432.63069193342
Iteration 260: Loss = -12432.630669493949
Iteration 270: Loss = -12432.630600617731
Iteration 280: Loss = -12432.630573971
Iteration 290: Loss = -12432.630530318613
Iteration 300: Loss = -12432.630512217484
Iteration 310: Loss = -12432.630469219008
Iteration 320: Loss = -12432.630475414662
1
Iteration 330: Loss = -12432.630447579899
Iteration 340: Loss = -12432.630428397371
Iteration 350: Loss = -12432.630410847323
Iteration 360: Loss = -12432.630416341508
1
Iteration 370: Loss = -12432.630426638605
2
Iteration 380: Loss = -12432.63037552273
Iteration 390: Loss = -12432.63040352694
1
Iteration 400: Loss = -12432.630359907022
Iteration 410: Loss = -12432.630351538957
Iteration 420: Loss = -12432.63030014502
Iteration 430: Loss = -12432.630367453366
1
Iteration 440: Loss = -12432.630302846737
2
Iteration 450: Loss = -12432.630272887072
Iteration 460: Loss = -12432.630321940649
1
Iteration 470: Loss = -12432.630258745048
Iteration 480: Loss = -12432.630285876272
1
Iteration 490: Loss = -12432.630287445663
2
Iteration 500: Loss = -12432.630231200283
Iteration 510: Loss = -12432.63029752657
1
Iteration 520: Loss = -12432.630257328856
2
Iteration 530: Loss = -12432.630322675781
3
Stopping early at iteration 530 due to no improvement.
pi: tensor([[0.8436, 0.1564],
        [0.1592, 0.8408]], dtype=torch.float64)
alpha: tensor([0.5045, 0.4955])
beta: tensor([[[0.1994, 0.1968],
         [0.0633, 0.1994]],

        [[0.3665, 0.1984],
         [0.5908, 0.3878]],

        [[0.6297, 0.1940],
         [0.0009, 0.7256]],

        [[0.7465, 0.2090],
         [0.7577, 0.6123]],

        [[0.3812, 0.1988],
         [0.1301, 0.7788]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -12570.061631485276
Iteration 10: Loss = -11988.363987366347
Iteration 20: Loss = -11988.363971324196
Iteration 30: Loss = -11988.363970498665
Iteration 40: Loss = -11988.363970498933
1
Iteration 50: Loss = -11988.363970498933
2
Iteration 60: Loss = -11988.363970498933
3
Stopping early at iteration 60 due to no improvement.
pi: tensor([[0.4745, 0.5255],
        [0.4675, 0.5325]], dtype=torch.float64)
alpha: tensor([0.4748, 0.5252])
beta: tensor([[[2.9419e-01, 9.1726e-02],
         [6.5557e-01, 2.9542e-01]],

        [[6.2533e-01, 1.0282e-01],
         [2.3151e-01, 6.7536e-01]],

        [[5.1223e-01, 1.0207e-01],
         [5.0946e-01, 3.9205e-04]],

        [[7.5918e-01, 1.1673e-01],
         [9.8810e-01, 7.8780e-01]],

        [[9.0574e-01, 1.0065e-01],
         [4.7184e-01, 3.4779e-01]]], dtype=torch.float64)
time is 0
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 4
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
Global Adjusted Rand Index: 0.03341061394774113
Average Adjusted Rand Index: 0.9761612713656115
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20096.565376859948
Iteration 100: Loss = -12433.592086217519
Iteration 200: Loss = -12433.322955865759
Iteration 300: Loss = -12433.215018616695
Iteration 400: Loss = -12432.882183257623
Iteration 500: Loss = -12432.152474616882
Iteration 600: Loss = -12431.712644822454
Iteration 700: Loss = -12431.639615507482
Iteration 800: Loss = -12431.611624546718
Iteration 900: Loss = -12431.590382363975
Iteration 1000: Loss = -12431.569157489426
Iteration 1100: Loss = -12431.54587044034
Iteration 1200: Loss = -12431.52079594938
Iteration 1300: Loss = -12431.495149788683
Iteration 1400: Loss = -12431.47030717833
Iteration 1500: Loss = -12431.446863529745
Iteration 1600: Loss = -12431.423825477086
Iteration 1700: Loss = -12431.399264507929
Iteration 1800: Loss = -12431.369884768423
Iteration 1900: Loss = -12431.330106263631
Iteration 2000: Loss = -12431.27110301622
Iteration 2100: Loss = -12431.181169593374
Iteration 2200: Loss = -12431.043261687924
Iteration 2300: Loss = -12303.742378432955
Iteration 2400: Loss = -12054.727394510657
Iteration 2500: Loss = -11919.145760857167
Iteration 2600: Loss = -11919.070936796037
Iteration 2700: Loss = -11919.048265048936
Iteration 2800: Loss = -11919.025528583756
Iteration 2900: Loss = -11919.017470989904
Iteration 3000: Loss = -11918.856286828903
Iteration 3100: Loss = -11918.852567381666
Iteration 3200: Loss = -11918.849622614644
Iteration 3300: Loss = -11918.847458947943
Iteration 3400: Loss = -11918.849995750817
1
Iteration 3500: Loss = -11918.844207839624
Iteration 3600: Loss = -11918.842771177886
Iteration 3700: Loss = -11918.841094027899
Iteration 3800: Loss = -11918.833115853864
Iteration 3900: Loss = -11918.837061329074
1
Iteration 4000: Loss = -11918.830749687078
Iteration 4100: Loss = -11918.830860042068
1
Iteration 4200: Loss = -11918.8362747609
2
Iteration 4300: Loss = -11918.82952223062
Iteration 4400: Loss = -11918.839377353344
1
Iteration 4500: Loss = -11918.827526840969
Iteration 4600: Loss = -11918.82708988192
Iteration 4700: Loss = -11918.834536259792
1
Iteration 4800: Loss = -11918.823551758873
Iteration 4900: Loss = -11918.824139964223
1
Iteration 5000: Loss = -11918.822902509552
Iteration 5100: Loss = -11918.823257633732
1
Iteration 5200: Loss = -11918.822412274752
Iteration 5300: Loss = -11918.82612375318
1
Iteration 5400: Loss = -11918.822053301541
Iteration 5500: Loss = -11918.824209055094
1
Iteration 5600: Loss = -11918.821556154158
Iteration 5700: Loss = -11918.819052401419
Iteration 5800: Loss = -11918.818587371146
Iteration 5900: Loss = -11918.819062382912
1
Iteration 6000: Loss = -11918.818833753721
2
Iteration 6100: Loss = -11918.818054209123
Iteration 6200: Loss = -11918.811117167368
Iteration 6300: Loss = -11918.813996464667
1
Iteration 6400: Loss = -11918.801410278265
Iteration 6500: Loss = -11918.799816550287
Iteration 6600: Loss = -11918.804094795922
1
Iteration 6700: Loss = -11918.799991285303
2
Iteration 6800: Loss = -11918.799560972793
Iteration 6900: Loss = -11918.801403397385
1
Iteration 7000: Loss = -11918.799422607974
Iteration 7100: Loss = -11918.799422937322
1
Iteration 7200: Loss = -11918.799242340565
Iteration 7300: Loss = -11918.799460057215
1
Iteration 7400: Loss = -11918.795628391521
Iteration 7500: Loss = -11918.804057222735
1
Iteration 7600: Loss = -11918.792525066368
Iteration 7700: Loss = -11918.992035949752
1
Iteration 7800: Loss = -11918.798346000827
2
Iteration 7900: Loss = -11918.79237751207
Iteration 8000: Loss = -11918.79286141364
1
Iteration 8100: Loss = -11918.792282928753
Iteration 8200: Loss = -11918.797528578509
1
Iteration 8300: Loss = -11918.792215399108
Iteration 8400: Loss = -11918.792133031655
Iteration 8500: Loss = -11918.835456833007
1
Iteration 8600: Loss = -11918.792088352162
Iteration 8700: Loss = -11918.792076804448
Iteration 8800: Loss = -11918.792081970241
1
Iteration 8900: Loss = -11918.79238357762
2
Iteration 9000: Loss = -11918.811363700945
3
Iteration 9100: Loss = -11918.870524749873
4
Iteration 9200: Loss = -11918.793624970142
5
Stopping early at iteration 9200 due to no improvement.
pi: tensor([[0.8021, 0.1979],
        [0.2392, 0.7608]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5117, 0.4883], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2930, 0.0917],
         [0.5674, 0.3105]],

        [[0.5498, 0.1025],
         [0.5935, 0.5502]],

        [[0.6956, 0.1019],
         [0.6594, 0.6731]],

        [[0.7050, 0.1168],
         [0.7298, 0.5000]],

        [[0.6242, 0.1008],
         [0.5327, 0.6718]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 4
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
Global Adjusted Rand Index: 0.9760961872484464
Average Adjusted Rand Index: 0.9761612713656115
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21350.24523726881
Iteration 100: Loss = -12433.642315895659
Iteration 200: Loss = -12433.094664052978
Iteration 300: Loss = -12432.673500479255
Iteration 400: Loss = -12432.282991223363
Iteration 500: Loss = -12431.988361044183
Iteration 600: Loss = -12431.814932555922
Iteration 700: Loss = -12431.718020970085
Iteration 800: Loss = -12431.665106256458
Iteration 900: Loss = -12431.634456941874
Iteration 1000: Loss = -12431.612152348805
Iteration 1100: Loss = -12431.593424828767
Iteration 1200: Loss = -12431.576684058906
Iteration 1300: Loss = -12431.561124474363
Iteration 1400: Loss = -12431.54604320833
Iteration 1500: Loss = -12431.531164978976
Iteration 1600: Loss = -12431.516178000324
Iteration 1700: Loss = -12431.501034924699
Iteration 1800: Loss = -12431.48573877726
Iteration 1900: Loss = -12431.470196192844
Iteration 2000: Loss = -12431.454379450173
Iteration 2100: Loss = -12431.437973477605
Iteration 2200: Loss = -12431.420620059624
Iteration 2300: Loss = -12431.401635163362
Iteration 2400: Loss = -12431.379752008957
Iteration 2500: Loss = -12431.352804948001
Iteration 2600: Loss = -12431.317366017154
Iteration 2700: Loss = -12431.267864826968
Iteration 2800: Loss = -12431.19691090257
Iteration 2900: Loss = -12431.09456240703
Iteration 3000: Loss = -12430.82506078896
Iteration 3100: Loss = -12156.679394280161
Iteration 3200: Loss = -12062.794121944782
Iteration 3300: Loss = -11923.76382720294
Iteration 3400: Loss = -11918.929739823578
Iteration 3500: Loss = -11918.895826205226
Iteration 3600: Loss = -11918.881033349646
Iteration 3700: Loss = -11918.8531147716
Iteration 3800: Loss = -11918.846441132468
Iteration 3900: Loss = -11918.84167603322
Iteration 4000: Loss = -11918.838005521518
Iteration 4100: Loss = -11918.834406862972
Iteration 4200: Loss = -11918.83066766012
Iteration 4300: Loss = -11918.854484187506
1
Iteration 4400: Loss = -11918.825061640553
Iteration 4500: Loss = -11918.823554721921
Iteration 4600: Loss = -11918.822198748436
Iteration 4700: Loss = -11918.824280971592
1
Iteration 4800: Loss = -11918.820018245466
Iteration 4900: Loss = -11918.819260618042
Iteration 5000: Loss = -11918.818182837496
Iteration 5100: Loss = -11918.8170595736
Iteration 5200: Loss = -11918.816612487393
Iteration 5300: Loss = -11918.815745722837
Iteration 5400: Loss = -11918.81672955667
1
Iteration 5500: Loss = -11918.81458584145
Iteration 5600: Loss = -11918.814401911966
Iteration 5700: Loss = -11918.81337512992
Iteration 5800: Loss = -11918.814043423758
1
Iteration 5900: Loss = -11918.811858184054
Iteration 6000: Loss = -11918.809853407025
Iteration 6100: Loss = -11918.809981129223
1
Iteration 6200: Loss = -11918.81272586262
2
Iteration 6300: Loss = -11918.808093062245
Iteration 6400: Loss = -11918.813054030561
1
Iteration 6500: Loss = -11918.806840979541
Iteration 6600: Loss = -11918.807746853081
1
Iteration 6700: Loss = -11918.803670770805
Iteration 6800: Loss = -11918.805793404475
1
Iteration 6900: Loss = -11918.803385820138
Iteration 7000: Loss = -11918.824721310406
1
Iteration 7100: Loss = -11918.8031916931
Iteration 7200: Loss = -11918.80308162467
Iteration 7300: Loss = -11918.803731950507
1
Iteration 7400: Loss = -11918.80285501376
Iteration 7500: Loss = -11918.884453293704
1
Iteration 7600: Loss = -11918.802713773717
Iteration 7700: Loss = -11918.80262437519
Iteration 7800: Loss = -11918.803119258344
1
Iteration 7900: Loss = -11918.802451797017
Iteration 8000: Loss = -11918.802668625296
1
Iteration 8100: Loss = -11918.803186100955
2
Iteration 8200: Loss = -11918.80224871752
Iteration 8300: Loss = -11918.802300730047
1
Iteration 8400: Loss = -11918.801915580214
Iteration 8500: Loss = -11918.804294976782
1
Iteration 8600: Loss = -11918.79690060436
Iteration 8700: Loss = -11918.809382002642
1
Iteration 8800: Loss = -11918.795267288819
Iteration 8900: Loss = -11918.814517100493
1
Iteration 9000: Loss = -11918.795044033064
Iteration 9100: Loss = -11918.8172728683
1
Iteration 9200: Loss = -11918.795433248777
2
Iteration 9300: Loss = -11918.79577152007
3
Iteration 9400: Loss = -11918.805248802975
4
Iteration 9500: Loss = -11918.794242273114
Iteration 9600: Loss = -11918.793213110923
Iteration 9700: Loss = -11918.805932554322
1
Iteration 9800: Loss = -11918.791594236547
Iteration 9900: Loss = -11918.796161393373
1
Iteration 10000: Loss = -11918.797248219404
2
Iteration 10100: Loss = -11918.794306263546
3
Iteration 10200: Loss = -11918.808655053515
4
Iteration 10300: Loss = -11918.800601722658
5
Stopping early at iteration 10300 due to no improvement.
pi: tensor([[0.8023, 0.1977],
        [0.2392, 0.7608]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5025, 0.4975], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2930, 0.0917],
         [0.7230, 0.3105]],

        [[0.6336, 0.1026],
         [0.7154, 0.6529]],

        [[0.5953, 0.1019],
         [0.7230, 0.6111]],

        [[0.7143, 0.1167],
         [0.6315, 0.5573]],

        [[0.6414, 0.1007],
         [0.7107, 0.6380]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 4
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
Global Adjusted Rand Index: 0.9760961872484464
Average Adjusted Rand Index: 0.9761612713656115
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25000.452349177056
Iteration 100: Loss = -12248.873865465664
Iteration 200: Loss = -12134.598350107026
Iteration 300: Loss = -12128.914060439445
Iteration 400: Loss = -12128.726798154412
Iteration 500: Loss = -12128.633193876682
Iteration 600: Loss = -12128.589701434159
Iteration 700: Loss = -12128.558442511685
Iteration 800: Loss = -12128.467940075881
Iteration 900: Loss = -12128.45016160427
Iteration 1000: Loss = -12128.434113391468
Iteration 1100: Loss = -12128.4222257478
Iteration 1200: Loss = -12128.403776232846
Iteration 1300: Loss = -12128.190621317155
Iteration 1400: Loss = -12125.007349763895
Iteration 1500: Loss = -12124.988152885668
Iteration 1600: Loss = -12124.980136589207
Iteration 1700: Loss = -12124.97629121985
Iteration 1800: Loss = -12124.972245274716
Iteration 1900: Loss = -12124.961455996905
Iteration 2000: Loss = -12124.758461629346
Iteration 2100: Loss = -12121.392782664429
Iteration 2200: Loss = -12116.433975202648
Iteration 2300: Loss = -12116.41684074777
Iteration 2400: Loss = -12116.349413529026
Iteration 2500: Loss = -12115.726356303836
Iteration 2600: Loss = -12115.714707490011
Iteration 2700: Loss = -12115.617549194327
Iteration 2800: Loss = -12115.583437468094
Iteration 2900: Loss = -12115.525652101822
Iteration 3000: Loss = -12112.905641439314
Iteration 3100: Loss = -12112.525455040412
Iteration 3200: Loss = -12107.369993927856
Iteration 3300: Loss = -12102.94857724802
Iteration 3400: Loss = -12079.999288351895
Iteration 3500: Loss = -12071.769763842676
Iteration 3600: Loss = -12065.54432923445
Iteration 3700: Loss = -12061.559817431302
Iteration 3800: Loss = -12050.359952960374
Iteration 3900: Loss = -12050.357371684067
Iteration 4000: Loss = -12044.470918483868
Iteration 4100: Loss = -12031.997623228652
Iteration 4200: Loss = -12013.351014802698
Iteration 4300: Loss = -12013.34902820395
Iteration 4400: Loss = -11999.465390116917
Iteration 4500: Loss = -11986.2952163173
Iteration 4600: Loss = -11973.7132693623
Iteration 4700: Loss = -11973.706533860584
Iteration 4800: Loss = -11955.56687919743
Iteration 4900: Loss = -11955.537029846326
Iteration 5000: Loss = -11930.217679437419
Iteration 5100: Loss = -11930.110315298945
Iteration 5200: Loss = -11918.808386082132
Iteration 5300: Loss = -11918.802355791624
Iteration 5400: Loss = -11918.802087718
Iteration 5500: Loss = -11918.80187958313
Iteration 5600: Loss = -11918.808075588899
1
Iteration 5700: Loss = -11918.801334337057
Iteration 5800: Loss = -11918.803225055777
1
Iteration 5900: Loss = -11918.802513666322
2
Iteration 6000: Loss = -11918.809266126238
3
Iteration 6100: Loss = -11918.80043603032
Iteration 6200: Loss = -11918.800250909242
Iteration 6300: Loss = -11918.797366013992
Iteration 6400: Loss = -11918.797794674905
1
Iteration 6500: Loss = -11918.797814259984
2
Iteration 6600: Loss = -11918.797590449578
3
Iteration 6700: Loss = -11918.800105577226
4
Iteration 6800: Loss = -11918.797115998608
Iteration 6900: Loss = -11918.79743763229
1
Iteration 7000: Loss = -11918.797053576835
Iteration 7100: Loss = -11918.797085647366
1
Iteration 7200: Loss = -11918.799487881313
2
Iteration 7300: Loss = -11918.797015808575
Iteration 7400: Loss = -11918.798604291524
1
Iteration 7500: Loss = -11918.797838213397
2
Iteration 7600: Loss = -11918.796932772431
Iteration 7700: Loss = -11918.802670001056
1
Iteration 7800: Loss = -11918.797171641647
2
Iteration 7900: Loss = -11918.797432772462
3
Iteration 8000: Loss = -11918.798753148145
4
Iteration 8100: Loss = -11918.79676712207
Iteration 8200: Loss = -11918.79668616625
Iteration 8300: Loss = -11918.79674356353
1
Iteration 8400: Loss = -11918.796615502995
Iteration 8500: Loss = -11918.796598133451
Iteration 8600: Loss = -11918.79662066845
1
Iteration 8700: Loss = -11918.801915617805
2
Iteration 8800: Loss = -11918.796513196658
Iteration 8900: Loss = -11918.797321701311
1
Iteration 9000: Loss = -11918.796567277981
2
Iteration 9100: Loss = -11918.796580808248
3
Iteration 9200: Loss = -11918.822473757238
4
Iteration 9300: Loss = -11918.79042110809
Iteration 9400: Loss = -11918.835827177772
1
Iteration 9500: Loss = -11918.788914126028
Iteration 9600: Loss = -11918.793301964735
1
Iteration 9700: Loss = -11918.788897899023
Iteration 9800: Loss = -11918.788894676003
Iteration 9900: Loss = -11918.789745743648
1
Iteration 10000: Loss = -11918.960732241843
2
Iteration 10100: Loss = -11918.791227742218
3
Iteration 10200: Loss = -11918.79375171775
4
Iteration 10300: Loss = -11918.823921642683
5
Stopping early at iteration 10300 due to no improvement.
pi: tensor([[0.7590, 0.2410],
        [0.1974, 0.8026]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4898, 0.5102], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3114, 0.0917],
         [0.6621, 0.2919]],

        [[0.6636, 0.1026],
         [0.5026, 0.5123]],

        [[0.5864, 0.1018],
         [0.6944, 0.5298]],

        [[0.7205, 0.1159],
         [0.5466, 0.6636]],

        [[0.6333, 0.1004],
         [0.6819, 0.5463]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 2
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 4
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
Global Adjusted Rand Index: 0.9760961872484464
Average Adjusted Rand Index: 0.9761612713656115
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20816.36207872337
Iteration 100: Loss = -12433.885838251525
Iteration 200: Loss = -12433.221889651239
Iteration 300: Loss = -12432.9189999271
Iteration 400: Loss = -12432.663040646556
Iteration 500: Loss = -12432.390743815107
Iteration 600: Loss = -12432.094232462883
Iteration 700: Loss = -12431.875565081144
Iteration 800: Loss = -12431.757149013287
Iteration 900: Loss = -12431.695268027686
Iteration 1000: Loss = -12431.660528281125
Iteration 1100: Loss = -12431.637625164347
Iteration 1200: Loss = -12431.619827958684
Iteration 1300: Loss = -12431.604792613234
Iteration 1400: Loss = -12431.591304979518
Iteration 1500: Loss = -12431.578716008615
Iteration 1600: Loss = -12431.566425134444
Iteration 1700: Loss = -12431.554040926885
Iteration 1800: Loss = -12431.54137521247
Iteration 1900: Loss = -12431.528268967855
Iteration 2000: Loss = -12431.514730499199
Iteration 2100: Loss = -12431.500762567634
Iteration 2200: Loss = -12431.486500411314
Iteration 2300: Loss = -12431.472018425808
Iteration 2400: Loss = -12431.457113357737
Iteration 2500: Loss = -12431.441696154061
Iteration 2600: Loss = -12431.425606112121
Iteration 2700: Loss = -12431.408087235684
Iteration 2800: Loss = -12431.388221040377
Iteration 2900: Loss = -12431.364363722638
Iteration 3000: Loss = -12431.333824358202
Iteration 3100: Loss = -12431.291945651577
Iteration 3200: Loss = -12431.232199941813
Iteration 3300: Loss = -12431.145767283817
Iteration 3400: Loss = -12431.0082008626
Iteration 3500: Loss = -12299.805178377377
Iteration 3600: Loss = -12116.76770422882
Iteration 3700: Loss = -11982.051742082562
Iteration 3800: Loss = -11923.550785915488
Iteration 3900: Loss = -11918.92529859656
Iteration 4000: Loss = -11918.900722255134
Iteration 4100: Loss = -11918.888754203952
Iteration 4200: Loss = -11918.875471134881
Iteration 4300: Loss = -11918.856124203468
Iteration 4400: Loss = -11918.850928779982
Iteration 4500: Loss = -11918.841215794755
Iteration 4600: Loss = -11918.838188570759
Iteration 4700: Loss = -11918.835580580419
Iteration 4800: Loss = -11918.832943357234
Iteration 4900: Loss = -11918.835254260026
1
Iteration 5000: Loss = -11918.828046939894
Iteration 5100: Loss = -11918.826655855353
Iteration 5200: Loss = -11918.825587727888
Iteration 5300: Loss = -11918.824619117731
Iteration 5400: Loss = -11918.823814001445
Iteration 5500: Loss = -11918.824014898459
1
Iteration 5600: Loss = -11918.822763957229
Iteration 5700: Loss = -11918.821949225483
Iteration 5800: Loss = -11918.82238502909
1
Iteration 5900: Loss = -11918.821882720693
Iteration 6000: Loss = -11918.820388213617
Iteration 6100: Loss = -11918.82012842074
Iteration 6200: Loss = -11918.824227998177
1
Iteration 6300: Loss = -11918.815898667921
Iteration 6400: Loss = -11918.813114081222
Iteration 6500: Loss = -11918.812867734465
Iteration 6600: Loss = -11918.812581907216
Iteration 6700: Loss = -11918.813395703612
1
Iteration 6800: Loss = -11918.812147650151
Iteration 6900: Loss = -11918.86373042974
1
Iteration 7000: Loss = -11918.811512909278
Iteration 7100: Loss = -11918.844037438748
1
Iteration 7200: Loss = -11918.813924013568
2
Iteration 7300: Loss = -11918.813461943717
3
Iteration 7400: Loss = -11918.811373796549
Iteration 7500: Loss = -11918.938944114576
1
Iteration 7600: Loss = -11918.807167014644
Iteration 7700: Loss = -11918.819312907117
1
Iteration 7800: Loss = -11918.806991670499
Iteration 7900: Loss = -11918.807080157105
1
Iteration 8000: Loss = -11918.806675660679
Iteration 8100: Loss = -11918.806102415816
Iteration 8200: Loss = -11918.803929883543
Iteration 8300: Loss = -11918.803802171646
Iteration 8400: Loss = -11918.80554464942
1
Iteration 8500: Loss = -11918.803530651989
Iteration 8600: Loss = -11918.80344049485
Iteration 8700: Loss = -11918.804277916699
1
Iteration 8800: Loss = -11918.803264636423
Iteration 8900: Loss = -11918.803701955303
1
Iteration 9000: Loss = -11918.803483989475
2
Iteration 9100: Loss = -11918.803541821442
3
Iteration 9200: Loss = -11918.803773584817
4
Iteration 9300: Loss = -11918.796528608795
Iteration 9400: Loss = -11918.797829820767
1
Iteration 9500: Loss = -11918.799896698401
2
Iteration 9600: Loss = -11918.796339799148
Iteration 9700: Loss = -11918.796225916636
Iteration 9800: Loss = -11918.796094042404
Iteration 9900: Loss = -11918.85241633771
1
Iteration 10000: Loss = -11918.865763142712
2
Iteration 10100: Loss = -11918.801916501407
3
Iteration 10200: Loss = -11918.813166697433
4
Iteration 10300: Loss = -11918.822853966745
5
Stopping early at iteration 10300 due to no improvement.
pi: tensor([[0.8039, 0.1961],
        [0.2400, 0.7600]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5098, 0.4902], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2920, 0.0922],
         [0.5667, 0.3116]],

        [[0.5542, 0.1024],
         [0.6402, 0.7277]],

        [[0.5894, 0.1023],
         [0.7095, 0.5324]],

        [[0.5728, 0.1167],
         [0.5252, 0.7235]],

        [[0.6889, 0.1009],
         [0.6193, 0.6991]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 2
tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 4
tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
Global Adjusted Rand Index: 0.9760961872484464
Average Adjusted Rand Index: 0.9761612713656115
11932.820331775833
[0.9760961872484464, 0.9760961872484464, 0.9760961872484464, 0.9760961872484464] [0.9761612713656115, 0.9761612713656115, 0.9761612713656115, 0.9761612713656115] [11918.793624970142, 11918.800601722658, 11918.823921642683, 11918.822853966745]
-----------------------------------------------------------------------------------------
This iteration is 15
True Objective function: Loss = -11736.411779745691
Iteration 0: Loss = -12253.830904997201
Iteration 10: Loss = -12253.830904997201
1
Iteration 20: Loss = -12253.830904997201
2
Iteration 30: Loss = -12253.830904997201
3
Stopping early at iteration 30 due to no improvement.
pi: tensor([[1.0000e+00, 1.3933e-17],
        [1.0000e+00, 1.4410e-30]], dtype=torch.float64)
alpha: tensor([1.0000e+00, 1.3597e-17])
beta: tensor([[[0.1942, 0.1846],
         [0.8770, 0.2530]],

        [[0.1467, 0.2527],
         [0.4185, 0.2152]],

        [[0.3498, 0.2510],
         [0.4562, 0.5268]],

        [[0.5070, 0.2189],
         [0.0701, 0.8036]],

        [[0.4935, 0.3004],
         [0.2870, 0.3112]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -12518.188868424393
Iteration 10: Loss = -12253.314825129457
Iteration 20: Loss = -12253.31071163462
Iteration 30: Loss = -12253.308937128597
Iteration 40: Loss = -12253.308180833114
Iteration 50: Loss = -12253.307874756367
Iteration 60: Loss = -12253.30783896966
Iteration 70: Loss = -12253.307796282481
Iteration 80: Loss = -12253.30784994914
1
Iteration 90: Loss = -12253.30786372788
2
Iteration 100: Loss = -12253.30790809759
3
Stopping early at iteration 100 due to no improvement.
pi: tensor([[0.2608, 0.7392],
        [0.4475, 0.5525]], dtype=torch.float64)
alpha: tensor([0.3771, 0.6229])
beta: tensor([[[0.1936, 0.1922],
         [0.0976, 0.1946]],

        [[0.2444, 0.1964],
         [0.7032, 0.6669]],

        [[0.7667, 0.1949],
         [0.3542, 0.5058]],

        [[0.3485, 0.1991],
         [0.3802, 0.8958]],

        [[0.1763, 0.1879],
         [0.2872, 0.1010]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23235.84519609468
Iteration 100: Loss = -11802.604635252252
Iteration 200: Loss = -11728.491781076526
Iteration 300: Loss = -11728.00275882206
Iteration 400: Loss = -11727.811241997573
Iteration 500: Loss = -11727.712120779663
Iteration 600: Loss = -11727.653024232925
Iteration 700: Loss = -11727.614558909201
Iteration 800: Loss = -11727.587913176036
Iteration 900: Loss = -11727.56860787215
Iteration 1000: Loss = -11727.554117060816
Iteration 1100: Loss = -11727.542933279405
Iteration 1200: Loss = -11727.534176344612
Iteration 1300: Loss = -11727.527035125622
Iteration 1400: Loss = -11727.521270878218
Iteration 1500: Loss = -11727.516465969164
Iteration 1600: Loss = -11727.512435532466
Iteration 1700: Loss = -11727.508992149653
Iteration 1800: Loss = -11727.506012928467
Iteration 1900: Loss = -11727.503502842326
Iteration 2000: Loss = -11727.501239726
Iteration 2100: Loss = -11727.499268096995
Iteration 2200: Loss = -11727.497555563907
Iteration 2300: Loss = -11727.496085571898
Iteration 2400: Loss = -11727.494733418058
Iteration 2500: Loss = -11727.493604354146
Iteration 2600: Loss = -11727.494118434746
1
Iteration 2700: Loss = -11727.491668568484
Iteration 2800: Loss = -11727.490760308068
Iteration 2900: Loss = -11727.490056051362
Iteration 3000: Loss = -11727.489415061196
Iteration 3100: Loss = -11727.489234080676
Iteration 3200: Loss = -11727.496571191068
1
Iteration 3300: Loss = -11727.487653882308
Iteration 3400: Loss = -11727.491847858335
1
Iteration 3500: Loss = -11727.486763199116
Iteration 3600: Loss = -11727.487640204643
1
Iteration 3700: Loss = -11727.4859804209
Iteration 3800: Loss = -11727.486715839674
1
Iteration 3900: Loss = -11727.485358892727
Iteration 4000: Loss = -11727.485043043089
Iteration 4100: Loss = -11727.484948220368
Iteration 4200: Loss = -11727.48459091736
Iteration 4300: Loss = -11727.484942951469
1
Iteration 4400: Loss = -11727.484129384382
Iteration 4500: Loss = -11727.485350901954
1
Iteration 4600: Loss = -11727.4837574333
Iteration 4700: Loss = -11727.483630014456
Iteration 4800: Loss = -11727.483415694218
Iteration 4900: Loss = -11727.485477650862
1
Iteration 5000: Loss = -11727.483109685883
Iteration 5100: Loss = -11727.484576516303
1
Iteration 5200: Loss = -11727.482904166918
Iteration 5300: Loss = -11727.482791345648
Iteration 5400: Loss = -11727.484051870819
1
Iteration 5500: Loss = -11727.482566120809
Iteration 5600: Loss = -11727.483900035273
1
Iteration 5700: Loss = -11727.491356155771
2
Iteration 5800: Loss = -11727.48394684392
3
Iteration 5900: Loss = -11727.482233779268
Iteration 6000: Loss = -11727.482443914785
1
Iteration 6100: Loss = -11727.482116725614
Iteration 6200: Loss = -11727.492255739611
1
Iteration 6300: Loss = -11727.48196141741
Iteration 6400: Loss = -11727.482351018494
1
Iteration 6500: Loss = -11727.488446035502
2
Iteration 6600: Loss = -11727.481860720205
Iteration 6700: Loss = -11727.482110591183
1
Iteration 6800: Loss = -11727.482189680795
2
Iteration 6900: Loss = -11727.481826965206
Iteration 7000: Loss = -11727.481657803846
Iteration 7100: Loss = -11727.483081352002
1
Iteration 7200: Loss = -11727.486585404447
2
Iteration 7300: Loss = -11727.485559487424
3
Iteration 7400: Loss = -11727.48715457973
4
Iteration 7500: Loss = -11727.483763765946
5
Stopping early at iteration 7500 due to no improvement.
pi: tensor([[0.7085, 0.2915],
        [0.2311, 0.7689]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5088, 0.4912], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2926, 0.0965],
         [0.5946, 0.2985]],

        [[0.6175, 0.0965],
         [0.6543, 0.5347]],

        [[0.5772, 0.0965],
         [0.6767, 0.6712]],

        [[0.7050, 0.0989],
         [0.6003, 0.5968]],

        [[0.6570, 0.0915],
         [0.6708, 0.6637]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
Global Adjusted Rand Index: 0.9760956718354579
Average Adjusted Rand Index: 0.9761604351641244
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24000.533079659337
Iteration 100: Loss = -12254.807697382164
Iteration 200: Loss = -12253.984862888932
Iteration 300: Loss = -12253.752745277154
Iteration 400: Loss = -12253.669939547523
Iteration 500: Loss = -12253.628481711467
Iteration 600: Loss = -12253.601395013966
Iteration 700: Loss = -12253.579017930344
Iteration 800: Loss = -12253.557113135923
Iteration 900: Loss = -12253.53360038462
Iteration 1000: Loss = -12253.507027021664
Iteration 1100: Loss = -12253.476590989774
Iteration 1200: Loss = -12253.44211433649
Iteration 1300: Loss = -12253.404474471716
Iteration 1400: Loss = -12253.366218574907
Iteration 1500: Loss = -12253.331337351126
Iteration 1600: Loss = -12253.303149594665
Iteration 1700: Loss = -12253.281867306107
Iteration 1800: Loss = -12253.265993110928
Iteration 1900: Loss = -12253.253855427705
Iteration 2000: Loss = -12253.244440251974
Iteration 2100: Loss = -12253.236911510638
Iteration 2200: Loss = -12253.230912077466
Iteration 2300: Loss = -12253.226078873422
Iteration 2400: Loss = -12253.222042838408
Iteration 2500: Loss = -12253.21883629196
Iteration 2600: Loss = -12253.21620622522
Iteration 2700: Loss = -12253.214087121107
Iteration 2800: Loss = -12253.21236284481
Iteration 2900: Loss = -12253.21095911618
Iteration 3000: Loss = -12253.209764523162
Iteration 3100: Loss = -12253.208877287003
Iteration 3200: Loss = -12253.208052582213
Iteration 3300: Loss = -12253.207369324802
Iteration 3400: Loss = -12253.206755535048
Iteration 3500: Loss = -12253.206270979845
Iteration 3600: Loss = -12253.205797377417
Iteration 3700: Loss = -12253.205302055676
Iteration 3800: Loss = -12253.204872829938
Iteration 3900: Loss = -12253.204435876261
Iteration 4000: Loss = -12253.204007722627
Iteration 4100: Loss = -12253.20353452332
Iteration 4200: Loss = -12253.203037472711
Iteration 4300: Loss = -12253.2025437147
Iteration 4400: Loss = -12253.202037849098
Iteration 4500: Loss = -12253.201512805004
Iteration 4600: Loss = -12253.200919342235
Iteration 4700: Loss = -12253.200313220728
Iteration 4800: Loss = -12253.199668303987
Iteration 4900: Loss = -12253.198962767408
Iteration 5000: Loss = -12253.19827315329
Iteration 5100: Loss = -12253.197540052885
Iteration 5200: Loss = -12253.196790888527
Iteration 5300: Loss = -12253.19601982889
Iteration 5400: Loss = -12253.19525611434
Iteration 5500: Loss = -12253.194418896253
Iteration 5600: Loss = -12253.193639155119
Iteration 5700: Loss = -12253.192805125243
Iteration 5800: Loss = -12253.192017571788
Iteration 5900: Loss = -12253.191229961933
Iteration 6000: Loss = -12253.19044953123
Iteration 6100: Loss = -12253.189671404161
Iteration 6200: Loss = -12253.188959331197
Iteration 6300: Loss = -12253.188204917908
Iteration 6400: Loss = -12253.188888487892
1
Iteration 6500: Loss = -12253.186839443735
Iteration 6600: Loss = -12253.186191899196
Iteration 6700: Loss = -12253.185630256483
Iteration 6800: Loss = -12253.194508562858
1
Iteration 6900: Loss = -12253.232730693817
2
Iteration 7000: Loss = -12253.183990472404
Iteration 7100: Loss = -12253.183821268105
Iteration 7200: Loss = -12253.307694383424
1
Iteration 7300: Loss = -12253.182588201076
Iteration 7400: Loss = -12253.236327272894
1
Iteration 7500: Loss = -12253.18176136631
Iteration 7600: Loss = -12253.181411774423
Iteration 7700: Loss = -12253.181261819069
Iteration 7800: Loss = -12253.180716912688
Iteration 7900: Loss = -12253.180438338579
Iteration 8000: Loss = -12253.181187178978
1
Iteration 8100: Loss = -12253.179877719103
Iteration 8200: Loss = -12253.179617756956
Iteration 8300: Loss = -12253.181165626094
1
Iteration 8400: Loss = -12253.179161480117
Iteration 8500: Loss = -12253.178937275561
Iteration 8600: Loss = -12253.782270161011
1
Iteration 8700: Loss = -12253.178556595209
Iteration 8800: Loss = -12253.178363236048
Iteration 8900: Loss = -12253.17829163048
Iteration 9000: Loss = -12253.178042515568
Iteration 9100: Loss = -12253.177880423627
Iteration 9200: Loss = -12253.177759324826
Iteration 9300: Loss = -12253.178538079917
1
Iteration 9400: Loss = -12253.177522752441
Iteration 9500: Loss = -12253.177423945035
Iteration 9600: Loss = -12253.190189855884
1
Iteration 9700: Loss = -12253.177165667359
Iteration 9800: Loss = -12253.177083805824
Iteration 9900: Loss = -12253.17699556482
Iteration 10000: Loss = -12253.177076460726
1
Iteration 10100: Loss = -12253.176835820772
Iteration 10200: Loss = -12253.176788330904
Iteration 10300: Loss = -12253.176709164301
Iteration 10400: Loss = -12253.17660050827
Iteration 10500: Loss = -12253.176571624786
Iteration 10600: Loss = -12253.176578428409
1
Iteration 10700: Loss = -12253.176455001942
Iteration 10800: Loss = -12253.176365579744
Iteration 10900: Loss = -12253.204036628784
1
Iteration 11000: Loss = -12253.17631550191
Iteration 11100: Loss = -12253.176237763348
Iteration 11200: Loss = -12253.1761846494
Iteration 11300: Loss = -12253.176160042227
Iteration 11400: Loss = -12253.176098806263
Iteration 11500: Loss = -12253.176078834265
Iteration 11600: Loss = -12253.17606221552
Iteration 11700: Loss = -12253.17599150355
Iteration 11800: Loss = -12253.17595069042
Iteration 11900: Loss = -12253.220386471008
1
Iteration 12000: Loss = -12253.175884632272
Iteration 12100: Loss = -12253.175860961224
Iteration 12200: Loss = -12253.457041016844
1
Iteration 12300: Loss = -12253.17582362351
Iteration 12400: Loss = -12253.175819726881
Iteration 12500: Loss = -12253.175771978573
Iteration 12600: Loss = -12253.17582884032
1
Iteration 12700: Loss = -12253.175789186482
2
Iteration 12800: Loss = -12253.17575047697
Iteration 12900: Loss = -12253.176094920802
1
Iteration 13000: Loss = -12253.175722901911
Iteration 13100: Loss = -12253.175688535759
Iteration 13200: Loss = -12253.175928196657
1
Iteration 13300: Loss = -12253.175612753432
Iteration 13400: Loss = -12253.175657190495
1
Iteration 13500: Loss = -12253.175742364341
2
Iteration 13600: Loss = -12253.175689333602
3
Iteration 13700: Loss = -12253.175649663459
4
Iteration 13800: Loss = -12253.175962049667
5
Stopping early at iteration 13800 due to no improvement.
pi: tensor([[2.9202e-04, 9.9971e-01],
        [2.2140e-02, 9.7786e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([3.8363e-04, 9.9962e-01], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2876, 0.1944],
         [0.5712, 0.1944]],

        [[0.6357, 0.2394],
         [0.6863, 0.7011]],

        [[0.6374, 0.2476],
         [0.6034, 0.6923]],

        [[0.5629, 0.2234],
         [0.6047, 0.6597]],

        [[0.6328, 0.2784],
         [0.6717, 0.6982]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.002264732809714258
Global Adjusted Rand Index: -0.0006470288592156272
Average Adjusted Rand Index: -0.0004529465619428516
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22999.24103225927
Iteration 100: Loss = -12254.30360940877
Iteration 200: Loss = -12253.765731920887
Iteration 300: Loss = -12253.661386216723
Iteration 400: Loss = -12253.608515990689
Iteration 500: Loss = -12253.573220236362
Iteration 600: Loss = -12253.54577885406
Iteration 700: Loss = -12253.522266842616
Iteration 800: Loss = -12253.500578393501
Iteration 900: Loss = -12253.47942853912
Iteration 1000: Loss = -12253.457750674235
Iteration 1100: Loss = -12253.435009928124
Iteration 1200: Loss = -12253.411011912409
Iteration 1300: Loss = -12253.386013036514
Iteration 1400: Loss = -12253.360544366395
Iteration 1500: Loss = -12253.334867220983
Iteration 1600: Loss = -12253.309268732393
Iteration 1700: Loss = -12253.283561753593
Iteration 1800: Loss = -12253.257329873899
Iteration 1900: Loss = -12253.230581713617
Iteration 2000: Loss = -12253.20332394081
Iteration 2100: Loss = -12253.176045357442
Iteration 2200: Loss = -12253.149332047218
Iteration 2300: Loss = -12253.123761395791
Iteration 2400: Loss = -12253.100003747062
Iteration 2500: Loss = -12253.078558911158
Iteration 2600: Loss = -12253.059923874249
Iteration 2700: Loss = -12253.04409408166
Iteration 2800: Loss = -12253.030935003631
Iteration 2900: Loss = -12253.020206891873
Iteration 3000: Loss = -12253.011366429306
Iteration 3100: Loss = -12253.004103440211
Iteration 3200: Loss = -12252.998009119581
Iteration 3300: Loss = -12252.992781095812
Iteration 3400: Loss = -12252.988290906625
Iteration 3500: Loss = -12252.984262958316
Iteration 3600: Loss = -12252.98232673813
Iteration 3700: Loss = -12252.977483394037
Iteration 3800: Loss = -12252.974390188912
Iteration 3900: Loss = -12252.971556493248
Iteration 4000: Loss = -12252.968592268779
Iteration 4100: Loss = -12252.965827055255
Iteration 4200: Loss = -12252.96274859773
Iteration 4300: Loss = -12252.959723230824
Iteration 4400: Loss = -12252.956276801298
Iteration 4500: Loss = -12252.95276282349
Iteration 4600: Loss = -12252.94851838324
Iteration 4700: Loss = -12252.944246063294
Iteration 4800: Loss = -12252.939114534836
Iteration 4900: Loss = -12252.93502009008
Iteration 5000: Loss = -12252.928223028353
Iteration 5100: Loss = -12252.921940996057
Iteration 5200: Loss = -12252.915366944351
Iteration 5300: Loss = -12252.906636675141
Iteration 5400: Loss = -12252.909481025943
1
Iteration 5500: Loss = -12252.868994659604
Iteration 5600: Loss = -12252.800287825577
Iteration 5700: Loss = -12252.54936940749
Iteration 5800: Loss = -12247.251290754813
Iteration 5900: Loss = -12246.484481284542
Iteration 6000: Loss = -12246.351160462795
Iteration 6100: Loss = -12246.11366957431
Iteration 6200: Loss = -12246.037406436184
Iteration 6300: Loss = -12246.011039935625
Iteration 6400: Loss = -12245.997417698549
Iteration 6500: Loss = -12245.989117807057
Iteration 6600: Loss = -12245.983461632683
Iteration 6700: Loss = -12245.979390614213
Iteration 6800: Loss = -12245.976378677122
Iteration 6900: Loss = -12245.97394925818
Iteration 7000: Loss = -12245.980583272658
1
Iteration 7100: Loss = -12245.970518908805
Iteration 7200: Loss = -12245.96915184435
Iteration 7300: Loss = -12245.968093278912
Iteration 7400: Loss = -12245.974510043385
1
Iteration 7500: Loss = -12245.966317282438
Iteration 7600: Loss = -12245.965636895027
Iteration 7700: Loss = -12245.965008853054
Iteration 7800: Loss = -12245.964451007958
Iteration 7900: Loss = -12245.963971947453
Iteration 8000: Loss = -12245.963499060754
Iteration 8100: Loss = -12245.970154376733
1
Iteration 8200: Loss = -12245.962771306293
Iteration 8300: Loss = -12245.962447684446
Iteration 8400: Loss = -12245.962150453193
Iteration 8500: Loss = -12245.962058410792
Iteration 8600: Loss = -12245.961650603385
Iteration 8700: Loss = -12245.966303402378
1
Iteration 8800: Loss = -12245.986243130734
2
Iteration 8900: Loss = -12245.961240917502
Iteration 9000: Loss = -12245.960892069766
Iteration 9100: Loss = -12245.961055185748
1
Iteration 9200: Loss = -12245.960598081267
Iteration 9300: Loss = -12245.960415775664
Iteration 9400: Loss = -12245.960407671157
Iteration 9500: Loss = -12245.960202905953
Iteration 9600: Loss = -12245.960121670647
Iteration 9700: Loss = -12245.960577091908
1
Iteration 9800: Loss = -12245.95986236218
Iteration 9900: Loss = -12245.95981597985
Iteration 10000: Loss = -12245.959804946615
Iteration 10100: Loss = -12245.95963190698
Iteration 10200: Loss = -12245.959596432633
Iteration 10300: Loss = -12245.971240355228
1
Iteration 10400: Loss = -12245.959427843885
Iteration 10500: Loss = -12245.96127515306
1
Iteration 10600: Loss = -12245.959426906265
Iteration 10700: Loss = -12245.959747329724
1
Iteration 10800: Loss = -12245.961494361993
2
Iteration 10900: Loss = -12246.006426438813
3
Iteration 11000: Loss = -12245.959132552402
Iteration 11100: Loss = -12245.959900790927
1
Iteration 11200: Loss = -12245.959025844872
Iteration 11300: Loss = -12245.959171965127
1
Iteration 11400: Loss = -12245.959222831136
2
Iteration 11500: Loss = -12245.99430119598
3
Iteration 11600: Loss = -12245.958897092001
Iteration 11700: Loss = -12246.01812639382
1
Iteration 11800: Loss = -12245.958921301752
2
Iteration 11900: Loss = -12245.978811977038
3
Iteration 12000: Loss = -12245.958787026733
Iteration 12100: Loss = -12245.975732999583
1
Iteration 12200: Loss = -12245.958748096478
Iteration 12300: Loss = -12245.97309650053
1
Iteration 12400: Loss = -12245.958688469926
Iteration 12500: Loss = -12245.95993826932
1
Iteration 12600: Loss = -12245.95868025247
Iteration 12700: Loss = -12246.018636927265
1
Iteration 12800: Loss = -12245.958630570742
Iteration 12900: Loss = -12245.959004421093
1
Iteration 13000: Loss = -12245.958585960678
Iteration 13100: Loss = -12245.95855828715
Iteration 13200: Loss = -12246.032436152516
1
Iteration 13300: Loss = -12245.958549644993
Iteration 13400: Loss = -12245.958516796865
Iteration 13500: Loss = -12245.980898710555
1
Iteration 13600: Loss = -12245.958517478986
2
Iteration 13700: Loss = -12245.958487469428
Iteration 13800: Loss = -12246.029984093013
1
Iteration 13900: Loss = -12245.958492447853
2
Iteration 14000: Loss = -12245.958449692791
Iteration 14100: Loss = -12246.06858942715
1
Iteration 14200: Loss = -12245.958514056436
2
Iteration 14300: Loss = -12245.983658681847
3
Iteration 14400: Loss = -12245.958453703057
4
Iteration 14500: Loss = -12245.968971730728
5
Stopping early at iteration 14500 due to no improvement.
pi: tensor([[1.4311e-07, 1.0000e+00],
        [1.0000e+00, 1.4073e-07]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9896, 0.0104], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1940, 0.0922],
         [0.6334, 0.2024]],

        [[0.5901, 0.1513],
         [0.5917, 0.7157]],

        [[0.5150, 0.2829],
         [0.5322, 0.7008]],

        [[0.7181, 0.0839],
         [0.5073, 0.7241]],

        [[0.6524, 0.1927],
         [0.6824, 0.5144]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.003478765038411364
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 59
Adjusted Rand Index: 0.007614171548597778
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 41
Adjusted Rand Index: 0.007614171548597778
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0025538476364862913
Global Adjusted Rand Index: -0.002198450369242598
Average Adjusted Rand Index: 0.004095268126764961
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22103.044267696274
Iteration 100: Loss = -12254.51318491645
Iteration 200: Loss = -12253.777980846773
Iteration 300: Loss = -12253.643992066412
Iteration 400: Loss = -12253.597666043413
Iteration 500: Loss = -12253.570761367666
Iteration 600: Loss = -12253.55049058193
Iteration 700: Loss = -12253.532942327009
Iteration 800: Loss = -12253.516365392928
Iteration 900: Loss = -12253.499692660185
Iteration 1000: Loss = -12253.481850797645
Iteration 1100: Loss = -12253.462237481732
Iteration 1200: Loss = -12253.440062904496
Iteration 1300: Loss = -12253.415026814746
Iteration 1400: Loss = -12253.38686133561
Iteration 1500: Loss = -12253.355755819432
Iteration 1600: Loss = -12253.322829215906
Iteration 1700: Loss = -12253.29014143087
Iteration 1800: Loss = -12253.259493728945
Iteration 1900: Loss = -12253.231274109381
Iteration 2000: Loss = -12253.204883600652
Iteration 2100: Loss = -12253.179473478607
Iteration 2200: Loss = -12253.154864170992
Iteration 2300: Loss = -12253.131035205159
Iteration 2400: Loss = -12253.108577624327
Iteration 2500: Loss = -12253.087927886214
Iteration 2600: Loss = -12253.06938460177
Iteration 2700: Loss = -12253.053166569072
Iteration 2800: Loss = -12253.039232035222
Iteration 2900: Loss = -12253.027619952567
Iteration 3000: Loss = -12253.018069706675
Iteration 3100: Loss = -12253.01015768244
Iteration 3200: Loss = -12253.003594539161
Iteration 3300: Loss = -12252.998001533024
Iteration 3400: Loss = -12252.993204615308
Iteration 3500: Loss = -12252.988993936244
Iteration 3600: Loss = -12252.985264206138
Iteration 3700: Loss = -12252.981820700452
Iteration 3800: Loss = -12252.978776503463
Iteration 3900: Loss = -12252.9757416141
Iteration 4000: Loss = -12252.973950900228
Iteration 4100: Loss = -12252.970063397637
Iteration 4200: Loss = -12252.96720623433
Iteration 4300: Loss = -12252.964354049227
Iteration 4400: Loss = -12252.96113032536
Iteration 4500: Loss = -12252.95793522603
Iteration 4600: Loss = -12252.954155915513
Iteration 4700: Loss = -12252.950293471367
Iteration 4800: Loss = -12252.945705597209
Iteration 4900: Loss = -12252.941075254506
Iteration 5000: Loss = -12252.935637857378
Iteration 5100: Loss = -12252.930234891412
Iteration 5200: Loss = -12252.924132306483
Iteration 5300: Loss = -12252.917775719752
Iteration 5400: Loss = -12252.90977659029
Iteration 5500: Loss = -12252.899151032516
Iteration 5600: Loss = -12252.880471904602
Iteration 5700: Loss = -12252.834983689523
Iteration 5800: Loss = -12252.69357512739
Iteration 5900: Loss = -12249.698519734566
Iteration 6000: Loss = -12246.532639626132
Iteration 6100: Loss = -12246.382039711732
Iteration 6200: Loss = -12246.13164112299
Iteration 6300: Loss = -12246.03992007426
Iteration 6400: Loss = -12246.011426361682
Iteration 6500: Loss = -12245.997232010814
Iteration 6600: Loss = -12245.988686544355
Iteration 6700: Loss = -12245.982998328785
Iteration 6800: Loss = -12245.978943392192
Iteration 6900: Loss = -12245.975934991991
Iteration 7000: Loss = -12245.973538286324
Iteration 7100: Loss = -12245.971642125978
Iteration 7200: Loss = -12245.970128940042
Iteration 7300: Loss = -12245.968869694421
Iteration 7400: Loss = -12245.967775617653
Iteration 7500: Loss = -12245.966864093758
Iteration 7600: Loss = -12246.222613714472
1
Iteration 7700: Loss = -12245.965411560943
Iteration 7800: Loss = -12245.964785633087
Iteration 7900: Loss = -12245.964265305061
Iteration 8000: Loss = -12245.995310592625
1
Iteration 8100: Loss = -12245.963365095648
Iteration 8200: Loss = -12246.048744779706
1
Iteration 8300: Loss = -12245.962629508282
Iteration 8400: Loss = -12245.962539751885
Iteration 8500: Loss = -12245.96274486368
1
Iteration 8600: Loss = -12245.961781462396
Iteration 8700: Loss = -12245.962756537967
1
Iteration 8800: Loss = -12245.96155123833
Iteration 8900: Loss = -12245.961160432222
Iteration 9000: Loss = -12245.962819921233
1
Iteration 9100: Loss = -12245.960806689203
Iteration 9200: Loss = -12245.960628648034
Iteration 9300: Loss = -12245.961003582064
1
Iteration 9400: Loss = -12245.960374125676
Iteration 9500: Loss = -12245.968063811124
1
Iteration 9600: Loss = -12245.960195392032
Iteration 9700: Loss = -12245.960029113694
Iteration 9800: Loss = -12245.994381115504
1
Iteration 9900: Loss = -12245.959862864082
Iteration 10000: Loss = -12245.95975516462
Iteration 10100: Loss = -12245.966638096075
1
Iteration 10200: Loss = -12245.959585625586
Iteration 10300: Loss = -12245.959535765884
Iteration 10400: Loss = -12245.98218646133
1
Iteration 10500: Loss = -12245.959387274528
Iteration 10600: Loss = -12245.959315144308
Iteration 10700: Loss = -12245.95987371799
1
Iteration 10800: Loss = -12245.95924490669
Iteration 10900: Loss = -12245.980165466091
1
Iteration 11000: Loss = -12245.959130888197
Iteration 11100: Loss = -12245.9602339042
1
Iteration 11200: Loss = -12245.959047486072
Iteration 11300: Loss = -12245.958990246963
Iteration 11400: Loss = -12245.959383177438
1
Iteration 11500: Loss = -12245.958914351564
Iteration 11600: Loss = -12245.958885989317
Iteration 11700: Loss = -12245.9611019355
1
Iteration 11800: Loss = -12245.95883498273
Iteration 11900: Loss = -12245.95881173185
Iteration 12000: Loss = -12245.977099520864
1
Iteration 12100: Loss = -12245.958784200864
Iteration 12200: Loss = -12245.958738844514
Iteration 12300: Loss = -12246.177605534773
1
Iteration 12400: Loss = -12245.958701521033
Iteration 12500: Loss = -12245.958673774041
Iteration 12600: Loss = -12245.958668139532
Iteration 12700: Loss = -12245.959461698132
1
Iteration 12800: Loss = -12245.958626302057
Iteration 12900: Loss = -12245.958609210984
Iteration 13000: Loss = -12245.958604697471
Iteration 13100: Loss = -12245.958821358257
1
Iteration 13200: Loss = -12245.958540198517
Iteration 13300: Loss = -12245.958516097051
Iteration 13400: Loss = -12245.98680176504
1
Iteration 13500: Loss = -12245.958552396163
2
Iteration 13600: Loss = -12245.958549185989
3
Iteration 13700: Loss = -12245.95852924471
4
Iteration 13800: Loss = -12245.961988207386
5
Stopping early at iteration 13800 due to no improvement.
pi: tensor([[2.5867e-07, 1.0000e+00],
        [1.0000e+00, 2.0186e-07]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0104, 0.9896], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2017, 0.0922],
         [0.5981, 0.1937]],

        [[0.7188, 0.1513],
         [0.6594, 0.6626]],

        [[0.7292, 0.2826],
         [0.6236, 0.5420]],

        [[0.6172, 0.0839],
         [0.5063, 0.5538]],

        [[0.6531, 0.1931],
         [0.5451, 0.6822]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.003478765038411364
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 41
Adjusted Rand Index: 0.007614171548597778
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 59
Adjusted Rand Index: 0.007614171548597778
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
Global Adjusted Rand Index: -0.002198450369242598
Average Adjusted Rand Index: 0.004095268126764961
11736.411779745691
[0.9760956718354579, -0.0006470288592156272, -0.002198450369242598, -0.002198450369242598] [0.9761604351641244, -0.0004529465619428516, 0.004095268126764961, 0.004095268126764961] [11727.483763765946, 12253.175962049667, 12245.968971730728, 12245.961988207386]
-----------------------------------------------------------------------------------------
This iteration is 16
True Objective function: Loss = -11952.82507092151
Iteration 0: Loss = -12615.73226640109
Iteration 10: Loss = -12395.054438864521
Iteration 20: Loss = -12394.803658926088
Iteration 30: Loss = -12394.745571973353
Iteration 40: Loss = -12394.722871269041
Iteration 50: Loss = -12394.710260792303
Iteration 60: Loss = -12394.701165690332
Iteration 70: Loss = -12394.69375204325
Iteration 80: Loss = -12394.687147008153
Iteration 90: Loss = -12394.68124785511
Iteration 100: Loss = -12394.675961404597
Iteration 110: Loss = -12394.671422059497
Iteration 120: Loss = -12394.667675853232
Iteration 130: Loss = -12394.664645019235
Iteration 140: Loss = -12394.662405857121
Iteration 150: Loss = -12394.66065904451
Iteration 160: Loss = -12394.659459093775
Iteration 170: Loss = -12394.658597536232
Iteration 180: Loss = -12394.65802530744
Iteration 190: Loss = -12394.657686453762
Iteration 200: Loss = -12394.657482741753
Iteration 210: Loss = -12394.657544210395
1
Iteration 220: Loss = -12394.657754447368
2
Iteration 230: Loss = -12394.658144859577
3
Stopping early at iteration 230 due to no improvement.
pi: tensor([[0.3695, 0.6305],
        [0.4247, 0.5753]], dtype=torch.float64)
alpha: tensor([0.4016, 0.5984])
beta: tensor([[[0.2060, 0.1936],
         [0.6552, 0.1933]],

        [[0.9377, 0.2090],
         [0.2708, 0.0559]],

        [[0.3344, 0.1914],
         [0.1293, 0.1454]],

        [[0.7123, 0.2057],
         [0.5383, 0.9760]],

        [[0.7585, 0.1974],
         [0.1159, 0.8674]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 7.13818506546007e-05
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 42
Adjusted Rand Index: -0.019124365424315677
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0027718337928344427
Average Adjusted Rand Index: -0.003810596714732215
Iteration 0: Loss = -12396.35382225369
Iteration 10: Loss = -12396.353821846978
Iteration 20: Loss = -12396.353821964127
1
Iteration 30: Loss = -12396.353822532734
2
Iteration 40: Loss = -12396.353825329852
3
Stopping early at iteration 40 due to no improvement.
pi: tensor([[3.5165e-12, 1.0000e+00],
        [1.4241e-10, 1.0000e+00]], dtype=torch.float64)
alpha: tensor([1.3871e-10, 1.0000e+00])
beta: tensor([[[0.2401, 0.1741],
         [0.8127, 0.1983]],

        [[0.8309, 0.2465],
         [0.5531, 0.8327]],

        [[0.7570, 0.1587],
         [0.1510, 0.1847]],

        [[0.0931, 0.2702],
         [0.2518, 0.7944]],

        [[0.0987, 0.1750],
         [0.2271, 0.4260]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 39
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21765.27053980468
Iteration 100: Loss = -12396.97454617263
Iteration 200: Loss = -12396.289547983482
Iteration 300: Loss = -12396.099950655615
Iteration 400: Loss = -12396.014732148004
Iteration 500: Loss = -12395.956720978791
Iteration 600: Loss = -12395.902381540465
Iteration 700: Loss = -12395.84423712262
Iteration 800: Loss = -12395.77920749121
Iteration 900: Loss = -12395.704106459569
Iteration 1000: Loss = -12395.613409147963
Iteration 1100: Loss = -12395.500878148472
Iteration 1200: Loss = -12395.364694491975
Iteration 1300: Loss = -12395.213069122281
Iteration 1400: Loss = -12395.065129009397
Iteration 1500: Loss = -12394.927649617024
Iteration 1600: Loss = -12394.798027134244
Iteration 1700: Loss = -12394.678211504717
Iteration 1800: Loss = -12394.57315031155
Iteration 1900: Loss = -12394.48742360158
Iteration 2000: Loss = -12394.418744025012
Iteration 2100: Loss = -12394.364937235936
Iteration 2200: Loss = -12394.324347526419
Iteration 2300: Loss = -12394.288315444957
Iteration 2400: Loss = -12394.237120470847
Iteration 2500: Loss = -12394.118803945405
Iteration 2600: Loss = -12393.928915375183
Iteration 2700: Loss = -12393.747664164774
Iteration 2800: Loss = -12393.567159535547
Iteration 2900: Loss = -12393.366512489785
Iteration 3000: Loss = -12393.12754845375
Iteration 3100: Loss = -12392.909992560382
Iteration 3200: Loss = -12392.792890194918
Iteration 3300: Loss = -12392.745201725882
Iteration 3400: Loss = -12392.724623965858
Iteration 3500: Loss = -12392.713298240918
Iteration 3600: Loss = -12392.704419041178
Iteration 3700: Loss = -12392.695226262826
Iteration 3800: Loss = -12392.683794146438
Iteration 3900: Loss = -12392.667998890412
Iteration 4000: Loss = -12392.643520877406
Iteration 4100: Loss = -12392.599604976338
Iteration 4200: Loss = -12392.515053793019
Iteration 4300: Loss = -12392.430463437438
Iteration 4400: Loss = -12392.406479571342
Iteration 4500: Loss = -12392.39416275917
Iteration 4600: Loss = -12392.387624273639
Iteration 4700: Loss = -12392.383491521787
Iteration 4800: Loss = -12392.380496708789
Iteration 4900: Loss = -12392.37833040467
Iteration 5000: Loss = -12392.376626598518
Iteration 5100: Loss = -12392.375292308447
Iteration 5200: Loss = -12392.374153569586
Iteration 5300: Loss = -12392.376292388544
1
Iteration 5400: Loss = -12392.37240376307
Iteration 5500: Loss = -12392.371697367818
Iteration 5600: Loss = -12392.371142861306
Iteration 5700: Loss = -12392.370552786626
Iteration 5800: Loss = -12392.370067395635
Iteration 5900: Loss = -12392.36958693766
Iteration 6000: Loss = -12392.369253788987
Iteration 6100: Loss = -12392.38400998689
1
Iteration 6200: Loss = -12392.368473239403
Iteration 6300: Loss = -12392.368180679989
Iteration 6400: Loss = -12392.367813182253
Iteration 6500: Loss = -12392.367556458046
Iteration 6600: Loss = -12392.367276523211
Iteration 6700: Loss = -12392.369675886903
1
Iteration 6800: Loss = -12392.366811689571
Iteration 6900: Loss = -12392.36655202321
Iteration 7000: Loss = -12392.366415434599
Iteration 7100: Loss = -12392.366139768217
Iteration 7200: Loss = -12392.37527886834
1
Iteration 7300: Loss = -12392.365816975045
Iteration 7400: Loss = -12392.365597381402
Iteration 7500: Loss = -12392.36545677906
Iteration 7600: Loss = -12392.366072501092
1
Iteration 7700: Loss = -12392.365474915128
2
Iteration 7800: Loss = -12392.398229512575
3
Iteration 7900: Loss = -12392.367161199749
4
Iteration 8000: Loss = -12392.364726773092
Iteration 8100: Loss = -12392.36506436264
1
Iteration 8200: Loss = -12392.364518874498
Iteration 8300: Loss = -12392.364421344259
Iteration 8400: Loss = -12392.364485373953
1
Iteration 8500: Loss = -12392.364215222871
Iteration 8600: Loss = -12392.36604294723
1
Iteration 8700: Loss = -12392.364221072372
2
Iteration 8800: Loss = -12392.364039343329
Iteration 8900: Loss = -12392.364417306635
1
Iteration 9000: Loss = -12392.364738691955
2
Iteration 9100: Loss = -12392.38410833167
3
Iteration 9200: Loss = -12392.364811621965
4
Iteration 9300: Loss = -12392.363601502131
Iteration 9400: Loss = -12392.369840791585
1
Iteration 9500: Loss = -12392.363446532721
Iteration 9600: Loss = -12392.363770559212
1
Iteration 9700: Loss = -12392.386688298131
2
Iteration 9800: Loss = -12392.364300954612
3
Iteration 9900: Loss = -12392.377178311548
4
Iteration 10000: Loss = -12392.36320613672
Iteration 10100: Loss = -12392.363445146211
1
Iteration 10200: Loss = -12392.36361482461
2
Iteration 10300: Loss = -12392.364574131127
3
Iteration 10400: Loss = -12392.408805829718
4
Iteration 10500: Loss = -12392.363007593523
Iteration 10600: Loss = -12392.363206979337
1
Iteration 10700: Loss = -12392.451916408945
2
Iteration 10800: Loss = -12392.362891055302
Iteration 10900: Loss = -12392.36319847669
1
Iteration 11000: Loss = -12392.363131903883
2
Iteration 11100: Loss = -12392.363044257541
3
Iteration 11200: Loss = -12392.37442829922
4
Iteration 11300: Loss = -12392.362767498165
Iteration 11400: Loss = -12392.362929579818
1
Iteration 11500: Loss = -12392.362774144023
2
Iteration 11600: Loss = -12392.362809634034
3
Iteration 11700: Loss = -12392.362685999746
Iteration 11800: Loss = -12392.362810891405
1
Iteration 11900: Loss = -12392.362804902052
2
Iteration 12000: Loss = -12392.362865076318
3
Iteration 12100: Loss = -12392.365361000979
4
Iteration 12200: Loss = -12392.362635890893
Iteration 12300: Loss = -12392.362700638581
1
Iteration 12400: Loss = -12392.366157871089
2
Iteration 12500: Loss = -12392.362587143763
Iteration 12600: Loss = -12392.362780420166
1
Iteration 12700: Loss = -12392.363770845346
2
Iteration 12800: Loss = -12392.362666948422
3
Iteration 12900: Loss = -12392.401881338195
4
Iteration 13000: Loss = -12392.492163785328
5
Stopping early at iteration 13000 due to no improvement.
pi: tensor([[4.8900e-02, 9.5110e-01],
        [9.9988e-01, 1.2324e-04]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7347, 0.2653], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1862, 0.2019],
         [0.7177, 0.2174]],

        [[0.6510, 0.2023],
         [0.5167, 0.6672]],

        [[0.6143, 0.2005],
         [0.5750, 0.6886]],

        [[0.6905, 0.2024],
         [0.5745, 0.6431]],

        [[0.6313, 0.2058],
         [0.6119, 0.7299]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.003504544956740773
time is 1
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.0020244543554292383
time is 2
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: -0.008377311141454773
time is 3
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: -0.010376639662651106
time is 4
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: -0.016979724985067522
Global Adjusted Rand Index: -0.0018848838727979294
Average Adjusted Rand Index: -0.008252535020268683
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22691.18175615207
Iteration 100: Loss = -12016.13775676068
Iteration 200: Loss = -11940.391082691525
Iteration 300: Loss = -11939.949324760268
Iteration 400: Loss = -11939.773087243871
Iteration 500: Loss = -11939.68976621508
Iteration 600: Loss = -11939.641960039698
Iteration 700: Loss = -11939.611198551225
Iteration 800: Loss = -11939.59017024836
Iteration 900: Loss = -11939.574984331171
Iteration 1000: Loss = -11939.563701841606
Iteration 1100: Loss = -11939.554964225414
Iteration 1200: Loss = -11939.54811270456
Iteration 1300: Loss = -11939.542583029635
Iteration 1400: Loss = -11939.538126310803
Iteration 1500: Loss = -11939.534360053492
Iteration 1600: Loss = -11939.531339729921
Iteration 1700: Loss = -11939.528455366173
Iteration 1800: Loss = -11939.526146311837
Iteration 1900: Loss = -11939.524083886892
Iteration 2000: Loss = -11939.522461045754
Iteration 2100: Loss = -11939.520275299177
Iteration 2200: Loss = -11939.518134215294
Iteration 2300: Loss = -11939.517324081993
Iteration 2400: Loss = -11939.515391135237
Iteration 2500: Loss = -11939.515096344576
Iteration 2600: Loss = -11939.504904375008
Iteration 2700: Loss = -11939.506513988452
1
Iteration 2800: Loss = -11939.508507025468
2
Iteration 2900: Loss = -11939.50197904365
Iteration 3000: Loss = -11939.50144712794
Iteration 3100: Loss = -11939.501130486557
Iteration 3200: Loss = -11939.500534688868
Iteration 3300: Loss = -11939.500138734635
Iteration 3400: Loss = -11939.49988340872
Iteration 3500: Loss = -11939.500447349219
1
Iteration 3600: Loss = -11939.501673965739
2
Iteration 3700: Loss = -11939.502252039238
3
Iteration 3800: Loss = -11939.498655210753
Iteration 3900: Loss = -11939.498331767722
Iteration 4000: Loss = -11939.498870875892
1
Iteration 4100: Loss = -11939.52139028091
2
Iteration 4200: Loss = -11939.497737285463
Iteration 4300: Loss = -11939.498971557843
1
Iteration 4400: Loss = -11939.499718784376
2
Iteration 4500: Loss = -11939.497436899372
Iteration 4600: Loss = -11939.497618394853
1
Iteration 4700: Loss = -11939.499321824369
2
Iteration 4800: Loss = -11939.496899123229
Iteration 4900: Loss = -11939.496754967971
Iteration 5000: Loss = -11939.496865928068
1
Iteration 5100: Loss = -11939.496590406246
Iteration 5200: Loss = -11939.49696564766
1
Iteration 5300: Loss = -11939.501555875257
2
Iteration 5400: Loss = -11939.496370434797
Iteration 5500: Loss = -11939.505305708777
1
Iteration 5600: Loss = -11939.496226916246
Iteration 5700: Loss = -11939.49624060151
1
Iteration 5800: Loss = -11939.496544563868
2
Iteration 5900: Loss = -11939.49615037019
Iteration 6000: Loss = -11939.499979230133
1
Iteration 6100: Loss = -11939.495930891295
Iteration 6200: Loss = -11939.495909198531
Iteration 6300: Loss = -11939.496434788003
1
Iteration 6400: Loss = -11939.495848350893
Iteration 6500: Loss = -11939.503228983214
1
Iteration 6600: Loss = -11939.495715805617
Iteration 6700: Loss = -11939.49597634248
1
Iteration 6800: Loss = -11939.495984058709
2
Iteration 6900: Loss = -11939.496280608737
3
Iteration 7000: Loss = -11939.495805313307
4
Iteration 7100: Loss = -11939.496144636561
5
Stopping early at iteration 7100 due to no improvement.
pi: tensor([[0.7205, 0.2795],
        [0.1968, 0.8032]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4789, 0.5211], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2959, 0.1054],
         [0.5628, 0.2938]],

        [[0.6198, 0.1192],
         [0.7243, 0.7222]],

        [[0.6787, 0.0981],
         [0.5688, 0.5557]],

        [[0.6924, 0.0959],
         [0.5740, 0.6044]],

        [[0.7175, 0.1003],
         [0.6036, 0.6070]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599967086364498
time is 1
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 2
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9207675179163246
Global Adjusted Rand Index: 0.968188848802325
Average Adjusted Rand Index: 0.9681520616402842
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21562.06102201554
Iteration 100: Loss = -12397.140636311216
Iteration 200: Loss = -12396.291087232981
Iteration 300: Loss = -12396.089588535466
Iteration 400: Loss = -12395.96942665316
Iteration 500: Loss = -12395.857722828738
Iteration 600: Loss = -12395.718549872438
Iteration 700: Loss = -12395.544937288863
Iteration 800: Loss = -12395.374466609874
Iteration 900: Loss = -12395.224483442435
Iteration 1000: Loss = -12395.08846872043
Iteration 1100: Loss = -12394.957960710173
Iteration 1200: Loss = -12394.834973116922
Iteration 1300: Loss = -12394.724373292094
Iteration 1400: Loss = -12394.626647061674
Iteration 1500: Loss = -12394.541962414056
Iteration 1600: Loss = -12394.471106956273
Iteration 1700: Loss = -12394.41387345498
Iteration 1800: Loss = -12394.36781315693
Iteration 1900: Loss = -12394.328671277348
Iteration 2000: Loss = -12394.288395926988
Iteration 2100: Loss = -12394.224937292893
Iteration 2200: Loss = -12394.102111018787
Iteration 2300: Loss = -12393.939168552095
Iteration 2400: Loss = -12393.781134227158
Iteration 2500: Loss = -12393.618009273558
Iteration 2600: Loss = -12393.434637586854
Iteration 2700: Loss = -12393.210090623417
Iteration 2800: Loss = -12392.979926960534
Iteration 2900: Loss = -12392.834707501022
Iteration 3000: Loss = -12392.770874756554
Iteration 3100: Loss = -12392.743096249513
Iteration 3200: Loss = -12392.729162065785
Iteration 3300: Loss = -12392.720924866862
Iteration 3400: Loss = -12392.71477544995
Iteration 3500: Loss = -12392.709253785564
Iteration 3600: Loss = -12392.703520750181
Iteration 3700: Loss = -12392.697003344862
Iteration 3800: Loss = -12392.689120825637
Iteration 3900: Loss = -12392.679234765825
Iteration 4000: Loss = -12392.666036945051
Iteration 4100: Loss = -12392.647394010857
Iteration 4200: Loss = -12392.61885419278
Iteration 4300: Loss = -12392.570423416302
Iteration 4400: Loss = -12392.489497151359
Iteration 4500: Loss = -12392.424489369485
Iteration 4600: Loss = -12392.402198282236
Iteration 4700: Loss = -12392.391809635223
Iteration 4800: Loss = -12392.38594691169
Iteration 4900: Loss = -12392.382198101212
Iteration 5000: Loss = -12392.379557040455
Iteration 5100: Loss = -12392.37758971317
Iteration 5200: Loss = -12392.382509268993
1
Iteration 5300: Loss = -12392.37487446765
Iteration 5400: Loss = -12392.373819997354
Iteration 5500: Loss = -12392.373037623383
Iteration 5600: Loss = -12392.372278470077
Iteration 5700: Loss = -12392.371596001556
Iteration 5800: Loss = -12392.378038342456
1
Iteration 5900: Loss = -12392.370505210552
Iteration 6000: Loss = -12392.370032979465
Iteration 6100: Loss = -12392.369622416074
Iteration 6200: Loss = -12392.369168477908
Iteration 6300: Loss = -12392.368848052589
Iteration 6400: Loss = -12392.368516852985
Iteration 6500: Loss = -12392.368219410146
Iteration 6600: Loss = -12392.3683666983
1
Iteration 6700: Loss = -12392.367604956456
Iteration 6800: Loss = -12392.367626306564
1
Iteration 6900: Loss = -12392.36719049464
Iteration 7000: Loss = -12392.366985484357
Iteration 7100: Loss = -12392.36670772529
Iteration 7200: Loss = -12392.366613255233
Iteration 7300: Loss = -12392.366157015455
Iteration 7400: Loss = -12392.36595516862
Iteration 7500: Loss = -12392.365795762273
Iteration 7600: Loss = -12392.365595378644
Iteration 7700: Loss = -12392.365483365691
Iteration 7800: Loss = -12392.366805104335
1
Iteration 7900: Loss = -12392.365160349878
Iteration 8000: Loss = -12392.366974465034
1
Iteration 8100: Loss = -12392.387529712516
2
Iteration 8200: Loss = -12392.36470744931
Iteration 8300: Loss = -12392.365270990516
1
Iteration 8400: Loss = -12392.375374049618
2
Iteration 8500: Loss = -12392.370025801563
3
Iteration 8600: Loss = -12392.36657200677
4
Iteration 8700: Loss = -12392.365658824348
5
Stopping early at iteration 8700 due to no improvement.
pi: tensor([[0.0012, 0.9988],
        [0.9505, 0.0495]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2677, 0.7323], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2185, 0.2019],
         [0.6908, 0.1856]],

        [[0.6918, 0.2018],
         [0.5131, 0.7306]],

        [[0.6029, 0.1999],
         [0.6593, 0.5299]],

        [[0.7014, 0.2007],
         [0.6064, 0.6610]],

        [[0.6835, 0.2052],
         [0.6066, 0.5282]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0035895661719517516
time is 1
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.0020244543554292383
time is 2
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.011501658457641777
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 56
Adjusted Rand Index: -0.010376639662651106
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.016979724985067522
Global Adjusted Rand Index: -0.002169481520893668
Average Adjusted Rand Index: -0.00889440872654828
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22243.089168803377
Iteration 100: Loss = -12396.767712694593
Iteration 200: Loss = -12396.300463843194
Iteration 300: Loss = -12396.16817457767
Iteration 400: Loss = -12396.097164239129
Iteration 500: Loss = -12396.045909094699
Iteration 600: Loss = -12395.99825385583
Iteration 700: Loss = -12395.94338447947
Iteration 800: Loss = -12395.870193858533
Iteration 900: Loss = -12395.774315509765
Iteration 1000: Loss = -12395.657524069351
Iteration 1100: Loss = -12395.50613982318
Iteration 1200: Loss = -12395.305468425386
Iteration 1300: Loss = -12395.077612056586
Iteration 1400: Loss = -12394.873488101548
Iteration 1500: Loss = -12394.696867885112
Iteration 1600: Loss = -12394.548891420853
Iteration 1700: Loss = -12394.437586431373
Iteration 1800: Loss = -12394.36130593804
Iteration 1900: Loss = -12394.305618338076
Iteration 2000: Loss = -12394.24477677733
Iteration 2100: Loss = -12394.096899618666
Iteration 2200: Loss = -12393.853638786011
Iteration 2300: Loss = -12393.63936261989
Iteration 2400: Loss = -12393.399289064198
Iteration 2500: Loss = -12393.114730628928
Iteration 2600: Loss = -12392.880943576929
Iteration 2700: Loss = -12392.784731161608
Iteration 2800: Loss = -12392.751668231218
Iteration 2900: Loss = -12392.737090697037
Iteration 3000: Loss = -12392.728584632858
Iteration 3100: Loss = -12392.72244061784
Iteration 3200: Loss = -12392.717407132726
Iteration 3300: Loss = -12392.71287675448
Iteration 3400: Loss = -12392.708064163719
Iteration 3500: Loss = -12392.70260389345
Iteration 3600: Loss = -12392.695952071424
Iteration 3700: Loss = -12392.687506775987
Iteration 3800: Loss = -12392.676238873228
Iteration 3900: Loss = -12392.660209053003
Iteration 4000: Loss = -12392.635331689917
Iteration 4100: Loss = -12392.591525647618
Iteration 4200: Loss = -12392.506198385994
Iteration 4300: Loss = -12392.41871240841
Iteration 4400: Loss = -12392.394169558796
Iteration 4500: Loss = -12392.384454053088
Iteration 4600: Loss = -12392.37952327277
Iteration 4700: Loss = -12392.376661313789
Iteration 4800: Loss = -12392.374738313043
Iteration 4900: Loss = -12392.37334517439
Iteration 5000: Loss = -12392.372224027637
Iteration 5100: Loss = -12392.371343998193
Iteration 5200: Loss = -12392.370627303526
Iteration 5300: Loss = -12392.370032351058
Iteration 5400: Loss = -12392.369973806228
Iteration 5500: Loss = -12392.369053994373
Iteration 5600: Loss = -12392.368618559572
Iteration 5700: Loss = -12392.368296531953
Iteration 5800: Loss = -12392.367910816449
Iteration 5900: Loss = -12392.36812847753
1
Iteration 6000: Loss = -12392.367315607526
Iteration 6100: Loss = -12392.367075817932
Iteration 6200: Loss = -12392.366813998544
Iteration 6300: Loss = -12392.366589425244
Iteration 6400: Loss = -12392.366445629239
Iteration 6500: Loss = -12392.366114789676
Iteration 6600: Loss = -12392.375528385024
1
Iteration 6700: Loss = -12392.365720461123
Iteration 6800: Loss = -12392.365692716681
Iteration 6900: Loss = -12392.365420145954
Iteration 7000: Loss = -12392.36524425565
Iteration 7100: Loss = -12392.365092158905
Iteration 7200: Loss = -12392.36561148883
1
Iteration 7300: Loss = -12392.364809712855
Iteration 7400: Loss = -12392.364686544966
Iteration 7500: Loss = -12392.364502870247
Iteration 7600: Loss = -12392.364506346146
1
Iteration 7700: Loss = -12392.377885184682
2
Iteration 7800: Loss = -12392.364406460663
Iteration 7900: Loss = -12392.365847599154
1
Iteration 8000: Loss = -12392.3694927522
2
Iteration 8100: Loss = -12392.367779155858
3
Iteration 8200: Loss = -12392.366102120153
4
Iteration 8300: Loss = -12392.369160943888
5
Stopping early at iteration 8300 due to no improvement.
pi: tensor([[9.1944e-04, 9.9908e-01],
        [9.4992e-01, 5.0079e-02]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2669, 0.7331], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2191, 0.2020],
         [0.6350, 0.1855]],

        [[0.5602, 0.2016],
         [0.6119, 0.5407]],

        [[0.6364, 0.2001],
         [0.5268, 0.6708]],

        [[0.5316, 0.1997],
         [0.6363, 0.7230]],

        [[0.6118, 0.2053],
         [0.7242, 0.6459]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0035895661719517516
time is 1
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.0020244543554292383
time is 2
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.011501658457641777
time is 3
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 56
Adjusted Rand Index: -0.010376639662651106
time is 4
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: -0.019670954084601195
Global Adjusted Rand Index: -0.0020716944825574057
Average Adjusted Rand Index: -0.009432654546455014
11952.82507092151
[-0.0018848838727979294, 0.968188848802325, -0.002169481520893668, -0.0020716944825574057] [-0.008252535020268683, 0.9681520616402842, -0.00889440872654828, -0.009432654546455014] [12392.492163785328, 11939.496144636561, 12392.365658824348, 12392.369160943888]
-----------------------------------------------------------------------------------------
This iteration is 17
True Objective function: Loss = -11977.770198707021
Iteration 0: Loss = -12607.39767496125
Iteration 10: Loss = -12455.48361972334
Iteration 20: Loss = -12455.447645541928
Iteration 30: Loss = -12455.444002021013
Iteration 40: Loss = -12455.443306688381
Iteration 50: Loss = -12455.443081553254
Iteration 60: Loss = -12455.442902535136
Iteration 70: Loss = -12455.44270140886
Iteration 80: Loss = -12455.442552259265
Iteration 90: Loss = -12455.442344209774
Iteration 100: Loss = -12455.442087165671
Iteration 110: Loss = -12455.441888961415
Iteration 120: Loss = -12455.441645045114
Iteration 130: Loss = -12455.441397215767
Iteration 140: Loss = -12455.441119395315
Iteration 150: Loss = -12455.44087910792
Iteration 160: Loss = -12455.440578494567
Iteration 170: Loss = -12455.440299524205
Iteration 180: Loss = -12455.440056601316
Iteration 190: Loss = -12455.43975487662
Iteration 200: Loss = -12455.43944020484
Iteration 210: Loss = -12455.439157312756
Iteration 220: Loss = -12455.438883426405
Iteration 230: Loss = -12455.438566193769
Iteration 240: Loss = -12455.438287555733
Iteration 250: Loss = -12455.438016593354
Iteration 260: Loss = -12455.437657529219
Iteration 270: Loss = -12455.437436644304
Iteration 280: Loss = -12455.43711655465
Iteration 290: Loss = -12455.436871982582
Iteration 300: Loss = -12455.436555717422
Iteration 310: Loss = -12455.43628737344
Iteration 320: Loss = -12455.436015992718
Iteration 330: Loss = -12455.435817108202
Iteration 340: Loss = -12455.435514293496
Iteration 350: Loss = -12455.435265159367
Iteration 360: Loss = -12455.435058286457
Iteration 370: Loss = -12455.434815044038
Iteration 380: Loss = -12455.43456467671
Iteration 390: Loss = -12455.434402152887
Iteration 400: Loss = -12455.43420795219
Iteration 410: Loss = -12455.43397147039
Iteration 420: Loss = -12455.433814127904
Iteration 430: Loss = -12455.43363038394
Iteration 440: Loss = -12455.433464499109
Iteration 450: Loss = -12455.433312250521
Iteration 460: Loss = -12455.43314682735
Iteration 470: Loss = -12455.432998511975
Iteration 480: Loss = -12455.432842363709
Iteration 490: Loss = -12455.432713255943
Iteration 500: Loss = -12455.432584927412
Iteration 510: Loss = -12455.432495371437
Iteration 520: Loss = -12455.432358755506
Iteration 530: Loss = -12455.43223345188
Iteration 540: Loss = -12455.432139563889
Iteration 550: Loss = -12455.431999392356
Iteration 560: Loss = -12455.431911753252
Iteration 570: Loss = -12455.431811866903
Iteration 580: Loss = -12455.43177410577
Iteration 590: Loss = -12455.4316181504
Iteration 600: Loss = -12455.431536714364
Iteration 610: Loss = -12455.43147401053
Iteration 620: Loss = -12455.431397570133
Iteration 630: Loss = -12455.431340466454
Iteration 640: Loss = -12455.431285995106
Iteration 650: Loss = -12455.431180509335
Iteration 660: Loss = -12455.431098029027
Iteration 670: Loss = -12455.431006119554
Iteration 680: Loss = -12455.430997525003
Iteration 690: Loss = -12455.430912287842
Iteration 700: Loss = -12455.430822023756
Iteration 710: Loss = -12455.43078899
Iteration 720: Loss = -12455.430723658652
Iteration 730: Loss = -12455.430641976056
Iteration 740: Loss = -12455.43058810185
Iteration 750: Loss = -12455.43055864854
Iteration 760: Loss = -12455.430457880651
Iteration 770: Loss = -12455.430417560208
Iteration 780: Loss = -12455.430362139674
Iteration 790: Loss = -12455.430365390734
1
Iteration 800: Loss = -12455.430274533346
Iteration 810: Loss = -12455.430227849205
Iteration 820: Loss = -12455.430205661623
Iteration 830: Loss = -12455.43009301513
Iteration 840: Loss = -12455.43007167786
Iteration 850: Loss = -12455.430030202338
Iteration 860: Loss = -12455.42996963262
Iteration 870: Loss = -12455.429934722753
Iteration 880: Loss = -12455.429873829798
Iteration 890: Loss = -12455.429811175545
Iteration 900: Loss = -12455.429769686456
Iteration 910: Loss = -12455.42976781507
Iteration 920: Loss = -12455.429731203669
Iteration 930: Loss = -12455.42972079844
Iteration 940: Loss = -12455.429629095574
Iteration 950: Loss = -12455.42961035693
Iteration 960: Loss = -12455.429554066957
Iteration 970: Loss = -12455.429504491165
Iteration 980: Loss = -12455.429414334854
Iteration 990: Loss = -12455.429425120466
1
Iteration 1000: Loss = -12455.429363808416
Iteration 1010: Loss = -12455.429349561357
Iteration 1020: Loss = -12455.429273536587
Iteration 1030: Loss = -12455.429254679679
Iteration 1040: Loss = -12455.429228977628
Iteration 1050: Loss = -12455.429187881606
Iteration 1060: Loss = -12455.429126782752
Iteration 1070: Loss = -12455.429122649262
Iteration 1080: Loss = -12455.429069150527
Iteration 1090: Loss = -12455.429010603017
Iteration 1100: Loss = -12455.429015069136
1
Iteration 1110: Loss = -12455.428952741462
Iteration 1120: Loss = -12455.428861178554
Iteration 1130: Loss = -12455.42884587631
Iteration 1140: Loss = -12455.428838953247
Iteration 1150: Loss = -12455.428798083483
Iteration 1160: Loss = -12455.428759995826
Iteration 1170: Loss = -12455.428701255862
Iteration 1180: Loss = -12455.428666993539
Iteration 1190: Loss = -12455.42862975927
Iteration 1200: Loss = -12455.428573922421
Iteration 1210: Loss = -12455.428560649405
Iteration 1220: Loss = -12455.428521682348
Iteration 1230: Loss = -12455.428483028267
Iteration 1240: Loss = -12455.428463747416
Iteration 1250: Loss = -12455.42843521595
Iteration 1260: Loss = -12455.428358676201
Iteration 1270: Loss = -12455.428353045172
Iteration 1280: Loss = -12455.428328839644
Iteration 1290: Loss = -12455.428282444145
Iteration 1300: Loss = -12455.428267453444
Iteration 1310: Loss = -12455.428238581284
Iteration 1320: Loss = -12455.428197247427
Iteration 1330: Loss = -12455.428184891218
Iteration 1340: Loss = -12455.428115631414
Iteration 1350: Loss = -12455.428106018604
Iteration 1360: Loss = -12455.42804121042
Iteration 1370: Loss = -12455.428032696513
Iteration 1380: Loss = -12455.428014491403
Iteration 1390: Loss = -12455.427957408674
Iteration 1400: Loss = -12455.42794738247
Iteration 1410: Loss = -12455.427933077033
Iteration 1420: Loss = -12455.42789615274
Iteration 1430: Loss = -12455.427837819252
Iteration 1440: Loss = -12455.427791456797
Iteration 1450: Loss = -12455.427759483786
Iteration 1460: Loss = -12455.427751040355
Iteration 1470: Loss = -12455.427716068563
Iteration 1480: Loss = -12455.427686308421
Iteration 1490: Loss = -12455.427687873504
1
Iteration 1500: Loss = -12455.427620519295
Iteration 1510: Loss = -12455.427633738333
1
Iteration 1520: Loss = -12455.427578093568
Iteration 1530: Loss = -12455.427559180436
Iteration 1540: Loss = -12455.427512947777
Iteration 1550: Loss = -12455.427455557185
Iteration 1560: Loss = -12455.427450742762
Iteration 1570: Loss = -12455.427388255792
Iteration 1580: Loss = -12455.427372403172
Iteration 1590: Loss = -12455.427352301249
Iteration 1600: Loss = -12455.427331088404
Iteration 1610: Loss = -12455.427333547143
1
Iteration 1620: Loss = -12455.427252264844
Iteration 1630: Loss = -12455.427258133766
1
Iteration 1640: Loss = -12455.427203067364
Iteration 1650: Loss = -12455.427177938838
Iteration 1660: Loss = -12455.427173724762
Iteration 1670: Loss = -12455.427113490672
Iteration 1680: Loss = -12455.427106262896
Iteration 1690: Loss = -12455.427073309398
Iteration 1700: Loss = -12455.42705315287
Iteration 1710: Loss = -12455.427039711009
Iteration 1720: Loss = -12455.426969543165
Iteration 1730: Loss = -12455.426919104551
Iteration 1740: Loss = -12455.426902891322
Iteration 1750: Loss = -12455.426939618776
1
Iteration 1760: Loss = -12455.426889622846
Iteration 1770: Loss = -12455.42687590232
Iteration 1780: Loss = -12455.426843826166
Iteration 1790: Loss = -12455.426824462915
Iteration 1800: Loss = -12455.426800217874
Iteration 1810: Loss = -12455.426801775617
1
Iteration 1820: Loss = -12455.426732627515
Iteration 1830: Loss = -12455.42672448553
Iteration 1840: Loss = -12455.426686735873
Iteration 1850: Loss = -12455.426679239134
Iteration 1860: Loss = -12455.426615978071
Iteration 1870: Loss = -12455.426649509829
1
Iteration 1880: Loss = -12455.426562879673
Iteration 1890: Loss = -12455.42656171076
Iteration 1900: Loss = -12455.426513344177
Iteration 1910: Loss = -12455.426494327661
Iteration 1920: Loss = -12455.426494382293
1
Iteration 1930: Loss = -12455.426471461984
Iteration 1940: Loss = -12455.426408870095
Iteration 1950: Loss = -12455.426380466524
Iteration 1960: Loss = -12455.426391623496
1
Iteration 1970: Loss = -12455.42638975046
2
Iteration 1980: Loss = -12455.426321973435
Iteration 1990: Loss = -12455.426287839651
Iteration 2000: Loss = -12455.426308335622
1
Iteration 2010: Loss = -12455.426278896068
Iteration 2020: Loss = -12455.426267940498
Iteration 2030: Loss = -12455.42622194411
Iteration 2040: Loss = -12455.426189701788
Iteration 2050: Loss = -12455.426183752752
Iteration 2060: Loss = -12455.426167034153
Iteration 2070: Loss = -12455.426116946626
Iteration 2080: Loss = -12455.426103517586
Iteration 2090: Loss = -12455.426104206508
1
Iteration 2100: Loss = -12455.426095370058
Iteration 2110: Loss = -12455.42604611748
Iteration 2120: Loss = -12455.426035579572
Iteration 2130: Loss = -12455.425972634444
Iteration 2140: Loss = -12455.425961415875
Iteration 2150: Loss = -12455.425940985344
Iteration 2160: Loss = -12455.425947216047
1
Iteration 2170: Loss = -12455.425926616963
Iteration 2180: Loss = -12455.425909910993
Iteration 2190: Loss = -12455.425878013037
Iteration 2200: Loss = -12455.425875998018
Iteration 2210: Loss = -12455.42584040861
Iteration 2220: Loss = -12455.425812558417
Iteration 2230: Loss = -12455.425813476368
1
Iteration 2240: Loss = -12455.425765225049
Iteration 2250: Loss = -12455.425730865043
Iteration 2260: Loss = -12455.425738659256
1
Iteration 2270: Loss = -12455.425728783217
Iteration 2280: Loss = -12455.425683307842
Iteration 2290: Loss = -12455.425680282473
Iteration 2300: Loss = -12455.425632847471
Iteration 2310: Loss = -12455.4256293974
Iteration 2320: Loss = -12455.425604004944
Iteration 2330: Loss = -12455.425547050458
Iteration 2340: Loss = -12455.425580585428
1
Iteration 2350: Loss = -12455.425571085643
2
Iteration 2360: Loss = -12455.425541637747
Iteration 2370: Loss = -12455.425508265074
Iteration 2380: Loss = -12455.42550062017
Iteration 2390: Loss = -12455.425555593289
1
Iteration 2400: Loss = -12455.425455848363
Iteration 2410: Loss = -12455.425438257686
Iteration 2420: Loss = -12455.425393635662
Iteration 2430: Loss = -12455.425421193911
1
Iteration 2440: Loss = -12455.425396299044
2
Iteration 2450: Loss = -12455.425348357698
Iteration 2460: Loss = -12455.425337953577
Iteration 2470: Loss = -12455.425330114682
Iteration 2480: Loss = -12455.425293101589
Iteration 2490: Loss = -12455.42530500421
1
Iteration 2500: Loss = -12455.425258134888
Iteration 2510: Loss = -12455.42524955036
Iteration 2520: Loss = -12455.425220822923
Iteration 2530: Loss = -12455.425208828032
Iteration 2540: Loss = -12455.42516705501
Iteration 2550: Loss = -12455.425181387867
1
Iteration 2560: Loss = -12455.425214500241
2
Iteration 2570: Loss = -12455.425182231784
3
Stopping early at iteration 2570 due to no improvement.
pi: tensor([[0.8193, 0.1807],
        [0.7575, 0.2425]], dtype=torch.float64)
alpha: tensor([0.8075, 0.1925])
beta: tensor([[[0.1993, 0.1996],
         [0.6160, 0.2033]],

        [[0.9073, 0.1983],
         [0.9883, 0.7878]],

        [[0.0735, 0.2057],
         [0.6654, 0.6538]],

        [[0.3790, 0.2058],
         [0.9717, 0.4998]],

        [[0.4197, 0.1968],
         [0.0587, 0.2971]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -12455.64837274676
Iteration 10: Loss = -12455.648396462637
1
Iteration 20: Loss = -12455.648422901933
2
Iteration 30: Loss = -12455.648485362473
3
Stopping early at iteration 30 due to no improvement.
pi: tensor([[4.3398e-31, 1.0000e+00],
        [6.6050e-09, 1.0000e+00]], dtype=torch.float64)
alpha: tensor([6.5061e-09, 1.0000e+00])
beta: tensor([[[0.2099, 0.1770],
         [0.8513, 0.2000]],

        [[0.4483, 0.1907],
         [0.1876, 0.2845]],

        [[0.0248, 0.2569],
         [0.2167, 0.1022]],

        [[0.1951, 0.3133],
         [0.1593, 0.8642]],

        [[0.4671, 0.1873],
         [0.0798, 0.4736]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21058.363580096593
Iteration 100: Loss = -12456.533799014274
Iteration 200: Loss = -12455.729595485196
Iteration 300: Loss = -12455.553525222536
Iteration 400: Loss = -12455.479501667678
Iteration 500: Loss = -12455.438238950393
Iteration 600: Loss = -12455.410498382396
Iteration 700: Loss = -12455.389823584195
Iteration 800: Loss = -12455.373225706233
Iteration 900: Loss = -12455.359115656565
Iteration 1000: Loss = -12455.346631381464
Iteration 1100: Loss = -12455.335104051332
Iteration 1200: Loss = -12455.323955980768
Iteration 1300: Loss = -12455.312896902025
Iteration 1400: Loss = -12455.301630907814
Iteration 1500: Loss = -12455.29005467893
Iteration 1600: Loss = -12455.278355080458
Iteration 1700: Loss = -12455.266651298574
Iteration 1800: Loss = -12455.255236522742
Iteration 1900: Loss = -12455.24438313987
Iteration 2000: Loss = -12455.234213640919
Iteration 2100: Loss = -12455.224755370664
Iteration 2200: Loss = -12455.215778626593
Iteration 2300: Loss = -12455.207212902953
Iteration 2400: Loss = -12455.198774365826
Iteration 2500: Loss = -12455.190323917057
Iteration 2600: Loss = -12455.181785117082
Iteration 2700: Loss = -12455.173237518326
Iteration 2800: Loss = -12455.164821885497
Iteration 2900: Loss = -12455.156658702635
Iteration 3000: Loss = -12455.148883901269
Iteration 3100: Loss = -12455.141757648442
Iteration 3200: Loss = -12455.134986922045
Iteration 3300: Loss = -12455.128253337356
Iteration 3400: Loss = -12455.120221513524
Iteration 3500: Loss = -12455.105677913134
Iteration 3600: Loss = -12455.04283110129
Iteration 3700: Loss = -12454.870780766989
Iteration 3800: Loss = -12454.841382873587
Iteration 3900: Loss = -12454.831700253262
Iteration 4000: Loss = -12454.826487540262
Iteration 4100: Loss = -12454.822974633056
Iteration 4200: Loss = -12454.820056153389
Iteration 4300: Loss = -12454.817488282131
Iteration 4400: Loss = -12454.815093215826
Iteration 4500: Loss = -12454.812675869547
Iteration 4600: Loss = -12454.81021859072
Iteration 4700: Loss = -12454.807398122264
Iteration 4800: Loss = -12454.805424103382
Iteration 4900: Loss = -12454.80073566562
Iteration 5000: Loss = -12454.798555994139
Iteration 5100: Loss = -12454.791410103124
Iteration 5200: Loss = -12454.792835038523
1
Iteration 5300: Loss = -12454.777649354915
Iteration 5400: Loss = -12454.77026828994
Iteration 5500: Loss = -12454.757854334466
Iteration 5600: Loss = -12454.745785048328
Iteration 5700: Loss = -12454.729883682234
Iteration 5800: Loss = -12454.713157713655
Iteration 5900: Loss = -12454.705930360518
Iteration 6000: Loss = -12454.681739241463
Iteration 6100: Loss = -12454.674045218053
Iteration 6200: Loss = -12454.66980462911
Iteration 6300: Loss = -12454.667776581393
Iteration 6400: Loss = -12454.676182777457
1
Iteration 6500: Loss = -12454.66459914601
Iteration 6600: Loss = -12454.664038417828
Iteration 6700: Loss = -12454.668348635338
1
Iteration 6800: Loss = -12454.659908099395
Iteration 6900: Loss = -12454.657825756272
Iteration 7000: Loss = -12454.656262662867
Iteration 7100: Loss = -12454.737065666555
1
Iteration 7200: Loss = -12454.653157475494
Iteration 7300: Loss = -12454.651489478105
Iteration 7400: Loss = -12454.651672641217
1
Iteration 7500: Loss = -12454.659705957256
2
Iteration 7600: Loss = -12454.850589098467
3
Iteration 7700: Loss = -12454.647980085532
Iteration 7800: Loss = -12454.64807207021
1
Iteration 7900: Loss = -12454.652055377646
2
Iteration 8000: Loss = -12454.684328107984
3
Iteration 8100: Loss = -12454.64911819175
4
Iteration 8200: Loss = -12454.645539441033
Iteration 8300: Loss = -12454.646029556046
1
Iteration 8400: Loss = -12454.64496309948
Iteration 8500: Loss = -12454.645300453547
1
Iteration 8600: Loss = -12454.663990628183
2
Iteration 8700: Loss = -12454.644313315313
Iteration 8800: Loss = -12454.663883377614
1
Iteration 8900: Loss = -12454.645888061415
2
Iteration 9000: Loss = -12454.679937336348
3
Iteration 9100: Loss = -12454.649626017153
4
Iteration 9200: Loss = -12454.644657598294
5
Stopping early at iteration 9200 due to no improvement.
pi: tensor([[0.1119, 0.8881],
        [0.4577, 0.5423]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9667, 0.0333], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2059, 0.1305],
         [0.6502, 0.2010]],

        [[0.6253, 0.2003],
         [0.6169, 0.6965]],

        [[0.7203, 0.2050],
         [0.6129, 0.7164]],

        [[0.6651, 0.2063],
         [0.6291, 0.5044]],

        [[0.5869, 0.1994],
         [0.6978, 0.6348]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.000465880155123834
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23643.46055353875
Iteration 100: Loss = -12456.100848269934
Iteration 200: Loss = -12455.55757962572
Iteration 300: Loss = -12455.442787341843
Iteration 400: Loss = -12455.386349783636
Iteration 500: Loss = -12455.350914643335
Iteration 600: Loss = -12455.324483101002
Iteration 700: Loss = -12455.301335119622
Iteration 800: Loss = -12455.278575965067
Iteration 900: Loss = -12455.255039004815
Iteration 1000: Loss = -12455.231681117939
Iteration 1100: Loss = -12455.211091909809
Iteration 1200: Loss = -12455.193902067671
Iteration 1300: Loss = -12455.176894078333
Iteration 1400: Loss = -12455.154007411298
Iteration 1500: Loss = -12455.108339471293
Iteration 1600: Loss = -12454.997068532442
Iteration 1700: Loss = -12454.913322280752
Iteration 1800: Loss = -12454.873266069466
Iteration 1900: Loss = -12454.850653613692
Iteration 2000: Loss = -12454.836669156633
Iteration 2100: Loss = -12454.827565875683
Iteration 2200: Loss = -12454.821267178806
Iteration 2300: Loss = -12454.816691911063
Iteration 2400: Loss = -12454.8129885228
Iteration 2500: Loss = -12454.809682173265
Iteration 2600: Loss = -12454.806506437677
Iteration 2700: Loss = -12454.803333118503
Iteration 2800: Loss = -12454.800203365006
Iteration 2900: Loss = -12454.797049416677
Iteration 3000: Loss = -12454.793848626672
Iteration 3100: Loss = -12454.790558316958
Iteration 3200: Loss = -12454.787076966664
Iteration 3300: Loss = -12454.783154628614
Iteration 3400: Loss = -12454.778639581653
Iteration 3500: Loss = -12454.773260383026
Iteration 3600: Loss = -12454.766392740641
Iteration 3700: Loss = -12454.758982546478
Iteration 3800: Loss = -12454.749143619174
Iteration 3900: Loss = -12454.739801866872
Iteration 4000: Loss = -12454.729949157621
Iteration 4100: Loss = -12454.72192357375
Iteration 4200: Loss = -12454.714685805438
Iteration 4300: Loss = -12454.709366689387
Iteration 4400: Loss = -12454.708774282737
Iteration 4500: Loss = -12454.699306972187
Iteration 4600: Loss = -12454.69760638164
Iteration 4700: Loss = -12454.687884728783
Iteration 4800: Loss = -12454.679681395965
Iteration 4900: Loss = -12454.671045296025
Iteration 5000: Loss = -12454.658769928194
Iteration 5100: Loss = -12454.651399349998
Iteration 5200: Loss = -12454.647026708297
Iteration 5300: Loss = -12454.646145209761
Iteration 5400: Loss = -12454.644484894874
Iteration 5500: Loss = -12454.644027160548
Iteration 5600: Loss = -12454.64385514715
Iteration 5700: Loss = -12454.643672137106
Iteration 5800: Loss = -12454.64562169689
1
Iteration 5900: Loss = -12454.643426253051
Iteration 6000: Loss = -12454.6513002699
1
Iteration 6100: Loss = -12454.643227623043
Iteration 6200: Loss = -12454.730710927948
1
Iteration 6300: Loss = -12454.643113955075
Iteration 6400: Loss = -12454.643066921013
Iteration 6500: Loss = -12454.643124168246
1
Iteration 6600: Loss = -12454.643001905051
Iteration 6700: Loss = -12454.643232680277
1
Iteration 6800: Loss = -12454.643066238385
2
Iteration 6900: Loss = -12454.643456685244
3
Iteration 7000: Loss = -12454.644127068459
4
Iteration 7100: Loss = -12454.642978351485
Iteration 7200: Loss = -12454.647801115098
1
Iteration 7300: Loss = -12454.64310981218
2
Iteration 7400: Loss = -12454.643033126455
3
Iteration 7500: Loss = -12454.694419555177
4
Iteration 7600: Loss = -12454.64296030105
Iteration 7700: Loss = -12454.643973420327
1
Iteration 7800: Loss = -12454.64346064367
2
Iteration 7900: Loss = -12454.642940169426
Iteration 8000: Loss = -12454.643096742537
1
Iteration 8100: Loss = -12454.643143950394
2
Iteration 8200: Loss = -12454.642937369614
Iteration 8300: Loss = -12454.644203632452
1
Iteration 8400: Loss = -12454.643007344543
2
Iteration 8500: Loss = -12454.645102344282
3
Iteration 8600: Loss = -12454.643770335271
4
Iteration 8700: Loss = -12454.64900972032
5
Stopping early at iteration 8700 due to no improvement.
pi: tensor([[0.5691, 0.4309],
        [0.9247, 0.0753]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0336, 0.9664], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2010, 0.1306],
         [0.6850, 0.2061]],

        [[0.6621, 0.1997],
         [0.6324, 0.5939]],

        [[0.6536, 0.2051],
         [0.5251, 0.7263]],

        [[0.6113, 0.2065],
         [0.6571, 0.7004]],

        [[0.5105, 0.1995],
         [0.5764, 0.6434]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.000465880155123834
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20726.88372420475
Iteration 100: Loss = -12455.801576103258
Iteration 200: Loss = -12455.43822620492
Iteration 300: Loss = -12455.394503132678
Iteration 400: Loss = -12455.373208308172
Iteration 500: Loss = -12455.357469298113
Iteration 600: Loss = -12455.344035390224
Iteration 700: Loss = -12455.331602391418
Iteration 800: Loss = -12455.319146787277
Iteration 900: Loss = -12455.305630516279
Iteration 1000: Loss = -12455.290444170223
Iteration 1100: Loss = -12455.273175650113
Iteration 1200: Loss = -12455.254667095089
Iteration 1300: Loss = -12455.23718182847
Iteration 1400: Loss = -12455.22250522236
Iteration 1500: Loss = -12455.210620242182
Iteration 1600: Loss = -12455.200457087434
Iteration 1700: Loss = -12455.191016456134
Iteration 1800: Loss = -12455.18186307401
Iteration 1900: Loss = -12455.172677483577
Iteration 2000: Loss = -12455.163483897706
Iteration 2100: Loss = -12455.154604186577
Iteration 2200: Loss = -12455.146279125049
Iteration 2300: Loss = -12455.138741087218
Iteration 2400: Loss = -12455.132157167238
Iteration 2500: Loss = -12455.126570521072
Iteration 2600: Loss = -12455.12187789274
Iteration 2700: Loss = -12455.11789469562
Iteration 2800: Loss = -12455.114536058327
Iteration 2900: Loss = -12455.11162402787
Iteration 3000: Loss = -12455.109019629226
Iteration 3100: Loss = -12455.106713080735
Iteration 3200: Loss = -12455.104543916583
Iteration 3300: Loss = -12455.102346990798
Iteration 3400: Loss = -12455.100144104465
Iteration 3500: Loss = -12455.097810603023
Iteration 3600: Loss = -12455.095241795192
Iteration 3700: Loss = -12455.09237805927
Iteration 3800: Loss = -12455.089103330984
Iteration 3900: Loss = -12455.085305926532
Iteration 4000: Loss = -12455.081071410144
Iteration 4100: Loss = -12455.076327416335
Iteration 4200: Loss = -12455.071271114068
Iteration 4300: Loss = -12455.065966436938
Iteration 4400: Loss = -12455.060696896786
Iteration 4500: Loss = -12455.05532630778
Iteration 4600: Loss = -12455.048734899136
Iteration 4700: Loss = -12455.020337543472
Iteration 4800: Loss = -12454.7923991202
Iteration 4900: Loss = -12454.774611350229
Iteration 5000: Loss = -12454.760773044285
Iteration 5100: Loss = -12454.747577683314
Iteration 5200: Loss = -12454.73580617764
Iteration 5300: Loss = -12454.726332050754
Iteration 5400: Loss = -12454.72033530468
Iteration 5500: Loss = -12454.715633927466
Iteration 5600: Loss = -12454.712838050267
Iteration 5700: Loss = -12454.710441098961
Iteration 5800: Loss = -12454.708996047366
Iteration 5900: Loss = -12454.707662459317
Iteration 6000: Loss = -12454.70919425724
1
Iteration 6100: Loss = -12454.706132766103
Iteration 6200: Loss = -12454.705599981493
Iteration 6300: Loss = -12454.769622965487
1
Iteration 6400: Loss = -12454.704984371649
Iteration 6500: Loss = -12454.704729765721
Iteration 6600: Loss = -12454.706911353569
1
Iteration 6700: Loss = -12454.704523284778
Iteration 6800: Loss = -12454.704439459883
Iteration 6900: Loss = -12454.704359095253
Iteration 7000: Loss = -12454.704528502169
1
Iteration 7100: Loss = -12454.704745501218
2
Iteration 7200: Loss = -12454.704612749969
3
Iteration 7300: Loss = -12454.70415705072
Iteration 7400: Loss = -12454.704187828767
1
Iteration 7500: Loss = -12454.704923181644
2
Iteration 7600: Loss = -12454.704159881516
3
Iteration 7700: Loss = -12454.703907777093
Iteration 7800: Loss = -12454.70435956521
1
Iteration 7900: Loss = -12454.705191416278
2
Iteration 8000: Loss = -12454.703709603988
Iteration 8100: Loss = -12454.703666479381
Iteration 8200: Loss = -12454.944892517917
1
Iteration 8300: Loss = -12454.703380302146
Iteration 8400: Loss = -12455.07146885418
1
Iteration 8500: Loss = -12454.703223751334
Iteration 8600: Loss = -12454.717587340383
1
Iteration 8700: Loss = -12454.703149119054
Iteration 8800: Loss = -12454.71622882516
1
Iteration 8900: Loss = -12454.703520264698
2
Iteration 9000: Loss = -12454.707401327976
3
Iteration 9100: Loss = -12454.702774727135
Iteration 9200: Loss = -12454.751678262766
1
Iteration 9300: Loss = -12454.70401376488
2
Iteration 9400: Loss = -12454.706740887324
3
Iteration 9500: Loss = -12454.72641172213
4
Iteration 9600: Loss = -12454.736680902603
5
Stopping early at iteration 9600 due to no improvement.
pi: tensor([[0.9952, 0.0048],
        [0.1821, 0.8179]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0298, 0.9702], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1997, 0.1308],
         [0.5229, 0.2043]],

        [[0.5487, 0.1984],
         [0.5890, 0.5496]],

        [[0.6366, 0.2040],
         [0.7259, 0.7103]],

        [[0.6997, 0.2061],
         [0.7158, 0.5901]],

        [[0.6959, 0.1986],
         [0.7197, 0.7172]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0031985518644306096
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.005066097656929834
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0010307241075050771
Average Adjusted Rand Index: -0.0003735091584998448
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21018.32284577303
Iteration 100: Loss = -12456.230193093348
Iteration 200: Loss = -12455.67860840127
Iteration 300: Loss = -12455.55141907822
Iteration 400: Loss = -12455.488730727211
Iteration 500: Loss = -12455.448443913467
Iteration 600: Loss = -12455.418350784064
Iteration 700: Loss = -12455.39425387014
Iteration 800: Loss = -12455.374200799617
Iteration 900: Loss = -12455.356660883745
Iteration 1000: Loss = -12455.34086242935
Iteration 1100: Loss = -12455.326097521425
Iteration 1200: Loss = -12455.311851320264
Iteration 1300: Loss = -12455.297792701627
Iteration 1400: Loss = -12455.283983265603
Iteration 1500: Loss = -12455.270448728934
Iteration 1600: Loss = -12455.257518422031
Iteration 1700: Loss = -12455.245563111635
Iteration 1800: Loss = -12455.23469988351
Iteration 1900: Loss = -12455.22485489201
Iteration 2000: Loss = -12455.215907402064
Iteration 2100: Loss = -12455.207676377871
Iteration 2200: Loss = -12455.199942003812
Iteration 2300: Loss = -12455.192532262567
Iteration 2400: Loss = -12455.185396125718
Iteration 2500: Loss = -12455.17831975921
Iteration 2600: Loss = -12455.171486962032
Iteration 2700: Loss = -12455.164822114948
Iteration 2800: Loss = -12455.15835030781
Iteration 2900: Loss = -12455.152247702787
Iteration 3000: Loss = -12455.14656460685
Iteration 3100: Loss = -12455.141261771585
Iteration 3200: Loss = -12455.136433544552
Iteration 3300: Loss = -12455.132038608488
Iteration 3400: Loss = -12455.128120145198
Iteration 3500: Loss = -12455.124595706382
Iteration 3600: Loss = -12455.121447543985
Iteration 3700: Loss = -12455.11868180691
Iteration 3800: Loss = -12455.116155125852
Iteration 3900: Loss = -12455.113804993798
Iteration 4000: Loss = -12455.11163077116
Iteration 4100: Loss = -12455.10943743022
Iteration 4200: Loss = -12455.106961807482
Iteration 4300: Loss = -12455.103392555948
Iteration 4400: Loss = -12455.09511146515
Iteration 4500: Loss = -12455.044360175358
Iteration 4600: Loss = -12454.845397670091
Iteration 4700: Loss = -12454.821926726976
Iteration 4800: Loss = -12454.813250926236
Iteration 4900: Loss = -12454.80801352638
Iteration 5000: Loss = -12454.8042351081
Iteration 5100: Loss = -12454.800683558211
Iteration 5200: Loss = -12454.797280059462
Iteration 5300: Loss = -12454.793456979593
Iteration 5400: Loss = -12454.789452688397
Iteration 5500: Loss = -12454.784508188359
Iteration 5600: Loss = -12454.779141528174
Iteration 5700: Loss = -12454.77727994929
Iteration 5800: Loss = -12454.765094812345
Iteration 5900: Loss = -12454.756771704262
Iteration 6000: Loss = -12454.746790881136
Iteration 6100: Loss = -12454.767101355303
1
Iteration 6200: Loss = -12454.72403732791
Iteration 6300: Loss = -12454.70527261835
Iteration 6400: Loss = -12454.689059626973
Iteration 6500: Loss = -12454.68852609871
Iteration 6600: Loss = -12454.667413783847
Iteration 6700: Loss = -12454.663292276178
Iteration 6800: Loss = -12454.66033694763
Iteration 6900: Loss = -12454.658578312672
Iteration 7000: Loss = -12454.657083459564
Iteration 7100: Loss = -12454.655273934057
Iteration 7200: Loss = -12454.654477137456
Iteration 7300: Loss = -12454.652410625009
Iteration 7400: Loss = -12454.656318124933
1
Iteration 7500: Loss = -12454.65390496192
2
Iteration 7600: Loss = -12454.704792274611
3
Iteration 7700: Loss = -12454.652899392824
4
Iteration 7800: Loss = -12454.649088758419
Iteration 7900: Loss = -12454.647302119545
Iteration 8000: Loss = -12454.648724898298
1
Iteration 8100: Loss = -12454.714440542559
2
Iteration 8200: Loss = -12454.743562437776
3
Iteration 8300: Loss = -12454.64700317768
Iteration 8400: Loss = -12454.645638174716
Iteration 8500: Loss = -12454.647770012101
1
Iteration 8600: Loss = -12454.678893007782
2
Iteration 8700: Loss = -12454.66440284771
3
Iteration 8800: Loss = -12454.65375760266
4
Iteration 8900: Loss = -12454.647799968046
5
Stopping early at iteration 8900 due to no improvement.
pi: tensor([[0.1278, 0.8722],
        [0.4728, 0.5272]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9668, 0.0332], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2059, 0.1304],
         [0.5165, 0.2008]],

        [[0.7063, 0.2004],
         [0.5540, 0.6844]],

        [[0.5311, 0.2054],
         [0.6706, 0.5442]],

        [[0.6523, 0.2062],
         [0.5543, 0.6271]],

        [[0.5981, 0.1995],
         [0.5879, 0.5177]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.000465880155123834
Average Adjusted Rand Index: 0.0
11977.770198707021
[-0.000465880155123834, -0.000465880155123834, -0.0010307241075050771, -0.000465880155123834] [0.0, 0.0, -0.0003735091584998448, 0.0] [12454.644657598294, 12454.64900972032, 12454.736680902603, 12454.647799968046]
-----------------------------------------------------------------------------------------
This iteration is 18
True Objective function: Loss = -11863.786861841018
Iteration 0: Loss = -12507.161074424852
Iteration 10: Loss = -12329.295959809706
Iteration 20: Loss = -12328.948485306119
Iteration 30: Loss = -12328.4034980815
Iteration 40: Loss = -12328.17360248988
Iteration 50: Loss = -12328.080681734438
Iteration 60: Loss = -12327.988783453688
Iteration 70: Loss = -12327.78246241952
Iteration 80: Loss = -12324.617338892376
Iteration 90: Loss = -11970.784730517958
Iteration 100: Loss = -11848.771302957352
Iteration 110: Loss = -11848.771281706613
Iteration 120: Loss = -11848.771290239092
1
Iteration 130: Loss = -11848.771290186198
2
Iteration 140: Loss = -11848.771290186198
3
Stopping early at iteration 140 due to no improvement.
pi: tensor([[0.7643, 0.2357],
        [0.2623, 0.7377]], dtype=torch.float64)
alpha: tensor([0.4865, 0.5135])
beta: tensor([[[0.2827, 0.1121],
         [0.1648, 0.2960]],

        [[0.1768, 0.0964],
         [0.4335, 0.3948]],

        [[0.3501, 0.0926],
         [0.2568, 0.6183]],

        [[0.8916, 0.1055],
         [0.5353, 0.7963]],

        [[0.2273, 0.1003],
         [0.9354, 0.0824]]], dtype=torch.float64)
time is 0
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824191749678101
Global Adjusted Rand Index: 0.9681923375482518
Average Adjusted Rand Index: 0.9684834901975574
Iteration 0: Loss = -12331.497645659021
Iteration 10: Loss = -12330.983380816248
Iteration 20: Loss = -12329.70408003663
Iteration 30: Loss = -12329.064613717735
Iteration 40: Loss = -12328.807433649545
Iteration 50: Loss = -12328.694457460537
Iteration 60: Loss = -12328.64372882604
Iteration 70: Loss = -12328.612991979004
Iteration 80: Loss = -12328.585900157952
Iteration 90: Loss = -12328.55542763617
Iteration 100: Loss = -12328.526444753128
Iteration 110: Loss = -12328.51007391417
Iteration 120: Loss = -12328.5046225205
Iteration 130: Loss = -12328.498427846447
Iteration 140: Loss = -12328.471774949801
Iteration 150: Loss = -12328.419046501564
Iteration 160: Loss = -12328.370493639404
Iteration 170: Loss = -12328.33709462174
Iteration 180: Loss = -12328.309645765006
Iteration 190: Loss = -12328.28150850253
Iteration 200: Loss = -12328.250579995596
Iteration 210: Loss = -12328.216560818006
Iteration 220: Loss = -12328.178972559806
Iteration 230: Loss = -12328.135977482003
Iteration 240: Loss = -12328.082774837185
Iteration 250: Loss = -12328.0046093008
Iteration 260: Loss = -12327.835857598315
Iteration 270: Loss = -12326.353762425202
Iteration 280: Loss = -12092.097620407632
Iteration 290: Loss = -11848.771829918822
Iteration 300: Loss = -11848.771309983178
Iteration 310: Loss = -11848.771284501609
Iteration 320: Loss = -11848.771290186198
1
Iteration 330: Loss = -11848.771290186198
2
Iteration 340: Loss = -11848.771290186198
3
Stopping early at iteration 340 due to no improvement.
pi: tensor([[0.7643, 0.2357],
        [0.2623, 0.7377]], dtype=torch.float64)
alpha: tensor([0.4865, 0.5135])
beta: tensor([[[0.2827, 0.1121],
         [0.2468, 0.2960]],

        [[0.7496, 0.0964],
         [0.2581, 0.1095]],

        [[0.8513, 0.0926],
         [0.6724, 0.4950]],

        [[0.4114, 0.1055],
         [0.7704, 0.3755]],

        [[0.8208, 0.1003],
         [0.3756, 0.1859]]], dtype=torch.float64)
time is 0
tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599982760199767
time is 2
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 4
tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824191749678101
Global Adjusted Rand Index: 0.9681923375482518
Average Adjusted Rand Index: 0.9684834901975574
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22028.05572178307
Iteration 100: Loss = -12331.190197751075
Iteration 200: Loss = -12330.748871647053
Iteration 300: Loss = -12330.609516324546
Iteration 400: Loss = -12330.511337787872
Iteration 500: Loss = -12330.419279127886
Iteration 600: Loss = -12330.31622828053
Iteration 700: Loss = -12330.1699114773
Iteration 800: Loss = -12329.900073040248
Iteration 900: Loss = -12329.441957794606
Iteration 1000: Loss = -12329.083320371434
Iteration 1100: Loss = -12328.869484300334
Iteration 1200: Loss = -12328.713257502248
Iteration 1300: Loss = -12328.589482107505
Iteration 1400: Loss = -12328.492899113773
Iteration 1500: Loss = -12328.419671230606
Iteration 1600: Loss = -12328.364055914197
Iteration 1700: Loss = -12328.319928120874
Iteration 1800: Loss = -12328.281295559775
Iteration 1900: Loss = -12328.24241717259
Iteration 2000: Loss = -12328.197934449494
Iteration 2100: Loss = -12328.143451327034
Iteration 2200: Loss = -12328.081039388238
Iteration 2300: Loss = -12328.02012252122
Iteration 2400: Loss = -12327.964911220797
Iteration 2500: Loss = -12327.912094819307
Iteration 2600: Loss = -12327.8639978036
Iteration 2700: Loss = -12327.82934698341
Iteration 2800: Loss = -12327.808161424025
Iteration 2900: Loss = -12327.794984206788
Iteration 3000: Loss = -12327.786653908612
Iteration 3100: Loss = -12327.782442277028
Iteration 3200: Loss = -12327.777891558439
Iteration 3300: Loss = -12327.775333628013
Iteration 3400: Loss = -12327.773499678438
Iteration 3500: Loss = -12327.77143383301
Iteration 3600: Loss = -12327.769756409469
Iteration 3700: Loss = -12327.768251274565
Iteration 3800: Loss = -12327.76674449488
Iteration 3900: Loss = -12327.765400915932
Iteration 4000: Loss = -12327.764184273712
Iteration 4100: Loss = -12327.762980087353
Iteration 4200: Loss = -12327.7631218706
1
Iteration 4300: Loss = -12327.760799040478
Iteration 4400: Loss = -12327.759908373211
Iteration 4500: Loss = -12327.759261424466
Iteration 4600: Loss = -12327.75809307227
Iteration 4700: Loss = -12327.75732266766
Iteration 4800: Loss = -12327.75658105868
Iteration 4900: Loss = -12327.755824318196
Iteration 5000: Loss = -12327.759301169346
1
Iteration 5100: Loss = -12327.754570822457
Iteration 5200: Loss = -12327.754022081263
Iteration 5300: Loss = -12327.753503578706
Iteration 5400: Loss = -12327.752923374577
Iteration 5500: Loss = -12327.752437066803
Iteration 5600: Loss = -12327.753460290478
1
Iteration 5700: Loss = -12327.7515899131
Iteration 5800: Loss = -12327.75117477152
Iteration 5900: Loss = -12327.750937422528
Iteration 6000: Loss = -12327.750457130143
Iteration 6100: Loss = -12327.75013443221
Iteration 6200: Loss = -12327.749846593959
Iteration 6300: Loss = -12327.749564587928
Iteration 6400: Loss = -12327.75271633267
1
Iteration 6500: Loss = -12327.749009573361
Iteration 6600: Loss = -12327.748771303859
Iteration 6700: Loss = -12327.748653670122
Iteration 6800: Loss = -12327.748306973112
Iteration 6900: Loss = -12327.748995413855
1
Iteration 7000: Loss = -12327.74793996499
Iteration 7100: Loss = -12327.747715888276
Iteration 7200: Loss = -12327.7476456295
Iteration 7300: Loss = -12327.955176440246
1
Iteration 7400: Loss = -12327.747256750714
Iteration 7500: Loss = -12327.75312261921
1
Iteration 7600: Loss = -12327.746970560387
Iteration 7700: Loss = -12327.76084394102
1
Iteration 7800: Loss = -12327.746678432104
Iteration 7900: Loss = -12327.749573005929
1
Iteration 8000: Loss = -12327.746523146003
Iteration 8100: Loss = -12327.746383451162
Iteration 8200: Loss = -12327.746785022835
1
Iteration 8300: Loss = -12327.746170839278
Iteration 8400: Loss = -12328.03619086893
1
Iteration 8500: Loss = -12327.745996003896
Iteration 8600: Loss = -12327.745868975802
Iteration 8700: Loss = -12328.363222201746
1
Iteration 8800: Loss = -12327.745763905607
Iteration 8900: Loss = -12327.745699485165
Iteration 9000: Loss = -12327.84740362619
1
Iteration 9100: Loss = -12327.74554835543
Iteration 9200: Loss = -12327.74553750955
Iteration 9300: Loss = -12327.745784845556
1
Iteration 9400: Loss = -12327.745471003742
Iteration 9500: Loss = -12327.745345508165
Iteration 9600: Loss = -12327.856526080997
1
Iteration 9700: Loss = -12327.745249987474
Iteration 9800: Loss = -12327.745210858417
Iteration 9900: Loss = -12327.759293390083
1
Iteration 10000: Loss = -12327.74510845372
Iteration 10100: Loss = -12327.747189277805
1
Iteration 10200: Loss = -12327.745050394566
Iteration 10300: Loss = -12327.753068182737
1
Iteration 10400: Loss = -12327.756309566546
2
Iteration 10500: Loss = -12327.7452768478
3
Iteration 10600: Loss = -12327.744916852334
Iteration 10700: Loss = -12327.745026678445
1
Iteration 10800: Loss = -12327.744928472994
2
Iteration 10900: Loss = -12327.897585436645
3
Iteration 11000: Loss = -12327.74785955121
4
Iteration 11100: Loss = -12327.76179794444
5
Stopping early at iteration 11100 due to no improvement.
pi: tensor([[4.5612e-04, 9.9954e-01],
        [1.2503e-01, 8.7497e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0130, 0.9870], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1438, 0.1897],
         [0.5168, 0.2048]],

        [[0.7040, 0.1493],
         [0.6063, 0.6960]],

        [[0.5689, 0.1668],
         [0.6626, 0.5277]],

        [[0.6740, 0.1648],
         [0.6813, 0.6329]],

        [[0.6422, 0.1944],
         [0.6551, 0.6802]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 59
Adjusted Rand Index: 0.01382061954603653
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0004929790577405324
Average Adjusted Rand Index: 0.002764123909207306
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21121.541895296225
Iteration 100: Loss = -12331.21727516114
Iteration 200: Loss = -12330.442753238625
Iteration 300: Loss = -12329.932945176159
Iteration 400: Loss = -12329.399425341902
Iteration 500: Loss = -12328.931864838984
Iteration 600: Loss = -12328.682851571546
Iteration 700: Loss = -12328.543604634098
Iteration 800: Loss = -12328.451013956248
Iteration 900: Loss = -12328.382710539892
Iteration 1000: Loss = -12328.328344669546
Iteration 1100: Loss = -12328.28098258136
Iteration 1200: Loss = -12328.234323082812
Iteration 1300: Loss = -12328.182015540597
Iteration 1400: Loss = -12328.117794424461
Iteration 1500: Loss = -12328.03931264504
Iteration 1600: Loss = -12327.952429541096
Iteration 1700: Loss = -12327.874144082822
Iteration 1800: Loss = -12327.831046565689
Iteration 1900: Loss = -12327.811321550631
Iteration 2000: Loss = -12327.7995955041
Iteration 2100: Loss = -12327.792420188573
Iteration 2200: Loss = -12327.787375858783
Iteration 2300: Loss = -12327.783400084596
Iteration 2400: Loss = -12327.779910677129
Iteration 2500: Loss = -12327.776775557175
Iteration 2600: Loss = -12327.773932103726
Iteration 2700: Loss = -12327.771492374419
Iteration 2800: Loss = -12327.76909274887
Iteration 2900: Loss = -12327.767036297597
Iteration 3000: Loss = -12327.765495584135
Iteration 3100: Loss = -12327.763669896827
Iteration 3200: Loss = -12327.762206296336
Iteration 3300: Loss = -12327.760890362793
Iteration 3400: Loss = -12327.759712455314
Iteration 3500: Loss = -12327.785253819355
1
Iteration 3600: Loss = -12327.757635909285
Iteration 3700: Loss = -12327.756746168146
Iteration 3800: Loss = -12327.757275297887
1
Iteration 3900: Loss = -12327.755156747004
Iteration 4000: Loss = -12327.754432979025
Iteration 4100: Loss = -12327.753749402984
Iteration 4200: Loss = -12327.75315271918
Iteration 4300: Loss = -12327.753138384292
Iteration 4400: Loss = -12327.752035081205
Iteration 4500: Loss = -12327.751520156708
Iteration 4600: Loss = -12327.751077866345
Iteration 4700: Loss = -12327.750658767145
Iteration 4800: Loss = -12327.784982941363
1
Iteration 4900: Loss = -12327.749901071038
Iteration 5000: Loss = -12327.749549285629
Iteration 5100: Loss = -12327.750935015369
1
Iteration 5200: Loss = -12327.748938374381
Iteration 5300: Loss = -12327.748658349587
Iteration 5400: Loss = -12327.748549989985
Iteration 5500: Loss = -12327.748163027607
Iteration 5600: Loss = -12327.748683797752
1
Iteration 5700: Loss = -12327.747742003556
Iteration 5800: Loss = -12327.747558143887
Iteration 5900: Loss = -12327.747333038122
Iteration 6000: Loss = -12327.747171003628
Iteration 6100: Loss = -12327.747316328803
1
Iteration 6200: Loss = -12327.746882369715
Iteration 6300: Loss = -12327.746745478466
Iteration 6400: Loss = -12327.746592464053
Iteration 6500: Loss = -12327.746471233744
Iteration 6600: Loss = -12327.748047616027
1
Iteration 6700: Loss = -12327.746247196028
Iteration 6800: Loss = -12327.746132553391
Iteration 6900: Loss = -12327.746026346143
Iteration 7000: Loss = -12327.745957786427
Iteration 7100: Loss = -12327.746013962054
1
Iteration 7200: Loss = -12327.745808654072
Iteration 7300: Loss = -12327.78668656799
1
Iteration 7400: Loss = -12327.745588268323
Iteration 7500: Loss = -12327.922720063823
1
Iteration 7600: Loss = -12327.745501369945
Iteration 7700: Loss = -12327.756578377333
1
Iteration 7800: Loss = -12327.745383791353
Iteration 7900: Loss = -12327.752724130947
1
Iteration 8000: Loss = -12327.745280427083
Iteration 8100: Loss = -12328.1494289194
1
Iteration 8200: Loss = -12327.745212931142
Iteration 8300: Loss = -12327.745121116512
Iteration 8400: Loss = -12327.745160321214
1
Iteration 8500: Loss = -12327.745042494662
Iteration 8600: Loss = -12327.745019092246
Iteration 8700: Loss = -12327.745127437322
1
Iteration 8800: Loss = -12327.744958492025
Iteration 8900: Loss = -12327.744982459322
1
Iteration 9000: Loss = -12327.744918377764
Iteration 9100: Loss = -12327.744898430517
Iteration 9200: Loss = -12327.852810711833
1
Iteration 9300: Loss = -12327.793765286095
2
Iteration 9400: Loss = -12327.759533695662
3
Iteration 9500: Loss = -12327.745537585804
4
Iteration 9600: Loss = -12327.942189449082
5
Stopping early at iteration 9600 due to no improvement.
pi: tensor([[4.2112e-04, 9.9958e-01],
        [1.2803e-01, 8.7197e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0132, 0.9868], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1437, 0.1895],
         [0.5532, 0.2069]],

        [[0.6781, 0.1497],
         [0.5250, 0.5225]],

        [[0.6168, 0.1665],
         [0.6313, 0.6535]],

        [[0.6021, 0.1639],
         [0.7263, 0.6190]],

        [[0.7258, 0.1928],
         [0.6544, 0.6486]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 59
Adjusted Rand Index: 0.01382061954603653
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0004929790577405324
Average Adjusted Rand Index: 0.002764123909207306
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20899.43765278926
Iteration 100: Loss = -12331.57942024254
Iteration 200: Loss = -12330.97202527198
Iteration 300: Loss = -12330.768447060771
Iteration 400: Loss = -12330.606787679642
Iteration 500: Loss = -12330.438145776809
Iteration 600: Loss = -12330.277131354163
Iteration 700: Loss = -12330.137660477989
Iteration 800: Loss = -12330.010781272907
Iteration 900: Loss = -12329.891016044614
Iteration 1000: Loss = -12329.766974191682
Iteration 1100: Loss = -12329.6066373373
Iteration 1200: Loss = -12329.322023304832
Iteration 1300: Loss = -12328.836968192069
Iteration 1400: Loss = -12328.526822946997
Iteration 1500: Loss = -12328.39707750297
Iteration 1600: Loss = -12328.326509868892
Iteration 1700: Loss = -12328.275941257638
Iteration 1800: Loss = -12328.229663058071
Iteration 1900: Loss = -12328.178488538673
Iteration 2000: Loss = -12328.11599632779
Iteration 2100: Loss = -12328.04276806145
Iteration 2200: Loss = -12327.96812665227
Iteration 2300: Loss = -12327.897219443212
Iteration 2400: Loss = -12327.83986132442
Iteration 2500: Loss = -12327.80654320134
Iteration 2600: Loss = -12327.790972196377
Iteration 2700: Loss = -12327.783995685286
Iteration 2800: Loss = -12327.815077980014
1
Iteration 2900: Loss = -12327.777949724674
Iteration 3000: Loss = -12327.775984608861
Iteration 3100: Loss = -12327.774224670382
Iteration 3200: Loss = -12327.772670089005
Iteration 3300: Loss = -12327.771241766251
Iteration 3400: Loss = -12327.769875364591
Iteration 3500: Loss = -12327.768618321012
Iteration 3600: Loss = -12327.76742251007
Iteration 3700: Loss = -12327.766263809961
Iteration 3800: Loss = -12327.765176881143
Iteration 3900: Loss = -12327.764137839076
Iteration 4000: Loss = -12327.766657775986
1
Iteration 4100: Loss = -12327.762246018625
Iteration 4200: Loss = -12327.761336268086
Iteration 4300: Loss = -12327.760812212582
Iteration 4400: Loss = -12327.759739609106
Iteration 4500: Loss = -12327.759087710108
Iteration 4600: Loss = -12327.758222814064
Iteration 4700: Loss = -12327.757524022678
Iteration 4800: Loss = -12327.757021225714
Iteration 4900: Loss = -12327.756203385297
Iteration 5000: Loss = -12327.755604899226
Iteration 5100: Loss = -12327.755109534142
Iteration 5200: Loss = -12327.754492900236
Iteration 5300: Loss = -12327.754007336745
Iteration 5400: Loss = -12327.753476954593
Iteration 5500: Loss = -12327.75300464738
Iteration 5600: Loss = -12327.752543727076
Iteration 5700: Loss = -12327.754798777072
1
Iteration 5800: Loss = -12327.751737663848
Iteration 5900: Loss = -12327.751348560241
Iteration 6000: Loss = -12327.751103998944
Iteration 6100: Loss = -12327.750610009845
Iteration 6200: Loss = -12327.76032286139
1
Iteration 6300: Loss = -12327.749977720383
Iteration 6400: Loss = -12327.749692392706
Iteration 6500: Loss = -12327.74938074664
Iteration 6600: Loss = -12327.749157992557
Iteration 6700: Loss = -12327.748982872434
Iteration 6800: Loss = -12327.74868257951
Iteration 6900: Loss = -12327.751388468621
1
Iteration 7000: Loss = -12327.823966766242
2
Iteration 7100: Loss = -12327.747991422384
Iteration 7200: Loss = -12327.750305548256
1
Iteration 7300: Loss = -12327.747650467209
Iteration 7400: Loss = -12327.77379612869
1
Iteration 7500: Loss = -12327.747330474664
Iteration 7600: Loss = -12327.747180900264
Iteration 7700: Loss = -12327.749755517467
1
Iteration 7800: Loss = -12327.746842122484
Iteration 7900: Loss = -12327.746742087213
Iteration 8000: Loss = -12327.747099012213
1
Iteration 8100: Loss = -12327.74651125917
Iteration 8200: Loss = -12327.746490502082
Iteration 8300: Loss = -12327.746302580394
Iteration 8400: Loss = -12327.74618658122
Iteration 8500: Loss = -12327.74610059093
Iteration 8600: Loss = -12327.746090315795
Iteration 8700: Loss = -12327.74592931507
Iteration 8800: Loss = -12327.745906198796
Iteration 8900: Loss = -12327.745832834378
Iteration 9000: Loss = -12327.745719124627
Iteration 9100: Loss = -12327.745590654808
Iteration 9200: Loss = -12327.74794647427
1
Iteration 9300: Loss = -12327.745489116021
Iteration 9400: Loss = -12327.745428621056
Iteration 9500: Loss = -12327.747803432901
1
Iteration 9600: Loss = -12327.745327828008
Iteration 9700: Loss = -12327.794893785269
1
Iteration 9800: Loss = -12327.745278067587
Iteration 9900: Loss = -12327.745231401652
Iteration 10000: Loss = -12327.804917656602
1
Iteration 10100: Loss = -12327.745120937017
Iteration 10200: Loss = -12327.746229937995
1
Iteration 10300: Loss = -12327.745000948857
Iteration 10400: Loss = -12327.835964320442
1
Iteration 10500: Loss = -12327.821019246636
2
Iteration 10600: Loss = -12327.797746129947
3
Iteration 10700: Loss = -12327.745249535323
4
Iteration 10800: Loss = -12327.747247331426
5
Stopping early at iteration 10800 due to no improvement.
pi: tensor([[8.7410e-01, 1.2590e-01],
        [9.9947e-01, 5.3406e-04]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9869, 0.0131], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2051, 0.1895],
         [0.5558, 0.1438]],

        [[0.7064, 0.1495],
         [0.5954, 0.7007]],

        [[0.5093, 0.1670],
         [0.7184, 0.6794]],

        [[0.6687, 0.1643],
         [0.6166, 0.6984]],

        [[0.5967, 0.1942],
         [0.6619, 0.6799]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 41
Adjusted Rand Index: 0.01382061954603653
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0004929790577405324
Average Adjusted Rand Index: 0.002764123909207306
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21591.890995246576
Iteration 100: Loss = -12331.073609641608
Iteration 200: Loss = -12330.719853751789
Iteration 300: Loss = -12330.627090477225
Iteration 400: Loss = -12330.55493554898
Iteration 500: Loss = -12330.489619344577
Iteration 600: Loss = -12330.441886741664
Iteration 700: Loss = -12330.392142702887
Iteration 800: Loss = -12330.327753997271
Iteration 900: Loss = -12330.224604176374
Iteration 1000: Loss = -12330.000544891063
Iteration 1100: Loss = -12329.210672174893
Iteration 1200: Loss = -12328.40443772128
Iteration 1300: Loss = -12328.21921990726
Iteration 1400: Loss = -12328.105087719
Iteration 1500: Loss = -12328.013585770856
Iteration 1600: Loss = -12327.922187715138
Iteration 1700: Loss = -12327.810404978452
Iteration 1800: Loss = -12327.760456407519
Iteration 1900: Loss = -12327.755793607099
Iteration 2000: Loss = -12327.754516334593
Iteration 2100: Loss = -12327.753879391203
Iteration 2200: Loss = -12327.753314689078
Iteration 2300: Loss = -12327.752862232563
Iteration 2400: Loss = -12327.752396068287
Iteration 2500: Loss = -12327.753289112125
1
Iteration 2600: Loss = -12327.751660911623
Iteration 2700: Loss = -12327.751308033468
Iteration 2800: Loss = -12327.750964796729
Iteration 2900: Loss = -12327.750600411877
Iteration 3000: Loss = -12327.750341512135
Iteration 3100: Loss = -12327.750030712174
Iteration 3200: Loss = -12327.74976058608
Iteration 3300: Loss = -12327.750023333185
1
Iteration 3400: Loss = -12327.749235756582
Iteration 3500: Loss = -12327.749579768904
1
Iteration 3600: Loss = -12327.748758294232
Iteration 3700: Loss = -12327.748496853508
Iteration 3800: Loss = -12327.748305427282
Iteration 3900: Loss = -12327.748129501479
Iteration 4000: Loss = -12327.749501979095
1
Iteration 4100: Loss = -12327.747733001484
Iteration 4200: Loss = -12327.74756919432
Iteration 4300: Loss = -12327.747456732077
Iteration 4400: Loss = -12327.747236333995
Iteration 4500: Loss = -12327.747508954846
1
Iteration 4600: Loss = -12327.74695517928
Iteration 4700: Loss = -12327.74738160372
1
Iteration 4800: Loss = -12327.746747936848
Iteration 4900: Loss = -12327.746581684778
Iteration 5000: Loss = -12327.74656264348
Iteration 5100: Loss = -12327.746365138488
Iteration 5200: Loss = -12327.759128640695
1
Iteration 5300: Loss = -12327.74618032091
Iteration 5400: Loss = -12327.746121251732
Iteration 5500: Loss = -12327.7460673873
Iteration 5600: Loss = -12327.74595594622
Iteration 5700: Loss = -12327.754555273683
1
Iteration 5800: Loss = -12327.745798221664
Iteration 5900: Loss = -12327.745672610394
Iteration 6000: Loss = -12327.745786522468
1
Iteration 6100: Loss = -12327.745549066716
Iteration 6200: Loss = -12327.745640929414
1
Iteration 6300: Loss = -12327.745430592735
Iteration 6400: Loss = -12327.745396460792
Iteration 6500: Loss = -12327.745699975078
1
Iteration 6600: Loss = -12327.745320734812
Iteration 6700: Loss = -12327.745276909087
Iteration 6800: Loss = -12327.745192415881
Iteration 6900: Loss = -12327.74514496189
Iteration 7000: Loss = -12327.745915831809
1
Iteration 7100: Loss = -12327.745073557253
Iteration 7200: Loss = -12327.746424146902
1
Iteration 7300: Loss = -12327.745056245953
Iteration 7400: Loss = -12327.781633548819
1
Iteration 7500: Loss = -12327.744947105539
Iteration 7600: Loss = -12328.053088747674
1
Iteration 7700: Loss = -12327.744915229481
Iteration 7800: Loss = -12327.744908644889
Iteration 7900: Loss = -12327.744884704696
Iteration 8000: Loss = -12327.744849743727
Iteration 8100: Loss = -12328.010972050584
1
Iteration 8200: Loss = -12327.744819846617
Iteration 8300: Loss = -12327.744767253636
Iteration 8400: Loss = -12327.836946079127
1
Iteration 8500: Loss = -12327.74472079402
Iteration 8600: Loss = -12327.744724447788
1
Iteration 8700: Loss = -12327.745553519326
2
Iteration 8800: Loss = -12327.744705392633
Iteration 8900: Loss = -12327.801267847117
1
Iteration 9000: Loss = -12327.74463145673
Iteration 9100: Loss = -12327.744629080906
Iteration 9200: Loss = -12327.745643732087
1
Iteration 9300: Loss = -12327.744638823613
2
Iteration 9400: Loss = -12327.744591178674
Iteration 9500: Loss = -12327.744821960734
1
Iteration 9600: Loss = -12327.744553804378
Iteration 9700: Loss = -12327.74591000274
1
Iteration 9800: Loss = -12327.744522714223
Iteration 9900: Loss = -12327.791586650987
1
Iteration 10000: Loss = -12327.744533757117
2
Iteration 10100: Loss = -12327.77144835941
3
Iteration 10200: Loss = -12327.756431545087
4
Iteration 10300: Loss = -12328.006662081189
5
Stopping early at iteration 10300 due to no improvement.
pi: tensor([[8.7218e-01, 1.2782e-01],
        [9.9985e-01, 1.4873e-04]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9867, 0.0133], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2069, 0.1896],
         [0.6633, 0.1437]],

        [[0.7289, 0.1500],
         [0.6237, 0.7084]],

        [[0.5502, 0.1656],
         [0.6320, 0.6084]],

        [[0.6637, 0.1639],
         [0.6829, 0.6163]],

        [[0.6702, 0.1927],
         [0.6469, 0.5153]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        1, 0, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 41
Adjusted Rand Index: 0.01382061954603653
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0004929790577405324
Average Adjusted Rand Index: 0.002764123909207306
11863.786861841018
[0.0004929790577405324, 0.0004929790577405324, 0.0004929790577405324, 0.0004929790577405324] [0.002764123909207306, 0.002764123909207306, 0.002764123909207306, 0.002764123909207306] [12327.76179794444, 12327.942189449082, 12327.747247331426, 12328.006662081189]
-----------------------------------------------------------------------------------------
This iteration is 19
True Objective function: Loss = -11960.557349129685
Iteration 0: Loss = -12679.629830143089
Iteration 10: Loss = -12478.618184590314
Iteration 20: Loss = -12478.589319212182
Iteration 30: Loss = -12478.589080087466
Iteration 40: Loss = -12478.589249584285
1
Iteration 50: Loss = -12478.589340957655
2
Iteration 60: Loss = -12478.589337299816
3
Stopping early at iteration 60 due to no improvement.
pi: tensor([[0.5960, 0.4040],
        [0.9801, 0.0199]], dtype=torch.float64)
alpha: tensor([0.7081, 0.2919])
beta: tensor([[[0.2002, 0.2066],
         [0.4604, 0.2020]],

        [[0.2601, 0.2029],
         [0.6639, 0.0702]],

        [[0.8735, 0.1976],
         [0.6297, 0.5981]],

        [[0.0497, 0.1965],
         [0.7456, 0.7550]],

        [[0.3813, 0.2018],
         [0.8965, 0.6572]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -12783.459029063382
Iteration 10: Loss = -12478.602623217812
Iteration 20: Loss = -12478.594738327063
Iteration 30: Loss = -12478.591465091493
Iteration 40: Loss = -12478.590101621721
Iteration 50: Loss = -12478.589470823066
Iteration 60: Loss = -12478.589257473182
Iteration 70: Loss = -12478.589122070138
Iteration 80: Loss = -12478.589155011894
1
Iteration 90: Loss = -12478.589150585658
2
Iteration 100: Loss = -12478.589186027093
3
Stopping early at iteration 100 due to no improvement.
pi: tensor([[0.2867, 0.7133],
        [0.2890, 0.7110]], dtype=torch.float64)
alpha: tensor([0.2885, 0.7115])
beta: tensor([[[0.2024, 0.2068],
         [0.4123, 0.2001]],

        [[0.3527, 0.2031],
         [0.0604, 0.4081]],

        [[0.1932, 0.1976],
         [0.7706, 0.6535]],

        [[0.8376, 0.1965],
         [0.1723, 0.8314]],

        [[0.9226, 0.2020],
         [0.1835, 0.5743]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20483.968701051523
Iteration 100: Loss = -12479.324255204181
Iteration 200: Loss = -12478.903827828306
Iteration 300: Loss = -12478.802416725459
Iteration 400: Loss = -12478.737325205975
Iteration 500: Loss = -12478.683278032704
Iteration 600: Loss = -12478.62853548895
Iteration 700: Loss = -12478.561436162245
Iteration 800: Loss = -12478.470856759244
Iteration 900: Loss = -12478.356794412291
Iteration 1000: Loss = -12478.123079825244
Iteration 1100: Loss = -12477.73974393681
Iteration 1200: Loss = -12477.419374601144
Iteration 1300: Loss = -12477.207834958605
Iteration 1400: Loss = -12477.049978598532
Iteration 1500: Loss = -12476.920196862951
Iteration 1600: Loss = -12476.809249186512
Iteration 1700: Loss = -12476.714506981238
Iteration 1800: Loss = -12476.634370282834
Iteration 1900: Loss = -12476.566720589919
Iteration 2000: Loss = -12476.50958388919
Iteration 2100: Loss = -12476.461027136294
Iteration 2200: Loss = -12476.419137299104
Iteration 2300: Loss = -12476.382275530397
Iteration 2400: Loss = -12476.348829365974
Iteration 2500: Loss = -12476.317276147858
Iteration 2600: Loss = -12476.286309761635
Iteration 2700: Loss = -12476.254863518328
Iteration 2800: Loss = -12476.222267894354
Iteration 2900: Loss = -12476.188532926433
Iteration 3000: Loss = -12476.154669342646
Iteration 3100: Loss = -12476.12231375719
Iteration 3200: Loss = -12476.093016747716
Iteration 3300: Loss = -12476.06760495111
Iteration 3400: Loss = -12476.046226046792
Iteration 3500: Loss = -12476.028457332206
Iteration 3600: Loss = -12476.013662848049
Iteration 3700: Loss = -12476.001365599685
Iteration 3800: Loss = -12475.990929426747
Iteration 3900: Loss = -12475.982016152482
Iteration 4000: Loss = -12475.974319163737
Iteration 4100: Loss = -12475.96763642848
Iteration 4200: Loss = -12475.961781444674
Iteration 4300: Loss = -12475.956616584357
Iteration 4400: Loss = -12475.951995749463
Iteration 4500: Loss = -12475.94784287388
Iteration 4600: Loss = -12475.94412500844
Iteration 4700: Loss = -12475.940768865095
Iteration 4800: Loss = -12475.937710672884
Iteration 4900: Loss = -12475.934938476968
Iteration 5000: Loss = -12475.932402535978
Iteration 5100: Loss = -12475.930078186027
Iteration 5200: Loss = -12475.927837840067
Iteration 5300: Loss = -12475.925790606543
Iteration 5400: Loss = -12475.923986709377
Iteration 5500: Loss = -12475.922198821392
Iteration 5600: Loss = -12475.920595591437
Iteration 5700: Loss = -12475.919161402082
Iteration 5800: Loss = -12475.917822321187
Iteration 5900: Loss = -12475.916546760336
Iteration 6000: Loss = -12475.915397727123
Iteration 6100: Loss = -12475.914319307994
Iteration 6200: Loss = -12475.913258005856
Iteration 6300: Loss = -12475.912332862197
Iteration 6400: Loss = -12475.911394093115
Iteration 6500: Loss = -12475.910527752627
Iteration 6600: Loss = -12475.909723164445
Iteration 6700: Loss = -12475.908898845964
Iteration 6800: Loss = -12475.916702632781
1
Iteration 6900: Loss = -12475.907462152676
Iteration 7000: Loss = -12475.906801478417
Iteration 7100: Loss = -12475.906674861626
Iteration 7200: Loss = -12475.905648347883
Iteration 7300: Loss = -12475.905060897128
Iteration 7400: Loss = -12475.904574737631
Iteration 7500: Loss = -12475.912204085638
1
Iteration 7600: Loss = -12475.903721378036
Iteration 7700: Loss = -12475.903290633107
Iteration 7800: Loss = -12475.902913558723
Iteration 7900: Loss = -12475.903039140447
1
Iteration 8000: Loss = -12475.902181264855
Iteration 8100: Loss = -12475.901913016824
Iteration 8200: Loss = -12475.93182958987
1
Iteration 8300: Loss = -12475.901344522128
Iteration 8400: Loss = -12475.901044017013
Iteration 8500: Loss = -12475.900757506315
Iteration 8600: Loss = -12475.902651141665
1
Iteration 8700: Loss = -12475.900218147395
Iteration 8800: Loss = -12475.899991797594
Iteration 8900: Loss = -12475.90528445802
1
Iteration 9000: Loss = -12475.899448639615
Iteration 9100: Loss = -12475.899060712642
Iteration 9200: Loss = -12475.89867415554
Iteration 9300: Loss = -12475.899544997614
1
Iteration 9400: Loss = -12475.898094370392
Iteration 9500: Loss = -12475.89790402836
Iteration 9600: Loss = -12475.897703405848
Iteration 9700: Loss = -12475.899916198106
1
Iteration 9800: Loss = -12475.897307044917
Iteration 9900: Loss = -12475.897234709777
Iteration 10000: Loss = -12476.310939502426
1
Iteration 10100: Loss = -12475.89689198132
Iteration 10200: Loss = -12475.896752704193
Iteration 10300: Loss = -12475.896613253808
Iteration 10400: Loss = -12475.897009740407
1
Iteration 10500: Loss = -12475.896303622963
Iteration 10600: Loss = -12475.89622382449
Iteration 10700: Loss = -12475.896247054927
1
Iteration 10800: Loss = -12475.896007560765
Iteration 10900: Loss = -12475.895934222575
Iteration 11000: Loss = -12475.896378505422
1
Iteration 11100: Loss = -12475.89574227383
Iteration 11200: Loss = -12475.895599388088
Iteration 11300: Loss = -12475.895545673044
Iteration 11400: Loss = -12475.89609355526
1
Iteration 11500: Loss = -12475.895447384395
Iteration 11600: Loss = -12475.895391565859
Iteration 11700: Loss = -12475.895294465154
Iteration 11800: Loss = -12475.895246266293
Iteration 11900: Loss = -12475.895671571729
1
Iteration 12000: Loss = -12475.895187660135
Iteration 12100: Loss = -12475.89513757262
Iteration 12200: Loss = -12475.895032932716
Iteration 12300: Loss = -12475.8951444935
1
Iteration 12400: Loss = -12475.894967234668
Iteration 12500: Loss = -12475.894850547455
Iteration 12600: Loss = -12475.894866310151
1
Iteration 12700: Loss = -12475.895275825
2
Iteration 12800: Loss = -12475.90644628201
3
Iteration 12900: Loss = -12475.894814518484
Iteration 13000: Loss = -12475.931824009844
1
Iteration 13100: Loss = -12475.894718196736
Iteration 13200: Loss = -12475.894654138468
Iteration 13300: Loss = -12475.895035693948
1
Iteration 13400: Loss = -12475.89462269582
Iteration 13500: Loss = -12475.927330366869
1
Iteration 13600: Loss = -12475.89456727163
Iteration 13700: Loss = -12475.894552192232
Iteration 13800: Loss = -12475.894579207243
1
Iteration 13900: Loss = -12475.894483868979
Iteration 14000: Loss = -12475.895340807856
1
Iteration 14100: Loss = -12475.902516878985
2
Iteration 14200: Loss = -12475.894402262666
Iteration 14300: Loss = -12475.894440605674
1
Iteration 14400: Loss = -12475.8945114617
2
Iteration 14500: Loss = -12475.905889879266
3
Iteration 14600: Loss = -12475.89432879553
Iteration 14700: Loss = -12476.392581566452
1
Iteration 14800: Loss = -12475.894436959848
2
Iteration 14900: Loss = -12475.894665299325
3
Iteration 15000: Loss = -12475.974565917262
4
Iteration 15100: Loss = -12475.894283456004
Iteration 15200: Loss = -12476.037888030003
1
Iteration 15300: Loss = -12475.89424915375
Iteration 15400: Loss = -12475.91711388072
1
Iteration 15500: Loss = -12475.895351302259
2
Iteration 15600: Loss = -12476.006305709552
3
Iteration 15700: Loss = -12475.894188043298
Iteration 15800: Loss = -12475.89419028376
1
Iteration 15900: Loss = -12475.896646500392
2
Iteration 16000: Loss = -12475.894379036456
3
Iteration 16100: Loss = -12475.894150493015
Iteration 16200: Loss = -12475.894211077306
1
Iteration 16300: Loss = -12475.894154342575
2
Iteration 16400: Loss = -12475.895275869609
3
Iteration 16500: Loss = -12475.894105419715
Iteration 16600: Loss = -12475.894001096298
Iteration 16700: Loss = -12475.895393872868
1
Iteration 16800: Loss = -12475.894027215852
2
Iteration 16900: Loss = -12475.929041442705
3
Iteration 17000: Loss = -12475.893974815881
Iteration 17100: Loss = -12475.895034882575
1
Iteration 17200: Loss = -12475.89400137697
2
Iteration 17300: Loss = -12475.893961394666
Iteration 17400: Loss = -12475.897195728276
1
Iteration 17500: Loss = -12475.893942604216
Iteration 17600: Loss = -12475.896801031571
1
Iteration 17700: Loss = -12475.894387088936
2
Iteration 17800: Loss = -12475.907215282074
3
Iteration 17900: Loss = -12475.895725704839
4
Iteration 18000: Loss = -12475.894010176937
5
Stopping early at iteration 18000 due to no improvement.
pi: tensor([[9.9999e-01, 5.1919e-06],
        [2.7560e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0201, 0.9799], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1985, 0.2752],
         [0.7166, 0.2007]],

        [[0.6626, 0.3003],
         [0.6554, 0.6184]],

        [[0.5067, 0.2681],
         [0.6434, 0.5543]],

        [[0.6797, 0.1972],
         [0.6336, 0.5319]],

        [[0.5008, 0.2109],
         [0.5961, 0.6217]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 2
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.00028700959432072443
Global Adjusted Rand Index: -0.0007361729337332871
Average Adjusted Rand Index: -0.0029539949725856758
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22429.650404959004
Iteration 100: Loss = -12480.115390450059
Iteration 200: Loss = -12479.238691907838
Iteration 300: Loss = -12478.939647803289
Iteration 400: Loss = -12478.78978238358
Iteration 500: Loss = -12478.689761955087
Iteration 600: Loss = -12478.608492358697
Iteration 700: Loss = -12478.53764011907
Iteration 800: Loss = -12478.482745617901
Iteration 900: Loss = -12478.447104464083
Iteration 1000: Loss = -12478.42033663307
Iteration 1100: Loss = -12478.38933763979
Iteration 1200: Loss = -12478.334227422594
Iteration 1300: Loss = -12478.220002867805
Iteration 1400: Loss = -12478.09380954154
Iteration 1500: Loss = -12477.966340404328
Iteration 1600: Loss = -12477.631375228973
Iteration 1700: Loss = -12477.315315707587
Iteration 1800: Loss = -12477.125135259052
Iteration 1900: Loss = -12476.96607175267
Iteration 2000: Loss = -12476.834503549528
Iteration 2100: Loss = -12476.727428104528
Iteration 2200: Loss = -12476.637812087038
Iteration 2300: Loss = -12476.563243942674
Iteration 2400: Loss = -12476.501767579515
Iteration 2500: Loss = -12476.45142212745
Iteration 2600: Loss = -12476.410337008307
Iteration 2700: Loss = -12476.376703987991
Iteration 2800: Loss = -12476.348674954497
Iteration 2900: Loss = -12476.32459099202
Iteration 3000: Loss = -12476.30309703555
Iteration 3100: Loss = -12476.283007294725
Iteration 3200: Loss = -12476.263541159286
Iteration 3300: Loss = -12476.243644273087
Iteration 3400: Loss = -12476.222541127934
Iteration 3500: Loss = -12476.199451480663
Iteration 3600: Loss = -12476.174197617502
Iteration 3700: Loss = -12476.146870804436
Iteration 3800: Loss = -12476.118549324636
Iteration 3900: Loss = -12476.090872427316
Iteration 4000: Loss = -12476.065474574678
Iteration 4100: Loss = -12476.04329057382
Iteration 4200: Loss = -12476.024532559093
Iteration 4300: Loss = -12476.009074224266
Iteration 4400: Loss = -12475.996317296904
Iteration 4500: Loss = -12475.985630408388
Iteration 4600: Loss = -12475.97654127169
Iteration 4700: Loss = -12475.968691627633
Iteration 4800: Loss = -12475.96175231681
Iteration 4900: Loss = -12475.955629400687
Iteration 5000: Loss = -12475.950426283478
Iteration 5100: Loss = -12475.946022009493
Iteration 5200: Loss = -12475.942213468685
Iteration 5300: Loss = -12475.93877503404
Iteration 5400: Loss = -12475.935763961526
Iteration 5500: Loss = -12475.932997307571
Iteration 5600: Loss = -12475.930507021274
Iteration 5700: Loss = -12475.928185676814
Iteration 5800: Loss = -12475.926129322235
Iteration 5900: Loss = -12475.924224887902
Iteration 6000: Loss = -12475.92246625213
Iteration 6100: Loss = -12475.920920551882
Iteration 6200: Loss = -12475.919485553026
Iteration 6300: Loss = -12475.918181613137
Iteration 6400: Loss = -12475.916991332775
Iteration 6500: Loss = -12475.915880763021
Iteration 6600: Loss = -12475.914800720127
Iteration 6700: Loss = -12475.913865530929
Iteration 6800: Loss = -12475.913041486992
Iteration 6900: Loss = -12475.912058898488
Iteration 7000: Loss = -12475.911260903666
Iteration 7100: Loss = -12475.910533034978
Iteration 7200: Loss = -12475.909799627012
Iteration 7300: Loss = -12475.909186262528
Iteration 7400: Loss = -12475.911677862701
1
Iteration 7500: Loss = -12475.908037876092
Iteration 7600: Loss = -12475.907459909573
Iteration 7700: Loss = -12475.906969280046
Iteration 7800: Loss = -12475.91383662209
1
Iteration 7900: Loss = -12475.90602790647
Iteration 8000: Loss = -12475.905594709568
Iteration 8100: Loss = -12475.992856438217
1
Iteration 8200: Loss = -12475.904711744088
Iteration 8300: Loss = -12475.904260609039
Iteration 8400: Loss = -12475.903805614158
Iteration 8500: Loss = -12475.904493631886
1
Iteration 8600: Loss = -12475.903108862216
Iteration 8700: Loss = -12475.902781482595
Iteration 8800: Loss = -12475.934197891063
1
Iteration 8900: Loss = -12475.902243730303
Iteration 9000: Loss = -12475.90196128196
Iteration 9100: Loss = -12475.901702081232
Iteration 9200: Loss = -12475.90188577324
1
Iteration 9300: Loss = -12475.90120446766
Iteration 9400: Loss = -12475.900928333413
Iteration 9500: Loss = -12475.90992528162
1
Iteration 9600: Loss = -12475.900476753895
Iteration 9700: Loss = -12475.90027935249
Iteration 9800: Loss = -12475.900004543608
Iteration 9900: Loss = -12475.900001481825
Iteration 10000: Loss = -12475.899597255897
Iteration 10100: Loss = -12475.89941015632
Iteration 10200: Loss = -12475.900265557786
1
Iteration 10300: Loss = -12475.899142519662
Iteration 10400: Loss = -12475.899019729668
Iteration 10500: Loss = -12475.89891515933
Iteration 10600: Loss = -12476.062213502273
1
Iteration 10700: Loss = -12475.898636819285
Iteration 10800: Loss = -12475.898365367648
Iteration 10900: Loss = -12475.897876613675
Iteration 11000: Loss = -12475.897900741009
1
Iteration 11100: Loss = -12475.897620375985
Iteration 11200: Loss = -12475.897514365362
Iteration 11300: Loss = -12475.897410277074
Iteration 11400: Loss = -12475.898104146925
1
Iteration 11500: Loss = -12475.89728229637
Iteration 11600: Loss = -12475.897197484508
Iteration 11700: Loss = -12475.898105390017
1
Iteration 11800: Loss = -12475.897080610326
Iteration 11900: Loss = -12475.897017778947
Iteration 12000: Loss = -12475.896985301917
Iteration 12100: Loss = -12475.896942024101
Iteration 12200: Loss = -12475.918659648723
1
Iteration 12300: Loss = -12475.89686221718
Iteration 12400: Loss = -12475.896820437072
Iteration 12500: Loss = -12475.896750494303
Iteration 12600: Loss = -12475.896639651597
Iteration 12700: Loss = -12475.897245514254
1
Iteration 12800: Loss = -12475.89588624859
Iteration 12900: Loss = -12475.909737236118
1
Iteration 13000: Loss = -12475.896434474316
2
Iteration 13100: Loss = -12475.90448404988
3
Iteration 13200: Loss = -12475.895745903497
Iteration 13300: Loss = -12476.293748474483
1
Iteration 13400: Loss = -12475.895739291718
Iteration 13500: Loss = -12475.89573027655
Iteration 13600: Loss = -12475.89691585644
1
Iteration 13700: Loss = -12475.895577198777
Iteration 13800: Loss = -12476.242695270217
1
Iteration 13900: Loss = -12475.895513927704
Iteration 14000: Loss = -12475.895767291902
1
Iteration 14100: Loss = -12475.925248896838
2
Iteration 14200: Loss = -12475.895315204853
Iteration 14300: Loss = -12475.895310325302
Iteration 14400: Loss = -12476.040312015086
1
Iteration 14500: Loss = -12475.895215103395
Iteration 14600: Loss = -12475.895276138572
1
Iteration 14700: Loss = -12475.895212603506
Iteration 14800: Loss = -12475.895309967835
1
Iteration 14900: Loss = -12475.895116103713
Iteration 15000: Loss = -12475.89507366387
Iteration 15100: Loss = -12475.895108367347
1
Iteration 15200: Loss = -12475.895045496038
Iteration 15300: Loss = -12476.044545504603
1
Iteration 15400: Loss = -12475.894903789336
Iteration 15500: Loss = -12475.897145708184
1
Iteration 15600: Loss = -12475.896292276111
2
Iteration 15700: Loss = -12475.894934139014
3
Iteration 15800: Loss = -12475.941466703303
4
Iteration 15900: Loss = -12475.894760513615
Iteration 16000: Loss = -12475.907940557365
1
Iteration 16100: Loss = -12475.89464344286
Iteration 16200: Loss = -12475.904593807043
1
Iteration 16300: Loss = -12475.896024036967
2
Iteration 16400: Loss = -12475.89458720187
Iteration 16500: Loss = -12475.89506956562
1
Iteration 16600: Loss = -12475.894573589458
Iteration 16700: Loss = -12475.979861386824
1
Iteration 16800: Loss = -12475.894538260784
Iteration 16900: Loss = -12475.894626736084
1
Iteration 17000: Loss = -12475.898412367455
2
Iteration 17100: Loss = -12475.894529139683
Iteration 17200: Loss = -12475.894962561975
1
Iteration 17300: Loss = -12475.89465588164
2
Iteration 17400: Loss = -12475.995670513148
3
Iteration 17500: Loss = -12475.894663405208
4
Iteration 17600: Loss = -12475.897112807332
5
Stopping early at iteration 17600 due to no improvement.
pi: tensor([[1.0000e+00, 3.2736e-07],
        [9.8966e-06, 9.9999e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9799, 0.0201], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2006, 0.2750],
         [0.5158, 0.1985]],

        [[0.7088, 0.3001],
         [0.6189, 0.7186]],

        [[0.5635, 0.2682],
         [0.5350, 0.6370]],

        [[0.6059, 0.1969],
         [0.6629, 0.5507]],

        [[0.6325, 0.2112],
         [0.6782, 0.5681]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.006464646464646465
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: -0.006464646464646465
time is 2
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
time is 3
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.00028700959432072443
Global Adjusted Rand Index: -0.0007361729337332871
Average Adjusted Rand Index: -0.0029539949725856758
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22969.909966331907
Iteration 100: Loss = -12479.675081886779
Iteration 200: Loss = -12478.935277799226
Iteration 300: Loss = -12478.749268731546
Iteration 400: Loss = -12478.630873479136
Iteration 500: Loss = -12478.514250976208
Iteration 600: Loss = -12478.475206137962
Iteration 700: Loss = -12478.45103908998
Iteration 800: Loss = -12478.401599175593
Iteration 900: Loss = -12477.733708472151
Iteration 1000: Loss = -12477.07455923545
Iteration 1100: Loss = -12476.76745047978
Iteration 1200: Loss = -12476.580855370004
Iteration 1300: Loss = -12476.461400784443
Iteration 1400: Loss = -12476.380408015186
Iteration 1500: Loss = -12476.32096257786
Iteration 1600: Loss = -12476.273298101287
Iteration 1700: Loss = -12476.231725121757
Iteration 1800: Loss = -12476.193118914363
Iteration 1900: Loss = -12476.156033431242
Iteration 2000: Loss = -12476.120459574091
Iteration 2100: Loss = -12476.087247213836
Iteration 2200: Loss = -12476.057656798534
Iteration 2300: Loss = -12476.032271865099
Iteration 2400: Loss = -12476.011262456956
Iteration 2500: Loss = -12475.994230831433
Iteration 2600: Loss = -12475.980607067433
Iteration 2700: Loss = -12475.96977174634
Iteration 2800: Loss = -12475.961063909685
Iteration 2900: Loss = -12475.953979759945
Iteration 3000: Loss = -12475.948065881797
Iteration 3100: Loss = -12475.943172152456
Iteration 3200: Loss = -12475.938967883703
Iteration 3300: Loss = -12475.93545501478
Iteration 3400: Loss = -12475.932403239602
Iteration 3500: Loss = -12475.929791100458
Iteration 3600: Loss = -12475.927362533146
Iteration 3700: Loss = -12475.925255910008
Iteration 3800: Loss = -12475.923347544525
Iteration 3900: Loss = -12475.921641457695
Iteration 4000: Loss = -12475.920096929112
Iteration 4100: Loss = -12475.918700617804
Iteration 4200: Loss = -12475.91742139025
Iteration 4300: Loss = -12475.91631480834
Iteration 4400: Loss = -12475.915270325722
Iteration 4500: Loss = -12475.914367103784
Iteration 4600: Loss = -12475.913491170742
Iteration 4700: Loss = -12475.912714065067
Iteration 4800: Loss = -12475.911957848177
Iteration 4900: Loss = -12475.911241543365
Iteration 5000: Loss = -12475.91059548593
Iteration 5100: Loss = -12475.909929337826
Iteration 5200: Loss = -12475.909260756365
Iteration 5300: Loss = -12475.908579190387
Iteration 5400: Loss = -12475.907916705422
Iteration 5500: Loss = -12475.90735197127
Iteration 5600: Loss = -12475.906903891762
Iteration 5700: Loss = -12475.906442671821
Iteration 5800: Loss = -12475.90592656533
Iteration 5900: Loss = -12475.905299556829
Iteration 6000: Loss = -12475.904791430967
Iteration 6100: Loss = -12475.904437543033
Iteration 6200: Loss = -12475.904079857739
Iteration 6300: Loss = -12475.903760053357
Iteration 6400: Loss = -12475.903435168697
Iteration 6500: Loss = -12475.903103108642
Iteration 6600: Loss = -12475.902697457244
Iteration 6700: Loss = -12475.902302732478
Iteration 6800: Loss = -12475.901876145608
Iteration 6900: Loss = -12475.901572483896
Iteration 7000: Loss = -12475.901338745445
Iteration 7100: Loss = -12475.90114702455
Iteration 7200: Loss = -12475.900939271183
Iteration 7300: Loss = -12475.900772677585
Iteration 7400: Loss = -12475.900661747419
Iteration 7500: Loss = -12475.9004952959
Iteration 7600: Loss = -12475.900342995106
Iteration 7700: Loss = -12475.900241269357
Iteration 7800: Loss = -12475.900062123124
Iteration 7900: Loss = -12475.937264524759
1
Iteration 8000: Loss = -12475.899797011412
Iteration 8100: Loss = -12475.89962897915
Iteration 8200: Loss = -12475.899757497371
1
Iteration 8300: Loss = -12475.898709559207
Iteration 8400: Loss = -12475.991208739018
1
Iteration 8500: Loss = -12475.898323160856
Iteration 8600: Loss = -12475.898183436502
Iteration 8700: Loss = -12475.89824213103
1
Iteration 8800: Loss = -12475.898046817261
Iteration 8900: Loss = -12475.897930179233
Iteration 9000: Loss = -12475.907166911555
1
Iteration 9100: Loss = -12475.897869398279
Iteration 9200: Loss = -12475.89776485123
Iteration 9300: Loss = -12476.360086605178
1
Iteration 9400: Loss = -12475.897737098994
Iteration 9500: Loss = -12475.897628173712
Iteration 9600: Loss = -12475.897564286079
Iteration 9700: Loss = -12475.899093464264
1
Iteration 9800: Loss = -12475.897441795436
Iteration 9900: Loss = -12475.897373455558
Iteration 10000: Loss = -12475.921745558484
1
Iteration 10100: Loss = -12475.897177042121
Iteration 10200: Loss = -12475.897141811067
Iteration 10300: Loss = -12475.897135684003
Iteration 10400: Loss = -12475.897724201626
1
Iteration 10500: Loss = -12475.89705048994
Iteration 10600: Loss = -12475.897015359153
Iteration 10700: Loss = -12475.91771814262
1
Iteration 10800: Loss = -12475.896936766901
Iteration 10900: Loss = -12475.896925410321
Iteration 11000: Loss = -12475.896885258053
Iteration 11100: Loss = -12475.89691275888
1
Iteration 11200: Loss = -12475.8968383185
Iteration 11300: Loss = -12475.89680382353
Iteration 11400: Loss = -12475.901445355383
1
Iteration 11500: Loss = -12475.896695408905
Iteration 11600: Loss = -12475.896643241727
Iteration 11700: Loss = -12475.896655894783
1
Iteration 11800: Loss = -12475.89756661347
2
Iteration 11900: Loss = -12475.896519927754
Iteration 12000: Loss = -12475.896417487345
Iteration 12100: Loss = -12475.896310087015
Iteration 12200: Loss = -12476.232329017637
1
Iteration 12300: Loss = -12475.896261166281
Iteration 12400: Loss = -12475.896228694759
Iteration 12500: Loss = -12475.896224156257
Iteration 12600: Loss = -12475.897016430174
1
Iteration 12700: Loss = -12475.895455903543
Iteration 12800: Loss = -12475.895432610938
Iteration 12900: Loss = -12476.034322942733
1
Iteration 13000: Loss = -12475.895329538042
Iteration 13100: Loss = -12475.89684830119
1
Iteration 13200: Loss = -12475.895316699245
Iteration 13300: Loss = -12475.89628327479
1
Iteration 13400: Loss = -12475.895373426687
2
Iteration 13500: Loss = -12475.896138571592
3
Iteration 13600: Loss = -12475.895218603771
Iteration 13700: Loss = -12475.895984400559
1
Iteration 13800: Loss = -12475.896489324565
2
Iteration 13900: Loss = -12475.895202912603
Iteration 14000: Loss = -12475.896179309073
1
Iteration 14100: Loss = -12475.895193240847
Iteration 14200: Loss = -12475.896897983437
1
Iteration 14300: Loss = -12475.895006846707
Iteration 14400: Loss = -12475.895686980795
1
Iteration 14500: Loss = -12475.894868699414
Iteration 14600: Loss = -12475.894677598468
Iteration 14700: Loss = -12475.894821566737
1
Iteration 14800: Loss = -12475.896964624131
2
Iteration 14900: Loss = -12475.894771853347
3
Iteration 15000: Loss = -12475.899747004514
4
Iteration 15100: Loss = -12475.895283058808
5
Stopping early at iteration 15100 due to no improvement.
pi: tensor([[9.9999e-01, 8.2497e-06],
        [4.2886e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0201, 0.9799], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1985, 0.2753],
         [0.6919, 0.2008]],

        [[0.6559, 0.3004],
         [0.7013, 0.5368]],

        [[0.7181, 0.2681],
         [0.5252, 0.6480]],

        [[0.7310, 0.1958],
         [0.6184, 0.5853]],

        [[0.5147, 0.2109],
         [0.6019, 0.5824]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 2
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.00028700959432072443
Global Adjusted Rand Index: -0.0007361729337332871
Average Adjusted Rand Index: -0.0029539949725856758
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21418.793039554996
Iteration 100: Loss = -12479.322764464136
Iteration 200: Loss = -12478.787929231772
Iteration 300: Loss = -12478.680199008584
Iteration 400: Loss = -12478.610174514646
Iteration 500: Loss = -12478.541707904977
Iteration 600: Loss = -12478.44246259121
Iteration 700: Loss = -12478.273158354836
Iteration 800: Loss = -12478.143354387335
Iteration 900: Loss = -12478.080412539097
Iteration 1000: Loss = -12478.02213346428
Iteration 1100: Loss = -12477.887317642555
Iteration 1200: Loss = -12477.568293346272
Iteration 1300: Loss = -12477.39744432549
Iteration 1400: Loss = -12477.339331356465
Iteration 1500: Loss = -12477.314116260877
Iteration 1600: Loss = -12477.299958879263
Iteration 1700: Loss = -12477.290039215182
Iteration 1800: Loss = -12477.281931260291
Iteration 1900: Loss = -12477.27476413932
Iteration 2000: Loss = -12477.26798958445
Iteration 2100: Loss = -12477.261379518768
Iteration 2200: Loss = -12477.254613709345
Iteration 2300: Loss = -12477.247492285185
Iteration 2400: Loss = -12477.23984415156
Iteration 2500: Loss = -12477.231421604882
Iteration 2600: Loss = -12477.222080205178
Iteration 2700: Loss = -12477.211853325418
Iteration 2800: Loss = -12477.200548701501
Iteration 2900: Loss = -12477.188307467688
Iteration 3000: Loss = -12477.17538492392
Iteration 3100: Loss = -12477.16215167985
Iteration 3200: Loss = -12477.149221763902
Iteration 3300: Loss = -12477.137096873987
Iteration 3400: Loss = -12477.126258018861
Iteration 3500: Loss = -12477.117047368798
Iteration 3600: Loss = -12477.109376268852
Iteration 3700: Loss = -12477.103193277806
Iteration 3800: Loss = -12477.098270609495
Iteration 3900: Loss = -12477.094267358252
Iteration 4000: Loss = -12477.09118254204
Iteration 4100: Loss = -12477.088553204676
Iteration 4200: Loss = -12477.086393231417
Iteration 4300: Loss = -12477.08458294379
Iteration 4400: Loss = -12477.083097238703
Iteration 4500: Loss = -12477.08174478583
Iteration 4600: Loss = -12477.080637129726
Iteration 4700: Loss = -12477.079586603897
Iteration 4800: Loss = -12477.078658777336
Iteration 4900: Loss = -12477.07789676912
Iteration 5000: Loss = -12477.077181103008
Iteration 5100: Loss = -12477.076490626314
Iteration 5200: Loss = -12477.075945354081
Iteration 5300: Loss = -12477.07541437365
Iteration 5400: Loss = -12477.074943376188
Iteration 5500: Loss = -12477.074520926588
Iteration 5600: Loss = -12477.074126449073
Iteration 5700: Loss = -12477.073806159076
Iteration 5800: Loss = -12477.073462215358
Iteration 5900: Loss = -12477.073182938246
Iteration 6000: Loss = -12477.072913577256
Iteration 6100: Loss = -12477.072650401293
Iteration 6200: Loss = -12477.072455162996
Iteration 6300: Loss = -12477.072225132986
Iteration 6400: Loss = -12477.072062656172
Iteration 6500: Loss = -12477.07188772418
Iteration 6600: Loss = -12477.071670705152
Iteration 6700: Loss = -12477.07154970185
Iteration 6800: Loss = -12477.07193332705
1
Iteration 6900: Loss = -12477.071326402829
Iteration 7000: Loss = -12477.07117175253
Iteration 7100: Loss = -12477.071903274002
1
Iteration 7200: Loss = -12477.071004304955
Iteration 7300: Loss = -12477.07098552654
Iteration 7400: Loss = -12477.072476167612
1
Iteration 7500: Loss = -12477.07276463898
2
Iteration 7600: Loss = -12477.070613775679
Iteration 7700: Loss = -12477.070597743164
Iteration 7800: Loss = -12477.070510726246
Iteration 7900: Loss = -12477.07051450356
1
Iteration 8000: Loss = -12477.070431806425
Iteration 8100: Loss = -12477.308892148236
1
Iteration 8200: Loss = -12477.070350185373
Iteration 8300: Loss = -12477.070265832208
Iteration 8400: Loss = -12477.117607900274
1
Iteration 8500: Loss = -12477.070155997246
Iteration 8600: Loss = -12477.070179829156
1
Iteration 8700: Loss = -12477.072093736118
2
Iteration 8800: Loss = -12477.070142161505
Iteration 8900: Loss = -12477.211969012444
1
Iteration 9000: Loss = -12477.070309117671
2
Iteration 9100: Loss = -12477.070072986997
Iteration 9200: Loss = -12477.070781909963
1
Iteration 9300: Loss = -12477.150755694243
2
Iteration 9400: Loss = -12477.070006709475
Iteration 9500: Loss = -12477.07322316872
1
Iteration 9600: Loss = -12477.07171141809
2
Iteration 9700: Loss = -12477.069939773462
Iteration 9800: Loss = -12477.070347072584
1
Iteration 9900: Loss = -12477.127924458204
2
Iteration 10000: Loss = -12477.07009585143
3
Iteration 10100: Loss = -12477.070160812382
4
Iteration 10200: Loss = -12477.08292570551
5
Stopping early at iteration 10200 due to no improvement.
pi: tensor([[0.9856, 0.0144],
        [0.9838, 0.0162]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0221, 0.9779], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2019, 0.2065],
         [0.5173, 0.2076]],

        [[0.6452, 0.2705],
         [0.6178, 0.6386]],

        [[0.6499, 0.0912],
         [0.5209, 0.5064]],

        [[0.6104, 0.1891],
         [0.5321, 0.6776]],

        [[0.5921, 0.2120],
         [0.5971, 0.5410]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1])
Difference count: 50
Adjusted Rand Index: -0.0007925468249603727
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.001433677255059433
Average Adjusted Rand Index: -0.00015850936499207452
11960.557349129685
[-0.0007361729337332871, -0.0007361729337332871, -0.0007361729337332871, -0.001433677255059433] [-0.0029539949725856758, -0.0029539949725856758, -0.0029539949725856758, -0.00015850936499207452] [12475.894010176937, 12475.897112807332, 12475.895283058808, 12477.08292570551]
-----------------------------------------------------------------------------------------
This iteration is 20
True Objective function: Loss = -11760.710648918353
Iteration 0: Loss = -12284.795154912323
Iteration 10: Loss = -12284.795154912323
1
Iteration 20: Loss = -12284.795154912325
2
Iteration 30: Loss = -12284.795154912366
3
Stopping early at iteration 30 due to no improvement.
pi: tensor([[1.0000e+00, 1.7612e-15],
        [1.0000e+00, 2.5929e-18]], dtype=torch.float64)
alpha: tensor([1.0000e+00, 1.6592e-15])
beta: tensor([[[0.1951, 0.1815],
         [0.9543, 0.2762]],

        [[0.9350, 0.1362],
         [0.6582, 0.1257]],

        [[0.7846, 0.2067],
         [0.4357, 0.9462]],

        [[0.2996, 0.2311],
         [0.9303, 0.6751]],

        [[0.8491, 0.2888],
         [0.8169, 0.9674]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 40
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 35
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -12356.492685785299
Iteration 10: Loss = -11751.689407820451
Iteration 20: Loss = -11751.649041657389
Iteration 30: Loss = -11751.649036890789
Iteration 40: Loss = -11751.649036890789
1
Iteration 50: Loss = -11751.649036890789
2
Iteration 60: Loss = -11751.649036890789
3
Stopping early at iteration 60 due to no improvement.
pi: tensor([[0.7943, 0.2057],
        [0.3305, 0.6695]], dtype=torch.float64)
alpha: tensor([0.5779, 0.4221])
beta: tensor([[[0.2881, 0.1006],
         [0.4470, 0.2918]],

        [[0.0382, 0.0940],
         [0.4266, 0.9066]],

        [[0.0378, 0.0910],
         [0.1414, 0.0799]],

        [[0.4048, 0.0927],
         [0.1788, 0.8639]],

        [[0.8413, 0.0941],
         [0.9891, 0.5427]]], dtype=torch.float64)
time is 0
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599529290626264
time is 4
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.9840249206208616
Average Adjusted Rand Index: 0.9839905858125253
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22349.021082537012
Iteration 100: Loss = -12232.671667547995
Iteration 200: Loss = -12232.160354652202
Iteration 300: Loss = -12232.018256882286
Iteration 400: Loss = -12231.893754093186
Iteration 500: Loss = -12231.681722568283
Iteration 600: Loss = -12231.231792874052
Iteration 700: Loss = -12230.41633132774
Iteration 800: Loss = -11806.649843348445
Iteration 900: Loss = -11779.933567923079
Iteration 1000: Loss = -11779.861809610937
Iteration 1100: Loss = -11779.813804900808
Iteration 1200: Loss = -11776.534737521186
Iteration 1300: Loss = -11776.297460740041
Iteration 1400: Loss = -11766.341537462731
Iteration 1500: Loss = -11766.328628498062
Iteration 1600: Loss = -11766.16017786316
Iteration 1700: Loss = -11766.154156933886
Iteration 1800: Loss = -11766.149361383208
Iteration 1900: Loss = -11766.14615885164
Iteration 2000: Loss = -11766.143594771545
Iteration 2100: Loss = -11766.141289613066
Iteration 2200: Loss = -11766.13870797445
Iteration 2300: Loss = -11766.130815261025
Iteration 2400: Loss = -11764.497645970638
Iteration 2500: Loss = -11764.390297626489
Iteration 2600: Loss = -11763.152824654222
Iteration 2700: Loss = -11761.721697206634
Iteration 2800: Loss = -11761.720434641318
Iteration 2900: Loss = -11761.719587693055
Iteration 3000: Loss = -11761.718826300745
Iteration 3100: Loss = -11761.71900930063
1
Iteration 3200: Loss = -11761.717732476316
Iteration 3300: Loss = -11761.716953150868
Iteration 3400: Loss = -11761.72577964655
1
Iteration 3500: Loss = -11761.715779789232
Iteration 3600: Loss = -11761.715815711363
1
Iteration 3700: Loss = -11761.71012341484
Iteration 3800: Loss = -11761.708889105968
Iteration 3900: Loss = -11753.291658870638
Iteration 4000: Loss = -11753.291107741828
Iteration 4100: Loss = -11753.302370047639
1
Iteration 4200: Loss = -11753.291427501494
2
Iteration 4300: Loss = -11753.288388427653
Iteration 4400: Loss = -11753.288101580541
Iteration 4500: Loss = -11753.287500847919
Iteration 4600: Loss = -11753.286991190713
Iteration 4700: Loss = -11753.286661868122
Iteration 4800: Loss = -11753.286422702806
Iteration 4900: Loss = -11753.28666392181
1
Iteration 5000: Loss = -11753.294903536183
2
Iteration 5100: Loss = -11753.285734904266
Iteration 5200: Loss = -11753.286062783473
1
Iteration 5300: Loss = -11753.287447745859
2
Iteration 5400: Loss = -11753.29078635303
3
Iteration 5500: Loss = -11753.285146023021
Iteration 5600: Loss = -11753.284651747157
Iteration 5700: Loss = -11753.284432326072
Iteration 5800: Loss = -11753.28433758948
Iteration 5900: Loss = -11753.28631602692
1
Iteration 6000: Loss = -11753.284083459488
Iteration 6100: Loss = -11753.284351450187
1
Iteration 6200: Loss = -11753.291913602845
2
Iteration 6300: Loss = -11753.285756938505
3
Iteration 6400: Loss = -11753.2838819868
Iteration 6500: Loss = -11753.283855691225
Iteration 6600: Loss = -11753.286229083813
1
Iteration 6700: Loss = -11753.283700004453
Iteration 6800: Loss = -11753.283954453875
1
Iteration 6900: Loss = -11753.28359997192
Iteration 7000: Loss = -11753.283769626649
1
Iteration 7100: Loss = -11753.2831349847
Iteration 7200: Loss = -11753.28317410051
1
Iteration 7300: Loss = -11753.284936228336
2
Iteration 7400: Loss = -11753.283048123247
Iteration 7500: Loss = -11753.28314817517
1
Iteration 7600: Loss = -11753.283033634847
Iteration 7700: Loss = -11753.282988934989
Iteration 7800: Loss = -11753.282947378395
Iteration 7900: Loss = -11753.282972880252
1
Iteration 8000: Loss = -11753.283013210952
2
Iteration 8100: Loss = -11753.28367229549
3
Iteration 8200: Loss = -11753.282870569123
Iteration 8300: Loss = -11753.285841545761
1
Iteration 8400: Loss = -11753.282812724477
Iteration 8500: Loss = -11753.28325936914
1
Iteration 8600: Loss = -11753.283504995738
2
Iteration 8700: Loss = -11753.282853284563
3
Iteration 8800: Loss = -11753.28377870232
4
Iteration 8900: Loss = -11753.291524559263
5
Stopping early at iteration 8900 due to no improvement.
pi: tensor([[0.2517, 0.7483],
        [0.7454, 0.2546]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4702, 0.5298], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2970, 0.1005],
         [0.5664, 0.2936]],

        [[0.6287, 0.0942],
         [0.5002, 0.5327]],

        [[0.5018, 0.0909],
         [0.6760, 0.7022]],

        [[0.5727, 0.0930],
         [0.5398, 0.6273]],

        [[0.5296, 0.0941],
         [0.5930, 0.7129]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599529290626264
time is 4
tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.041366518063159115
Average Adjusted Rand Index: 0.9839905858125253
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20869.650193028672
Iteration 100: Loss = -12285.144121814044
Iteration 200: Loss = -12284.109602740158
Iteration 300: Loss = -12283.755805528497
Iteration 400: Loss = -12283.466240087675
Iteration 500: Loss = -12283.16454442369
Iteration 600: Loss = -12282.856505026617
Iteration 700: Loss = -12282.586819280174
Iteration 800: Loss = -12282.370751318547
Iteration 900: Loss = -12282.208768271244
Iteration 1000: Loss = -12282.092340752526
Iteration 1100: Loss = -12282.00940343632
Iteration 1200: Loss = -12281.950460031063
Iteration 1300: Loss = -12281.909931047016
Iteration 1400: Loss = -12281.883314448754
Iteration 1500: Loss = -12281.867084259884
Iteration 1600: Loss = -12281.857339167009
Iteration 1700: Loss = -12281.851292442108
Iteration 1800: Loss = -12281.84694533917
Iteration 1900: Loss = -12281.842944276657
Iteration 2000: Loss = -12281.83592256312
Iteration 2100: Loss = -12281.600229281707
Iteration 2200: Loss = -12066.846791734564
Iteration 2300: Loss = -11872.119846807884
Iteration 2400: Loss = -11802.949088364874
Iteration 2500: Loss = -11788.194444944382
Iteration 2600: Loss = -11776.449758221923
Iteration 2700: Loss = -11776.30660175134
Iteration 2800: Loss = -11776.250094644764
Iteration 2900: Loss = -11760.422395143309
Iteration 3000: Loss = -11760.373872622911
Iteration 3100: Loss = -11760.344842970913
Iteration 3200: Loss = -11753.364016623558
Iteration 3300: Loss = -11753.352900527749
Iteration 3400: Loss = -11753.34663738733
Iteration 3500: Loss = -11753.33901950082
Iteration 3600: Loss = -11753.333972256178
Iteration 3700: Loss = -11753.329520894527
Iteration 3800: Loss = -11753.32580025299
Iteration 3900: Loss = -11753.32363596051
Iteration 4000: Loss = -11753.319725051648
Iteration 4100: Loss = -11753.31789655223
Iteration 4200: Loss = -11753.315054226116
Iteration 4300: Loss = -11753.313515570182
Iteration 4400: Loss = -11753.312044061257
Iteration 4500: Loss = -11753.309712036149
Iteration 4600: Loss = -11753.30807193762
Iteration 4700: Loss = -11753.30609076431
Iteration 4800: Loss = -11753.302133881341
Iteration 4900: Loss = -11753.289338594032
Iteration 5000: Loss = -11753.28617710052
Iteration 5100: Loss = -11753.283828705691
Iteration 5200: Loss = -11753.301328445645
1
Iteration 5300: Loss = -11753.281446251325
Iteration 5400: Loss = -11753.279514435293
Iteration 5500: Loss = -11753.276998566238
Iteration 5600: Loss = -11753.275473243775
Iteration 5700: Loss = -11753.274870599198
Iteration 5800: Loss = -11753.274769676653
Iteration 5900: Loss = -11753.27388799258
Iteration 6000: Loss = -11753.27343204091
Iteration 6100: Loss = -11753.277954022326
1
Iteration 6200: Loss = -11753.272679047348
Iteration 6300: Loss = -11753.272349995132
Iteration 6400: Loss = -11753.27998620947
1
Iteration 6500: Loss = -11753.27167293798
Iteration 6600: Loss = -11753.271408345363
Iteration 6700: Loss = -11753.271104721485
Iteration 6800: Loss = -11753.270890213931
Iteration 6900: Loss = -11753.270644547092
Iteration 7000: Loss = -11753.27313273733
1
Iteration 7100: Loss = -11753.270263367762
Iteration 7200: Loss = -11753.286295963242
1
Iteration 7300: Loss = -11753.27012233712
Iteration 7400: Loss = -11753.272436677084
1
Iteration 7500: Loss = -11753.269908860637
Iteration 7600: Loss = -11753.269397793229
Iteration 7700: Loss = -11753.272974577063
1
Iteration 7800: Loss = -11753.269028347142
Iteration 7900: Loss = -11753.310002225475
1
Iteration 8000: Loss = -11753.268410372399
Iteration 8100: Loss = -11753.270873908716
1
Iteration 8200: Loss = -11753.266649917266
Iteration 8300: Loss = -11753.266554909305
Iteration 8400: Loss = -11753.266876668644
1
Iteration 8500: Loss = -11753.26634744339
Iteration 8600: Loss = -11753.29700992537
1
Iteration 8700: Loss = -11753.266186390907
Iteration 8800: Loss = -11753.266823119015
1
Iteration 8900: Loss = -11753.267265076067
2
Iteration 9000: Loss = -11753.26795764709
3
Iteration 9100: Loss = -11753.289900247051
4
Iteration 9200: Loss = -11753.272288209793
5
Stopping early at iteration 9200 due to no improvement.
pi: tensor([[0.2629, 0.7371],
        [0.7482, 0.2518]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5298, 0.4702], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2936, 0.1005],
         [0.7278, 0.2971]],

        [[0.6848, 0.0941],
         [0.6236, 0.5294]],

        [[0.5864, 0.0909],
         [0.5802, 0.5129]],

        [[0.6691, 0.0930],
         [0.6288, 0.7020]],

        [[0.5487, 0.0941],
         [0.5518, 0.5410]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 0
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 2
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599529290626264
time is 4
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.041366518063159115
Average Adjusted Rand Index: 0.9839905858125253
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22283.286629712868
Iteration 100: Loss = -11981.663104549472
Iteration 200: Loss = -11784.922679063116
Iteration 300: Loss = -11783.88358688852
Iteration 400: Loss = -11783.655121114334
Iteration 500: Loss = -11783.548879262284
Iteration 600: Loss = -11783.488619938953
Iteration 700: Loss = -11783.450521517167
Iteration 800: Loss = -11783.424569351295
Iteration 900: Loss = -11783.406012261303
Iteration 1000: Loss = -11783.39218252871
Iteration 1100: Loss = -11783.38166731004
Iteration 1200: Loss = -11783.373331684354
Iteration 1300: Loss = -11783.36668387635
Iteration 1400: Loss = -11783.361285225466
Iteration 1500: Loss = -11783.35676945411
Iteration 1600: Loss = -11783.354898026895
Iteration 1700: Loss = -11783.349887502234
Iteration 1800: Loss = -11783.34719081785
Iteration 1900: Loss = -11783.344887128607
Iteration 2000: Loss = -11783.342888548104
Iteration 2100: Loss = -11783.341144486241
Iteration 2200: Loss = -11783.339593729355
Iteration 2300: Loss = -11783.338273052275
Iteration 2400: Loss = -11783.337056320299
Iteration 2500: Loss = -11783.338647748722
1
Iteration 2600: Loss = -11783.346836224891
2
Iteration 2700: Loss = -11783.33413824906
Iteration 2800: Loss = -11783.333741159784
Iteration 2900: Loss = -11783.33275025182
Iteration 3000: Loss = -11783.332061135683
Iteration 3100: Loss = -11783.331960364678
Iteration 3200: Loss = -11783.330965314513
Iteration 3300: Loss = -11783.33043720494
Iteration 3400: Loss = -11783.329975450612
Iteration 3500: Loss = -11783.329545576402
Iteration 3600: Loss = -11783.33230536417
1
Iteration 3700: Loss = -11783.32882543186
Iteration 3800: Loss = -11783.335553948413
1
Iteration 3900: Loss = -11783.32831614209
Iteration 4000: Loss = -11783.330756467703
1
Iteration 4100: Loss = -11783.32815734505
Iteration 4200: Loss = -11783.33157992573
1
Iteration 4300: Loss = -11783.327335692578
Iteration 4400: Loss = -11783.327167658645
Iteration 4500: Loss = -11783.326945790484
Iteration 4600: Loss = -11783.329018910643
1
Iteration 4700: Loss = -11783.3265653503
Iteration 4800: Loss = -11783.32887098314
1
Iteration 4900: Loss = -11783.326308064823
Iteration 5000: Loss = -11783.327032404344
1
Iteration 5100: Loss = -11783.326055730497
Iteration 5200: Loss = -11783.32626049797
1
Iteration 5300: Loss = -11783.325836775542
Iteration 5400: Loss = -11783.325815578108
Iteration 5500: Loss = -11783.325637554246
Iteration 5600: Loss = -11783.325602204823
Iteration 5700: Loss = -11783.325546004946
Iteration 5800: Loss = -11783.325486604834
Iteration 5900: Loss = -11783.331758139559
1
Iteration 6000: Loss = -11783.332424098653
2
Iteration 6100: Loss = -11783.326066100748
3
Iteration 6200: Loss = -11783.325934404118
4
Iteration 6300: Loss = -11783.32513620103
Iteration 6400: Loss = -11783.325530835262
1
Iteration 6500: Loss = -11783.32499555777
Iteration 6600: Loss = -11783.32512762527
1
Iteration 6700: Loss = -11783.325043499846
2
Iteration 6800: Loss = -11783.317402990477
Iteration 6900: Loss = -11783.318368747245
1
Iteration 7000: Loss = -11783.317284709638
Iteration 7100: Loss = -11783.317272760325
Iteration 7200: Loss = -11783.318125457155
1
Iteration 7300: Loss = -11783.31721675068
Iteration 7400: Loss = -11783.317175219538
Iteration 7500: Loss = -11783.317139788374
Iteration 7600: Loss = -11783.397773783263
1
Iteration 7700: Loss = -11783.317074942606
Iteration 7800: Loss = -11783.31955243015
1
Iteration 7900: Loss = -11783.31703696786
Iteration 8000: Loss = -11783.3172880574
1
Iteration 8100: Loss = -11783.339904350129
2
Iteration 8200: Loss = -11783.318940764673
3
Iteration 8300: Loss = -11783.381570663703
4
Iteration 8400: Loss = -11783.316955054499
Iteration 8500: Loss = -11783.323460456242
1
Iteration 8600: Loss = -11783.316916020103
Iteration 8700: Loss = -11783.31787063129
1
Iteration 8800: Loss = -11783.326372524249
2
Iteration 8900: Loss = -11783.316859315804
Iteration 9000: Loss = -11783.317502414087
1
Iteration 9100: Loss = -11783.38231806403
2
Iteration 9200: Loss = -11783.316907776632
3
Iteration 9300: Loss = -11783.318568690232
4
Iteration 9400: Loss = -11783.335953698495
5
Stopping early at iteration 9400 due to no improvement.
pi: tensor([[0.6557, 0.3443],
        [0.3528, 0.6472]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4742, 0.5258], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2991, 0.1005],
         [0.5572, 0.2927]],

        [[0.5354, 0.0940],
         [0.5099, 0.6276]],

        [[0.6122, 0.0909],
         [0.7137, 0.6402]],

        [[0.5196, 0.0927],
         [0.6607, 0.5423]],

        [[0.6861, 0.0939],
         [0.6280, 0.6430]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.96
time is 2
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.9599529290626264
time is 4
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.34917578378640723
Average Adjusted Rand Index: 0.9839905858125253
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24156.455701082432
Iteration 100: Loss = -12285.119178441866
Iteration 200: Loss = -12284.678903609409
Iteration 300: Loss = -12284.600808779587
Iteration 400: Loss = -12284.551183802994
Iteration 500: Loss = -12284.512679945365
Iteration 600: Loss = -12284.47003433425
Iteration 700: Loss = -12284.389390596334
Iteration 800: Loss = -12283.997573312057
Iteration 900: Loss = -12282.706077392308
Iteration 1000: Loss = -12281.837882753867
Iteration 1100: Loss = -12281.00096485624
Iteration 1200: Loss = -12280.67805411676
Iteration 1300: Loss = -12074.960410503476
Iteration 1400: Loss = -11889.420667271428
Iteration 1500: Loss = -11853.525381011626
Iteration 1600: Loss = -11853.415244272404
Iteration 1700: Loss = -11841.925360549007
Iteration 1800: Loss = -11826.79120980993
Iteration 1900: Loss = -11826.752180742937
Iteration 2000: Loss = -11822.07062694676
Iteration 2100: Loss = -11822.05697481147
Iteration 2200: Loss = -11822.052710665386
Iteration 2300: Loss = -11822.041366686854
Iteration 2400: Loss = -11809.260807750934
Iteration 2500: Loss = -11808.212232566475
Iteration 2600: Loss = -11808.00238517593
Iteration 2700: Loss = -11807.283620589735
Iteration 2800: Loss = -11807.275585343104
Iteration 2900: Loss = -11785.59962858776
Iteration 3000: Loss = -11785.313652780142
Iteration 3100: Loss = -11768.477349330182
Iteration 3200: Loss = -11760.359399197378
Iteration 3300: Loss = -11760.356758124135
Iteration 3400: Loss = -11760.353482914485
Iteration 3500: Loss = -11760.36216289965
1
Iteration 3600: Loss = -11760.35913057272
2
Iteration 3700: Loss = -11760.351094227011
Iteration 3800: Loss = -11760.350422596635
Iteration 3900: Loss = -11760.329817566488
Iteration 4000: Loss = -11760.32935599736
Iteration 4100: Loss = -11760.339414146498
1
Iteration 4200: Loss = -11760.328754971859
Iteration 4300: Loss = -11760.328340951295
Iteration 4400: Loss = -11753.325722436664
Iteration 4500: Loss = -11753.299576363708
Iteration 4600: Loss = -11753.300069448704
1
Iteration 4700: Loss = -11753.299276643742
Iteration 4800: Loss = -11753.301597826166
1
Iteration 4900: Loss = -11753.29903969796
Iteration 5000: Loss = -11753.30268963209
1
Iteration 5100: Loss = -11753.300749558508
2
Iteration 5200: Loss = -11753.302714687796
3
Iteration 5300: Loss = -11753.297000269911
Iteration 5400: Loss = -11753.298387179027
1
Iteration 5500: Loss = -11753.30105753812
2
Iteration 5600: Loss = -11753.300623738127
3
Iteration 5700: Loss = -11753.296654982993
Iteration 5800: Loss = -11753.29655811152
Iteration 5900: Loss = -11753.296477088421
Iteration 6000: Loss = -11753.29627440533
Iteration 6100: Loss = -11753.296209014205
Iteration 6200: Loss = -11753.29637668784
1
Iteration 6300: Loss = -11753.296332615564
2
Iteration 6400: Loss = -11753.296211135892
3
Iteration 6500: Loss = -11753.299245090979
4
Iteration 6600: Loss = -11753.296900840169
5
Stopping early at iteration 6600 due to no improvement.
pi: tensor([[0.2526, 0.7474],
        [0.7403, 0.2597]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4703, 0.5297], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2970, 0.1005],
         [0.5625, 0.2936]],

        [[0.6080, 0.0942],
         [0.7145, 0.7170]],

        [[0.6282, 0.0909],
         [0.6573, 0.5837]],

        [[0.6522, 0.0930],
         [0.5039, 0.5912]],

        [[0.5252, 0.0941],
         [0.5418, 0.7236]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 1
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.96
time is 2
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.9599529290626264
time is 4
tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 100
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.041366518063159115
Average Adjusted Rand Index: 0.9839905858125253
11760.710648918353
[0.041366518063159115, 0.041366518063159115, 0.34917578378640723, 0.041366518063159115] [0.9839905858125253, 0.9839905858125253, 0.9839905858125253, 0.9839905858125253] [11753.291524559263, 11753.272288209793, 11783.335953698495, 11753.296900840169]
-----------------------------------------------------------------------------------------
This iteration is 21
True Objective function: Loss = -11760.423457448027
Iteration 0: Loss = -12319.83182691631
Iteration 10: Loss = -12319.83182691631
1
Iteration 20: Loss = -12319.83182691631
2
Iteration 30: Loss = -12319.83182691631
3
Stopping early at iteration 30 due to no improvement.
pi: tensor([[1.0000e+00, 5.9895e-22],
        [1.0000e+00, 2.7521e-26]], dtype=torch.float64)
alpha: tensor([1.0000e+00, 6.1245e-22])
beta: tensor([[[0.1961, 0.1527],
         [0.7474, 0.2105]],

        [[0.3346, 0.2104],
         [0.7998, 0.0912]],

        [[0.0859, 0.2404],
         [0.9319, 0.2683]],

        [[0.6623, 0.2367],
         [0.7271, 0.9283]],

        [[0.8931, 0.1744],
         [0.8296, 0.7023]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -12536.868622948567
Iteration 10: Loss = -12318.963244308665
Iteration 20: Loss = -12318.962961016234
Iteration 30: Loss = -12318.96295211957
Iteration 40: Loss = -12318.962971245133
1
Iteration 50: Loss = -12318.962959539936
2
Iteration 60: Loss = -12318.96294499988
Iteration 70: Loss = -12318.962986191349
1
Iteration 80: Loss = -12318.963000470925
2
Iteration 90: Loss = -12318.962970380813
3
Stopping early at iteration 90 due to no improvement.
pi: tensor([[0.2980, 0.7020],
        [0.6341, 0.3659]], dtype=torch.float64)
alpha: tensor([0.4746, 0.5254])
beta: tensor([[[0.1961, 0.1878],
         [0.2458, 0.1961]],

        [[0.7881, 0.1984],
         [0.8508, 0.8317]],

        [[0.7318, 0.1992],
         [0.4500, 0.3391]],

        [[0.1585, 0.2008],
         [0.0939, 0.8350]],

        [[0.2193, 0.1944],
         [0.9279, 0.0078]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21972.71265433262
Iteration 100: Loss = -12320.370246623834
Iteration 200: Loss = -12319.763333685465
Iteration 300: Loss = -12319.583954558251
Iteration 400: Loss = -12319.504497946744
Iteration 500: Loss = -12319.461126639799
Iteration 600: Loss = -12319.434073942986
Iteration 700: Loss = -12319.415383780817
Iteration 800: Loss = -12319.400828288295
Iteration 900: Loss = -12319.387779145642
Iteration 1000: Loss = -12319.374476473153
Iteration 1100: Loss = -12319.359657766337
Iteration 1200: Loss = -12319.341775568742
Iteration 1300: Loss = -12319.318597728367
Iteration 1400: Loss = -12319.28597085145
Iteration 1500: Loss = -12319.234527243087
Iteration 1600: Loss = -12319.137800562441
Iteration 1700: Loss = -12318.901558872307
Iteration 1800: Loss = -12318.515476059672
Iteration 1900: Loss = -12318.334004513608
Iteration 2000: Loss = -12318.245637110507
Iteration 2100: Loss = -12318.20310218195
Iteration 2200: Loss = -12318.179632655381
Iteration 2300: Loss = -12318.164690843134
Iteration 2400: Loss = -12318.154014409278
Iteration 2500: Loss = -12318.145658917563
Iteration 2600: Loss = -12318.138454999294
Iteration 2700: Loss = -12318.131683500518
Iteration 2800: Loss = -12318.124746923868
Iteration 2900: Loss = -12318.117006865597
Iteration 3000: Loss = -12318.107839032296
Iteration 3100: Loss = -12318.096595601028
Iteration 3200: Loss = -12318.083645085344
Iteration 3300: Loss = -12318.070840271643
Iteration 3400: Loss = -12318.059812673491
Iteration 3500: Loss = -12318.050829998658
Iteration 3600: Loss = -12318.043320645125
Iteration 3700: Loss = -12318.037010598087
Iteration 3800: Loss = -12318.03191090711
Iteration 3900: Loss = -12318.027877825989
Iteration 4000: Loss = -12318.024744427223
Iteration 4100: Loss = -12318.022413914983
Iteration 4200: Loss = -12318.020660544236
Iteration 4300: Loss = -12318.01930112309
Iteration 4400: Loss = -12318.018397366262
Iteration 4500: Loss = -12318.017644716341
Iteration 4600: Loss = -12318.01709500014
Iteration 4700: Loss = -12318.016625525064
Iteration 4800: Loss = -12318.016307979293
Iteration 4900: Loss = -12318.016002479408
Iteration 5000: Loss = -12318.015772502311
Iteration 5100: Loss = -12318.01554912104
Iteration 5200: Loss = -12318.015384043165
Iteration 5300: Loss = -12318.015183867345
Iteration 5400: Loss = -12318.015068910267
Iteration 5500: Loss = -12318.014897730398
Iteration 5600: Loss = -12318.01476189391
Iteration 5700: Loss = -12318.014643645669
Iteration 5800: Loss = -12318.014565440364
Iteration 5900: Loss = -12318.014466439989
Iteration 6000: Loss = -12318.014337233693
Iteration 6100: Loss = -12318.014232940333
Iteration 6200: Loss = -12318.014204376392
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 22%|██▏       | 22/100 [8:28:27<30:00:49, 1385.25s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 23%|██▎       | 23/100 [8:50:42<29:18:08, 1369.99s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 24%|██▍       | 24/100 [9:16:35<30:05:00, 1425.01s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 25%|██▌       | 25/100 [9:40:18<29:40:43, 1424.58s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 26%|██▌       | 26/100 [10:00:53<28:06:48, 1367.68s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 27%|██▋       | 27/100 [10:22:43<27:22:58, 1350.39s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 28%|██▊       | 28/100 [10:36:50<23:59:01, 1199.18s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 29%|██▉       | 29/100 [11:07:45<27:31:48, 1395.89s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 30%|███       | 30/100 [11:29:11<26:30:11, 1363.03s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 31%|███       | 31/100 [12:01:46<29:31:49, 1540.71s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 32%|███▏      | 32/100 [12:33:44<31:14:08, 1653.65s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
