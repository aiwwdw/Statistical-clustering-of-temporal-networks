nohup: ignoring input
  0%|          | 0/100 [00:00<?, ?it/s]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/adj_generator.py:108: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1720538439675/work/torch/csrc/utils/tensor_new.cpp:278.)
  Y = torch.tensor(Y)
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  1%|          | 1/100 [21:34<35:35:39, 1294.34s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  2%|▏         | 2/100 [53:46<45:26:38, 1669.37s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  3%|▎         | 3/100 [1:20:35<44:14:07, 1641.72s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  4%|▍         | 4/100 [1:45:48<42:26:02, 1591.28s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  5%|▌         | 5/100 [2:12:26<42:03:05, 1593.53s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  6%|▌         | 6/100 [2:33:45<38:48:53, 1486.53s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  7%|▋         | 7/100 [3:01:07<39:42:53, 1537.35s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  8%|▊         | 8/100 [3:29:53<40:49:24, 1597.44s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
  9%|▉         | 9/100 [3:59:06<41:36:42, 1646.18s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 10%|█         | 10/100 [4:22:38<39:20:46, 1573.85s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 11%|█         | 11/100 [4:49:41<39:16:54, 1588.93s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 12%|█▏        | 12/100 [5:15:55<38:43:38, 1584.30s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 13%|█▎        | 13/100 [5:30:21<33:01:30, 1366.56s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 14%|█▍        | 14/100 [5:52:49<32:30:46, 1361.01s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 15%|█▌        | 15/100 [6:19:00<33:37:58, 1424.45s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 16%|█▌        | 16/100 [6:41:13<32:35:41, 1396.93s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 17%|█▋        | 17/100 [7:06:07<32:52:39, 1426.01s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 18%|█▊        | 18/100 [7:33:10<33:49:44, 1485.18s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 19%|█▉        | 19/100 [7:58:07<33:29:47, 1488.73s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 20%|██        | 20/100 [8:26:30<34:30:43, 1553.05s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 21%|██        | 21/100 [8:58:05<36:19:59, 1655.68s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
-----------------------------------------------------------------------------------------
This iteration is 0
True Objective function: Loss = -10821.238928183191
Iteration 0: Loss = -10960.411434914658
Iteration 10: Loss = -10949.514771234035
Iteration 20: Loss = -10949.417209743367
Iteration 30: Loss = -10949.35302500752
Iteration 40: Loss = -10949.324184909681
Iteration 50: Loss = -10949.31027941114
Iteration 60: Loss = -10949.300670633658
Iteration 70: Loss = -10949.292072702303
Iteration 80: Loss = -10949.283044548223
Iteration 90: Loss = -10949.272311807305
Iteration 100: Loss = -10949.259040440513
Iteration 110: Loss = -10949.243124358854
Iteration 120: Loss = -10949.225952472209
Iteration 130: Loss = -10949.209287374395
Iteration 140: Loss = -10949.194218351458
Iteration 150: Loss = -10949.180902205931
Iteration 160: Loss = -10949.169398408068
Iteration 170: Loss = -10949.159652704004
Iteration 180: Loss = -10949.15167977508
Iteration 190: Loss = -10949.145383574187
Iteration 200: Loss = -10949.14064172
Iteration 210: Loss = -10949.137355161803
Iteration 220: Loss = -10949.135462951846
Iteration 230: Loss = -10949.134689568984
Iteration 240: Loss = -10949.134993049958
1
Iteration 250: Loss = -10949.136229044523
2
Iteration 260: Loss = -10949.13815041369
3
Stopping early at iteration 260 due to no improvement.
pi: tensor([[8.8332e-01, 1.1668e-01],
        [9.9995e-01, 4.7908e-05]], dtype=torch.float64)
alpha: tensor([0.8958, 0.1042])
beta: tensor([[[0.1551, 0.1768],
         [0.1968, 0.2077]],

        [[0.4380, 0.1861],
         [0.5945, 0.1760]],

        [[0.9240, 0.1708],
         [0.4166, 0.3166]],

        [[0.6519, 0.1737],
         [0.3122, 0.8038]],

        [[0.1243, 0.1844],
         [0.1916, 0.8328]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -10956.985658573718
Iteration 10: Loss = -10949.592653074566
Iteration 20: Loss = -10949.470347250654
Iteration 30: Loss = -10949.384174395136
Iteration 40: Loss = -10949.347155200046
Iteration 50: Loss = -10949.327602107887
Iteration 60: Loss = -10949.314749035066
Iteration 70: Loss = -10949.304900889347
Iteration 80: Loss = -10949.296291613504
Iteration 90: Loss = -10949.287653860889
Iteration 100: Loss = -10949.277903279492
Iteration 110: Loss = -10949.265956676296
Iteration 120: Loss = -10949.251070988452
Iteration 130: Loss = -10949.233829078563
Iteration 140: Loss = -10949.215742986135
Iteration 150: Loss = -10949.198231655826
Iteration 160: Loss = -10949.181610213649
Iteration 170: Loss = -10949.165816563042
Iteration 180: Loss = -10949.150546631121
Iteration 190: Loss = -10949.13539534451
Iteration 200: Loss = -10949.120011955618
Iteration 210: Loss = -10949.104272987319
Iteration 220: Loss = -10949.088079526027
Iteration 230: Loss = -10949.071381343074
Iteration 240: Loss = -10949.054354437787
Iteration 250: Loss = -10949.037091737053
Iteration 260: Loss = -10949.019670011508
Iteration 270: Loss = -10949.002063910484
Iteration 280: Loss = -10948.984294740012
Iteration 290: Loss = -10948.966311479506
Iteration 300: Loss = -10948.948060462233
Iteration 310: Loss = -10948.929586281685
Iteration 320: Loss = -10948.910727002907
Iteration 330: Loss = -10948.891578926028
Iteration 340: Loss = -10948.872036390334
Iteration 350: Loss = -10948.852149049531
Iteration 360: Loss = -10948.831829699253
Iteration 370: Loss = -10948.81113202509
Iteration 380: Loss = -10948.789873588807
Iteration 390: Loss = -10948.768159986977
Iteration 400: Loss = -10948.745900186705
Iteration 410: Loss = -10948.723045747707
Iteration 420: Loss = -10948.699536887412
Iteration 430: Loss = -10948.675350247786
Iteration 440: Loss = -10948.650293239849
Iteration 450: Loss = -10948.624370623664
Iteration 460: Loss = -10948.597356961
Iteration 470: Loss = -10948.569282435277
Iteration 480: Loss = -10948.53981588707
Iteration 490: Loss = -10948.508749920762
Iteration 500: Loss = -10948.47583344483
Iteration 510: Loss = -10948.440612089431
Iteration 520: Loss = -10948.402577637213
Iteration 530: Loss = -10948.361020066603
Iteration 540: Loss = -10948.314799574557
Iteration 550: Loss = -10948.262356711684
Iteration 560: Loss = -10948.200758901809
Iteration 570: Loss = -10948.12470918818
Iteration 580: Loss = -10948.021358917727
Iteration 590: Loss = -10947.84409564671
Iteration 600: Loss = -10946.960505464778
Iteration 610: Loss = -10938.37143484067
Iteration 620: Loss = -10933.808527994745
Iteration 630: Loss = -10932.283882454982
Iteration 640: Loss = -10930.678209595242
Iteration 650: Loss = -10930.054509706326
Iteration 660: Loss = -10929.81822742729
Iteration 670: Loss = -10929.647240546667
Iteration 680: Loss = -10929.379560362004
Iteration 690: Loss = -10927.872755269613
Iteration 700: Loss = -10905.033558640644
Iteration 710: Loss = -10791.637134612822
Iteration 720: Loss = -10791.627948912788
Iteration 730: Loss = -10791.628414870622
1
Iteration 740: Loss = -10791.628405549753
2
Iteration 750: Loss = -10791.628436771036
3
Stopping early at iteration 750 due to no improvement.
pi: tensor([[0.7297, 0.2703],
        [0.2786, 0.7214]], dtype=torch.float64)
alpha: tensor([0.5305, 0.4695])
beta: tensor([[[0.1903, 0.0953],
         [0.5453, 0.2608]],

        [[0.2575, 0.1019],
         [0.4876, 0.7125]],

        [[0.4482, 0.0977],
         [0.0126, 0.6493]],

        [[0.9295, 0.1017],
         [0.7409, 0.1392]],

        [[0.7395, 0.0928],
         [0.7416, 0.3416]]], dtype=torch.float64)
time is 0
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 92
Adjusted Rand Index: 0.7026374762971901
time is 1
tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 97
Adjusted Rand Index: 0.8824200963560147
time is 2
tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 100
Adjusted Rand Index: 1.0
time is 3
tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448509923071951
time is 4
tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599990596656818
Global Adjusted Rand Index: 0.8758480931220337
Average Adjusted Rand Index: 0.8779815249252163
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24032.38370915272
Iteration 100: Loss = -10951.462985907783
Iteration 200: Loss = -10950.188526709553
Iteration 300: Loss = -10949.877785024024
Iteration 400: Loss = -10949.758760965056
Iteration 500: Loss = -10949.697657103112
Iteration 600: Loss = -10949.659728771838
Iteration 700: Loss = -10949.633338450183
Iteration 800: Loss = -10949.613352648517
Iteration 900: Loss = -10949.597014895091
Iteration 1000: Loss = -10949.58227624001
Iteration 1100: Loss = -10949.56739488914
Iteration 1200: Loss = -10949.550355811229
Iteration 1300: Loss = -10949.527711946133
Iteration 1400: Loss = -10949.493979349048
Iteration 1500: Loss = -10949.444644111154
Iteration 1600: Loss = -10949.392645894119
Iteration 1700: Loss = -10949.355642960687
Iteration 1800: Loss = -10949.329221769385
Iteration 1900: Loss = -10949.30608741448
Iteration 2000: Loss = -10949.282981419996
Iteration 2100: Loss = -10949.258509731882
Iteration 2200: Loss = -10949.232101133664
Iteration 2300: Loss = -10949.203603801656
Iteration 2400: Loss = -10949.17363333942
Iteration 2500: Loss = -10949.142832797548
Iteration 2600: Loss = -10949.11229977176
Iteration 2700: Loss = -10949.083151558772
Iteration 2800: Loss = -10949.056015668997
Iteration 2900: Loss = -10949.030417540052
Iteration 3000: Loss = -10949.00411090222
Iteration 3100: Loss = -10948.97556751043
Iteration 3200: Loss = -10948.943949457444
Iteration 3300: Loss = -10948.90664303343
Iteration 3400: Loss = -10948.864537640653
Iteration 3500: Loss = -10948.820364409194
Iteration 3600: Loss = -10948.76886712538
Iteration 3700: Loss = -10948.706257884383
Iteration 3800: Loss = -10948.624552501688
Iteration 3900: Loss = -10948.523288448496
Iteration 4000: Loss = -10948.408208178385
Iteration 4100: Loss = -10948.288975482028
Iteration 4200: Loss = -10948.195758701606
Iteration 4300: Loss = -10948.158564511654
Iteration 4400: Loss = -10948.150443608169
Iteration 4500: Loss = -10948.152924761751
1
Iteration 4600: Loss = -10948.1486649738
Iteration 4700: Loss = -10948.148558174185
Iteration 4800: Loss = -10948.149206019192
1
Iteration 4900: Loss = -10948.14833756409
Iteration 5000: Loss = -10948.148318409238
Iteration 5100: Loss = -10948.150038278745
1
Iteration 5200: Loss = -10948.148119545676
Iteration 5300: Loss = -10948.148058064839
Iteration 5400: Loss = -10948.148755590666
1
Iteration 5500: Loss = -10948.147862191958
Iteration 5600: Loss = -10948.14775675872
Iteration 5700: Loss = -10948.147683532045
Iteration 5800: Loss = -10948.147781186935
1
Iteration 5900: Loss = -10948.147550196374
Iteration 6000: Loss = -10948.147407655615
Iteration 6100: Loss = -10948.147504896651
1
Iteration 6200: Loss = -10948.148643374378
2
Iteration 6300: Loss = -10948.147534509368
3
Iteration 6400: Loss = -10948.147165891287
Iteration 6500: Loss = -10948.148254471427
1
Iteration 6600: Loss = -10948.146995649118
Iteration 6700: Loss = -10948.146861017582
Iteration 6800: Loss = -10948.146832612767
Iteration 6900: Loss = -10948.147343429253
1
Iteration 7000: Loss = -10948.149169955203
2
Iteration 7100: Loss = -10948.146580469147
Iteration 7200: Loss = -10948.14651826553
Iteration 7300: Loss = -10948.14637958199
Iteration 7400: Loss = -10948.146362545616
Iteration 7500: Loss = -10948.150691016906
1
Iteration 7600: Loss = -10948.146365985847
2
Iteration 7700: Loss = -10948.154096422004
3
Iteration 7800: Loss = -10948.146186270853
Iteration 7900: Loss = -10948.146395326874
1
Iteration 8000: Loss = -10948.146039754014
Iteration 8100: Loss = -10948.147173527675
1
Iteration 8200: Loss = -10948.145960004262
Iteration 8300: Loss = -10948.148158422622
1
Iteration 8400: Loss = -10948.145883322171
Iteration 8500: Loss = -10948.189612757496
1
Iteration 8600: Loss = -10948.145784336419
Iteration 8700: Loss = -10948.145776966448
Iteration 8800: Loss = -10948.147802863714
1
Iteration 8900: Loss = -10948.145738464624
Iteration 9000: Loss = -10948.145794999451
1
Iteration 9100: Loss = -10948.145996316209
2
Iteration 9200: Loss = -10948.149601870035
3
Iteration 9300: Loss = -10948.175286307076
4
Iteration 9400: Loss = -10948.151454867098
5
Stopping early at iteration 9400 due to no improvement.
pi: tensor([[0.2965, 0.7035],
        [0.9982, 0.0018]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6791, 0.3209], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1464, 0.1685],
         [0.7221, 0.1848]],

        [[0.5521, 0.1676],
         [0.5382, 0.5053]],

        [[0.5365, 0.1624],
         [0.5256, 0.7302]],

        [[0.5223, 0.1629],
         [0.5094, 0.6131]],

        [[0.5323, 0.1648],
         [0.7067, 0.5265]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.004097011376522661
time is 1
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.009615483514529753
time is 2
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 59
Adjusted Rand Index: 0.023487100444052993
time is 3
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.004554135448802703
time is 4
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 64
Adjusted Rand Index: 0.06998044845260952
Global Adjusted Rand Index: 0.025126402631710173
Average Adjusted Rand Index: 0.022346835847303525
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22049.606907933823
Iteration 100: Loss = -10951.890314003693
Iteration 200: Loss = -10950.37825008801
Iteration 300: Loss = -10949.878403698278
Iteration 400: Loss = -10949.682753085433
Iteration 500: Loss = -10949.598899482302
Iteration 600: Loss = -10949.556895953372
Iteration 700: Loss = -10949.530844246174
Iteration 800: Loss = -10949.510965670148
Iteration 900: Loss = -10949.493080130775
Iteration 1000: Loss = -10949.47518894718
Iteration 1100: Loss = -10949.456258181226
Iteration 1200: Loss = -10949.436216577624
Iteration 1300: Loss = -10949.415597015617
Iteration 1400: Loss = -10949.395011361692
Iteration 1500: Loss = -10949.374450452824
Iteration 1600: Loss = -10949.353892811781
Iteration 1700: Loss = -10949.333033897363
Iteration 1800: Loss = -10949.311534345557
Iteration 1900: Loss = -10949.288873047246
Iteration 2000: Loss = -10949.264889051226
Iteration 2100: Loss = -10949.239647094342
Iteration 2200: Loss = -10949.213518747125
Iteration 2300: Loss = -10949.18725332823
Iteration 2400: Loss = -10949.161661714248
Iteration 2500: Loss = -10949.137264998588
Iteration 2600: Loss = -10949.114585949184
Iteration 2700: Loss = -10949.09429886782
Iteration 2800: Loss = -10949.076798148832
Iteration 2900: Loss = -10949.061874535344
Iteration 3000: Loss = -10949.048413156255
Iteration 3100: Loss = -10949.034909819313
Iteration 3200: Loss = -10949.01988109404
Iteration 3300: Loss = -10949.001367283592
Iteration 3400: Loss = -10948.977110076736
Iteration 3500: Loss = -10948.948398739538
Iteration 3600: Loss = -10948.916511471733
Iteration 3700: Loss = -10948.882232840528
Iteration 3800: Loss = -10948.85263937173
Iteration 3900: Loss = -10948.805503375908
Iteration 4000: Loss = -10948.757792941724
Iteration 4100: Loss = -10948.699953543526
Iteration 4200: Loss = -10948.63348164692
Iteration 4300: Loss = -10948.551763572861
Iteration 4400: Loss = -10948.458636989868
Iteration 4500: Loss = -10948.359420204995
Iteration 4600: Loss = -10948.259897020303
Iteration 4700: Loss = -10948.18813706058
Iteration 4800: Loss = -10948.158397220652
Iteration 4900: Loss = -10948.150352175135
Iteration 5000: Loss = -10948.148494789491
Iteration 5100: Loss = -10948.150185636388
1
Iteration 5200: Loss = -10948.147902160665
Iteration 5300: Loss = -10948.147800772269
Iteration 5400: Loss = -10948.147988661065
1
Iteration 5500: Loss = -10948.147692138906
Iteration 5600: Loss = -10948.148593063223
1
Iteration 5700: Loss = -10948.147599413447
Iteration 5800: Loss = -10948.147560485879
Iteration 5900: Loss = -10948.147980775571
1
Iteration 6000: Loss = -10948.147369269522
Iteration 6100: Loss = -10948.147331623008
Iteration 6200: Loss = -10948.14727136966
Iteration 6300: Loss = -10948.147301021567
1
Iteration 6400: Loss = -10948.14710699294
Iteration 6500: Loss = -10948.158884245679
1
Iteration 6600: Loss = -10948.147005289287
Iteration 6700: Loss = -10948.148606926401
1
Iteration 6800: Loss = -10948.146884152547
Iteration 6900: Loss = -10948.146841761907
Iteration 7000: Loss = -10948.147336149807
1
Iteration 7100: Loss = -10948.146693425393
Iteration 7200: Loss = -10948.146630694657
Iteration 7300: Loss = -10948.147940743675
1
Iteration 7400: Loss = -10948.146505998042
Iteration 7500: Loss = -10948.146459418533
Iteration 7600: Loss = -10948.146408897082
Iteration 7700: Loss = -10948.146374105268
Iteration 7800: Loss = -10948.147331429613
1
Iteration 7900: Loss = -10948.146262369124
Iteration 8000: Loss = -10948.1504111132
1
Iteration 8100: Loss = -10948.146151962595
Iteration 8200: Loss = -10948.147984471587
1
Iteration 8300: Loss = -10948.146053436381
Iteration 8400: Loss = -10948.147688799905
1
Iteration 8500: Loss = -10948.145971983651
Iteration 8600: Loss = -10948.235866483086
1
Iteration 8700: Loss = -10948.145933021351
Iteration 8800: Loss = -10948.14589130234
Iteration 8900: Loss = -10948.146037345423
1
Iteration 9000: Loss = -10948.147225722587
2
Iteration 9100: Loss = -10948.319747472719
3
Iteration 9200: Loss = -10948.145987315775
4
Iteration 9300: Loss = -10948.206980706836
5
Stopping early at iteration 9300 due to no improvement.
pi: tensor([[0.2950, 0.7050],
        [0.9977, 0.0023]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6834, 0.3166], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1464, 0.1692],
         [0.5185, 0.1849]],

        [[0.6666, 0.1663],
         [0.5871, 0.5686]],

        [[0.6511, 0.1633],
         [0.7233, 0.5549]],

        [[0.7081, 0.1621],
         [0.5520, 0.6158]],

        [[0.6435, 0.1655],
         [0.5189, 0.5217]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.004097011376522661
time is 1
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.009615483514529753
time is 2
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.01655372473487028
time is 3
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.009836629103553438
time is 4
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 64
Adjusted Rand Index: 0.06998044845260952
Global Adjusted Rand Index: 0.025126402631710173
Average Adjusted Rand Index: 0.02201665943641713
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20601.915006430983
Iteration 100: Loss = -10950.709632472663
Iteration 200: Loss = -10949.946873283841
Iteration 300: Loss = -10949.797152691868
Iteration 400: Loss = -10949.714563490872
Iteration 500: Loss = -10949.6553833682
Iteration 600: Loss = -10949.605025777842
Iteration 700: Loss = -10949.554350842629
Iteration 800: Loss = -10949.495874238897
Iteration 900: Loss = -10949.427402284498
Iteration 1000: Loss = -10949.359947322826
Iteration 1100: Loss = -10949.306759652449
Iteration 1200: Loss = -10949.268578355797
Iteration 1300: Loss = -10949.239414341568
Iteration 1400: Loss = -10949.214364113906
Iteration 1500: Loss = -10949.191017017227
Iteration 1600: Loss = -10949.1684425801
Iteration 1700: Loss = -10949.146446376768
Iteration 1800: Loss = -10949.125342277392
Iteration 1900: Loss = -10949.10556670516
Iteration 2000: Loss = -10949.087295368487
Iteration 2100: Loss = -10949.070815054552
Iteration 2200: Loss = -10949.05583181128
Iteration 2300: Loss = -10949.04175462566
Iteration 2400: Loss = -10949.0274502516
Iteration 2500: Loss = -10949.011097737153
Iteration 2600: Loss = -10948.990485333765
Iteration 2700: Loss = -10948.963554766307
Iteration 2800: Loss = -10948.928812151784
Iteration 2900: Loss = -10948.888109240635
Iteration 3000: Loss = -10948.84496972901
Iteration 3100: Loss = -10948.79765555366
Iteration 3200: Loss = -10948.7443088132
Iteration 3300: Loss = -10948.679687962924
Iteration 3400: Loss = -10948.602327263172
Iteration 3500: Loss = -10948.51198101672
Iteration 3600: Loss = -10948.42095847394
Iteration 3700: Loss = -10948.330109733679
Iteration 3800: Loss = -10948.24466184601
Iteration 3900: Loss = -10948.185325689905
Iteration 4000: Loss = -10948.160418165078
Iteration 4100: Loss = -10948.152825637835
Iteration 4200: Loss = -10948.15072884694
Iteration 4300: Loss = -10948.150172253141
Iteration 4400: Loss = -10948.149933174207
Iteration 4500: Loss = -10948.14976318941
Iteration 4600: Loss = -10948.149712180826
Iteration 4700: Loss = -10948.149511189456
Iteration 4800: Loss = -10948.161852347564
1
Iteration 4900: Loss = -10948.149406452882
Iteration 5000: Loss = -10948.149204675872
Iteration 5100: Loss = -10948.14913915861
Iteration 5200: Loss = -10948.14909164671
Iteration 5300: Loss = -10948.149544277649
1
Iteration 5400: Loss = -10948.148777571501
Iteration 5500: Loss = -10948.148606247596
Iteration 5600: Loss = -10948.150274450169
1
Iteration 5700: Loss = -10948.148363360757
Iteration 5800: Loss = -10948.149442690265
1
Iteration 5900: Loss = -10948.14816807457
Iteration 6000: Loss = -10948.148155110206
Iteration 6100: Loss = -10948.147928406017
Iteration 6200: Loss = -10948.147843090686
Iteration 6300: Loss = -10948.148255484339
1
Iteration 6400: Loss = -10948.147811558203
Iteration 6500: Loss = -10948.1506669715
1
Iteration 6600: Loss = -10948.148574613615
2
Iteration 6700: Loss = -10948.15038235763
3
Iteration 6800: Loss = -10948.147231832723
Iteration 6900: Loss = -10948.147136800115
Iteration 7000: Loss = -10948.147263299648
1
Iteration 7100: Loss = -10948.169885824695
2
Iteration 7200: Loss = -10948.146885464677
Iteration 7300: Loss = -10948.146808979338
Iteration 7400: Loss = -10948.146917188986
1
Iteration 7500: Loss = -10948.146740013563
Iteration 7600: Loss = -10948.155559387167
1
Iteration 7700: Loss = -10948.146511520119
Iteration 7800: Loss = -10948.14852556763
1
Iteration 7900: Loss = -10948.146403609331
Iteration 8000: Loss = -10948.14672937462
1
Iteration 8100: Loss = -10948.14626556042
Iteration 8200: Loss = -10948.146247140243
Iteration 8300: Loss = -10948.146132736656
Iteration 8400: Loss = -10948.14639324645
1
Iteration 8500: Loss = -10948.14641148569
2
Iteration 8600: Loss = -10948.146059598022
Iteration 8700: Loss = -10948.146137527367
1
Iteration 8800: Loss = -10948.1517191856
2
Iteration 8900: Loss = -10948.157323106141
3
Iteration 9000: Loss = -10948.174396750554
4
Iteration 9100: Loss = -10948.145779456108
Iteration 9200: Loss = -10948.248342209301
1
Iteration 9300: Loss = -10948.156235048586
2
Iteration 9400: Loss = -10948.146688504072
3
Iteration 9500: Loss = -10948.147427430817
4
Iteration 9600: Loss = -10948.146348152808
5
Stopping early at iteration 9600 due to no improvement.
pi: tensor([[0.2938, 0.7062],
        [0.9980, 0.0020]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6779, 0.3221], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1464, 0.1687],
         [0.6562, 0.1850]],

        [[0.5334, 0.1676],
         [0.5360, 0.5711]],

        [[0.6081, 0.1625],
         [0.6527, 0.6700]],

        [[0.5100, 0.1631],
         [0.7284, 0.6373]],

        [[0.5858, 0.1645],
         [0.5791, 0.6367]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.004097011376522661
time is 1
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.009615483514529753
time is 2
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 59
Adjusted Rand Index: 0.023487100444052993
time is 3
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 8.227054964072745e-05
time is 4
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 63
Adjusted Rand Index: 0.059011339865683146
Global Adjusted Rand Index: 0.022554340352466
Average Adjusted Rand Index: 0.019258641150085853
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22997.138582388336
Iteration 100: Loss = -10951.220451413
Iteration 200: Loss = -10950.219324893234
Iteration 300: Loss = -10949.844633786059
Iteration 400: Loss = -10949.700633754912
Iteration 500: Loss = -10949.63789985739
Iteration 600: Loss = -10949.603149901
Iteration 700: Loss = -10949.57797664849
Iteration 800: Loss = -10949.554888239798
Iteration 900: Loss = -10949.529683799781
Iteration 1000: Loss = -10949.499203320673
Iteration 1100: Loss = -10949.462970249977
Iteration 1200: Loss = -10949.425519767876
Iteration 1300: Loss = -10949.39308710286
Iteration 1400: Loss = -10949.366581232434
Iteration 1500: Loss = -10949.34411137211
Iteration 1600: Loss = -10949.323609444222
Iteration 1700: Loss = -10949.303729609097
Iteration 1800: Loss = -10949.283454618479
Iteration 1900: Loss = -10949.262049894898
Iteration 2000: Loss = -10949.239064934629
Iteration 2100: Loss = -10949.214125110317
Iteration 2200: Loss = -10949.187183510889
Iteration 2300: Loss = -10949.15853645248
Iteration 2400: Loss = -10949.128813129708
Iteration 2500: Loss = -10949.099071740751
Iteration 2600: Loss = -10949.070165894726
Iteration 2700: Loss = -10949.042551602446
Iteration 2800: Loss = -10949.015846625562
Iteration 2900: Loss = -10948.988697439934
Iteration 3000: Loss = -10948.959971232105
Iteration 3100: Loss = -10948.928579658706
Iteration 3200: Loss = -10948.89381983309
Iteration 3300: Loss = -10948.85742072828
Iteration 3400: Loss = -10948.838549567366
Iteration 3500: Loss = -10948.776321541609
Iteration 3600: Loss = -10948.764869988754
Iteration 3700: Loss = -10948.668671262414
Iteration 3800: Loss = -10948.621491085041
Iteration 3900: Loss = -10948.533036549185
Iteration 4000: Loss = -10948.408628916433
Iteration 4100: Loss = -10948.304287782727
Iteration 4200: Loss = -10948.21409060113
Iteration 4300: Loss = -10948.166648127424
Iteration 4400: Loss = -10948.152450247353
Iteration 4500: Loss = -10948.149211442973
Iteration 4600: Loss = -10948.148522806066
Iteration 4700: Loss = -10948.148349990457
Iteration 4800: Loss = -10948.148247490033
Iteration 4900: Loss = -10948.148166147505
Iteration 5000: Loss = -10948.148091287327
Iteration 5100: Loss = -10948.148180015323
1
Iteration 5200: Loss = -10948.14820972137
2
Iteration 5300: Loss = -10948.148184643864
3
Iteration 5400: Loss = -10948.147816794264
Iteration 5500: Loss = -10948.14934810544
1
Iteration 5600: Loss = -10948.147679638563
Iteration 5700: Loss = -10948.148640552583
1
Iteration 5800: Loss = -10948.147528008472
Iteration 5900: Loss = -10948.147441062058
Iteration 6000: Loss = -10948.149284253186
1
Iteration 6100: Loss = -10948.14729869872
Iteration 6200: Loss = -10948.151912697534
1
Iteration 6300: Loss = -10948.147162677078
Iteration 6400: Loss = -10948.148597801815
1
Iteration 6500: Loss = -10948.147271279813
2
Iteration 6600: Loss = -10948.151766115181
3
Iteration 6700: Loss = -10948.147208869208
4
Iteration 6800: Loss = -10948.147295747796
5
Stopping early at iteration 6800 due to no improvement.
pi: tensor([[0.2947, 0.7053],
        [0.9941, 0.0059]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.6770, 0.3230], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1463, 0.1687],
         [0.5325, 0.1849]],

        [[0.5756, 0.1674],
         [0.7245, 0.7233]],

        [[0.5974, 0.1625],
         [0.5225, 0.6831]],

        [[0.5850, 0.1631],
         [0.6679, 0.7236]],

        [[0.7020, 0.1646],
         [0.7286, 0.5553]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1])
Difference count: 56
Adjusted Rand Index: 0.008289621581116523
time is 1
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 0, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 57
Adjusted Rand Index: 0.009615483514529753
time is 2
tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.01655372473487028
time is 3
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 8.227054964072745e-05
time is 4
tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 64
Adjusted Rand Index: 0.06998044845260952
Global Adjusted Rand Index: 0.023817007045320342
Average Adjusted Rand Index: 0.02090430976655336
10821.238928183191
[0.025126402631710173, 0.025126402631710173, 0.022554340352466, 0.023817007045320342] [0.022346835847303525, 0.02201665943641713, 0.019258641150085853, 0.02090430976655336] [10948.151454867098, 10948.206980706836, 10948.146348152808, 10948.147295747796]
-----------------------------------------------------------------------------------------
This iteration is 1
True Objective function: Loss = -10903.596473201182
Iteration 0: Loss = -11048.831249014678
Iteration 10: Loss = -10915.781347804543
Iteration 20: Loss = -10915.869802232137
1
Iteration 30: Loss = -10915.934218090288
2
Iteration 40: Loss = -10915.975032913027
3
Stopping early at iteration 40 due to no improvement.
pi: tensor([[2.5600e-04, 9.9974e-01],
        [1.6725e-02, 9.8328e-01]], dtype=torch.float64)
alpha: tensor([0.0177, 0.9823])
beta: tensor([[[0.2479, 0.2173],
         [0.7661, 0.1575]],

        [[0.2760, 0.1749],
         [0.2604, 0.0809]],

        [[0.8955, 0.1734],
         [0.3502, 0.9815]],

        [[0.4651, 0.2360],
         [0.1621, 0.0103]],

        [[0.9345, 0.2255],
         [0.8719, 0.4862]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -10916.824732177525
Iteration 10: Loss = -10916.824669943177
Iteration 20: Loss = -10916.824585252825
Iteration 30: Loss = -10916.824562588778
Iteration 40: Loss = -10916.824397209448
Iteration 50: Loss = -10916.82435094025
Iteration 60: Loss = -10916.823729343423
Iteration 70: Loss = -10916.822233506464
Iteration 80: Loss = -10916.818377254656
Iteration 90: Loss = -10916.808513143029
Iteration 100: Loss = -10916.784071501243
Iteration 110: Loss = -10916.728333211355
Iteration 120: Loss = -10916.622095627146
Iteration 130: Loss = -10916.47113670855
Iteration 140: Loss = -10916.320494593669
Iteration 150: Loss = -10916.207754162373
Iteration 160: Loss = -10916.135878895353
Iteration 170: Loss = -10916.093095797392
Iteration 180: Loss = -10916.06817282429
Iteration 190: Loss = -10916.053758118818
Iteration 200: Loss = -10916.045441717704
Iteration 210: Loss = -10916.040635192476
Iteration 220: Loss = -10916.03783119187
Iteration 230: Loss = -10916.036210637765
Iteration 240: Loss = -10916.035270080076
Iteration 250: Loss = -10916.034725272008
Iteration 260: Loss = -10916.034419495298
Iteration 270: Loss = -10916.034227430153
Iteration 280: Loss = -10916.03415353314
Iteration 290: Loss = -10916.034103092947
Iteration 300: Loss = -10916.03402795716
Iteration 310: Loss = -10916.034045858436
1
Iteration 320: Loss = -10916.034018709403
Iteration 330: Loss = -10916.03399811533
Iteration 340: Loss = -10916.03401148014
1
Iteration 350: Loss = -10916.034021799405
2
Iteration 360: Loss = -10916.033996732362
Iteration 370: Loss = -10916.034023716136
1
Iteration 380: Loss = -10916.034015874107
2
Iteration 390: Loss = -10916.033990032312
Iteration 400: Loss = -10916.033990735488
1
Iteration 410: Loss = -10916.03397930257
Iteration 420: Loss = -10916.0339937746
1
Iteration 430: Loss = -10916.033988915122
2
Iteration 440: Loss = -10916.033972557714
Iteration 450: Loss = -10916.033993512834
1
Iteration 460: Loss = -10916.033980122922
2
Iteration 470: Loss = -10916.0339918158
3
Stopping early at iteration 470 due to no improvement.
pi: tensor([[9.8522e-01, 1.4777e-02],
        [1.0000e+00, 5.8823e-60]], dtype=torch.float64)
alpha: tensor([0.9843, 0.0157])
beta: tensor([[[0.1577, 0.2182],
         [0.5200, 0.2477]],

        [[0.6807, 0.1743],
         [0.1358, 0.0945]],

        [[0.5031, 0.1722],
         [0.3142, 0.0644]],

        [[0.0143, 0.2376],
         [0.2783, 0.6392]],

        [[0.2129, 0.2273],
         [0.7953, 0.3801]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21286.318505667416
Iteration 100: Loss = -10917.836968018675
Iteration 200: Loss = -10916.948643334817
Iteration 300: Loss = -10916.510907031416
Iteration 400: Loss = -10916.082771308935
Iteration 500: Loss = -10915.612483159886
Iteration 600: Loss = -10915.393995898152
Iteration 700: Loss = -10915.266161259955
Iteration 800: Loss = -10915.160201201183
Iteration 900: Loss = -10915.065838510227
Iteration 1000: Loss = -10914.977702895307
Iteration 1100: Loss = -10914.886687395205
Iteration 1200: Loss = -10914.768900493535
Iteration 1300: Loss = -10914.524316365954
Iteration 1400: Loss = -10913.873219984536
Iteration 1500: Loss = -10912.758328396296
Iteration 1600: Loss = -10911.79007324311
Iteration 1700: Loss = -10911.290301807845
Iteration 1800: Loss = -10911.00216894406
Iteration 1900: Loss = -10910.81589935021
Iteration 2000: Loss = -10910.684821351071
Iteration 2100: Loss = -10910.58641657178
Iteration 2200: Loss = -10910.507869931813
Iteration 2300: Loss = -10910.44209478735
Iteration 2400: Loss = -10910.38438083397
Iteration 2500: Loss = -10910.331146679224
Iteration 2600: Loss = -10910.279047503373
Iteration 2700: Loss = -10910.224428917532
Iteration 2800: Loss = -10910.162991699419
Iteration 2900: Loss = -10910.090895770614
Iteration 3000: Loss = -10910.009387811793
Iteration 3100: Loss = -10909.928703710266
Iteration 3200: Loss = -10909.860436350747
Iteration 3300: Loss = -10909.807711897634
Iteration 3400: Loss = -10909.767678898452
Iteration 3500: Loss = -10909.737048115205
Iteration 3600: Loss = -10909.71291265332
Iteration 3700: Loss = -10909.69364403248
Iteration 3800: Loss = -10909.677779873016
Iteration 3900: Loss = -10909.664702038688
Iteration 4000: Loss = -10909.653648809883
Iteration 4100: Loss = -10909.64419417094
Iteration 4200: Loss = -10909.636116715325
Iteration 4300: Loss = -10909.629091776194
Iteration 4400: Loss = -10909.62296879358
Iteration 4500: Loss = -10909.617516227943
Iteration 4600: Loss = -10909.612767323028
Iteration 4700: Loss = -10909.608495327204
Iteration 4800: Loss = -10909.604662332687
Iteration 4900: Loss = -10909.601231672219
Iteration 5000: Loss = -10909.59811500639
Iteration 5100: Loss = -10909.595301991114
Iteration 5200: Loss = -10909.592757791435
Iteration 5300: Loss = -10909.590490413832
Iteration 5400: Loss = -10909.588328071193
Iteration 5500: Loss = -10909.586381405905
Iteration 5600: Loss = -10909.584568384284
Iteration 5700: Loss = -10909.582923915981
Iteration 5800: Loss = -10909.581430014152
Iteration 5900: Loss = -10909.579996667164
Iteration 6000: Loss = -10909.578728919181
Iteration 6100: Loss = -10909.577528979577
Iteration 6200: Loss = -10909.576394303476
Iteration 6300: Loss = -10909.575329469902
Iteration 6400: Loss = -10909.574357476344
Iteration 6500: Loss = -10909.573463343564
Iteration 6600: Loss = -10909.572606393493
Iteration 6700: Loss = -10909.57179077104
Iteration 6800: Loss = -10909.5710583436
Iteration 6900: Loss = -10909.570360423777
Iteration 7000: Loss = -10909.569677735455
Iteration 7100: Loss = -10909.569032745312
Iteration 7200: Loss = -10909.568462463872
Iteration 7300: Loss = -10909.56791736572
Iteration 7400: Loss = -10909.586991801707
1
Iteration 7500: Loss = -10909.566876521869
Iteration 7600: Loss = -10909.56643228314
Iteration 7700: Loss = -10909.565992680225
Iteration 7800: Loss = -10909.565681323582
Iteration 7900: Loss = -10909.565172288452
Iteration 8000: Loss = -10909.56480011037
Iteration 8100: Loss = -10910.205779664791
1
Iteration 8200: Loss = -10909.56408348076
Iteration 8300: Loss = -10909.563772236725
Iteration 8400: Loss = -10909.563446434207
Iteration 8500: Loss = -10909.566765792053
1
Iteration 8600: Loss = -10909.562908693686
Iteration 8700: Loss = -10909.562639663693
Iteration 8800: Loss = -10909.562400143417
Iteration 8900: Loss = -10909.563521140988
1
Iteration 9000: Loss = -10909.561902492087
Iteration 9100: Loss = -10909.562019589734
1
Iteration 9200: Loss = -10909.561958983448
2
Iteration 9300: Loss = -10909.56127359243
Iteration 9400: Loss = -10909.561102274738
Iteration 9500: Loss = -10909.562322720569
1
Iteration 9600: Loss = -10909.56075563443
Iteration 9700: Loss = -10909.560622956247
Iteration 9800: Loss = -10909.561000808393
1
Iteration 9900: Loss = -10909.56030342764
Iteration 10000: Loss = -10909.560204842306
Iteration 10100: Loss = -10909.560068614355
Iteration 10200: Loss = -10909.560106872514
1
Iteration 10300: Loss = -10909.561602184116
2
Iteration 10400: Loss = -10909.55973337109
Iteration 10500: Loss = -10909.55961555261
Iteration 10600: Loss = -10909.560055031925
1
Iteration 10700: Loss = -10909.55942275601
Iteration 10800: Loss = -10909.559371215844
Iteration 10900: Loss = -10909.566434199844
1
Iteration 11000: Loss = -10909.559186934834
Iteration 11100: Loss = -10909.559166891628
Iteration 11200: Loss = -10909.559926591368
1
Iteration 11300: Loss = -10909.558987168371
Iteration 11400: Loss = -10909.55895665634
Iteration 11500: Loss = -10909.558922220127
Iteration 11600: Loss = -10909.558860336343
Iteration 11700: Loss = -10909.558777922482
Iteration 11800: Loss = -10909.558921516787
1
Iteration 11900: Loss = -10909.560202568597
2
Iteration 12000: Loss = -10909.558697810831
Iteration 12100: Loss = -10909.571705595847
1
Iteration 12200: Loss = -10909.558546125061
Iteration 12300: Loss = -10909.558537230934
Iteration 12400: Loss = -10910.059422077098
1
Iteration 12500: Loss = -10909.558479435971
Iteration 12600: Loss = -10909.558443769054
Iteration 12700: Loss = -10909.558483900813
1
Iteration 12800: Loss = -10909.55865311907
2
Iteration 12900: Loss = -10909.558365552104
Iteration 13000: Loss = -10909.558346087877
Iteration 13100: Loss = -10909.562918352507
1
Iteration 13200: Loss = -10909.558331472535
Iteration 13300: Loss = -10909.55832970427
Iteration 13400: Loss = -10909.639985577114
1
Iteration 13500: Loss = -10909.558402904817
2
Iteration 13600: Loss = -10909.558281300573
Iteration 13700: Loss = -10909.919078530354
1
Iteration 13800: Loss = -10909.558261802944
Iteration 13900: Loss = -10909.558233576508
Iteration 14000: Loss = -10909.615727029877
1
Iteration 14100: Loss = -10909.558243697467
2
Iteration 14200: Loss = -10909.558187728111
Iteration 14300: Loss = -10909.558214541503
1
Iteration 14400: Loss = -10909.55830047864
2
Iteration 14500: Loss = -10909.558927493803
3
Iteration 14600: Loss = -10909.558163876183
Iteration 14700: Loss = -10909.559164231261
1
Iteration 14800: Loss = -10909.558154705512
Iteration 14900: Loss = -10909.563579591626
1
Iteration 15000: Loss = -10909.558152693719
Iteration 15100: Loss = -10909.563170487956
1
Iteration 15200: Loss = -10909.559587505177
2
Iteration 15300: Loss = -10909.560850237742
3
Iteration 15400: Loss = -10909.558074833425
Iteration 15500: Loss = -10909.56103399759
1
Iteration 15600: Loss = -10909.55823578577
2
Iteration 15700: Loss = -10909.558166008412
3
Iteration 15800: Loss = -10909.558385563223
4
Iteration 15900: Loss = -10909.55809777615
5
Stopping early at iteration 15900 due to no improvement.
pi: tensor([[9.9999e-01, 9.0470e-06],
        [3.2163e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0101, 0.9899], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1362, 0.2721],
         [0.6199, 0.1603]],

        [[0.7212, 0.1619],
         [0.5546, 0.6222]],

        [[0.6572, 0.1714],
         [0.7179, 0.5958]],

        [[0.5571, 0.0612],
         [0.5221, 0.6568]],

        [[0.5095, 0.2722],
         [0.7104, 0.6890]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0025538476364862913
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0016672277529296718
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: 0.0009711150057751206
Average Adjusted Rand Index: 0.0008489082118456738
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24295.317838357376
Iteration 100: Loss = -10918.151754294819
Iteration 200: Loss = -10916.858534929142
Iteration 300: Loss = -10916.349053115375
Iteration 400: Loss = -10915.39005180839
Iteration 500: Loss = -10915.098458982466
Iteration 600: Loss = -10914.94445685997
Iteration 700: Loss = -10914.83260198428
Iteration 800: Loss = -10914.735955802753
Iteration 900: Loss = -10914.627539235977
Iteration 1000: Loss = -10914.44530848805
Iteration 1100: Loss = -10913.237006035952
Iteration 1200: Loss = -10911.927522285183
Iteration 1300: Loss = -10911.541521599447
Iteration 1400: Loss = -10911.386073875896
Iteration 1500: Loss = -10911.307478885665
Iteration 1600: Loss = -10911.261193362452
Iteration 1700: Loss = -10911.23095511469
Iteration 1800: Loss = -10911.20984939692
Iteration 1900: Loss = -10911.194329013837
Iteration 2000: Loss = -10911.182543972078
Iteration 2100: Loss = -10911.173252246237
Iteration 2200: Loss = -10911.165840722611
Iteration 2300: Loss = -10911.159765307602
Iteration 2400: Loss = -10911.154679585718
Iteration 2500: Loss = -10911.150465578501
Iteration 2600: Loss = -10911.146868590522
Iteration 2700: Loss = -10911.143777716487
Iteration 2800: Loss = -10911.141098774591
Iteration 2900: Loss = -10911.138768708268
Iteration 3000: Loss = -10911.136688791734
Iteration 3100: Loss = -10911.134833908724
Iteration 3200: Loss = -10911.13325969579
Iteration 3300: Loss = -10911.131770160917
Iteration 3400: Loss = -10911.130477342813
Iteration 3500: Loss = -10911.129339931778
Iteration 3600: Loss = -10911.128283155655
Iteration 3700: Loss = -10911.127294795657
Iteration 3800: Loss = -10911.126434506537
Iteration 3900: Loss = -10911.125552997104
Iteration 4000: Loss = -10911.124850493889
Iteration 4100: Loss = -10911.12419268989
Iteration 4200: Loss = -10911.123553314308
Iteration 4300: Loss = -10911.122985869963
Iteration 4400: Loss = -10911.12247194236
Iteration 4500: Loss = -10911.121968934269
Iteration 4600: Loss = -10911.121496989083
Iteration 4700: Loss = -10911.121104033664
Iteration 4800: Loss = -10911.120687302962
Iteration 4900: Loss = -10911.120321510482
Iteration 5000: Loss = -10911.119981469508
Iteration 5100: Loss = -10911.119643381926
Iteration 5200: Loss = -10911.1193351609
Iteration 5300: Loss = -10911.11910085959
Iteration 5400: Loss = -10911.1188571524
Iteration 5500: Loss = -10911.118593374582
Iteration 5600: Loss = -10911.118385101518
Iteration 5700: Loss = -10911.119625756639
1
Iteration 5800: Loss = -10911.117971307964
Iteration 5900: Loss = -10911.11775966736
Iteration 6000: Loss = -10911.117609748551
Iteration 6100: Loss = -10911.117434308177
Iteration 6200: Loss = -10911.117289097647
Iteration 6300: Loss = -10911.117121074427
Iteration 6400: Loss = -10911.11707185897
Iteration 6500: Loss = -10911.116854283995
Iteration 6600: Loss = -10911.117082798924
1
Iteration 6700: Loss = -10911.116585287657
Iteration 6800: Loss = -10911.116503365587
Iteration 6900: Loss = -10911.116399740326
Iteration 7000: Loss = -10911.116276432462
Iteration 7100: Loss = -10911.11638346445
1
Iteration 7200: Loss = -10911.116088631527
Iteration 7300: Loss = -10911.11797963826
1
Iteration 7400: Loss = -10911.115972740125
Iteration 7500: Loss = -10911.11587093541
Iteration 7600: Loss = -10911.120958040414
1
Iteration 7700: Loss = -10911.115749412582
Iteration 7800: Loss = -10911.115688629965
Iteration 7900: Loss = -10911.115657114324
Iteration 8000: Loss = -10911.11556576419
Iteration 8100: Loss = -10911.11551893823
Iteration 8200: Loss = -10911.115471959089
Iteration 8300: Loss = -10911.115422783869
Iteration 8400: Loss = -10911.1154103904
Iteration 8500: Loss = -10911.115917267789
1
Iteration 8600: Loss = -10911.201324430438
2
Iteration 8700: Loss = -10911.115262080444
Iteration 8800: Loss = -10911.115214073603
Iteration 8900: Loss = -10911.130105909937
1
Iteration 9000: Loss = -10911.115157309563
Iteration 9100: Loss = -10911.115117988884
Iteration 9200: Loss = -10911.15632895309
1
Iteration 9300: Loss = -10911.115063049761
Iteration 9400: Loss = -10911.115025991889
Iteration 9500: Loss = -10911.164711648014
1
Iteration 9600: Loss = -10911.114986029159
Iteration 9700: Loss = -10911.114971630257
Iteration 9800: Loss = -10911.115215147225
1
Iteration 9900: Loss = -10911.114952711872
Iteration 10000: Loss = -10911.11490956494
Iteration 10100: Loss = -10911.114868313638
Iteration 10200: Loss = -10911.11579006907
1
Iteration 10300: Loss = -10911.114829351558
Iteration 10400: Loss = -10911.114811275955
Iteration 10500: Loss = -10911.167473866071
1
Iteration 10600: Loss = -10911.114784503881
Iteration 10700: Loss = -10911.114793548812
1
Iteration 10800: Loss = -10911.115548603493
2
Iteration 10900: Loss = -10911.114828691338
3
Iteration 11000: Loss = -10911.114742446687
Iteration 11100: Loss = -10911.114743134463
1
Iteration 11200: Loss = -10911.11503353298
2
Iteration 11300: Loss = -10911.114735381127
Iteration 11400: Loss = -10911.127031838263
1
Iteration 11500: Loss = -10911.11470209964
Iteration 11600: Loss = -10911.115550553279
1
Iteration 11700: Loss = -10911.114675102981
Iteration 11800: Loss = -10911.120382288333
1
Iteration 11900: Loss = -10911.11465532516
Iteration 12000: Loss = -10911.212178208656
1
Iteration 12100: Loss = -10911.114695120797
2
Iteration 12200: Loss = -10911.115065221165
3
Iteration 12300: Loss = -10911.114627815341
Iteration 12400: Loss = -10911.114773045061
1
Iteration 12500: Loss = -10911.142107034319
2
Iteration 12600: Loss = -10911.114623217913
Iteration 12700: Loss = -10911.295776876306
1
Iteration 12800: Loss = -10911.114611327801
Iteration 12900: Loss = -10911.132278579042
1
Iteration 13000: Loss = -10911.118593722915
2
Iteration 13100: Loss = -10911.136475968196
3
Iteration 13200: Loss = -10911.193576428619
4
Iteration 13300: Loss = -10911.15414742967
5
Stopping early at iteration 13300 due to no improvement.
pi: tensor([[1.0000e+00, 3.0146e-07],
        [2.5683e-01, 7.4317e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9321, 0.0679], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1603, 0.2111],
         [0.5403, 0.2196]],

        [[0.6461, 0.1774],
         [0.6008, 0.7130]],

        [[0.6932, 0.1388],
         [0.5111, 0.6600]],

        [[0.6018, 0.0942],
         [0.5634, 0.5992]],

        [[0.6834, 0.0703],
         [0.6886, 0.6142]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.002056465888134684
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0010149900615556472
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0005780186400410129
Average Adjusted Rand Index: 0.0011969665072433413
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23160.57014297909
Iteration 100: Loss = -10916.87690339753
Iteration 200: Loss = -10915.914472913724
Iteration 300: Loss = -10914.999126734761
Iteration 400: Loss = -10914.639520370421
Iteration 500: Loss = -10913.992116456295
Iteration 600: Loss = -10912.716714975591
Iteration 700: Loss = -10912.018417337698
Iteration 800: Loss = -10911.68135001059
Iteration 900: Loss = -10911.499846314498
Iteration 1000: Loss = -10911.393782982394
Iteration 1100: Loss = -10911.327069261288
Iteration 1200: Loss = -10911.282417104845
Iteration 1300: Loss = -10911.250873830666
Iteration 1400: Loss = -10911.227716067333
Iteration 1500: Loss = -10911.210124920879
Iteration 1600: Loss = -10911.196413123136
Iteration 1700: Loss = -10911.185511771428
Iteration 1800: Loss = -10911.17664473139
Iteration 1900: Loss = -10911.169389217648
Iteration 2000: Loss = -10911.163320521518
Iteration 2100: Loss = -10911.158183251306
Iteration 2200: Loss = -10911.153793787988
Iteration 2300: Loss = -10911.150008586468
Iteration 2400: Loss = -10911.146754110256
Iteration 2500: Loss = -10911.143875530446
Iteration 2600: Loss = -10911.141343859535
Iteration 2700: Loss = -10911.139099086517
Iteration 2800: Loss = -10911.137129303765
Iteration 2900: Loss = -10911.135346154535
Iteration 3000: Loss = -10911.133781620987
Iteration 3100: Loss = -10911.132311270176
Iteration 3200: Loss = -10911.131031980494
Iteration 3300: Loss = -10911.129852367174
Iteration 3400: Loss = -10911.128750306787
Iteration 3500: Loss = -10911.127789484872
Iteration 3600: Loss = -10911.126869768246
Iteration 3700: Loss = -10911.126050865289
Iteration 3800: Loss = -10911.125308850027
Iteration 3900: Loss = -10911.12465478008
Iteration 4000: Loss = -10911.12398112158
Iteration 4100: Loss = -10911.123404442764
Iteration 4200: Loss = -10911.122875453362
Iteration 4300: Loss = -10911.12231443441
Iteration 4400: Loss = -10911.12186748147
Iteration 4500: Loss = -10911.12143424121
Iteration 4600: Loss = -10911.121022922198
Iteration 4700: Loss = -10911.12063310827
Iteration 4800: Loss = -10911.120322495426
Iteration 4900: Loss = -10911.119967965979
Iteration 5000: Loss = -10911.119662615078
Iteration 5100: Loss = -10911.119333017697
Iteration 5200: Loss = -10911.119091494378
Iteration 5300: Loss = -10911.11893976279
Iteration 5400: Loss = -10911.118574197822
Iteration 5500: Loss = -10911.118343188062
Iteration 5600: Loss = -10911.118163042156
Iteration 5700: Loss = -10911.117926765884
Iteration 5800: Loss = -10911.11858459223
1
Iteration 5900: Loss = -10911.117572451563
Iteration 6000: Loss = -10911.117447164015
Iteration 6100: Loss = -10911.117332310774
Iteration 6200: Loss = -10911.117101146829
Iteration 6300: Loss = -10911.116961422302
Iteration 6400: Loss = -10911.116836038955
Iteration 6500: Loss = -10911.1167409613
Iteration 6600: Loss = -10911.117168057439
1
Iteration 6700: Loss = -10911.116517297838
Iteration 6800: Loss = -10911.116434275762
Iteration 6900: Loss = -10911.116369085754
Iteration 7000: Loss = -10911.116215293148
Iteration 7100: Loss = -10911.11645515198
1
Iteration 7200: Loss = -10911.11606582523
Iteration 7300: Loss = -10911.116019903293
Iteration 7400: Loss = -10911.11590846694
Iteration 7500: Loss = -10911.116431735258
1
Iteration 7600: Loss = -10911.115774414886
Iteration 7700: Loss = -10911.115698665253
Iteration 7800: Loss = -10911.115662565766
Iteration 7900: Loss = -10911.115596882692
Iteration 8000: Loss = -10911.115509408832
Iteration 8100: Loss = -10911.115486419272
Iteration 8200: Loss = -10911.115419269614
Iteration 8300: Loss = -10911.116290605753
1
Iteration 8400: Loss = -10911.117263666067
2
Iteration 8500: Loss = -10911.151275197863
3
Iteration 8600: Loss = -10911.115236146712
Iteration 8700: Loss = -10911.116008438757
1
Iteration 8800: Loss = -10911.115159541818
Iteration 8900: Loss = -10911.1181423668
1
Iteration 9000: Loss = -10911.115132140822
Iteration 9100: Loss = -10911.244431830222
1
Iteration 9200: Loss = -10911.115070286065
Iteration 9300: Loss = -10911.115043490703
Iteration 9400: Loss = -10911.115044777205
1
Iteration 9500: Loss = -10911.11501933043
Iteration 9600: Loss = -10911.114957301817
Iteration 9700: Loss = -10911.155451215289
1
Iteration 9800: Loss = -10911.114918919626
Iteration 9900: Loss = -10911.114883546865
Iteration 10000: Loss = -10911.189685681928
1
Iteration 10100: Loss = -10911.114866685615
Iteration 10200: Loss = -10911.114847752367
Iteration 10300: Loss = -10911.114875563893
1
Iteration 10400: Loss = -10911.114910192051
2
Iteration 10500: Loss = -10911.114773638867
Iteration 10600: Loss = -10911.114764377653
Iteration 10700: Loss = -10911.115120909606
1
Iteration 10800: Loss = -10911.114733837772
Iteration 10900: Loss = -10911.11474300632
1
Iteration 11000: Loss = -10911.116111212215
2
Iteration 11100: Loss = -10911.114728696668
Iteration 11200: Loss = -10911.114704373622
Iteration 11300: Loss = -10911.205271918392
1
Iteration 11400: Loss = -10911.114713007772
2
Iteration 11500: Loss = -10911.114681323357
Iteration 11600: Loss = -10911.132390462404
1
Iteration 11700: Loss = -10911.201181291635
2
Iteration 11800: Loss = -10911.115587117562
3
Iteration 11900: Loss = -10911.114682333884
4
Iteration 12000: Loss = -10911.147498530248
5
Stopping early at iteration 12000 due to no improvement.
pi: tensor([[7.4571e-01, 2.5429e-01],
        [5.4310e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0668, 0.9332], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2196, 0.2127],
         [0.7233, 0.1607]],

        [[0.5209, 0.1788],
         [0.5826, 0.7002]],

        [[0.5080, 0.1380],
         [0.5224, 0.6728]],

        [[0.6506, 0.0942],
         [0.6965, 0.5464]],

        [[0.6733, 0.0703],
         [0.5234, 0.5936]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0015310678579180683
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.002056465888134684
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0010149900615556472
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0005780186400410129
Average Adjusted Rand Index: 0.0011969665072433413
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20361.90830193974
Iteration 100: Loss = -10917.28032076951
Iteration 200: Loss = -10916.69837374468
Iteration 300: Loss = -10916.312896991403
Iteration 400: Loss = -10915.712623823712
Iteration 500: Loss = -10915.364333672678
Iteration 600: Loss = -10915.153527048724
Iteration 700: Loss = -10915.0145552058
Iteration 800: Loss = -10914.942012530248
Iteration 900: Loss = -10914.893977118263
Iteration 1000: Loss = -10914.846283096082
Iteration 1100: Loss = -10914.78089090603
Iteration 1200: Loss = -10914.637153147129
Iteration 1300: Loss = -10914.07863482726
Iteration 1400: Loss = -10912.441578974807
Iteration 1500: Loss = -10911.270543722954
Iteration 1600: Loss = -10910.849209547241
Iteration 1700: Loss = -10910.625708785925
Iteration 1800: Loss = -10910.49742577785
Iteration 1900: Loss = -10910.404085105009
Iteration 2000: Loss = -10910.326108622248
Iteration 2100: Loss = -10910.251924821912
Iteration 2200: Loss = -10910.170326347272
Iteration 2300: Loss = -10910.07024070422
Iteration 2400: Loss = -10909.956594838768
Iteration 2500: Loss = -10909.858765378967
Iteration 2600: Loss = -10909.790570617684
Iteration 2700: Loss = -10909.744492736887
Iteration 2800: Loss = -10909.711966202733
Iteration 2900: Loss = -10909.688050449191
Iteration 3000: Loss = -10909.66973170153
Iteration 3100: Loss = -10909.655288346878
Iteration 3200: Loss = -10909.643598399107
Iteration 3300: Loss = -10909.634056543537
Iteration 3400: Loss = -10909.626045302659
Iteration 3500: Loss = -10909.619267799613
Iteration 3600: Loss = -10909.613451092839
Iteration 3700: Loss = -10909.608463528191
Iteration 3800: Loss = -10909.604143023189
Iteration 3900: Loss = -10909.600289222908
Iteration 4000: Loss = -10909.596891105275
Iteration 4100: Loss = -10909.59390697299
Iteration 4200: Loss = -10909.59118733944
Iteration 4300: Loss = -10909.588803857096
Iteration 4400: Loss = -10909.58660390395
Iteration 4500: Loss = -10909.58461146203
Iteration 4600: Loss = -10909.582865025124
Iteration 4700: Loss = -10909.581189561106
Iteration 4800: Loss = -10909.579712940576
Iteration 4900: Loss = -10909.578336577148
Iteration 5000: Loss = -10909.577090461267
Iteration 5100: Loss = -10909.575948446436
Iteration 5200: Loss = -10909.574884944112
Iteration 5300: Loss = -10909.57386962612
Iteration 5400: Loss = -10909.572990875244
Iteration 5500: Loss = -10909.57213103552
Iteration 5600: Loss = -10909.571313492037
Iteration 5700: Loss = -10909.570595005072
Iteration 5800: Loss = -10909.569925944097
Iteration 5900: Loss = -10909.569301761057
Iteration 6000: Loss = -10909.568647061162
Iteration 6100: Loss = -10909.568063953264
Iteration 6200: Loss = -10909.56750275156
Iteration 6300: Loss = -10909.567076544423
Iteration 6400: Loss = -10909.566570177014
Iteration 6500: Loss = -10909.566131822487
Iteration 6600: Loss = -10909.565702833488
Iteration 6700: Loss = -10909.565324512032
Iteration 6800: Loss = -10909.564928377344
Iteration 6900: Loss = -10909.564572857244
Iteration 7000: Loss = -10909.564224702995
Iteration 7100: Loss = -10909.563909218456
Iteration 7200: Loss = -10909.563635180175
Iteration 7300: Loss = -10909.563333519463
Iteration 7400: Loss = -10909.56314045468
Iteration 7500: Loss = -10909.56280257151
Iteration 7600: Loss = -10909.56254712282
Iteration 7700: Loss = -10909.562288045394
Iteration 7800: Loss = -10909.562828827526
1
Iteration 7900: Loss = -10909.561838667703
Iteration 8000: Loss = -10909.561657381573
Iteration 8100: Loss = -10909.561452029013
Iteration 8200: Loss = -10909.687938842413
1
Iteration 8300: Loss = -10909.561055239485
Iteration 8400: Loss = -10909.560907255816
Iteration 8500: Loss = -10909.560705796417
Iteration 8600: Loss = -10909.57109382247
1
Iteration 8700: Loss = -10909.560410645927
Iteration 8800: Loss = -10909.56026573489
Iteration 8900: Loss = -10909.560123134868
Iteration 9000: Loss = -10909.637972290953
1
Iteration 9100: Loss = -10909.559928241966
Iteration 9200: Loss = -10909.559809870525
Iteration 9300: Loss = -10909.559669752058
Iteration 9400: Loss = -10910.346203529376
1
Iteration 9500: Loss = -10909.559508143831
Iteration 9600: Loss = -10909.559441180696
Iteration 9700: Loss = -10909.559327408393
Iteration 9800: Loss = -10909.559263332456
Iteration 9900: Loss = -10909.559711444112
1
Iteration 10000: Loss = -10909.559094278748
Iteration 10100: Loss = -10909.559224380802
1
Iteration 10200: Loss = -10909.5589224528
Iteration 10300: Loss = -10909.55887936875
Iteration 10400: Loss = -10909.558868045018
Iteration 10500: Loss = -10909.558777428694
Iteration 10600: Loss = -10909.558878612745
1
Iteration 10700: Loss = -10909.558634065543
Iteration 10800: Loss = -10909.558649682866
1
Iteration 10900: Loss = -10909.558598035057
Iteration 11000: Loss = -10909.560672192934
1
Iteration 11100: Loss = -10909.558515604022
Iteration 11200: Loss = -10909.558525032211
1
Iteration 11300: Loss = -10909.567397616476
2
Iteration 11400: Loss = -10909.55844487439
Iteration 11500: Loss = -10909.558435230798
Iteration 11600: Loss = -10909.636425302737
1
Iteration 11700: Loss = -10909.558468537667
2
Iteration 11800: Loss = -10909.558363538341
Iteration 11900: Loss = -10909.55836858814
1
Iteration 12000: Loss = -10909.569675958526
2
Iteration 12100: Loss = -10909.558345019557
Iteration 12200: Loss = -10909.558326339615
Iteration 12300: Loss = -10909.600421776988
1
Iteration 12400: Loss = -10909.558326309805
Iteration 12500: Loss = -10909.55830283188
Iteration 12600: Loss = -10909.558256333676
Iteration 12700: Loss = -10909.558251541233
Iteration 12800: Loss = -10909.558221393636
Iteration 12900: Loss = -10909.558549175934
1
Iteration 13000: Loss = -10909.558209632924
Iteration 13100: Loss = -10909.558214557834
1
Iteration 13200: Loss = -10909.5591566089
2
Iteration 13300: Loss = -10909.558179232303
Iteration 13400: Loss = -10909.558230567649
1
Iteration 13500: Loss = -10909.558209694507
2
Iteration 13600: Loss = -10909.558110230959
Iteration 13700: Loss = -10909.561204152022
1
Iteration 13800: Loss = -10909.558085525141
Iteration 13900: Loss = -10909.558167086101
1
Iteration 14000: Loss = -10909.558083879872
Iteration 14100: Loss = -10909.558320118455
1
Iteration 14200: Loss = -10909.558050334977
Iteration 14300: Loss = -10909.558053565423
1
Iteration 14400: Loss = -10909.558521175071
2
Iteration 14500: Loss = -10909.558014189437
Iteration 14600: Loss = -10909.57818306114
1
Iteration 14700: Loss = -10909.558015903805
2
Iteration 14800: Loss = -10909.558305742636
3
Iteration 14900: Loss = -10909.558293945453
4
Iteration 15000: Loss = -10909.558152981419
5
Stopping early at iteration 15000 due to no improvement.
pi: tensor([[1.0000e+00, 2.8655e-08],
        [6.9329e-06, 9.9999e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9899, 0.0101], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1603, 0.2723],
         [0.7289, 0.1361]],

        [[0.6788, 0.1618],
         [0.5769, 0.6732]],

        [[0.5996, 0.1714],
         [0.6734, 0.5370]],

        [[0.6613, 0.0612],
         [0.5983, 0.6268]],

        [[0.5265, 0.2715],
         [0.5563, 0.5389]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0044444444444444444
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0025538476364862913
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
Global Adjusted Rand Index: 0.0009711150057751206
Average Adjusted Rand Index: 0.0008489082118456738
10903.596473201182
[0.0009711150057751206, 0.0005780186400410129, 0.0005780186400410129, 0.0009711150057751206] [0.0008489082118456738, 0.0011969665072433413, 0.0011969665072433413, 0.0008489082118456738] [10909.55809777615, 10911.15414742967, 10911.147498530248, 10909.558152981419]
-----------------------------------------------------------------------------------------
This iteration is 2
True Objective function: Loss = -10790.489215516836
Iteration 0: Loss = -10925.743007733503
Iteration 10: Loss = -10920.075990727264
Iteration 20: Loss = -10919.248525277924
Iteration 30: Loss = -10918.766860464788
Iteration 40: Loss = -10917.883261614807
Iteration 50: Loss = -10917.766391641566
Iteration 60: Loss = -10917.750595756173
Iteration 70: Loss = -10917.747819416316
Iteration 80: Loss = -10917.747284126413
Iteration 90: Loss = -10917.747129601077
Iteration 100: Loss = -10917.747150237337
1
Iteration 110: Loss = -10917.747106310324
Iteration 120: Loss = -10917.74711779859
1
Iteration 130: Loss = -10917.747127872995
2
Iteration 140: Loss = -10917.747102109084
Iteration 150: Loss = -10917.747080342016
Iteration 160: Loss = -10917.747078886745
Iteration 170: Loss = -10917.74712292638
1
Iteration 180: Loss = -10917.747099050119
2
Iteration 190: Loss = -10917.74709878866
3
Stopping early at iteration 190 due to no improvement.
pi: tensor([[0.0481, 0.9519],
        [0.0711, 0.9289]], dtype=torch.float64)
alpha: tensor([0.0695, 0.9305])
beta: tensor([[[0.2257, 0.1888],
         [0.8458, 0.1527]],

        [[0.2967, 0.2088],
         [0.5856, 0.7014]],

        [[0.9392, 0.1760],
         [0.4717, 0.8306]],

        [[0.0047, 0.1796],
         [0.6721, 0.3312]],

        [[0.4035, 0.2557],
         [0.8604, 0.8334]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 38
Adjusted Rand Index: 0.01126387000386094
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.021933873838361234
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 36
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.017313838488058987
Global Adjusted Rand Index: 0.012339780777788394
Average Adjusted Rand Index: 0.0031767810708326375
Iteration 0: Loss = -10923.430209059847
Iteration 10: Loss = -10923.430234402475
1
Iteration 20: Loss = -10923.429860398757
Iteration 30: Loss = -10923.28899406889
Iteration 40: Loss = -10919.363651545382
Iteration 50: Loss = -10919.0520886055
Iteration 60: Loss = -10918.311973355008
Iteration 70: Loss = -10918.004776712272
Iteration 80: Loss = -10917.857782849442
Iteration 90: Loss = -10917.749256965191
Iteration 100: Loss = -10917.74730926806
Iteration 110: Loss = -10917.747196815602
Iteration 120: Loss = -10917.747124239635
Iteration 130: Loss = -10917.747090628112
Iteration 140: Loss = -10917.74710402967
1
Iteration 150: Loss = -10917.747124521735
2
Iteration 160: Loss = -10917.747123744832
3
Stopping early at iteration 160 due to no improvement.
pi: tensor([[0.9289, 0.0711],
        [0.9519, 0.0481]], dtype=torch.float64)
alpha: tensor([0.9305, 0.0695])
beta: tensor([[[0.1527, 0.1888],
         [0.6373, 0.2257]],

        [[0.4126, 0.2088],
         [0.7746, 0.7933]],

        [[0.8453, 0.1760],
         [0.4106, 0.6561]],

        [[0.4864, 0.1796],
         [0.6855, 0.5685]],

        [[0.6878, 0.2557],
         [0.3251, 0.0114]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 62
Adjusted Rand Index: 0.01126387000386094
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.021933873838361234
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 64
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.017313838488058987
Global Adjusted Rand Index: 0.012339780777788394
Average Adjusted Rand Index: 0.0031767810708326375
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24027.849125516823
Iteration 100: Loss = -10923.250699481492
Iteration 200: Loss = -10921.593892500265
Iteration 300: Loss = -10920.878013679188
Iteration 400: Loss = -10919.851331994949
Iteration 500: Loss = -10919.087538565418
Iteration 600: Loss = -10918.410045353425
Iteration 700: Loss = -10917.973616223495
Iteration 800: Loss = -10917.713310308774
Iteration 900: Loss = -10917.547001423933
Iteration 1000: Loss = -10917.440516242676
Iteration 1100: Loss = -10917.372190895265
Iteration 1200: Loss = -10917.330546067247
Iteration 1300: Loss = -10917.308348749037
Iteration 1400: Loss = -10917.29398621544
Iteration 1500: Loss = -10917.283455588326
Iteration 1600: Loss = -10917.274981214552
Iteration 1700: Loss = -10917.267788836905
Iteration 1800: Loss = -10917.261482412236
Iteration 1900: Loss = -10917.25598598491
Iteration 2000: Loss = -10917.251130915842
Iteration 2100: Loss = -10917.24679322763
Iteration 2200: Loss = -10917.242863515774
Iteration 2300: Loss = -10917.239451979889
Iteration 2400: Loss = -10917.236369394015
Iteration 2500: Loss = -10917.23360186569
Iteration 2600: Loss = -10917.231120891971
Iteration 2700: Loss = -10917.228866198864
Iteration 2800: Loss = -10917.226875091515
Iteration 2900: Loss = -10917.225102652543
Iteration 3000: Loss = -10917.223472301366
Iteration 3100: Loss = -10917.222005686765
Iteration 3200: Loss = -10917.220703297311
Iteration 3300: Loss = -10917.219468717154
Iteration 3400: Loss = -10917.218347179576
Iteration 3500: Loss = -10917.217366417273
Iteration 3600: Loss = -10917.216433426433
Iteration 3700: Loss = -10917.215618643748
Iteration 3800: Loss = -10917.214816648755
Iteration 3900: Loss = -10917.214160593605
Iteration 4000: Loss = -10917.213432135197
Iteration 4100: Loss = -10917.212828597005
Iteration 4200: Loss = -10917.212262519597
Iteration 4300: Loss = -10917.21173747137
Iteration 4400: Loss = -10917.214293148036
1
Iteration 4500: Loss = -10917.210737966887
Iteration 4600: Loss = -10917.210304219316
Iteration 4700: Loss = -10917.209980142894
Iteration 4800: Loss = -10917.209550862926
Iteration 4900: Loss = -10917.209192815326
Iteration 5000: Loss = -10917.208954160573
Iteration 5100: Loss = -10917.208533374682
Iteration 5200: Loss = -10917.21368576568
1
Iteration 5300: Loss = -10917.207983577919
Iteration 5400: Loss = -10917.207676526446
Iteration 5500: Loss = -10917.20744232348
Iteration 5600: Loss = -10917.20723740612
Iteration 5700: Loss = -10917.207019476858
Iteration 5800: Loss = -10917.20681561005
Iteration 5900: Loss = -10917.21354276732
1
Iteration 6000: Loss = -10917.206421801082
Iteration 6100: Loss = -10917.206238070814
Iteration 6200: Loss = -10917.206217568646
Iteration 6300: Loss = -10917.205910992554
Iteration 6400: Loss = -10917.205792353865
Iteration 6500: Loss = -10917.20564432461
Iteration 6600: Loss = -10917.205500748178
Iteration 6700: Loss = -10917.20539670941
Iteration 6800: Loss = -10917.20529800668
Iteration 6900: Loss = -10917.20519240189
Iteration 7000: Loss = -10917.205077979426
Iteration 7100: Loss = -10917.205419459075
1
Iteration 7200: Loss = -10917.204866430046
Iteration 7300: Loss = -10917.204835555718
Iteration 7400: Loss = -10917.204692286812
Iteration 7500: Loss = -10917.204642542429
Iteration 7600: Loss = -10917.204520962257
Iteration 7700: Loss = -10917.204487886438
Iteration 7800: Loss = -10917.204388832835
Iteration 7900: Loss = -10917.204291460897
Iteration 8000: Loss = -10917.204281849117
Iteration 8100: Loss = -10917.204188976444
Iteration 8200: Loss = -10917.20414947593
Iteration 8300: Loss = -10917.20446958523
1
Iteration 8400: Loss = -10917.204069914931
Iteration 8500: Loss = -10917.204009477935
Iteration 8600: Loss = -10917.308479502282
1
Iteration 8700: Loss = -10917.203912122946
Iteration 8800: Loss = -10917.248798886922
1
Iteration 8900: Loss = -10917.2038284284
Iteration 9000: Loss = -10917.475440878263
1
Iteration 9100: Loss = -10917.203771997996
Iteration 9200: Loss = -10917.203743220598
Iteration 9300: Loss = -10917.205519760255
1
Iteration 9400: Loss = -10917.203672781587
Iteration 9500: Loss = -10917.203632294468
Iteration 9600: Loss = -10917.203802671665
1
Iteration 9700: Loss = -10917.20360900547
Iteration 9800: Loss = -10917.203564869029
Iteration 9900: Loss = -10917.203642425613
1
Iteration 10000: Loss = -10917.203508374423
Iteration 10100: Loss = -10917.20401321862
1
Iteration 10200: Loss = -10917.203577207934
2
Iteration 10300: Loss = -10917.203457439993
Iteration 10400: Loss = -10917.26213277172
1
Iteration 10500: Loss = -10917.20339726248
Iteration 10600: Loss = -10917.582738314137
1
Iteration 10700: Loss = -10917.203392668918
Iteration 10800: Loss = -10917.203601357893
1
Iteration 10900: Loss = -10917.203417974535
2
Iteration 11000: Loss = -10917.203536775367
3
Iteration 11100: Loss = -10917.210207949665
4
Iteration 11200: Loss = -10917.248289180727
5
Stopping early at iteration 11200 due to no improvement.
pi: tensor([[9.2108e-01, 7.8918e-02],
        [9.9995e-01, 5.0211e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9761, 0.0239], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1533, 0.2361],
         [0.5390, 0.2493]],

        [[0.5789, 0.2082],
         [0.7275, 0.5627]],

        [[0.5242, 0.1822],
         [0.6799, 0.5933]],

        [[0.6406, 0.1843],
         [0.6414, 0.5392]],

        [[0.5442, 0.2490],
         [0.5596, 0.5695]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 62
Adjusted Rand Index: 0.01126387000386094
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.021933873838361234
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 64
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.01737152632365585
Global Adjusted Rand Index: 0.013748610421887675
Average Adjusted Rand Index: 0.003165243503713265
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21293.51586145268
Iteration 100: Loss = -10923.922435925197
Iteration 200: Loss = -10922.326103600331
Iteration 300: Loss = -10920.70700936523
Iteration 400: Loss = -10919.857861959568
Iteration 500: Loss = -10919.268632458772
Iteration 600: Loss = -10918.770278178274
Iteration 700: Loss = -10918.36820473766
Iteration 800: Loss = -10917.932123644243
Iteration 900: Loss = -10917.470660638763
Iteration 1000: Loss = -10917.371357435193
Iteration 1100: Loss = -10917.325786028772
Iteration 1200: Loss = -10917.301622607696
Iteration 1300: Loss = -10917.286980341003
Iteration 1400: Loss = -10917.276672061258
Iteration 1500: Loss = -10917.268289694019
Iteration 1600: Loss = -10917.261023618143
Iteration 1700: Loss = -10917.254586250594
Iteration 1800: Loss = -10917.249041889447
Iteration 1900: Loss = -10917.244110954334
Iteration 2000: Loss = -10917.23985798492
Iteration 2100: Loss = -10917.236129905412
Iteration 2200: Loss = -10917.232967572685
Iteration 2300: Loss = -10917.230249719823
Iteration 2400: Loss = -10917.227886400195
Iteration 2500: Loss = -10917.225834781066
Iteration 2600: Loss = -10917.224054852708
Iteration 2700: Loss = -10917.222462662188
Iteration 2800: Loss = -10917.221059909532
Iteration 2900: Loss = -10917.219851812353
Iteration 3000: Loss = -10917.218748716368
Iteration 3100: Loss = -10917.217778712067
Iteration 3200: Loss = -10917.216831769392
Iteration 3300: Loss = -10917.21595540617
Iteration 3400: Loss = -10917.215223960915
Iteration 3500: Loss = -10917.2144879569
Iteration 3600: Loss = -10917.21384030499
Iteration 3700: Loss = -10917.213208880916
Iteration 3800: Loss = -10917.212628131112
Iteration 3900: Loss = -10917.212174493272
Iteration 4000: Loss = -10917.2116023929
Iteration 4100: Loss = -10917.211169523081
Iteration 4200: Loss = -10917.210681845616
Iteration 4300: Loss = -10917.210297883716
Iteration 4400: Loss = -10917.210020175047
Iteration 4500: Loss = -10917.209504609378
Iteration 4600: Loss = -10917.209212644417
Iteration 4700: Loss = -10917.208962925888
Iteration 4800: Loss = -10917.208575473487
Iteration 4900: Loss = -10917.208968938701
1
Iteration 5000: Loss = -10917.20802845654
Iteration 5100: Loss = -10917.207765709638
Iteration 5200: Loss = -10917.20753148688
Iteration 5300: Loss = -10917.207315664802
Iteration 5400: Loss = -10917.207704277604
1
Iteration 5500: Loss = -10917.20689439633
Iteration 5600: Loss = -10917.206693470755
Iteration 5700: Loss = -10917.206494640279
Iteration 5800: Loss = -10917.206342583144
Iteration 5900: Loss = -10917.20627057483
Iteration 6000: Loss = -10917.206033530343
Iteration 6100: Loss = -10917.216833570727
1
Iteration 6200: Loss = -10917.205740224603
Iteration 6300: Loss = -10917.205641638524
Iteration 6400: Loss = -10917.205495143267
Iteration 6500: Loss = -10917.205337002795
Iteration 6600: Loss = -10917.205862699893
1
Iteration 6700: Loss = -10917.205136493672
Iteration 6800: Loss = -10917.2050290396
Iteration 6900: Loss = -10917.205010757625
Iteration 7000: Loss = -10917.204863878549
Iteration 7100: Loss = -10917.204998515374
1
Iteration 7200: Loss = -10917.204652211352
Iteration 7300: Loss = -10917.20711727555
1
Iteration 7400: Loss = -10917.204508182514
Iteration 7500: Loss = -10917.204486879098
Iteration 7600: Loss = -10917.204401576115
Iteration 7700: Loss = -10917.204361208944
Iteration 7800: Loss = -10917.20425103428
Iteration 7900: Loss = -10917.242910357414
1
Iteration 8000: Loss = -10917.204181364925
Iteration 8100: Loss = -10917.225344827773
1
Iteration 8200: Loss = -10917.204055371954
Iteration 8300: Loss = -10917.211231031335
1
Iteration 8400: Loss = -10917.203920318943
Iteration 8500: Loss = -10917.35222881338
1
Iteration 8600: Loss = -10917.20388880898
Iteration 8700: Loss = -10917.203826700466
Iteration 8800: Loss = -10917.203870483676
1
Iteration 8900: Loss = -10917.20378376741
Iteration 9000: Loss = -10917.39965577593
1
Iteration 9100: Loss = -10917.203722767903
Iteration 9200: Loss = -10917.203669501157
Iteration 9300: Loss = -10917.212362162325
1
Iteration 9400: Loss = -10917.203607410342
Iteration 9500: Loss = -10917.203566021697
Iteration 9600: Loss = -10917.204143629338
1
Iteration 9700: Loss = -10917.203536862035
Iteration 9800: Loss = -10917.230166735206
1
Iteration 9900: Loss = -10917.203512305667
Iteration 10000: Loss = -10917.203503757075
Iteration 10100: Loss = -10917.70987634067
1
Iteration 10200: Loss = -10917.203423674819
Iteration 10300: Loss = -10917.203400329776
Iteration 10400: Loss = -10917.32848971122
1
Iteration 10500: Loss = -10917.203434563262
2
Iteration 10600: Loss = -10917.203378928183
Iteration 10700: Loss = -10917.203349862988
Iteration 10800: Loss = -10917.203623225745
1
Iteration 10900: Loss = -10917.203327666859
Iteration 11000: Loss = -10917.249661651393
1
Iteration 11100: Loss = -10917.203746096111
2
Iteration 11200: Loss = -10917.203326545856
Iteration 11300: Loss = -10917.206230291204
1
Iteration 11400: Loss = -10917.204188109406
2
Iteration 11500: Loss = -10917.203280036825
Iteration 11600: Loss = -10917.20503683939
1
Iteration 11700: Loss = -10917.205111732328
2
Iteration 11800: Loss = -10917.204549245698
3
Iteration 11900: Loss = -10917.244054732375
4
Iteration 12000: Loss = -10917.203246498058
Iteration 12100: Loss = -10917.20983803952
1
Iteration 12200: Loss = -10917.208873165504
2
Iteration 12300: Loss = -10917.203295269017
3
Iteration 12400: Loss = -10917.203209089908
Iteration 12500: Loss = -10917.204159734789
1
Iteration 12600: Loss = -10917.203182859706
Iteration 12700: Loss = -10917.20629778896
1
Iteration 12800: Loss = -10917.203184007116
2
Iteration 12900: Loss = -10917.25103970682
3
Iteration 13000: Loss = -10917.20318216831
Iteration 13100: Loss = -10917.203175937344
Iteration 13200: Loss = -10917.203821874102
1
Iteration 13300: Loss = -10917.203267377867
2
Iteration 13400: Loss = -10917.206652265058
3
Iteration 13500: Loss = -10917.203180951123
4
Iteration 13600: Loss = -10917.267859957805
5
Stopping early at iteration 13600 due to no improvement.
pi: tensor([[9.2064e-01, 7.9357e-02],
        [9.9999e-01, 1.2678e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9760, 0.0240], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1532, 0.2351],
         [0.6348, 0.2496]],

        [[0.7057, 0.2079],
         [0.6873, 0.7101]],

        [[0.6976, 0.1821],
         [0.6538, 0.6143]],

        [[0.6952, 0.1843],
         [0.5676, 0.6502]],

        [[0.6294, 0.2489],
         [0.7242, 0.7222]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 62
Adjusted Rand Index: 0.01126387000386094
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.021933873838361234
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 64
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 4
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.01737152632365585
Global Adjusted Rand Index: 0.013748610421887675
Average Adjusted Rand Index: 0.003165243503713265
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19855.964421072127
Iteration 100: Loss = -10923.704262428662
Iteration 200: Loss = -10922.97306815915
Iteration 300: Loss = -10919.972630235117
Iteration 400: Loss = -10918.619380002385
Iteration 500: Loss = -10918.259664858384
Iteration 600: Loss = -10918.095001220407
Iteration 700: Loss = -10917.953903151056
Iteration 800: Loss = -10917.694770550417
Iteration 900: Loss = -10917.518054111863
Iteration 1000: Loss = -10917.457590221855
Iteration 1100: Loss = -10917.41578430389
Iteration 1200: Loss = -10917.38463403565
Iteration 1300: Loss = -10917.360421786057
Iteration 1400: Loss = -10917.340826166384
Iteration 1500: Loss = -10917.324701252552
Iteration 1600: Loss = -10917.311020442254
Iteration 1700: Loss = -10917.299308542657
Iteration 1800: Loss = -10917.289035734751
Iteration 1900: Loss = -10917.279900113417
Iteration 2000: Loss = -10917.27171170458
Iteration 2100: Loss = -10917.264337940102
Iteration 2200: Loss = -10917.257714161873
Iteration 2300: Loss = -10917.251695221636
Iteration 2400: Loss = -10917.246340569067
Iteration 2500: Loss = -10917.24156880836
Iteration 2600: Loss = -10917.237338136465
Iteration 2700: Loss = -10917.233642970512
Iteration 2800: Loss = -10917.230421935876
Iteration 2900: Loss = -10917.227632926235
Iteration 3000: Loss = -10917.225205747978
Iteration 3100: Loss = -10917.223127866911
Iteration 3200: Loss = -10917.221388912822
Iteration 3300: Loss = -10917.21982860784
Iteration 3400: Loss = -10917.21845760216
Iteration 3500: Loss = -10917.21736496094
Iteration 3600: Loss = -10917.216227005889
Iteration 3700: Loss = -10917.21534335748
Iteration 3800: Loss = -10917.214506009897
Iteration 3900: Loss = -10917.213708710637
Iteration 4000: Loss = -10917.213060732824
Iteration 4100: Loss = -10917.212454767028
Iteration 4200: Loss = -10917.211906001905
Iteration 4300: Loss = -10917.211587397369
Iteration 4400: Loss = -10917.210896300925
Iteration 4500: Loss = -10917.210680573487
Iteration 4600: Loss = -10917.210022214142
Iteration 4700: Loss = -10917.20966133361
Iteration 4800: Loss = -10917.20939395668
Iteration 4900: Loss = -10917.20897883599
Iteration 5000: Loss = -10917.210084405688
1
Iteration 5100: Loss = -10917.208344565464
Iteration 5200: Loss = -10917.208043667948
Iteration 5300: Loss = -10917.207829437337
Iteration 5400: Loss = -10917.207527387494
Iteration 5500: Loss = -10917.214461549283
1
Iteration 5600: Loss = -10917.207069769735
Iteration 5700: Loss = -10917.206899591003
Iteration 5800: Loss = -10917.206679919578
Iteration 5900: Loss = -10917.20653873189
Iteration 6000: Loss = -10917.206726611243
1
Iteration 6100: Loss = -10917.20617508605
Iteration 6200: Loss = -10917.20600935113
Iteration 6300: Loss = -10917.20587741011
Iteration 6400: Loss = -10917.205724829986
Iteration 6500: Loss = -10917.205877551765
1
Iteration 6600: Loss = -10917.20547820362
Iteration 6700: Loss = -10917.219934062805
1
Iteration 6800: Loss = -10917.205265856173
Iteration 6900: Loss = -10917.20515388
Iteration 7000: Loss = -10917.20502327232
Iteration 7100: Loss = -10917.204950189407
Iteration 7200: Loss = -10917.205507838895
1
Iteration 7300: Loss = -10917.204759915096
Iteration 7400: Loss = -10917.206997361603
1
Iteration 7500: Loss = -10917.204614488568
Iteration 7600: Loss = -10917.20484313
1
Iteration 7700: Loss = -10917.204412377163
Iteration 7800: Loss = -10917.208908354205
1
Iteration 7900: Loss = -10917.204306293574
Iteration 8000: Loss = -10917.205821340101
1
Iteration 8100: Loss = -10917.204205163733
Iteration 8200: Loss = -10917.3275835579
1
Iteration 8300: Loss = -10917.204088155488
Iteration 8400: Loss = -10917.20401835691
Iteration 8500: Loss = -10917.218702371389
1
Iteration 8600: Loss = -10917.203941631367
Iteration 8700: Loss = -10917.203897680174
Iteration 8800: Loss = -10917.22173906423
1
Iteration 8900: Loss = -10917.203852779981
Iteration 9000: Loss = -10917.203778582172
Iteration 9100: Loss = -10917.32048620331
1
Iteration 9200: Loss = -10917.203706507102
Iteration 9300: Loss = -10917.203695207658
Iteration 9400: Loss = -10917.81441096397
1
Iteration 9500: Loss = -10917.203629809108
Iteration 9600: Loss = -10917.203611367786
Iteration 9700: Loss = -10917.20355804431
Iteration 9800: Loss = -10917.203680545897
1
Iteration 9900: Loss = -10917.203535713274
Iteration 10000: Loss = -10917.20539546844
1
Iteration 10100: Loss = -10917.203508306524
Iteration 10200: Loss = -10917.203479215894
Iteration 10300: Loss = -10917.246633180805
1
Iteration 10400: Loss = -10917.203420519194
Iteration 10500: Loss = -10917.21186818729
1
Iteration 10600: Loss = -10917.204091449368
2
Iteration 10700: Loss = -10917.203367956683
Iteration 10800: Loss = -10917.20342238634
1
Iteration 10900: Loss = -10917.203360208341
Iteration 11000: Loss = -10917.205166965874
1
Iteration 11100: Loss = -10917.203332592857
Iteration 11200: Loss = -10917.20369069387
1
Iteration 11300: Loss = -10917.20329460807
Iteration 11400: Loss = -10917.203933455414
1
Iteration 11500: Loss = -10917.204388366492
2
Iteration 11600: Loss = -10917.220390559607
3
Iteration 11700: Loss = -10917.205424222871
4
Iteration 11800: Loss = -10917.211974863621
5
Stopping early at iteration 11800 due to no improvement.
pi: tensor([[3.4398e-05, 9.9997e-01],
        [7.7766e-02, 9.2223e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0237, 0.9763], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2495, 0.2361],
         [0.7247, 0.1541]],

        [[0.7243, 0.2095],
         [0.6350, 0.5196]],

        [[0.5245, 0.1819],
         [0.6022, 0.7265]],

        [[0.7280, 0.1845],
         [0.5507, 0.5306]],

        [[0.6775, 0.2502],
         [0.7004, 0.5736]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 38
Adjusted Rand Index: 0.01126387000386094
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.021933873838361234
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 36
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.01737152632365585
Global Adjusted Rand Index: 0.013748610421887675
Average Adjusted Rand Index: 0.003165243503713265
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22778.357955846284
Iteration 100: Loss = -10922.862772911207
Iteration 200: Loss = -10920.714835615076
Iteration 300: Loss = -10920.087124206491
Iteration 400: Loss = -10919.606198816946
Iteration 500: Loss = -10919.104656786738
Iteration 600: Loss = -10918.446207510411
Iteration 700: Loss = -10917.745152402722
Iteration 800: Loss = -10917.302227862516
Iteration 900: Loss = -10917.046926810015
Iteration 1000: Loss = -10916.894962816768
Iteration 1100: Loss = -10916.801745566218
Iteration 1200: Loss = -10916.740909675758
Iteration 1300: Loss = -10916.698830493917
Iteration 1400: Loss = -10916.668210606942
Iteration 1500: Loss = -10916.64513355321
Iteration 1600: Loss = -10916.62707420611
Iteration 1700: Loss = -10916.612523311353
Iteration 1800: Loss = -10916.600465233007
Iteration 1900: Loss = -10916.590375453472
Iteration 2000: Loss = -10916.58172229955
Iteration 2100: Loss = -10916.574272800846
Iteration 2200: Loss = -10916.567797726098
Iteration 2300: Loss = -10916.562132511288
Iteration 2400: Loss = -10916.557119205469
Iteration 2500: Loss = -10916.552750958534
Iteration 2600: Loss = -10916.548814282283
Iteration 2700: Loss = -10916.545320546807
Iteration 2800: Loss = -10916.542221229936
Iteration 2900: Loss = -10916.539477240449
Iteration 3000: Loss = -10916.536982047342
Iteration 3100: Loss = -10916.534741463214
Iteration 3200: Loss = -10916.53277864496
Iteration 3300: Loss = -10916.530966425193
Iteration 3400: Loss = -10916.529329351437
Iteration 3500: Loss = -10916.52786702008
Iteration 3600: Loss = -10916.526523659943
Iteration 3700: Loss = -10916.525305504401
Iteration 3800: Loss = -10916.52417448293
Iteration 3900: Loss = -10916.523181205373
Iteration 4000: Loss = -10916.522258991994
Iteration 4100: Loss = -10916.52142873442
Iteration 4200: Loss = -10916.520623238015
Iteration 4300: Loss = -10916.519931934932
Iteration 4400: Loss = -10916.519295219066
Iteration 4500: Loss = -10916.518693193233
Iteration 4600: Loss = -10916.518120455721
Iteration 4700: Loss = -10916.517631680927
Iteration 4800: Loss = -10916.51713452511
Iteration 4900: Loss = -10916.51670077884
Iteration 5000: Loss = -10916.5162904214
Iteration 5100: Loss = -10916.515896587953
Iteration 5200: Loss = -10916.5155520825
Iteration 5300: Loss = -10916.515225842031
Iteration 5400: Loss = -10916.51489908723
Iteration 5500: Loss = -10916.51457114191
Iteration 5600: Loss = -10916.51433277414
Iteration 5700: Loss = -10916.514050330117
Iteration 5800: Loss = -10916.51381721988
Iteration 5900: Loss = -10916.513542597326
Iteration 6000: Loss = -10916.513334850282
Iteration 6100: Loss = -10916.513150620493
Iteration 6200: Loss = -10916.512950267615
Iteration 6300: Loss = -10916.512753591946
Iteration 6400: Loss = -10916.512569177059
Iteration 6500: Loss = -10916.512385156298
Iteration 6600: Loss = -10916.512240853775
Iteration 6700: Loss = -10916.512053667455
Iteration 6800: Loss = -10916.511941089351
Iteration 6900: Loss = -10916.511801331455
Iteration 7000: Loss = -10916.511696033738
Iteration 7100: Loss = -10916.51156932559
Iteration 7200: Loss = -10916.511413759925
Iteration 7300: Loss = -10916.511330369256
Iteration 7400: Loss = -10916.511223627795
Iteration 7500: Loss = -10916.51110639001
Iteration 7600: Loss = -10916.511038299766
Iteration 7700: Loss = -10916.510918631559
Iteration 7800: Loss = -10916.510867821255
Iteration 7900: Loss = -10916.510767995234
Iteration 8000: Loss = -10916.51072122737
Iteration 8100: Loss = -10916.51062755739
Iteration 8200: Loss = -10916.510841795753
1
Iteration 8300: Loss = -10916.533559962869
2
Iteration 8400: Loss = -10916.535119008366
3
Iteration 8500: Loss = -10916.510505953353
Iteration 8600: Loss = -10916.510343324204
Iteration 8700: Loss = -10916.531471374461
1
Iteration 8800: Loss = -10916.510203763457
Iteration 8900: Loss = -10916.528419819291
1
Iteration 9000: Loss = -10916.510400124831
2
Iteration 9100: Loss = -10916.6337063318
3
Iteration 9200: Loss = -10916.510060161658
Iteration 9300: Loss = -10916.510553268035
1
Iteration 9400: Loss = -10916.511373617222
2
Iteration 9500: Loss = -10916.509951602577
Iteration 9600: Loss = -10916.510159468264
1
Iteration 9700: Loss = -10916.509884280513
Iteration 9800: Loss = -10916.50982912618
Iteration 9900: Loss = -10916.509816184856
Iteration 10000: Loss = -10916.530420167928
1
Iteration 10100: Loss = -10916.509746466621
Iteration 10200: Loss = -10916.50976482567
1
Iteration 10300: Loss = -10916.510812922626
2
Iteration 10400: Loss = -10916.509717478331
Iteration 10500: Loss = -10916.509711579158
Iteration 10600: Loss = -10916.513599438545
1
Iteration 10700: Loss = -10916.50962030438
Iteration 10800: Loss = -10916.509618431148
Iteration 10900: Loss = -10916.527773668262
1
Iteration 11000: Loss = -10916.50958995827
Iteration 11100: Loss = -10916.509586777282
Iteration 11200: Loss = -10916.592891040726
1
Iteration 11300: Loss = -10916.509530506964
Iteration 11400: Loss = -10916.510513416255
1
Iteration 11500: Loss = -10916.509546568413
2
Iteration 11600: Loss = -10916.509631209237
3
Iteration 11700: Loss = -10916.509525112204
Iteration 11800: Loss = -10916.509527425931
1
Iteration 11900: Loss = -10916.588179479539
2
Iteration 12000: Loss = -10916.509507347491
Iteration 12100: Loss = -10916.6245671149
1
Iteration 12200: Loss = -10916.509441484422
Iteration 12300: Loss = -10916.512503142947
1
Iteration 12400: Loss = -10916.5094997365
2
Iteration 12500: Loss = -10916.509444711211
3
Iteration 12600: Loss = -10916.511050736543
4
Iteration 12700: Loss = -10916.509445863501
5
Stopping early at iteration 12700 due to no improvement.
pi: tensor([[9.9996e-01, 4.0504e-05],
        [8.2506e-03, 9.9175e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0105, 0.9895], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1530, 0.1696],
         [0.5660, 0.1588]],

        [[0.6759, 0.1243],
         [0.5557, 0.6917]],

        [[0.5488, 0.2291],
         [0.6429, 0.6588]],

        [[0.5019, 0.1461],
         [0.6839, 0.6325]],

        [[0.5668, 0.2731],
         [0.6288, 0.7098]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0007748402262652058
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 38
Adjusted Rand Index: -0.016594385273424505
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0])
Difference count: 53
Adjusted Rand Index: -0.014778186472389411
Global Adjusted Rand Index: 0.0009873644648894107
Average Adjusted Rand Index: -0.007412475596839036
10790.489215516836
[0.013748610421887675, 0.013748610421887675, 0.013748610421887675, 0.0009873644648894107] [0.003165243503713265, 0.003165243503713265, 0.003165243503713265, -0.007412475596839036] [10917.248289180727, 10917.267859957805, 10917.211974863621, 10916.509445863501]
-----------------------------------------------------------------------------------------
This iteration is 3
True Objective function: Loss = -10952.501582448027
Iteration 0: Loss = -11110.586350693799
Iteration 10: Loss = -11022.878686295713
Iteration 20: Loss = -11022.727369308212
Iteration 30: Loss = -11022.675724287732
Iteration 40: Loss = -11022.65071539518
Iteration 50: Loss = -11022.635381062484
Iteration 60: Loss = -11022.624162352853
Iteration 70: Loss = -11022.614908544569
Iteration 80: Loss = -11022.606925252225
Iteration 90: Loss = -11022.599755873165
Iteration 100: Loss = -11022.593426359723
Iteration 110: Loss = -11022.587668015916
Iteration 120: Loss = -11022.582570486373
Iteration 130: Loss = -11022.578017444215
Iteration 140: Loss = -11022.573924182729
Iteration 150: Loss = -11022.570182166019
Iteration 160: Loss = -11022.566879767723
Iteration 170: Loss = -11022.563901861844
Iteration 180: Loss = -11022.56117878687
Iteration 190: Loss = -11022.55872900065
Iteration 200: Loss = -11022.556507755633
Iteration 210: Loss = -11022.554438376208
Iteration 220: Loss = -11022.552583624107
Iteration 230: Loss = -11022.550868161927
Iteration 240: Loss = -11022.54930353399
Iteration 250: Loss = -11022.547839068557
Iteration 260: Loss = -11022.54651534927
Iteration 270: Loss = -11022.545257233205
Iteration 280: Loss = -11022.544138180201
Iteration 290: Loss = -11022.543040793904
Iteration 300: Loss = -11022.542080222054
Iteration 310: Loss = -11022.54113035848
Iteration 320: Loss = -11022.540262161821
Iteration 330: Loss = -11022.539474643465
Iteration 340: Loss = -11022.53870142218
Iteration 350: Loss = -11022.537982482621
Iteration 360: Loss = -11022.537338430759
Iteration 370: Loss = -11022.53670852624
Iteration 380: Loss = -11022.536123522183
Iteration 390: Loss = -11022.535568129522
Iteration 400: Loss = -11022.535056537394
Iteration 410: Loss = -11022.53454704441
Iteration 420: Loss = -11022.53411404853
Iteration 430: Loss = -11022.53363131072
Iteration 440: Loss = -11022.533207227541
Iteration 450: Loss = -11022.532787671422
Iteration 460: Loss = -11022.532420495481
Iteration 470: Loss = -11022.532107014045
Iteration 480: Loss = -11022.531776051672
Iteration 490: Loss = -11022.531477930566
Iteration 500: Loss = -11022.531105651085
Iteration 510: Loss = -11022.530824659942
Iteration 520: Loss = -11022.530506569059
Iteration 530: Loss = -11022.530245294161
Iteration 540: Loss = -11022.530019792432
Iteration 550: Loss = -11022.529794424507
Iteration 560: Loss = -11022.529556689035
Iteration 570: Loss = -11022.529280601724
Iteration 580: Loss = -11022.529114869985
Iteration 590: Loss = -11022.528890107531
Iteration 600: Loss = -11022.52867636279
Iteration 610: Loss = -11022.528538715113
Iteration 620: Loss = -11022.528364445556
Iteration 630: Loss = -11022.528153308498
Iteration 640: Loss = -11022.52805294571
Iteration 650: Loss = -11022.527837857831
Iteration 660: Loss = -11022.527698500906
Iteration 670: Loss = -11022.527572553365
Iteration 680: Loss = -11022.527475708379
Iteration 690: Loss = -11022.527312191973
Iteration 700: Loss = -11022.527225804213
Iteration 710: Loss = -11022.527059461641
Iteration 720: Loss = -11022.526946017928
Iteration 730: Loss = -11022.52685765944
Iteration 740: Loss = -11022.52669157121
Iteration 750: Loss = -11022.526645070775
Iteration 760: Loss = -11022.526521284683
Iteration 770: Loss = -11022.526449830264
Iteration 780: Loss = -11022.526337755726
Iteration 790: Loss = -11022.526257542606
Iteration 800: Loss = -11022.526185504694
Iteration 810: Loss = -11022.52608259565
Iteration 820: Loss = -11022.526010822932
Iteration 830: Loss = -11022.525913569121
Iteration 840: Loss = -11022.525870166297
Iteration 850: Loss = -11022.52579228683
Iteration 860: Loss = -11022.525757342168
Iteration 870: Loss = -11022.525678499422
Iteration 880: Loss = -11022.525624049576
Iteration 890: Loss = -11022.525518394921
Iteration 900: Loss = -11022.52548476634
Iteration 910: Loss = -11022.525439340505
Iteration 920: Loss = -11022.52536877642
Iteration 930: Loss = -11022.525325313924
Iteration 940: Loss = -11022.525266830831
Iteration 950: Loss = -11022.52522884197
Iteration 960: Loss = -11022.525215014623
Iteration 970: Loss = -11022.5251174917
Iteration 980: Loss = -11022.52509641489
Iteration 990: Loss = -11022.52502062687
Iteration 1000: Loss = -11022.525001526152
Iteration 1010: Loss = -11022.52495351086
Iteration 1020: Loss = -11022.524945455743
Iteration 1030: Loss = -11022.524925873902
Iteration 1040: Loss = -11022.524876441317
Iteration 1050: Loss = -11022.524858245291
Iteration 1060: Loss = -11022.524844836344
Iteration 1070: Loss = -11022.524783217883
Iteration 1080: Loss = -11022.52474406358
Iteration 1090: Loss = -11022.524766156721
1
Iteration 1100: Loss = -11022.52475407033
2
Iteration 1110: Loss = -11022.524684471719
Iteration 1120: Loss = -11022.524639947707
Iteration 1130: Loss = -11022.524660685202
1
Iteration 1140: Loss = -11022.524603388858
Iteration 1150: Loss = -11022.524602987578
Iteration 1160: Loss = -11022.524599017288
Iteration 1170: Loss = -11022.524551098162
Iteration 1180: Loss = -11022.524538659527
Iteration 1190: Loss = -11022.52453504699
Iteration 1200: Loss = -11022.524553736339
1
Iteration 1210: Loss = -11022.524505276648
Iteration 1220: Loss = -11022.524484362248
Iteration 1230: Loss = -11022.524490882268
1
Iteration 1240: Loss = -11022.524490877111
2
Iteration 1250: Loss = -11022.524451183437
Iteration 1260: Loss = -11022.524410030039
Iteration 1270: Loss = -11022.524422551656
1
Iteration 1280: Loss = -11022.524395223985
Iteration 1290: Loss = -11022.524418017389
1
Iteration 1300: Loss = -11022.524419276091
2
Iteration 1310: Loss = -11022.524387602176
Iteration 1320: Loss = -11022.52441444074
1
Iteration 1330: Loss = -11022.524406585628
2
Iteration 1340: Loss = -11022.524370230845
Iteration 1350: Loss = -11022.524372935595
1
Iteration 1360: Loss = -11022.524364664494
Iteration 1370: Loss = -11022.524351359267
Iteration 1380: Loss = -11022.524347437795
Iteration 1390: Loss = -11022.524371555386
1
Iteration 1400: Loss = -11022.524343437633
Iteration 1410: Loss = -11022.524344862766
1
Iteration 1420: Loss = -11022.524376406573
2
Iteration 1430: Loss = -11022.524352915938
3
Stopping early at iteration 1430 due to no improvement.
pi: tensor([[0.5243, 0.4757],
        [0.4353, 0.5647]], dtype=torch.float64)
alpha: tensor([0.4775, 0.5225])
beta: tensor([[[0.1766, 0.1603],
         [0.6079, 0.1493]],

        [[0.4772, 0.1555],
         [0.3666, 0.7010]],

        [[0.7479, 0.1763],
         [0.0958, 0.2272]],

        [[0.4198, 0.1612],
         [0.8920, 0.7071]],

        [[0.3748, 0.1567],
         [0.8682, 0.1262]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.03897614180264656
time is 1
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 42
Adjusted Rand Index: 0.01627147128527899
time is 2
tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 0, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 41
Adjusted Rand Index: 0.022706671700840775
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 34
Adjusted Rand Index: 0.09356639247943596
time is 4
tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 30
Adjusted Rand Index: 0.15199358598785037
Global Adjusted Rand Index: 0.06375053127079193
Average Adjusted Rand Index: 0.06470285265121054
Iteration 0: Loss = -11034.755078968437
Iteration 10: Loss = -11023.938294679232
Iteration 20: Loss = -11023.00529352071
Iteration 30: Loss = -11022.720594490333
Iteration 40: Loss = -11022.576810411105
Iteration 50: Loss = -11022.201702575916
Iteration 60: Loss = -11022.107121235582
Iteration 70: Loss = -11022.114252177023
1
Iteration 80: Loss = -11022.124523696224
2
Iteration 90: Loss = -11022.133520827934
3
Stopping early at iteration 90 due to no improvement.
pi: tensor([[1.6950e-05, 9.9998e-01],
        [7.7213e-02, 9.2279e-01]], dtype=torch.float64)
alpha: tensor([0.0707, 0.9293])
beta: tensor([[[0.1320, 0.1118],
         [0.7807, 0.1651]],

        [[0.9137, 0.1373],
         [0.7038, 0.3017]],

        [[0.2895, 0.2012],
         [0.7948, 0.4666]],

        [[0.3613, 0.1446],
         [0.6886, 0.2872]],

        [[0.1992, 0.1158],
         [0.8764, 0.5075]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: 0.0059174737657959825
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
Global Adjusted Rand Index: 0.00038770756378241793
Average Adjusted Rand Index: 0.0013468228023497618
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23443.67294921804
Iteration 100: Loss = -11025.590872521936
Iteration 200: Loss = -11025.140011068079
Iteration 300: Loss = -11024.847348799427
Iteration 400: Loss = -11024.541705889229
Iteration 500: Loss = -11024.338974731207
Iteration 600: Loss = -11024.167265868577
Iteration 700: Loss = -11023.997067038867
Iteration 800: Loss = -11023.838820206278
Iteration 900: Loss = -11023.699103858187
Iteration 1000: Loss = -11023.574781614383
Iteration 1100: Loss = -11023.457368039979
Iteration 1200: Loss = -11023.339452198314
Iteration 1300: Loss = -11023.217254998577
Iteration 1400: Loss = -11023.089716877666
Iteration 1500: Loss = -11022.958050974878
Iteration 1600: Loss = -11022.826933417393
Iteration 1700: Loss = -11022.70377303427
Iteration 1800: Loss = -11022.596654113022
Iteration 1900: Loss = -11022.511141170666
Iteration 2000: Loss = -11022.447054542825
Iteration 2100: Loss = -11022.402343681119
Iteration 2200: Loss = -11022.371810656226
Iteration 2300: Loss = -11022.353105161197
Iteration 2400: Loss = -11022.341585664248
Iteration 2500: Loss = -11022.333922658976
Iteration 2600: Loss = -11022.328379834667
Iteration 2700: Loss = -11022.32384112041
Iteration 2800: Loss = -11022.320147891814
Iteration 2900: Loss = -11022.315440065162
Iteration 3000: Loss = -11022.310913958434
Iteration 3100: Loss = -11022.324459659665
1
Iteration 3200: Loss = -11022.300247340927
Iteration 3300: Loss = -11022.29386814984
Iteration 3400: Loss = -11022.287125128938
Iteration 3500: Loss = -11022.27869304347
Iteration 3600: Loss = -11022.269940630957
Iteration 3700: Loss = -11022.26966875236
Iteration 3800: Loss = -11022.251230092072
Iteration 3900: Loss = -11022.24202453567
Iteration 4000: Loss = -11022.234049215112
Iteration 4100: Loss = -11022.226140514918
Iteration 4200: Loss = -11022.220121866629
Iteration 4300: Loss = -11022.215400241064
Iteration 4400: Loss = -11022.211871927426
Iteration 4500: Loss = -11022.20925604546
Iteration 4600: Loss = -11022.207295217462
Iteration 4700: Loss = -11022.205658907153
Iteration 4800: Loss = -11022.204685393308
Iteration 4900: Loss = -11022.202818092876
Iteration 5000: Loss = -11022.20117207551
Iteration 5100: Loss = -11022.199501570252
Iteration 5200: Loss = -11022.198258035987
Iteration 5300: Loss = -11022.193627257198
Iteration 5400: Loss = -11022.189883192583
Iteration 5500: Loss = -11022.182262415774
Iteration 5600: Loss = -11022.167922000575
Iteration 5700: Loss = -11022.133753581731
Iteration 5800: Loss = -11021.831637879008
Iteration 5900: Loss = -10909.420649496842
Iteration 6000: Loss = -10907.874893972888
Iteration 6100: Loss = -10902.9998543859
Iteration 6200: Loss = -10902.947645030767
Iteration 6300: Loss = -10902.935660815074
Iteration 6400: Loss = -10902.928719720807
Iteration 6500: Loss = -10902.920464069484
Iteration 6600: Loss = -10902.910110409852
Iteration 6700: Loss = -10902.905552175296
Iteration 6800: Loss = -10902.903765498937
Iteration 6900: Loss = -10902.900267151917
Iteration 7000: Loss = -10902.894855470679
Iteration 7100: Loss = -10902.893776334067
Iteration 7200: Loss = -10902.8923208906
Iteration 7300: Loss = -10902.891030818355
Iteration 7400: Loss = -10902.890139435658
Iteration 7500: Loss = -10902.889361406429
Iteration 7600: Loss = -10902.888660543034
Iteration 7700: Loss = -10902.887751794528
Iteration 7800: Loss = -10902.88667431424
Iteration 7900: Loss = -10902.885617784726
Iteration 8000: Loss = -10902.888246849821
1
Iteration 8100: Loss = -10902.88447800129
Iteration 8200: Loss = -10902.886151106977
1
Iteration 8300: Loss = -10902.883625566616
Iteration 8400: Loss = -10902.883225806845
Iteration 8500: Loss = -10902.88288346683
Iteration 8600: Loss = -10902.892633181773
1
Iteration 8700: Loss = -10902.882063170806
Iteration 8800: Loss = -10902.882991762835
1
Iteration 8900: Loss = -10902.88177271767
Iteration 9000: Loss = -10902.881930438232
1
Iteration 9100: Loss = -10902.881715098867
Iteration 9200: Loss = -10902.881833454734
1
Iteration 9300: Loss = -10902.881736760748
2
Iteration 9400: Loss = -10902.881667446483
Iteration 9500: Loss = -10902.953386896344
1
Iteration 9600: Loss = -10902.881644425079
Iteration 9700: Loss = -10902.881633333529
Iteration 9800: Loss = -10902.913824625493
1
Iteration 9900: Loss = -10902.881521178277
Iteration 10000: Loss = -10902.881811874162
1
Iteration 10100: Loss = -10902.881416745062
Iteration 10200: Loss = -10902.881822194837
1
Iteration 10300: Loss = -10902.88112150982
Iteration 10400: Loss = -10902.878809250957
Iteration 10500: Loss = -10902.878415332936
Iteration 10600: Loss = -10902.886128231068
1
Iteration 10700: Loss = -10902.877898926637
Iteration 10800: Loss = -10902.87692649403
Iteration 10900: Loss = -10902.925783474055
1
Iteration 11000: Loss = -10902.875100597059
Iteration 11100: Loss = -10902.875289392581
1
Iteration 11200: Loss = -10902.888430102688
2
Iteration 11300: Loss = -10902.876241347478
3
Iteration 11400: Loss = -10902.87763705902
4
Iteration 11500: Loss = -10902.87515967576
5
Stopping early at iteration 11500 due to no improvement.
pi: tensor([[0.7614, 0.2386],
        [0.2445, 0.7555]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4883, 0.5117], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1977, 0.0983],
         [0.5853, 0.2522]],

        [[0.5452, 0.0978],
         [0.5049, 0.7254]],

        [[0.5925, 0.1182],
         [0.6073, 0.5761]],

        [[0.6590, 0.1033],
         [0.6865, 0.7022]],

        [[0.6946, 0.0978],
         [0.6667, 0.6962]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 94
Adjusted Rand Index: 0.7721069260785004
time is 1
tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 93
Adjusted Rand Index: 0.7369552685595733
time is 2
tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 1])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 95
Adjusted Rand Index: 0.8080762963757459
time is 3
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448436976567872
time is 4
tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 94
Adjusted Rand Index: 0.7720997823854144
Global Adjusted Rand Index: 0.788119507680331
Average Adjusted Rand Index: 0.7868163942112043
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20644.21866480533
Iteration 100: Loss = -11025.61000239469
Iteration 200: Loss = -11024.947877712277
Iteration 300: Loss = -11024.367744599058
Iteration 400: Loss = -11023.881177257173
Iteration 500: Loss = -11023.607278096795
Iteration 600: Loss = -11023.406367380372
Iteration 700: Loss = -11023.23484881203
Iteration 800: Loss = -11023.076812031779
Iteration 900: Loss = -11022.92304790362
Iteration 1000: Loss = -11022.775515204492
Iteration 1100: Loss = -11022.644151409433
Iteration 1200: Loss = -11022.538689671192
Iteration 1300: Loss = -11022.45864377095
Iteration 1400: Loss = -11022.397433564915
Iteration 1500: Loss = -11022.351283306063
Iteration 1600: Loss = -11022.32068812047
Iteration 1700: Loss = -11022.300898464546
Iteration 1800: Loss = -11022.286498184405
Iteration 1900: Loss = -11022.274192181807
Iteration 2000: Loss = -11022.262523546535
Iteration 2100: Loss = -11022.251276365323
Iteration 2200: Loss = -11022.240804463263
Iteration 2300: Loss = -11022.231741992726
Iteration 2400: Loss = -11022.224441108858
Iteration 2500: Loss = -11022.218959791231
Iteration 2600: Loss = -11022.215196958608
Iteration 2700: Loss = -11022.212627632061
Iteration 2800: Loss = -11022.220300822066
1
Iteration 2900: Loss = -11022.209757112296
Iteration 3000: Loss = -11022.208927483633
Iteration 3100: Loss = -11022.208268758484
Iteration 3200: Loss = -11022.207612486662
Iteration 3300: Loss = -11022.20698099372
Iteration 3400: Loss = -11022.206266105573
Iteration 3500: Loss = -11022.21247159746
1
Iteration 3600: Loss = -11022.2044567351
Iteration 3700: Loss = -11022.203290796158
Iteration 3800: Loss = -11022.202215546216
Iteration 3900: Loss = -11022.201921960976
Iteration 4000: Loss = -11022.19834044035
Iteration 4100: Loss = -11022.195892773785
Iteration 4200: Loss = -11022.189474961078
Iteration 4300: Loss = -11022.179571296007
Iteration 4400: Loss = -11022.183026461002
1
Iteration 4500: Loss = -11022.091394295205
Iteration 4600: Loss = -10923.348557180645
Iteration 4700: Loss = -10903.495100068487
Iteration 4800: Loss = -10903.020715536573
Iteration 4900: Loss = -10902.964358993982
Iteration 5000: Loss = -10902.935184954747
Iteration 5100: Loss = -10902.930839138016
Iteration 5200: Loss = -10902.924300045957
Iteration 5300: Loss = -10902.919701671828
Iteration 5400: Loss = -10902.914352884898
Iteration 5500: Loss = -10902.908783348046
Iteration 5600: Loss = -10902.90427911365
Iteration 5700: Loss = -10902.902381015132
Iteration 5800: Loss = -10902.894140363625
Iteration 5900: Loss = -10902.894527060305
1
Iteration 6000: Loss = -10902.89218987359
Iteration 6100: Loss = -10902.891383820826
Iteration 6200: Loss = -10902.897768186325
1
Iteration 6300: Loss = -10902.890528450296
Iteration 6400: Loss = -10902.890106936002
Iteration 6500: Loss = -10902.897254303969
1
Iteration 6600: Loss = -10902.889163344826
Iteration 6700: Loss = -10902.886536876009
Iteration 6800: Loss = -10902.888475764234
1
Iteration 6900: Loss = -10902.885518543246
Iteration 7000: Loss = -10902.885277537887
Iteration 7100: Loss = -10902.885098822699
Iteration 7200: Loss = -10902.884851096293
Iteration 7300: Loss = -10902.885016893571
1
Iteration 7400: Loss = -10902.890865043772
2
Iteration 7500: Loss = -10902.884155032818
Iteration 7600: Loss = -10902.884486064628
1
Iteration 7700: Loss = -10902.883706325103
Iteration 7800: Loss = -10902.883701368313
Iteration 7900: Loss = -10902.883297566275
Iteration 8000: Loss = -10902.882271814711
Iteration 8100: Loss = -10902.881657309763
Iteration 8200: Loss = -10902.883063426512
1
Iteration 8300: Loss = -10902.88155256989
Iteration 8400: Loss = -10902.882487185962
1
Iteration 8500: Loss = -10902.881143111783
Iteration 8600: Loss = -10902.878273169339
Iteration 8700: Loss = -10902.878426755138
1
Iteration 8800: Loss = -10902.87820306161
Iteration 8900: Loss = -10902.911388065984
1
Iteration 9000: Loss = -10902.878210765848
2
Iteration 9100: Loss = -10902.878216763682
3
Iteration 9200: Loss = -10902.878521898147
4
Iteration 9300: Loss = -10902.878182527256
Iteration 9400: Loss = -10902.878519084059
1
Iteration 9500: Loss = -10902.87817756347
Iteration 9600: Loss = -10902.878302569126
1
Iteration 9700: Loss = -10902.878180822925
2
Iteration 9800: Loss = -10902.878304526954
3
Iteration 9900: Loss = -10902.898800177585
4
Iteration 10000: Loss = -10902.878545615293
5
Stopping early at iteration 10000 due to no improvement.
pi: tensor([[0.7551, 0.2449],
        [0.2389, 0.7611]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5120, 0.4880], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2522, 0.0983],
         [0.6566, 0.1977]],

        [[0.6608, 0.0976],
         [0.6979, 0.6190]],

        [[0.5199, 0.1181],
         [0.7219, 0.6333]],

        [[0.5184, 0.1032],
         [0.6759, 0.5857]],

        [[0.5442, 0.0978],
         [0.7021, 0.6791]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 6
Adjusted Rand Index: 0.7721069260785004
time is 1
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 7
Adjusted Rand Index: 0.7369552685595733
time is 2
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 5
Adjusted Rand Index: 0.8080762963757459
time is 3
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8448436976567872
time is 4
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 6
Adjusted Rand Index: 0.7720997823854144
Global Adjusted Rand Index: 0.788119507680331
Average Adjusted Rand Index: 0.7868163942112043
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21287.16900137661
Iteration 100: Loss = -11026.548842412527
Iteration 200: Loss = -11025.319762170724
Iteration 300: Loss = -11024.859434880995
Iteration 400: Loss = -11024.650437439981
Iteration 500: Loss = -11024.514801954834
Iteration 600: Loss = -11024.392857048128
Iteration 700: Loss = -11024.26578825706
Iteration 800: Loss = -11024.126479571718
Iteration 900: Loss = -11023.975223607444
Iteration 1000: Loss = -11023.819969716229
Iteration 1100: Loss = -11023.670360450174
Iteration 1200: Loss = -11023.527663206132
Iteration 1300: Loss = -11023.38510143101
Iteration 1400: Loss = -11023.238093672
Iteration 1500: Loss = -11023.088329685994
Iteration 1600: Loss = -11022.939171034554
Iteration 1700: Loss = -11022.800823173688
Iteration 1800: Loss = -11022.6798459239
Iteration 1900: Loss = -11022.583084874399
Iteration 2000: Loss = -11022.513380409127
Iteration 2100: Loss = -11022.464225440248
Iteration 2200: Loss = -11022.424987307748
Iteration 2300: Loss = -11022.394435861448
Iteration 2400: Loss = -11022.369258396102
Iteration 2500: Loss = -11022.350285243196
Iteration 2600: Loss = -11022.337664049095
Iteration 2700: Loss = -11022.328128048248
Iteration 2800: Loss = -11022.320033993246
Iteration 2900: Loss = -11022.312290134598
Iteration 3000: Loss = -11022.304051812487
Iteration 3100: Loss = -11022.294915665942
Iteration 3200: Loss = -11022.284494403197
Iteration 3300: Loss = -11022.272867556947
Iteration 3400: Loss = -11022.260462040831
Iteration 3500: Loss = -11022.248087668753
Iteration 3600: Loss = -11022.236718759505
Iteration 3700: Loss = -11022.227333061104
Iteration 3800: Loss = -11022.223343865233
Iteration 3900: Loss = -11022.215352960073
Iteration 4000: Loss = -11022.212125600667
Iteration 4100: Loss = -11022.210683430483
Iteration 4200: Loss = -11022.20829686377
Iteration 4300: Loss = -11022.206926371955
Iteration 4400: Loss = -11022.205756548276
Iteration 4500: Loss = -11022.203999754955
Iteration 4600: Loss = -11022.202149405575
Iteration 4700: Loss = -11022.19967470003
Iteration 4800: Loss = -11022.195017489732
Iteration 4900: Loss = -11022.188341134803
Iteration 5000: Loss = -11022.175392010884
Iteration 5100: Loss = -11022.144161312244
Iteration 5200: Loss = -11021.695174602468
Iteration 5300: Loss = -10905.074487461665
Iteration 5400: Loss = -10903.32855016644
Iteration 5500: Loss = -10903.202036628767
Iteration 5600: Loss = -10903.059913634033
Iteration 5700: Loss = -10903.048206775422
Iteration 5800: Loss = -10903.013791219424
Iteration 5900: Loss = -10902.959070742834
Iteration 6000: Loss = -10902.934020050274
Iteration 6100: Loss = -10902.92586011629
Iteration 6200: Loss = -10902.92312621121
Iteration 6300: Loss = -10902.919634930648
Iteration 6400: Loss = -10902.916725127274
Iteration 6500: Loss = -10902.909644806543
Iteration 6600: Loss = -10902.905200799052
Iteration 6700: Loss = -10902.905608919811
1
Iteration 6800: Loss = -10902.903968432167
Iteration 6900: Loss = -10902.902234221818
Iteration 7000: Loss = -10902.910901187432
1
Iteration 7100: Loss = -10902.89569837388
Iteration 7200: Loss = -10902.894217009796
Iteration 7300: Loss = -10902.893834722574
Iteration 7400: Loss = -10902.893671763424
Iteration 7500: Loss = -10902.89606834979
1
Iteration 7600: Loss = -10902.891834756894
Iteration 7700: Loss = -10902.88881568717
Iteration 7800: Loss = -10902.888338047052
Iteration 7900: Loss = -10902.889885275514
1
Iteration 8000: Loss = -10902.8871594579
Iteration 8100: Loss = -10902.88554910617
Iteration 8200: Loss = -10902.911883685249
1
Iteration 8300: Loss = -10902.884170453448
Iteration 8400: Loss = -10902.883895091001
Iteration 8500: Loss = -10902.883122063757
Iteration 8600: Loss = -10902.895859529093
1
Iteration 8700: Loss = -10902.88270205512
Iteration 8800: Loss = -10903.055758601544
1
Iteration 8900: Loss = -10902.882146058186
Iteration 9000: Loss = -10902.88262915165
1
Iteration 9100: Loss = -10902.881805369578
Iteration 9200: Loss = -10902.881930261397
1
Iteration 9300: Loss = -10902.882669164566
2
Iteration 9400: Loss = -10902.881422077484
Iteration 9500: Loss = -10902.882554567257
1
Iteration 9600: Loss = -10902.898564409054
2
Iteration 9700: Loss = -10902.895329109193
3
Iteration 9800: Loss = -10902.884098476587
4
Iteration 9900: Loss = -10902.890837319192
5
Stopping early at iteration 9900 due to no improvement.
pi: tensor([[0.7554, 0.2446],
        [0.2392, 0.7608]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5125, 0.4875], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2520, 0.0982],
         [0.6391, 0.1979]],

        [[0.6157, 0.0975],
         [0.6857, 0.5111]],

        [[0.6789, 0.1181],
         [0.5105, 0.6768]],

        [[0.6142, 0.1030],
         [0.7146, 0.6777]],

        [[0.6487, 0.0978],
         [0.5007, 0.7260]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 6
Adjusted Rand Index: 0.7721069260785004
time is 1
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 7
Adjusted Rand Index: 0.7369552685595733
time is 2
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 5
Adjusted Rand Index: 0.8080762963757459
time is 3
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8448436976567872
time is 4
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 6
Adjusted Rand Index: 0.7720997823854144
Global Adjusted Rand Index: 0.788119507680331
Average Adjusted Rand Index: 0.7868163942112043
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21698.301234104776
Iteration 100: Loss = -11026.45467476031
Iteration 200: Loss = -11025.525699759826
Iteration 300: Loss = -11025.148280986215
Iteration 400: Loss = -11024.94634809985
Iteration 500: Loss = -11024.824675751104
Iteration 600: Loss = -11024.741318786844
Iteration 700: Loss = -11024.674534542393
Iteration 800: Loss = -11024.60977061765
Iteration 900: Loss = -11024.53604170328
Iteration 1000: Loss = -11024.444402916652
Iteration 1100: Loss = -11024.32596294886
Iteration 1200: Loss = -11024.170638121243
Iteration 1300: Loss = -11023.967642024263
Iteration 1400: Loss = -11023.71912077509
Iteration 1500: Loss = -11023.439827474347
Iteration 1600: Loss = -11023.12448137185
Iteration 1700: Loss = -11022.84060621969
Iteration 1800: Loss = -11022.633215076896
Iteration 1900: Loss = -11022.509624528406
Iteration 2000: Loss = -11022.440032294533
Iteration 2100: Loss = -11022.397309299944
Iteration 2200: Loss = -11022.375059329286
Iteration 2300: Loss = -11022.366691961744
Iteration 2400: Loss = -11022.362393145402
Iteration 2500: Loss = -11022.359469016681
Iteration 2600: Loss = -11022.356640147711
Iteration 2700: Loss = -11022.353410955102
Iteration 2800: Loss = -11022.349304914953
Iteration 2900: Loss = -11022.343971631557
Iteration 3000: Loss = -11022.336636445138
Iteration 3100: Loss = -11022.32638796861
Iteration 3200: Loss = -11022.31190782843
Iteration 3300: Loss = -11022.292708520195
Iteration 3400: Loss = -11022.276994782613
Iteration 3500: Loss = -11022.251652130108
Iteration 3600: Loss = -11022.238704588795
Iteration 3700: Loss = -11022.230847484376
Iteration 3800: Loss = -11022.22604764959
Iteration 3900: Loss = -11022.224423907073
Iteration 4000: Loss = -11022.221020428598
Iteration 4100: Loss = -11022.219202647071
Iteration 4200: Loss = -11022.218160599637
Iteration 4300: Loss = -11022.217296550763
Iteration 4400: Loss = -11022.222582140626
1
Iteration 4500: Loss = -11022.216322575005
Iteration 4600: Loss = -11022.21609837736
Iteration 4700: Loss = -11022.215656638367
Iteration 4800: Loss = -11022.215623853264
Iteration 4900: Loss = -11022.214956953041
Iteration 5000: Loss = -11022.216016861945
1
Iteration 5100: Loss = -11022.213832677833
Iteration 5200: Loss = -11022.213565690954
Iteration 5300: Loss = -11022.212520155235
Iteration 5400: Loss = -11022.211383830165
Iteration 5500: Loss = -11022.209390536766
Iteration 5600: Loss = -11022.206461163767
Iteration 5700: Loss = -11022.201343121373
Iteration 5800: Loss = -11022.190459454025
Iteration 5900: Loss = -11022.049919225237
Iteration 6000: Loss = -10912.569873610064
Iteration 6100: Loss = -10905.030949236187
Iteration 6200: Loss = -10903.706090974312
Iteration 6300: Loss = -10903.482147403587
Iteration 6400: Loss = -10903.262768879931
Iteration 6500: Loss = -10903.235022408144
Iteration 6600: Loss = -10903.185186692486
Iteration 6700: Loss = -10903.125887168257
Iteration 6800: Loss = -10903.040483612376
Iteration 6900: Loss = -10903.018290294036
Iteration 7000: Loss = -10903.00427182966
Iteration 7100: Loss = -10902.991266723891
Iteration 7200: Loss = -10902.970311996798
Iteration 7300: Loss = -10902.964039055587
Iteration 7400: Loss = -10902.955170797886
Iteration 7500: Loss = -10902.959880450491
1
Iteration 7600: Loss = -10902.937729564677
Iteration 7700: Loss = -10902.932250464153
Iteration 7800: Loss = -10902.928372381766
Iteration 7900: Loss = -10902.921398266588
Iteration 8000: Loss = -10902.920560746377
Iteration 8100: Loss = -10902.927823676519
1
Iteration 8200: Loss = -10902.917100881255
Iteration 8300: Loss = -10902.919105248397
1
Iteration 8400: Loss = -10902.913026175678
Iteration 8500: Loss = -10902.912362256513
Iteration 8600: Loss = -10902.909624440004
Iteration 8700: Loss = -10902.9083565218
Iteration 8800: Loss = -10902.906527893974
Iteration 8900: Loss = -10902.913505830778
1
Iteration 9000: Loss = -10902.905095163893
Iteration 9100: Loss = -10902.898141738386
Iteration 9200: Loss = -10902.893329692262
Iteration 9300: Loss = -10902.89346598166
1
Iteration 9400: Loss = -10902.892990481769
Iteration 9500: Loss = -10902.905495709996
1
Iteration 9600: Loss = -10902.895194256917
2
Iteration 9700: Loss = -10902.911227660878
3
Iteration 9800: Loss = -10902.892474272609
Iteration 9900: Loss = -10902.890875029516
Iteration 10000: Loss = -10902.891172384723
1
Iteration 10100: Loss = -10902.899641898408
2
Iteration 10200: Loss = -10902.903498807118
3
Iteration 10300: Loss = -10902.976865575041
4
Iteration 10400: Loss = -10902.889133208708
Iteration 10500: Loss = -10902.888419110142
Iteration 10600: Loss = -10902.884859760865
Iteration 10700: Loss = -10902.883518286091
Iteration 10800: Loss = -10902.882966712019
Iteration 10900: Loss = -10902.911301744398
1
Iteration 11000: Loss = -10902.878846966358
Iteration 11100: Loss = -10902.886892000422
1
Iteration 11200: Loss = -10902.878833674362
Iteration 11300: Loss = -10902.878981843369
1
Iteration 11400: Loss = -10902.879192884284
2
Iteration 11500: Loss = -10902.880114451009
3
Iteration 11600: Loss = -10903.035999049654
4
Iteration 11700: Loss = -10902.878708721133
Iteration 11800: Loss = -10902.878384712312
Iteration 11900: Loss = -10902.891199122421
1
Iteration 12000: Loss = -10902.877754544472
Iteration 12100: Loss = -10902.878590314613
1
Iteration 12200: Loss = -10902.877355209315
Iteration 12300: Loss = -10902.878166110691
1
Iteration 12400: Loss = -10902.877269301438
Iteration 12500: Loss = -10902.881550254726
1
Iteration 12600: Loss = -10902.877235321095
Iteration 12700: Loss = -10902.898383961387
1
Iteration 12800: Loss = -10902.920902818609
2
Iteration 12900: Loss = -10902.88083504568
3
Iteration 13000: Loss = -10902.877605879154
4
Iteration 13100: Loss = -10902.87761346711
5
Stopping early at iteration 13100 due to no improvement.
pi: tensor([[0.7552, 0.2448],
        [0.2387, 0.7613]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5117, 0.4883], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2523, 0.0983],
         [0.7164, 0.1977]],

        [[0.5712, 0.0977],
         [0.6747, 0.6363]],

        [[0.6170, 0.1182],
         [0.6418, 0.6021]],

        [[0.7047, 0.1030],
         [0.5065, 0.5464]],

        [[0.5071, 0.0979],
         [0.5795, 0.5834]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 6
Adjusted Rand Index: 0.7721069260785004
time is 1
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1])
Difference count: 7
Adjusted Rand Index: 0.7369552685595733
time is 2
tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0])
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 5
Adjusted Rand Index: 0.8080762963757459
time is 3
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8448436976567872
time is 4
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 6
Adjusted Rand Index: 0.7720997823854144
Global Adjusted Rand Index: 0.788119507680331
Average Adjusted Rand Index: 0.7868163942112043
10952.501582448027
[0.788119507680331, 0.788119507680331, 0.788119507680331, 0.788119507680331] [0.7868163942112043, 0.7868163942112043, 0.7868163942112043, 0.7868163942112043] [10902.87515967576, 10902.878545615293, 10902.890837319192, 10902.87761346711]
-----------------------------------------------------------------------------------------
This iteration is 4
True Objective function: Loss = -11064.267104828867
Iteration 0: Loss = -11300.293853594672
Iteration 10: Loss = -11125.245424725188
Iteration 20: Loss = -11125.158319546805
Iteration 30: Loss = -11125.13592635531
Iteration 40: Loss = -11125.121951365292
Iteration 50: Loss = -11125.1091903671
Iteration 60: Loss = -11125.09598540252
Iteration 70: Loss = -11125.081979173423
Iteration 80: Loss = -11125.067145819336
Iteration 90: Loss = -11125.051279886477
Iteration 100: Loss = -11125.034395152607
Iteration 110: Loss = -11125.016456389172
Iteration 120: Loss = -11124.997346007702
Iteration 130: Loss = -11124.976963704077
Iteration 140: Loss = -11124.955234156452
Iteration 150: Loss = -11124.931807088615
Iteration 160: Loss = -11124.906556247774
Iteration 170: Loss = -11124.879175552845
Iteration 180: Loss = -11124.849260285551
Iteration 190: Loss = -11124.816160801696
Iteration 200: Loss = -11124.779357856194
Iteration 210: Loss = -11124.737688514128
Iteration 220: Loss = -11124.689959557714
Iteration 230: Loss = -11124.63419130245
Iteration 240: Loss = -11124.567516452333
Iteration 250: Loss = -11124.485743407302
Iteration 260: Loss = -11124.382339305026
Iteration 270: Loss = -11124.247459937613
Iteration 280: Loss = -11124.069030477605
Iteration 290: Loss = -11123.843265916541
Iteration 300: Loss = -11123.612719432445
Iteration 310: Loss = -11123.468942391948
Iteration 320: Loss = -11123.426775651316
Iteration 330: Loss = -11123.428424270493
1
Iteration 340: Loss = -11123.438090280775
2
Iteration 350: Loss = -11123.445646187962
3
Stopping early at iteration 350 due to no improvement.
pi: tensor([[0.0457, 0.9543],
        [0.0511, 0.9489]], dtype=torch.float64)
alpha: tensor([0.0516, 0.9484])
beta: tensor([[[0.2337, 0.1945],
         [0.1728, 0.1609]],

        [[0.2295, 0.1655],
         [0.6658, 0.0212]],

        [[0.1261, 0.2384],
         [0.1319, 0.6651]],

        [[0.3141, 0.1676],
         [0.1114, 0.5951]],

        [[0.5434, 0.1985],
         [0.3724, 0.4942]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: -0.014100938522947548
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 9.6196983717923e-05
Average Adjusted Rand Index: -0.0028201877045895096
Iteration 0: Loss = -11126.210483648734
Iteration 10: Loss = -11125.308602013683
Iteration 20: Loss = -11124.073716347057
Iteration 30: Loss = -11123.69671718746
Iteration 40: Loss = -11123.582764481367
Iteration 50: Loss = -11123.533894285742
Iteration 60: Loss = -11123.494001953371
Iteration 70: Loss = -11123.469641375037
Iteration 80: Loss = -11123.460580056444
Iteration 90: Loss = -11123.45752881684
Iteration 100: Loss = -11123.456359148106
Iteration 110: Loss = -11123.455859488315
Iteration 120: Loss = -11123.455661949629
Iteration 130: Loss = -11123.455543194368
Iteration 140: Loss = -11123.455467349318
Iteration 150: Loss = -11123.455449328045
Iteration 160: Loss = -11123.455436716076
Iteration 170: Loss = -11123.455410110013
Iteration 180: Loss = -11123.455431277127
1
Iteration 190: Loss = -11123.455430493037
2
Iteration 200: Loss = -11123.45542751799
3
Stopping early at iteration 200 due to no improvement.
pi: tensor([[0.9507, 0.0493],
        [0.9557, 0.0443]], dtype=torch.float64)
alpha: tensor([0.9502, 0.0498])
beta: tensor([[[0.1610, 0.1947],
         [0.4390, 0.2343]],

        [[0.3961, 0.1651],
         [0.4419, 0.0100]],

        [[0.9165, 0.2393],
         [0.7964, 0.7324]],

        [[0.8801, 0.1671],
         [0.7096, 0.1652]],

        [[0.3347, 0.1991],
         [0.2573, 0.0408]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: -0.014100938522947548
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 9.6196983717923e-05
Average Adjusted Rand Index: -0.0028201877045895096
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22571.688933942187
Iteration 100: Loss = -11127.82703985492
Iteration 200: Loss = -11126.30071049719
Iteration 300: Loss = -11125.864970130064
Iteration 400: Loss = -11125.70699164415
Iteration 500: Loss = -11125.63519384978
Iteration 600: Loss = -11125.59434148963
Iteration 700: Loss = -11125.563285183993
Iteration 800: Loss = -11125.528834034507
Iteration 900: Loss = -11125.461285672947
Iteration 1000: Loss = -11125.137294084914
Iteration 1100: Loss = -11124.648857522441
Iteration 1200: Loss = -11124.30734650022
Iteration 1300: Loss = -11124.040190511263
Iteration 1400: Loss = -11123.835881470915
Iteration 1500: Loss = -11123.674130577718
Iteration 1600: Loss = -11123.54575513863
Iteration 1700: Loss = -11123.439120975354
Iteration 1800: Loss = -11123.347863302015
Iteration 1900: Loss = -11123.26570574592
Iteration 2000: Loss = -11123.186630801287
Iteration 2100: Loss = -11123.1107507768
Iteration 2200: Loss = -11123.043741311449
Iteration 2300: Loss = -11122.992036148604
Iteration 2400: Loss = -11122.954428450537
Iteration 2500: Loss = -11122.926107970154
Iteration 2600: Loss = -11122.908652076221
Iteration 2700: Loss = -11122.899989814032
Iteration 2800: Loss = -11122.895546516982
Iteration 2900: Loss = -11122.892992535379
Iteration 3000: Loss = -11122.891333359392
Iteration 3100: Loss = -11122.889993574807
Iteration 3200: Loss = -11122.888826554785
Iteration 3300: Loss = -11122.88774654701
Iteration 3400: Loss = -11122.886772085909
Iteration 3500: Loss = -11122.885819739511
Iteration 3600: Loss = -11122.884907895716
Iteration 3700: Loss = -11122.88408533183
Iteration 3800: Loss = -11122.883275817341
Iteration 3900: Loss = -11122.882489047273
Iteration 4000: Loss = -11122.881800767665
Iteration 4100: Loss = -11122.88524930961
1
Iteration 4200: Loss = -11122.88047397794
Iteration 4300: Loss = -11122.879891345
Iteration 4400: Loss = -11122.88076789575
1
Iteration 4500: Loss = -11122.878734987846
Iteration 4600: Loss = -11122.878231317683
Iteration 4700: Loss = -11122.877838718785
Iteration 4800: Loss = -11122.881740020573
1
Iteration 4900: Loss = -11122.876835788682
Iteration 5000: Loss = -11122.87657528946
Iteration 5100: Loss = -11122.87605464185
Iteration 5200: Loss = -11122.875715157015
Iteration 5300: Loss = -11122.875961471504
1
Iteration 5400: Loss = -11122.879734127639
2
Iteration 5500: Loss = -11122.874800561503
Iteration 5600: Loss = -11122.87439409188
Iteration 5700: Loss = -11122.874284597276
Iteration 5800: Loss = -11122.874433215675
1
Iteration 5900: Loss = -11122.873612551206
Iteration 6000: Loss = -11122.873381940606
Iteration 6100: Loss = -11122.873179091923
Iteration 6200: Loss = -11122.873741988316
1
Iteration 6300: Loss = -11122.872901535766
Iteration 6400: Loss = -11122.882320959088
1
Iteration 6500: Loss = -11122.872333818981
Iteration 6600: Loss = -11122.872948279482
1
Iteration 6700: Loss = -11122.8720248992
Iteration 6800: Loss = -11122.872170780436
1
Iteration 6900: Loss = -11122.8717596971
Iteration 7000: Loss = -11122.871681783748
Iteration 7100: Loss = -11122.871492229804
Iteration 7200: Loss = -11122.871384719618
Iteration 7300: Loss = -11122.871280116578
Iteration 7400: Loss = -11122.87217675368
1
Iteration 7500: Loss = -11122.870990614465
Iteration 7600: Loss = -11122.871391362534
1
Iteration 7700: Loss = -11122.87083066145
Iteration 7800: Loss = -11122.870842791248
1
Iteration 7900: Loss = -11122.870675099186
Iteration 8000: Loss = -11122.871278152896
1
Iteration 8100: Loss = -11122.870497220478
Iteration 8200: Loss = -11122.870743706453
1
Iteration 8300: Loss = -11122.870375068349
Iteration 8400: Loss = -11122.874872664144
1
Iteration 8500: Loss = -11122.870246427285
Iteration 8600: Loss = -11122.87016619788
Iteration 8700: Loss = -11122.870142942244
Iteration 8800: Loss = -11122.870076116043
Iteration 8900: Loss = -11122.871592799178
1
Iteration 9000: Loss = -11122.870032708925
Iteration 9100: Loss = -11122.869927416259
Iteration 9200: Loss = -11122.901131424085
1
Iteration 9300: Loss = -11122.869829332723
Iteration 9400: Loss = -11122.869848542528
1
Iteration 9500: Loss = -11122.86995774156
2
Iteration 9600: Loss = -11122.869725088036
Iteration 9700: Loss = -11122.869745896
1
Iteration 9800: Loss = -11122.869723236985
Iteration 9900: Loss = -11122.869642845943
Iteration 10000: Loss = -11123.22670075936
1
Iteration 10100: Loss = -11122.869590104601
Iteration 10200: Loss = -11122.869567212578
Iteration 10300: Loss = -11122.87873667989
1
Iteration 10400: Loss = -11122.869506117377
Iteration 10500: Loss = -11122.869538146335
1
Iteration 10600: Loss = -11122.870314299073
2
Iteration 10700: Loss = -11122.869455787439
Iteration 10800: Loss = -11122.879450333752
1
Iteration 10900: Loss = -11122.869414428815
Iteration 11000: Loss = -11122.869456314258
1
Iteration 11100: Loss = -11122.869477962722
2
Iteration 11200: Loss = -11122.869413398545
Iteration 11300: Loss = -11122.869733824307
1
Iteration 11400: Loss = -11122.869345210212
Iteration 11500: Loss = -11122.871061679678
1
Iteration 11600: Loss = -11122.87346175633
2
Iteration 11700: Loss = -11122.869324789357
Iteration 11800: Loss = -11122.869871239558
1
Iteration 11900: Loss = -11122.883961561358
2
Iteration 12000: Loss = -11122.869260123494
Iteration 12100: Loss = -11122.979434518347
1
Iteration 12200: Loss = -11122.869277780233
2
Iteration 12300: Loss = -11122.917305970777
3
Iteration 12400: Loss = -11122.869245910464
Iteration 12500: Loss = -11122.87111200771
1
Iteration 12600: Loss = -11122.869324007814
2
Iteration 12700: Loss = -11122.869264737994
3
Iteration 12800: Loss = -11122.87422721742
4
Iteration 12900: Loss = -11122.870882308915
5
Stopping early at iteration 12900 due to no improvement.
pi: tensor([[9.3481e-01, 6.5193e-02],
        [9.9982e-01, 1.7592e-04]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8110, 0.1890], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1613, 0.1828],
         [0.6453, 0.2141]],

        [[0.6081, 0.1670],
         [0.5491, 0.7228]],

        [[0.6171, 0.2362],
         [0.5934, 0.7057]],

        [[0.6707, 0.1720],
         [0.5687, 0.6951]],

        [[0.7258, 0.1968],
         [0.7112, 0.6910]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: -0.014100938522947548
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 9.6196983717923e-05
Average Adjusted Rand Index: -0.0028201877045895096
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22608.88229293258
Iteration 100: Loss = -11126.915084637729
Iteration 200: Loss = -11125.99919693687
Iteration 300: Loss = -11125.760129192717
Iteration 400: Loss = -11125.659161523356
Iteration 500: Loss = -11125.591707121856
Iteration 600: Loss = -11125.5104898449
Iteration 700: Loss = -11125.243304647143
Iteration 800: Loss = -11124.523574503199
Iteration 900: Loss = -11124.138682615001
Iteration 1000: Loss = -11123.899884981454
Iteration 1100: Loss = -11123.728988718221
Iteration 1200: Loss = -11123.60150698584
Iteration 1300: Loss = -11123.499207294555
Iteration 1400: Loss = -11123.415323762705
Iteration 1500: Loss = -11123.341868810394
Iteration 1600: Loss = -11123.271355780771
Iteration 1700: Loss = -11123.199523899846
Iteration 1800: Loss = -11123.126753370578
Iteration 1900: Loss = -11123.059289325092
Iteration 2000: Loss = -11123.005099269927
Iteration 2100: Loss = -11122.965703538166
Iteration 2200: Loss = -11122.935786801354
Iteration 2300: Loss = -11122.916359876635
Iteration 2400: Loss = -11122.90614842505
Iteration 2500: Loss = -11122.900582223081
Iteration 2600: Loss = -11122.897301558372
Iteration 2700: Loss = -11122.894942374645
Iteration 2800: Loss = -11122.893138130079
Iteration 2900: Loss = -11122.891528096976
Iteration 3000: Loss = -11122.89007642494
Iteration 3100: Loss = -11122.88876959333
Iteration 3200: Loss = -11122.887549896048
Iteration 3300: Loss = -11122.886395239211
Iteration 3400: Loss = -11122.88532391101
Iteration 3500: Loss = -11122.884319207213
Iteration 3600: Loss = -11122.88344234609
Iteration 3700: Loss = -11122.88253335169
Iteration 3800: Loss = -11122.88192304717
Iteration 3900: Loss = -11122.880990585589
Iteration 4000: Loss = -11122.880293343573
Iteration 4100: Loss = -11122.879716575662
Iteration 4200: Loss = -11122.879048594226
Iteration 4300: Loss = -11122.891826780782
1
Iteration 4400: Loss = -11122.877857675223
Iteration 4500: Loss = -11122.877394673158
Iteration 4600: Loss = -11122.876924586595
Iteration 4700: Loss = -11122.876458926126
Iteration 4800: Loss = -11122.876170220448
Iteration 4900: Loss = -11122.875616608764
Iteration 5000: Loss = -11122.876058736381
1
Iteration 5100: Loss = -11122.874972797837
Iteration 5200: Loss = -11122.87461903708
Iteration 5300: Loss = -11122.874280961087
Iteration 5400: Loss = -11122.874795034595
1
Iteration 5500: Loss = -11122.87373120736
Iteration 5600: Loss = -11122.875864789241
1
Iteration 5700: Loss = -11122.87323468728
Iteration 5800: Loss = -11122.873596164069
1
Iteration 5900: Loss = -11122.872869355424
Iteration 6000: Loss = -11122.87264899572
Iteration 6100: Loss = -11122.87301853561
1
Iteration 6200: Loss = -11122.872274340856
Iteration 6300: Loss = -11122.872133430528
Iteration 6400: Loss = -11122.874683446622
1
Iteration 6500: Loss = -11122.872620440856
2
Iteration 6600: Loss = -11122.87216599555
3
Iteration 6700: Loss = -11122.871825969358
Iteration 6800: Loss = -11122.871427957029
Iteration 6900: Loss = -11122.871286358766
Iteration 7000: Loss = -11122.872401144867
1
Iteration 7100: Loss = -11122.872618784728
2
Iteration 7200: Loss = -11122.872420977967
3
Iteration 7300: Loss = -11122.870844630834
Iteration 7400: Loss = -11122.87079291879
Iteration 7500: Loss = -11122.870695764972
Iteration 7600: Loss = -11122.874421752824
1
Iteration 7700: Loss = -11122.870522503921
Iteration 7800: Loss = -11122.870453339414
Iteration 7900: Loss = -11122.88478523713
1
Iteration 8000: Loss = -11122.870283740498
Iteration 8100: Loss = -11122.870233455686
Iteration 8200: Loss = -11122.870789293465
1
Iteration 8300: Loss = -11122.870127927428
Iteration 8400: Loss = -11122.874648803889
1
Iteration 8500: Loss = -11122.870050995776
Iteration 8600: Loss = -11122.883084721621
1
Iteration 8700: Loss = -11122.869910691512
Iteration 8800: Loss = -11122.869903084455
Iteration 8900: Loss = -11122.870055961133
1
Iteration 9000: Loss = -11122.869813485351
Iteration 9100: Loss = -11122.869726868827
Iteration 9200: Loss = -11122.870093454267
1
Iteration 9300: Loss = -11122.869702394742
Iteration 9400: Loss = -11122.869686538723
Iteration 9500: Loss = -11122.869743810938
1
Iteration 9600: Loss = -11122.869588831356
Iteration 9700: Loss = -11122.87014279017
1
Iteration 9800: Loss = -11122.869607537685
2
Iteration 9900: Loss = -11122.869539387562
Iteration 10000: Loss = -11122.948924952056
1
Iteration 10100: Loss = -11122.870282935022
2
Iteration 10200: Loss = -11122.869537473898
Iteration 10300: Loss = -11122.873661827784
1
Iteration 10400: Loss = -11122.873348447549
2
Iteration 10500: Loss = -11122.869444166952
Iteration 10600: Loss = -11122.898835341428
1
Iteration 10700: Loss = -11122.869396900405
Iteration 10800: Loss = -11122.869456235032
1
Iteration 10900: Loss = -11122.899913088995
2
Iteration 11000: Loss = -11122.869407589644
3
Iteration 11100: Loss = -11122.870082662526
4
Iteration 11200: Loss = -11122.906065747655
5
Stopping early at iteration 11200 due to no improvement.
pi: tensor([[9.3544e-01, 6.4564e-02],
        [9.9967e-01, 3.3081e-04]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8097, 0.1903], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1614, 0.1825],
         [0.6949, 0.2137]],

        [[0.6914, 0.1668],
         [0.7185, 0.5420]],

        [[0.5885, 0.2369],
         [0.5034, 0.6995]],

        [[0.5451, 0.1719],
         [0.6399, 0.5359]],

        [[0.6630, 0.1966],
         [0.6194, 0.6766]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: -0.014100938522947548
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 9.6196983717923e-05
Average Adjusted Rand Index: -0.0028201877045895096
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22177.42990274045
Iteration 100: Loss = -11126.994693538552
Iteration 200: Loss = -11126.088009099805
Iteration 300: Loss = -11125.822517590364
Iteration 400: Loss = -11125.705092770238
Iteration 500: Loss = -11125.641953904606
Iteration 600: Loss = -11125.603705848745
Iteration 700: Loss = -11125.577867449578
Iteration 800: Loss = -11125.55805961289
Iteration 900: Loss = -11125.540784474393
Iteration 1000: Loss = -11125.52348677726
Iteration 1100: Loss = -11125.503891688208
Iteration 1200: Loss = -11125.47977206429
Iteration 1300: Loss = -11125.448946630777
Iteration 1400: Loss = -11125.409706366587
Iteration 1500: Loss = -11125.361270310546
Iteration 1600: Loss = -11125.302024754541
Iteration 1700: Loss = -11125.221803184466
Iteration 1800: Loss = -11125.110772853815
Iteration 1900: Loss = -11125.026221465756
Iteration 2000: Loss = -11124.976821446144
Iteration 2100: Loss = -11124.930481477124
Iteration 2200: Loss = -11124.882641192622
Iteration 2300: Loss = -11124.836451015593
Iteration 2400: Loss = -11124.79762033597
Iteration 2500: Loss = -11124.771676989207
Iteration 2600: Loss = -11124.758085853124
Iteration 2700: Loss = -11124.752431573183
Iteration 2800: Loss = -11124.748680506646
Iteration 2900: Loss = -11124.746331893028
Iteration 3000: Loss = -11124.744649151791
Iteration 3100: Loss = -11124.743324095924
Iteration 3200: Loss = -11124.742164264433
Iteration 3300: Loss = -11124.741463664468
Iteration 3400: Loss = -11124.740962245736
Iteration 3500: Loss = -11124.740496878509
Iteration 3600: Loss = -11124.740528975486
1
Iteration 3700: Loss = -11124.73972255824
Iteration 3800: Loss = -11124.739869739546
1
Iteration 3900: Loss = -11124.739177369998
Iteration 4000: Loss = -11124.738903472035
Iteration 4100: Loss = -11124.74251032142
1
Iteration 4200: Loss = -11124.738475625298
Iteration 4300: Loss = -11124.73826718591
Iteration 4400: Loss = -11124.738057989127
Iteration 4500: Loss = -11124.737882660309
Iteration 4600: Loss = -11124.737697843004
Iteration 4700: Loss = -11124.737546916305
Iteration 4800: Loss = -11124.737425013773
Iteration 4900: Loss = -11124.737275588348
Iteration 5000: Loss = -11124.739884583976
1
Iteration 5100: Loss = -11124.737022023646
Iteration 5200: Loss = -11124.736897000392
Iteration 5300: Loss = -11124.736783675622
Iteration 5400: Loss = -11124.736689204214
Iteration 5500: Loss = -11124.737423089191
1
Iteration 5600: Loss = -11124.736508774035
Iteration 5700: Loss = -11124.736387603385
Iteration 5800: Loss = -11124.736303608848
Iteration 5900: Loss = -11124.736217073909
Iteration 6000: Loss = -11124.736717816053
1
Iteration 6100: Loss = -11124.7361095075
Iteration 6200: Loss = -11124.73607907513
Iteration 6300: Loss = -11124.735978693649
Iteration 6400: Loss = -11124.735888949392
Iteration 6500: Loss = -11124.73586697477
Iteration 6600: Loss = -11124.735766477646
Iteration 6700: Loss = -11124.735826123882
1
Iteration 6800: Loss = -11124.735682556944
Iteration 6900: Loss = -11124.738137982255
1
Iteration 7000: Loss = -11124.735584054517
Iteration 7100: Loss = -11124.735730888962
1
Iteration 7200: Loss = -11124.735535838503
Iteration 7300: Loss = -11124.771142654696
1
Iteration 7400: Loss = -11124.735425595045
Iteration 7500: Loss = -11124.737120557156
1
Iteration 7600: Loss = -11124.7656898883
2
Iteration 7700: Loss = -11124.735359834898
Iteration 7800: Loss = -11124.735457708812
1
Iteration 7900: Loss = -11124.764811812662
2
Iteration 8000: Loss = -11124.735302221367
Iteration 8100: Loss = -11124.809979437758
1
Iteration 8200: Loss = -11124.735247888693
Iteration 8300: Loss = -11124.735227489615
Iteration 8400: Loss = -11124.75402395627
1
Iteration 8500: Loss = -11124.735187048203
Iteration 8600: Loss = -11124.735195557403
1
Iteration 8700: Loss = -11124.77115237839
2
Iteration 8800: Loss = -11124.735148604339
Iteration 8900: Loss = -11124.7351076518
Iteration 9000: Loss = -11124.737334235411
1
Iteration 9100: Loss = -11124.735148053516
2
Iteration 9200: Loss = -11124.735072618309
Iteration 9300: Loss = -11124.735115655454
1
Iteration 9400: Loss = -11124.735042385346
Iteration 9500: Loss = -11124.735107908982
1
Iteration 9600: Loss = -11124.735157662531
2
Iteration 9700: Loss = -11124.735051056321
3
Iteration 9800: Loss = -11124.795913761078
4
Iteration 9900: Loss = -11124.735000219496
Iteration 10000: Loss = -11124.735025436461
1
Iteration 10100: Loss = -11124.734981431096
Iteration 10200: Loss = -11124.73512698614
1
Iteration 10300: Loss = -11124.734947697345
Iteration 10400: Loss = -11124.734970000147
1
Iteration 10500: Loss = -11124.735173480181
2
Iteration 10600: Loss = -11124.735000929115
3
Iteration 10700: Loss = -11124.734965291396
4
Iteration 10800: Loss = -11124.747868575747
5
Stopping early at iteration 10800 due to no improvement.
pi: tensor([[9.2772e-01, 7.2279e-02],
        [9.9992e-01, 7.7330e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9913e-01, 8.7154e-04], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1694, 0.1806],
         [0.5952, 0.1145]],

        [[0.5232, 0.1460],
         [0.5393, 0.6571]],

        [[0.6882, 0.1345],
         [0.6145, 0.5900]],

        [[0.6830, 0.1381],
         [0.5602, 0.6956]],

        [[0.7109, 0.1212],
         [0.7070, 0.6766]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.002442591574126975
Global Adjusted Rand Index: 3.206515228510384e-05
Average Adjusted Rand Index: 0.000488518314825395
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22225.11767049507
Iteration 100: Loss = -11127.027416582585
Iteration 200: Loss = -11125.978553419776
Iteration 300: Loss = -11125.623379133222
Iteration 400: Loss = -11125.33757394723
Iteration 500: Loss = -11124.965809524996
Iteration 600: Loss = -11124.709500049845
Iteration 700: Loss = -11124.48430350517
Iteration 800: Loss = -11124.260173335619
Iteration 900: Loss = -11124.059097089395
Iteration 1000: Loss = -11123.886640813998
Iteration 1100: Loss = -11123.73956257913
Iteration 1200: Loss = -11123.607690040815
Iteration 1300: Loss = -11123.491677562686
Iteration 1400: Loss = -11123.391000121514
Iteration 1500: Loss = -11123.303205072774
Iteration 1600: Loss = -11123.226209912842
Iteration 1700: Loss = -11123.155362364736
Iteration 1800: Loss = -11123.089020915002
Iteration 1900: Loss = -11123.030896710608
Iteration 2000: Loss = -11122.984963476298
Iteration 2100: Loss = -11122.950393743164
Iteration 2200: Loss = -11122.92493175269
Iteration 2300: Loss = -11122.909020915537
Iteration 2400: Loss = -11122.900351061731
Iteration 2500: Loss = -11122.895690931622
Iteration 2600: Loss = -11122.89299499754
Iteration 2700: Loss = -11122.891344998272
Iteration 2800: Loss = -11122.890115707669
Iteration 2900: Loss = -11122.889076427158
Iteration 3000: Loss = -11122.888108755928
Iteration 3100: Loss = -11122.88725634166
Iteration 3200: Loss = -11122.886376441593
Iteration 3300: Loss = -11122.885565218368
Iteration 3400: Loss = -11122.884835242972
Iteration 3500: Loss = -11122.884077954748
Iteration 3600: Loss = -11122.883387226915
Iteration 3700: Loss = -11122.882679157308
Iteration 3800: Loss = -11122.882031807694
Iteration 3900: Loss = -11122.881422304155
Iteration 4000: Loss = -11122.880817895999
Iteration 4100: Loss = -11122.890000716578
1
Iteration 4200: Loss = -11122.879718317374
Iteration 4300: Loss = -11122.879163490532
Iteration 4400: Loss = -11122.890544800968
1
Iteration 4500: Loss = -11122.878227717832
Iteration 4600: Loss = -11122.87779359271
Iteration 4700: Loss = -11122.878023966396
1
Iteration 4800: Loss = -11122.876899089919
Iteration 4900: Loss = -11122.877668793182
1
Iteration 5000: Loss = -11122.876160556061
Iteration 5100: Loss = -11122.87599153416
Iteration 5200: Loss = -11122.875481542977
Iteration 5300: Loss = -11122.876953199971
1
Iteration 5400: Loss = -11122.874832923997
Iteration 5500: Loss = -11122.87493152491
1
Iteration 5600: Loss = -11122.874297150252
Iteration 5700: Loss = -11122.874628353307
1
Iteration 5800: Loss = -11122.87382051284
Iteration 5900: Loss = -11122.873819472037
Iteration 6000: Loss = -11122.873272309404
Iteration 6100: Loss = -11122.87310422786
Iteration 6200: Loss = -11122.872911006101
Iteration 6300: Loss = -11122.872714914176
Iteration 6400: Loss = -11122.872534369219
Iteration 6500: Loss = -11122.872365747562
Iteration 6600: Loss = -11122.8728203454
1
Iteration 6700: Loss = -11122.872084667186
Iteration 6800: Loss = -11122.872557764938
1
Iteration 6900: Loss = -11122.87176294683
Iteration 7000: Loss = -11122.871669954699
Iteration 7100: Loss = -11122.87151099091
Iteration 7200: Loss = -11122.871374416292
Iteration 7300: Loss = -11122.871265508715
Iteration 7400: Loss = -11122.871161827958
Iteration 7500: Loss = -11122.871036617851
Iteration 7600: Loss = -11122.871189643083
1
Iteration 7700: Loss = -11122.90886129941
2
Iteration 7800: Loss = -11122.870802239308
Iteration 7900: Loss = -11122.870998656095
1
Iteration 8000: Loss = -11122.933121129276
2
Iteration 8100: Loss = -11122.87052921948
Iteration 8200: Loss = -11122.871751948673
1
Iteration 8300: Loss = -11122.870419081322
Iteration 8400: Loss = -11122.887716808082
1
Iteration 8500: Loss = -11122.870278647986
Iteration 8600: Loss = -11122.870311097126
1
Iteration 8700: Loss = -11122.870145556522
Iteration 8800: Loss = -11122.870075909714
Iteration 8900: Loss = -11122.870368310278
1
Iteration 9000: Loss = -11122.87004603705
Iteration 9100: Loss = -11122.869949488035
Iteration 9200: Loss = -11122.869899997775
Iteration 9300: Loss = -11122.870538887217
1
Iteration 9400: Loss = -11122.869801866253
Iteration 9500: Loss = -11122.869794300961
Iteration 9600: Loss = -11122.925688521034
1
Iteration 9700: Loss = -11122.869730003757
Iteration 9800: Loss = -11122.869673689622
Iteration 9900: Loss = -11122.88649266781
1
Iteration 10000: Loss = -11122.869595079288
Iteration 10100: Loss = -11122.869641044636
1
Iteration 10200: Loss = -11122.869575693901
Iteration 10300: Loss = -11122.870379474141
1
Iteration 10400: Loss = -11122.869538598747
Iteration 10500: Loss = -11122.869479322946
Iteration 10600: Loss = -11122.870414403513
1
Iteration 10700: Loss = -11122.892519360412
2
Iteration 10800: Loss = -11122.86946124583
Iteration 10900: Loss = -11122.870668972058
1
Iteration 11000: Loss = -11122.869449466585
Iteration 11100: Loss = -11122.870285843142
1
Iteration 11200: Loss = -11122.869422240043
Iteration 11300: Loss = -11122.869647834174
1
Iteration 11400: Loss = -11122.869566387244
2
Iteration 11500: Loss = -11122.869851323334
3
Iteration 11600: Loss = -11123.044681858071
4
Iteration 11700: Loss = -11122.86932577722
Iteration 11800: Loss = -11122.869684644773
1
Iteration 11900: Loss = -11122.883721640244
2
Iteration 12000: Loss = -11122.869322437953
Iteration 12100: Loss = -11122.869355544432
1
Iteration 12200: Loss = -11122.869337334034
2
Iteration 12300: Loss = -11122.869270726254
Iteration 12400: Loss = -11122.869364981809
1
Iteration 12500: Loss = -11122.869259665713
Iteration 12600: Loss = -11122.869358286476
1
Iteration 12700: Loss = -11122.86918794485
Iteration 12800: Loss = -11122.869382952165
1
Iteration 12900: Loss = -11122.86976179991
2
Iteration 13000: Loss = -11122.873483081985
3
Iteration 13100: Loss = -11122.870972748999
4
Iteration 13200: Loss = -11122.869300617107
5
Stopping early at iteration 13200 due to no improvement.
pi: tensor([[9.3474e-01, 6.5263e-02],
        [9.9985e-01, 1.5473e-04]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8108, 0.1892], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1611, 0.1829],
         [0.5440, 0.2142]],

        [[0.6087, 0.1672],
         [0.5409, 0.5556]],

        [[0.5584, 0.2361],
         [0.6715, 0.7063]],

        [[0.5722, 0.1723],
         [0.6255, 0.7015]],

        [[0.5457, 0.1971],
         [0.6152, 0.5024]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: -0.014100938522947548
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 9.6196983717923e-05
Average Adjusted Rand Index: -0.0028201877045895096
11064.267104828867
[9.6196983717923e-05, 9.6196983717923e-05, 3.206515228510384e-05, 9.6196983717923e-05] [-0.0028201877045895096, -0.0028201877045895096, 0.000488518314825395, -0.0028201877045895096] [11122.870882308915, 11122.906065747655, 11124.747868575747, 11122.869300617107]
-----------------------------------------------------------------------------------------
This iteration is 5
True Objective function: Loss = -10938.1728503145
Iteration 0: Loss = -11051.287744539137
Iteration 10: Loss = -11051.28774527727
1
Iteration 20: Loss = -11051.287745767919
2
Iteration 30: Loss = -11051.287564992756
Iteration 40: Loss = -11051.269827210032
Iteration 50: Loss = -11050.859857598683
Iteration 60: Loss = -11050.76496333665
Iteration 70: Loss = -11050.528883551973
Iteration 80: Loss = -11046.569319551092
Iteration 90: Loss = -11045.899603426193
Iteration 100: Loss = -11045.898756725743
Iteration 110: Loss = -11045.90343803137
1
Iteration 120: Loss = -11045.904432748135
2
Iteration 130: Loss = -11045.904589421349
3
Stopping early at iteration 130 due to no improvement.
pi: tensor([[9.5513e-01, 4.4871e-02],
        [1.0000e+00, 5.1346e-46]], dtype=torch.float64)
alpha: tensor([0.9580, 0.0420])
beta: tensor([[[0.1621, 0.2322],
         [0.7561, 0.2154]],

        [[0.3291, 0.1442],
         [0.4142, 0.2534]],

        [[0.3903, 0.0698],
         [0.1645, 0.7369]],

        [[0.5727, 0.1962],
         [0.1372, 0.5813]],

        [[0.8152, 0.1894],
         [0.4142, 0.1318]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 47
Adjusted Rand Index: 0.002427096100870114
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 6.489882528926777e-05
Average Adjusted Rand Index: 0.00032849619252034234
Iteration 0: Loss = -11262.252555808012
Iteration 10: Loss = -11050.273023272663
Iteration 20: Loss = -11050.243254635716
Iteration 30: Loss = -11050.09246847433
Iteration 40: Loss = -11049.643708208498
Iteration 50: Loss = -11049.243529278629
Iteration 60: Loss = -11049.13531801224
Iteration 70: Loss = -11049.115149502584
Iteration 80: Loss = -11049.110971484095
Iteration 90: Loss = -11049.109965318528
Iteration 100: Loss = -11049.109484678898
Iteration 110: Loss = -11049.109165082888
Iteration 120: Loss = -11049.10888947304
Iteration 130: Loss = -11049.108541436637
Iteration 140: Loss = -11049.108197010984
Iteration 150: Loss = -11049.10786095614
Iteration 160: Loss = -11049.10750573651
Iteration 170: Loss = -11049.107187819238
Iteration 180: Loss = -11049.10682273527
Iteration 190: Loss = -11049.10650559666
Iteration 200: Loss = -11049.106229130177
Iteration 210: Loss = -11049.10596513916
Iteration 220: Loss = -11049.105752553874
Iteration 230: Loss = -11049.10553876038
Iteration 240: Loss = -11049.105450460856
Iteration 250: Loss = -11049.105330044278
Iteration 260: Loss = -11049.105339985595
1
Iteration 270: Loss = -11049.105391237612
2
Iteration 280: Loss = -11049.10549851201
3
Stopping early at iteration 280 due to no improvement.
pi: tensor([[0.8940, 0.1060],
        [0.7434, 0.2566]], dtype=torch.float64)
alpha: tensor([0.8752, 0.1248])
beta: tensor([[[0.1554, 0.1959],
         [0.1831, 0.2228]],

        [[0.6604, 0.1747],
         [0.0271, 0.7945]],

        [[0.5157, 0.1686],
         [0.7087, 0.4656]],

        [[0.9767, 0.1885],
         [0.4175, 0.0937]],

        [[0.1449, 0.1869],
         [0.4787, 0.9296]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 50
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -3.1873015856761384e-05
Average Adjusted Rand Index: -0.00015692302765368048
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23082.237426171003
Iteration 100: Loss = -11053.184453211836
Iteration 200: Loss = -11051.752728869033
Iteration 300: Loss = -11051.331462946679
Iteration 400: Loss = -11051.132106199719
Iteration 500: Loss = -11050.821631875328
Iteration 600: Loss = -11049.390155336296
Iteration 700: Loss = -11048.930485256114
Iteration 800: Loss = -11048.748738412982
Iteration 900: Loss = -11048.591494240234
Iteration 1000: Loss = -11048.452385636021
Iteration 1100: Loss = -11048.324234849315
Iteration 1200: Loss = -11048.185046200068
Iteration 1300: Loss = -11047.973768758267
Iteration 1400: Loss = -11045.990764735945
Iteration 1500: Loss = -11040.342470608372
Iteration 1600: Loss = -11034.437620578583
Iteration 1700: Loss = -11032.117698362283
Iteration 1800: Loss = -11030.317303622096
Iteration 1900: Loss = -11023.22819972363
Iteration 2000: Loss = -10930.14970654318
Iteration 2100: Loss = -10915.896217460031
Iteration 2200: Loss = -10911.440128705119
Iteration 2300: Loss = -10910.932650260302
Iteration 2400: Loss = -10910.21796196802
Iteration 2500: Loss = -10909.019474851326
Iteration 2600: Loss = -10908.891382292028
Iteration 2700: Loss = -10908.218412565948
Iteration 2800: Loss = -10908.158972675958
Iteration 2900: Loss = -10908.128999660776
Iteration 3000: Loss = -10908.103161248604
Iteration 3100: Loss = -10908.06021595713
Iteration 3200: Loss = -10908.020346873456
Iteration 3300: Loss = -10907.949191957854
Iteration 3400: Loss = -10907.931215763065
Iteration 3500: Loss = -10907.917796535385
Iteration 3600: Loss = -10907.910586450296
Iteration 3700: Loss = -10907.9082142498
Iteration 3800: Loss = -10907.902547049176
Iteration 3900: Loss = -10907.898997260916
Iteration 4000: Loss = -10907.896497079897
Iteration 4100: Loss = -10907.891356525013
Iteration 4200: Loss = -10907.889460261687
Iteration 4300: Loss = -10907.888801194607
Iteration 4400: Loss = -10907.882839070007
Iteration 4500: Loss = -10907.8840620571
1
Iteration 4600: Loss = -10907.879943320195
Iteration 4700: Loss = -10907.878271378493
Iteration 4800: Loss = -10907.8582121662
Iteration 4900: Loss = -10907.693907548542
Iteration 5000: Loss = -10907.55043567747
Iteration 5100: Loss = -10907.548605052809
Iteration 5200: Loss = -10907.547026524768
Iteration 5300: Loss = -10907.546187043661
Iteration 5400: Loss = -10907.543696667599
Iteration 5500: Loss = -10907.539655973844
Iteration 5600: Loss = -10907.535250553714
Iteration 5700: Loss = -10907.53107588671
Iteration 5800: Loss = -10907.528703705917
Iteration 5900: Loss = -10907.526200062255
Iteration 6000: Loss = -10907.484860266499
Iteration 6100: Loss = -10907.48690903822
1
Iteration 6200: Loss = -10907.483732446648
Iteration 6300: Loss = -10907.48279379026
Iteration 6400: Loss = -10907.482184885486
Iteration 6500: Loss = -10907.479018608774
Iteration 6600: Loss = -10907.474523030382
Iteration 6700: Loss = -10907.473368886202
Iteration 6800: Loss = -10907.469537024854
Iteration 6900: Loss = -10907.469339268278
Iteration 7000: Loss = -10907.469060563366
Iteration 7100: Loss = -10907.469959133045
1
Iteration 7200: Loss = -10907.468391539549
Iteration 7300: Loss = -10907.468029550413
Iteration 7400: Loss = -10907.467731486444
Iteration 7500: Loss = -10907.46755327735
Iteration 7600: Loss = -10907.468587864005
1
Iteration 7700: Loss = -10907.466430133405
Iteration 7800: Loss = -10907.474544643132
1
Iteration 7900: Loss = -10907.466259585031
Iteration 8000: Loss = -10907.465463750967
Iteration 8100: Loss = -10907.455026200194
Iteration 8200: Loss = -10907.516915528167
1
Iteration 8300: Loss = -10907.444855097105
Iteration 8400: Loss = -10907.446860319695
1
Iteration 8500: Loss = -10907.443979270161
Iteration 8600: Loss = -10907.450239201804
1
Iteration 8700: Loss = -10907.443660755514
Iteration 8800: Loss = -10907.443551836854
Iteration 8900: Loss = -10907.454048780239
1
Iteration 9000: Loss = -10907.44326169801
Iteration 9100: Loss = -10907.44274362454
Iteration 9200: Loss = -10907.426383800115
Iteration 9300: Loss = -10907.42166708895
Iteration 9400: Loss = -10907.433980223259
1
Iteration 9500: Loss = -10907.501116573449
2
Iteration 9600: Loss = -10907.420746428259
Iteration 9700: Loss = -10907.420398604418
Iteration 9800: Loss = -10907.426884941631
1
Iteration 9900: Loss = -10907.431248665696
2
Iteration 10000: Loss = -10907.420018461447
Iteration 10100: Loss = -10907.421872417879
1
Iteration 10200: Loss = -10907.420055013006
2
Iteration 10300: Loss = -10907.429672990538
3
Iteration 10400: Loss = -10907.420669930021
4
Iteration 10500: Loss = -10907.42322586894
5
Stopping early at iteration 10500 due to no improvement.
pi: tensor([[0.7805, 0.2195],
        [0.2192, 0.7808]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5076, 0.4924], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2520, 0.1098],
         [0.6693, 0.2009]],

        [[0.5750, 0.1025],
         [0.5020, 0.6132]],

        [[0.5565, 0.0988],
         [0.6822, 0.5722]],

        [[0.7142, 0.0921],
         [0.7272, 0.6300]],

        [[0.5430, 0.1067],
         [0.5496, 0.7167]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824127248460211
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208070336283601
time is 2
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 4
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9368975462072318
Average Adjusted Rand Index: 0.936642384392164
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21699.086698253726
Iteration 100: Loss = -11052.172814363166
Iteration 200: Loss = -11050.816813441721
Iteration 300: Loss = -11050.099013393557
Iteration 400: Loss = -11049.831790791353
Iteration 500: Loss = -11049.666413222221
Iteration 600: Loss = -11049.496303951468
Iteration 700: Loss = -11049.254153653743
Iteration 800: Loss = -11048.393740701633
Iteration 900: Loss = -11047.490553269847
Iteration 1000: Loss = -11046.977749048168
Iteration 1100: Loss = -11046.481644397467
Iteration 1200: Loss = -11044.973847772664
Iteration 1300: Loss = -11042.57616028644
Iteration 1400: Loss = -11037.796901332507
Iteration 1500: Loss = -11037.158140719057
Iteration 1600: Loss = -11034.46840504593
Iteration 1700: Loss = -11032.411931078257
Iteration 1800: Loss = -10973.796192620044
Iteration 1900: Loss = -10923.55295441888
Iteration 2000: Loss = -10914.506244661732
Iteration 2100: Loss = -10913.77435820223
Iteration 2200: Loss = -10912.42223012699
Iteration 2300: Loss = -10911.85315072735
Iteration 2400: Loss = -10910.909803840806
Iteration 2500: Loss = -10910.290817547186
Iteration 2600: Loss = -10910.243255379624
Iteration 2700: Loss = -10910.229166429837
Iteration 2800: Loss = -10910.217842999426
Iteration 2900: Loss = -10910.208849391958
Iteration 3000: Loss = -10910.198759919
Iteration 3100: Loss = -10910.14676473024
Iteration 3200: Loss = -10910.093638576585
Iteration 3300: Loss = -10910.071166736292
Iteration 3400: Loss = -10910.05677922312
Iteration 3500: Loss = -10910.049148109363
Iteration 3600: Loss = -10910.03431393089
Iteration 3700: Loss = -10910.03123925649
Iteration 3800: Loss = -10909.883874047255
Iteration 3900: Loss = -10909.89131383583
1
Iteration 4000: Loss = -10909.872776887669
Iteration 4100: Loss = -10909.873606624316
1
Iteration 4200: Loss = -10909.859291003408
Iteration 4300: Loss = -10909.844206665104
Iteration 4400: Loss = -10909.835600230484
Iteration 4500: Loss = -10909.841453362224
1
Iteration 4600: Loss = -10909.830327419011
Iteration 4700: Loss = -10909.180138297594
Iteration 4800: Loss = -10909.163656017257
Iteration 4900: Loss = -10909.162381248316
Iteration 5000: Loss = -10909.112844923384
Iteration 5100: Loss = -10909.114968579845
1
Iteration 5200: Loss = -10909.100686897564
Iteration 5300: Loss = -10909.100056286716
Iteration 5400: Loss = -10909.118910893696
1
Iteration 5500: Loss = -10909.081157554245
Iteration 5600: Loss = -10909.055260359431
Iteration 5700: Loss = -10909.053606299347
Iteration 5800: Loss = -10909.056974358795
1
Iteration 5900: Loss = -10909.050889967772
Iteration 6000: Loss = -10909.044125807848
Iteration 6100: Loss = -10909.043435986647
Iteration 6200: Loss = -10909.042866234757
Iteration 6300: Loss = -10909.034160256113
Iteration 6400: Loss = -10909.021943106922
Iteration 6500: Loss = -10909.021610797028
Iteration 6600: Loss = -10909.019126160412
Iteration 6700: Loss = -10908.874556213304
Iteration 6800: Loss = -10908.869493124117
Iteration 6900: Loss = -10908.867437958164
Iteration 7000: Loss = -10908.859450270747
Iteration 7100: Loss = -10908.871270320913
1
Iteration 7200: Loss = -10908.904282409318
2
Iteration 7300: Loss = -10908.865565034559
3
Iteration 7400: Loss = -10908.85654125832
Iteration 7500: Loss = -10908.856561261045
1
Iteration 7600: Loss = -10908.859171017531
2
Iteration 7700: Loss = -10908.856049401755
Iteration 7800: Loss = -10908.943141279204
1
Iteration 7900: Loss = -10908.856442946788
2
Iteration 8000: Loss = -10908.85570403159
Iteration 8100: Loss = -10908.855719797313
1
Iteration 8200: Loss = -10908.856764658085
2
Iteration 8300: Loss = -10908.854050129929
Iteration 8400: Loss = -10908.846922882298
Iteration 8500: Loss = -10908.84688058426
Iteration 8600: Loss = -10908.846807742713
Iteration 8700: Loss = -10908.84707721517
1
Iteration 8800: Loss = -10908.846742950043
Iteration 8900: Loss = -10908.846255638524
Iteration 9000: Loss = -10908.845310825804
Iteration 9100: Loss = -10908.844912700242
Iteration 9200: Loss = -10908.849453995725
1
Iteration 9300: Loss = -10908.829199530084
Iteration 9400: Loss = -10908.828488958277
Iteration 9500: Loss = -10908.835452294834
1
Iteration 9600: Loss = -10908.828340845599
Iteration 9700: Loss = -10908.83334910508
1
Iteration 9800: Loss = -10908.834069183995
2
Iteration 9900: Loss = -10908.828587204409
3
Iteration 10000: Loss = -10908.828717449456
4
Iteration 10100: Loss = -10908.829597506416
5
Stopping early at iteration 10100 due to no improvement.
pi: tensor([[0.7870, 0.2130],
        [0.2084, 0.7916]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4890, 0.5110], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2014, 0.1095],
         [0.7010, 0.2497]],

        [[0.5724, 0.1027],
         [0.5171, 0.5501]],

        [[0.6226, 0.1004],
         [0.7111, 0.5890]],

        [[0.6537, 0.0919],
         [0.6152, 0.7179]],

        [[0.6171, 0.1071],
         [0.6629, 0.6893]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824127248460211
time is 1
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208070336283601
time is 2
tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8448351863643042
time is 3
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
time is 4
tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.90611588536808
Average Adjusted Rand Index: 0.9057706768502864
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21130.509017795677
Iteration 100: Loss = -11051.313087607896
Iteration 200: Loss = -11050.324897358427
Iteration 300: Loss = -11049.74521006484
Iteration 400: Loss = -11049.559221017606
Iteration 500: Loss = -11049.43018854762
Iteration 600: Loss = -11049.29540682861
Iteration 700: Loss = -11049.120973614785
Iteration 800: Loss = -11048.880565207828
Iteration 900: Loss = -11048.544573499808
Iteration 1000: Loss = -11047.345194515594
Iteration 1100: Loss = -11043.128365952674
Iteration 1200: Loss = -11036.141782199582
Iteration 1300: Loss = -11034.407879917475
Iteration 1400: Loss = -11031.56405598312
Iteration 1500: Loss = -11002.785872015005
Iteration 1600: Loss = -10925.523575714296
Iteration 1700: Loss = -10916.177348715939
Iteration 1800: Loss = -10911.926440599718
Iteration 1900: Loss = -10911.604876257858
Iteration 2000: Loss = -10911.381030392111
Iteration 2100: Loss = -10909.683202515133
Iteration 2200: Loss = -10908.520095531543
Iteration 2300: Loss = -10907.946770575452
Iteration 2400: Loss = -10907.745384807873
Iteration 2500: Loss = -10907.710534301163
Iteration 2600: Loss = -10907.663657433624
Iteration 2700: Loss = -10907.627344520914
Iteration 2800: Loss = -10907.617910640085
Iteration 2900: Loss = -10907.610338656554
Iteration 3000: Loss = -10907.603444928563
Iteration 3100: Loss = -10907.601283876453
Iteration 3200: Loss = -10907.581591395807
Iteration 3300: Loss = -10907.578179733522
Iteration 3400: Loss = -10907.577721866452
Iteration 3500: Loss = -10907.573546642807
Iteration 3600: Loss = -10907.57255961636
Iteration 3700: Loss = -10907.572456653641
Iteration 3800: Loss = -10907.574561306257
1
Iteration 3900: Loss = -10907.571311639018
Iteration 4000: Loss = -10907.568668153743
Iteration 4100: Loss = -10907.568159516084
Iteration 4200: Loss = -10907.56702002858
Iteration 4300: Loss = -10907.561420468535
Iteration 4400: Loss = -10907.556871407505
Iteration 4500: Loss = -10907.555862638192
Iteration 4600: Loss = -10907.554408728098
Iteration 4700: Loss = -10907.555564628661
1
Iteration 4800: Loss = -10907.552968610255
Iteration 4900: Loss = -10907.552615192424
Iteration 5000: Loss = -10907.551922084589
Iteration 5100: Loss = -10907.552598102235
1
Iteration 5200: Loss = -10907.550741589564
Iteration 5300: Loss = -10907.547234783891
Iteration 5400: Loss = -10907.537684882074
Iteration 5500: Loss = -10907.496466437418
Iteration 5600: Loss = -10907.495908565055
Iteration 5700: Loss = -10907.49568418526
Iteration 5800: Loss = -10907.496242861485
1
Iteration 5900: Loss = -10907.505598493457
2
Iteration 6000: Loss = -10907.494329116163
Iteration 6100: Loss = -10907.493906666625
Iteration 6200: Loss = -10907.503083926465
1
Iteration 6300: Loss = -10907.493593395264
Iteration 6400: Loss = -10907.493246944361
Iteration 6500: Loss = -10907.492300960606
Iteration 6600: Loss = -10907.491987722657
Iteration 6700: Loss = -10907.491779751079
Iteration 6800: Loss = -10907.500295964166
1
Iteration 6900: Loss = -10907.496835164085
2
Iteration 7000: Loss = -10907.491100042971
Iteration 7100: Loss = -10907.494528297213
1
Iteration 7200: Loss = -10907.488158708305
Iteration 7300: Loss = -10907.487711480384
Iteration 7400: Loss = -10907.487583116597
Iteration 7500: Loss = -10907.487285800076
Iteration 7600: Loss = -10907.48693148799
Iteration 7700: Loss = -10907.477430788967
Iteration 7800: Loss = -10907.477864835791
1
Iteration 7900: Loss = -10907.476382988789
Iteration 8000: Loss = -10907.476288174708
Iteration 8100: Loss = -10907.476202025233
Iteration 8200: Loss = -10907.476092771009
Iteration 8300: Loss = -10907.489761997458
1
Iteration 8400: Loss = -10907.475997891419
Iteration 8500: Loss = -10907.475941414792
Iteration 8600: Loss = -10907.47596716339
1
Iteration 8700: Loss = -10907.475918006645
Iteration 8800: Loss = -10907.488299849187
1
Iteration 8900: Loss = -10907.475877881152
Iteration 9000: Loss = -10907.483303559065
1
Iteration 9100: Loss = -10907.47576758929
Iteration 9200: Loss = -10907.476287686251
1
Iteration 9300: Loss = -10907.475947210978
2
Iteration 9400: Loss = -10907.47556673342
Iteration 9500: Loss = -10907.478849068224
1
Iteration 9600: Loss = -10907.47569210329
2
Iteration 9700: Loss = -10907.474827723405
Iteration 9800: Loss = -10907.475138918555
1
Iteration 9900: Loss = -10907.490178218146
2
Iteration 10000: Loss = -10907.532048520872
3
Iteration 10100: Loss = -10907.474676276346
Iteration 10200: Loss = -10907.474753174554
1
Iteration 10300: Loss = -10907.50969737239
2
Iteration 10400: Loss = -10907.45585126057
Iteration 10500: Loss = -10907.455249587609
Iteration 10600: Loss = -10907.444389547354
Iteration 10700: Loss = -10907.445443881978
1
Iteration 10800: Loss = -10907.456384368577
2
Iteration 10900: Loss = -10907.443868601642
Iteration 11000: Loss = -10907.454889794553
1
Iteration 11100: Loss = -10907.52985434803
2
Iteration 11200: Loss = -10907.43922816482
Iteration 11300: Loss = -10907.440418527469
1
Iteration 11400: Loss = -10907.430940082324
Iteration 11500: Loss = -10907.427918982918
Iteration 11600: Loss = -10907.427743437142
Iteration 11700: Loss = -10907.428329834709
1
Iteration 11800: Loss = -10907.432892978784
2
Iteration 11900: Loss = -10907.431408118657
3
Iteration 12000: Loss = -10907.425665790643
Iteration 12100: Loss = -10907.425659815695
Iteration 12200: Loss = -10907.495825604832
1
Iteration 12300: Loss = -10907.426871745574
2
Iteration 12400: Loss = -10907.426342332188
3
Iteration 12500: Loss = -10907.440863500184
4
Iteration 12600: Loss = -10907.523889750675
5
Stopping early at iteration 12600 due to no improvement.
pi: tensor([[0.7834, 0.2166],
        [0.2203, 0.7797]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5127, 0.4873], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2500, 0.1089],
         [0.7242, 0.2025]],

        [[0.6223, 0.1031],
         [0.5678, 0.7244]],

        [[0.6670, 0.0981],
         [0.5420, 0.6337]],

        [[0.5260, 0.0912],
         [0.5919, 0.5880]],

        [[0.5566, 0.1068],
         [0.6874, 0.6867]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824127248460211
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208070336283601
time is 2
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 4
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9368975462072318
Average Adjusted Rand Index: 0.936642384392164
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24975.24011472193
Iteration 100: Loss = -11050.447031283105
Iteration 200: Loss = -11048.699346433363
Iteration 300: Loss = -11047.192658876947
Iteration 400: Loss = -11045.108887543185
Iteration 500: Loss = -10938.575912841612
Iteration 600: Loss = -10908.319714306612
Iteration 700: Loss = -10907.650729367482
Iteration 800: Loss = -10907.553381542843
Iteration 900: Loss = -10907.50187608072
Iteration 1000: Loss = -10907.488605805174
Iteration 1100: Loss = -10907.479345534068
Iteration 1200: Loss = -10907.471725721602
Iteration 1300: Loss = -10907.45954547081
Iteration 1400: Loss = -10907.435744353003
Iteration 1500: Loss = -10907.430169397125
Iteration 1600: Loss = -10907.427262375295
Iteration 1700: Loss = -10907.425185160178
Iteration 1800: Loss = -10907.423492129557
Iteration 1900: Loss = -10907.422015977656
Iteration 2000: Loss = -10907.420687038737
Iteration 2100: Loss = -10907.419521068574
Iteration 2200: Loss = -10907.418434414372
Iteration 2300: Loss = -10907.41731465068
Iteration 2400: Loss = -10907.416025822833
Iteration 2500: Loss = -10907.414370169437
Iteration 2600: Loss = -10907.412853751986
Iteration 2700: Loss = -10907.411448896046
Iteration 2800: Loss = -10907.410546188967
Iteration 2900: Loss = -10907.410197777544
Iteration 3000: Loss = -10907.410301083688
1
Iteration 3100: Loss = -10907.409661347498
Iteration 3200: Loss = -10907.417063159546
1
Iteration 3300: Loss = -10907.409230338933
Iteration 3400: Loss = -10907.4097406081
1
Iteration 3500: Loss = -10907.408962206036
Iteration 3600: Loss = -10907.413645828816
1
Iteration 3700: Loss = -10907.409982209652
2
Iteration 3800: Loss = -10907.411480792804
3
Iteration 3900: Loss = -10907.411142842617
4
Iteration 4000: Loss = -10907.4086238398
Iteration 4100: Loss = -10907.408354603054
Iteration 4200: Loss = -10907.408353275236
Iteration 4300: Loss = -10907.409590270805
1
Iteration 4400: Loss = -10907.408468077507
2
Iteration 4500: Loss = -10907.408689951031
3
Iteration 4600: Loss = -10907.408636465301
4
Iteration 4700: Loss = -10907.408043484575
Iteration 4800: Loss = -10907.407978185867
Iteration 4900: Loss = -10907.411150976293
1
Iteration 5000: Loss = -10907.408397701834
2
Iteration 5100: Loss = -10907.407515932795
Iteration 5200: Loss = -10907.407695325881
1
Iteration 5300: Loss = -10907.407934703275
2
Iteration 5400: Loss = -10907.409025207447
3
Iteration 5500: Loss = -10907.411781934574
4
Iteration 5600: Loss = -10907.407911691598
5
Stopping early at iteration 5600 due to no improvement.
pi: tensor([[0.7809, 0.2191],
        [0.2198, 0.7802]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5088, 0.4912], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2516, 0.1096],
         [0.7043, 0.2011]],

        [[0.5573, 0.1025],
         [0.5294, 0.6749]],

        [[0.6741, 0.0988],
         [0.6612, 0.6880]],

        [[0.6109, 0.0920],
         [0.6205, 0.6535]],

        [[0.6489, 0.1067],
         [0.5342, 0.6006]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824127248460211
time is 1
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208070336283601
time is 2
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599960816486464
time is 3
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599982760199767
time is 4
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
Global Adjusted Rand Index: 0.9368975462072318
Average Adjusted Rand Index: 0.936642384392164
10938.1728503145
[0.9368975462072318, 0.90611588536808, 0.9368975462072318, 0.9368975462072318] [0.936642384392164, 0.9057706768502864, 0.936642384392164, 0.936642384392164] [10907.42322586894, 10908.829597506416, 10907.523889750675, 10907.407911691598]
-----------------------------------------------------------------------------------------
This iteration is 6
True Objective function: Loss = -10804.705035829336
Iteration 0: Loss = -11100.323237244971
Iteration 10: Loss = -10909.338615493556
Iteration 20: Loss = -10909.31765922498
Iteration 30: Loss = -10909.308386023375
Iteration 40: Loss = -10909.303426247805
Iteration 50: Loss = -10909.300567037626
Iteration 60: Loss = -10909.298826667165
Iteration 70: Loss = -10909.297664268046
Iteration 80: Loss = -10909.296866355759
Iteration 90: Loss = -10909.296174571631
Iteration 100: Loss = -10909.295513742605
Iteration 110: Loss = -10909.294850755206
Iteration 120: Loss = -10909.294147860963
Iteration 130: Loss = -10909.293393883641
Iteration 140: Loss = -10909.292526606305
Iteration 150: Loss = -10909.29153792641
Iteration 160: Loss = -10909.290449201524
Iteration 170: Loss = -10909.289099538282
Iteration 180: Loss = -10909.28747635231
Iteration 190: Loss = -10909.285487554462
Iteration 200: Loss = -10909.282970983999
Iteration 210: Loss = -10909.27984735341
Iteration 220: Loss = -10909.275556693226
Iteration 230: Loss = -10909.269739919464
Iteration 240: Loss = -10909.261458467581
Iteration 250: Loss = -10909.249094371738
Iteration 260: Loss = -10909.229901904444
Iteration 270: Loss = -10909.198400378973
Iteration 280: Loss = -10909.145135962104
Iteration 290: Loss = -10909.055952973322
Iteration 300: Loss = -10908.923265783937
Iteration 310: Loss = -10908.77802947115
Iteration 320: Loss = -10908.683185879856
Iteration 330: Loss = -10908.65263429776
Iteration 340: Loss = -10908.65536254298
1
Iteration 350: Loss = -10908.668988433634
2
Iteration 360: Loss = -10908.685584645922
3
Stopping early at iteration 360 due to no improvement.
pi: tensor([[0.5733, 0.4267],
        [0.6125, 0.3875]], dtype=torch.float64)
alpha: tensor([0.5918, 0.4082])
beta: tensor([[[0.1438, 0.1552],
         [0.2340, 0.1835]],

        [[0.9133, 0.1593],
         [0.7550, 0.7489]],

        [[0.1299, 0.1644],
         [0.8067, 0.5432]],

        [[0.0315, 0.1695],
         [0.7910, 0.9268]],

        [[0.9633, 0.1602],
         [0.6446, 0.7199]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 72
Adjusted Rand Index: 0.18718695433106883
time is 1
tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 56
Adjusted Rand Index: -0.010454485805520318
time is 2
tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.00552400596663465
time is 3
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 58
Adjusted Rand Index: 0.017697285360478262
time is 4
tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.0329703390513439
Global Adjusted Rand Index: 0.04078643306445179
Average Adjusted Rand Index: 0.04658481978080107
Iteration 0: Loss = -11010.182586507319
Iteration 10: Loss = -10908.078455817466
Iteration 20: Loss = -10908.029324716357
Iteration 30: Loss = -10908.036657750235
1
Iteration 40: Loss = -10908.048527592247
2
Iteration 50: Loss = -10908.060463091031
3
Stopping early at iteration 50 due to no improvement.
pi: tensor([[0.1428, 0.8572],
        [0.1215, 0.8785]], dtype=torch.float64)
alpha: tensor([0.1227, 0.8773])
beta: tensor([[[0.2151, 0.1677],
         [0.0474, 0.1521]],

        [[0.7046, 0.1730],
         [0.8106, 0.7528]],

        [[0.4796, 0.1841],
         [0.9256, 0.7604]],

        [[0.1140, 0.2028],
         [0.3175, 0.2848]],

        [[0.5427, 0.1713],
         [0.9282, 0.2405]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 38
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.00877236457352431
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.004878730976372607
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0039055633266170944
Average Adjusted Rand Index: 0.002730219109979383
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23005.273538336776
Iteration 100: Loss = -10911.71004832797
Iteration 200: Loss = -10910.195311530622
Iteration 300: Loss = -10909.590831475338
Iteration 400: Loss = -10909.24507652322
Iteration 500: Loss = -10908.926911287954
Iteration 600: Loss = -10908.595855558287
Iteration 700: Loss = -10908.318755833998
Iteration 800: Loss = -10908.072692384103
Iteration 900: Loss = -10907.818317382209
Iteration 1000: Loss = -10907.606833875065
Iteration 1100: Loss = -10907.458475248739
Iteration 1200: Loss = -10907.35316826562
Iteration 1300: Loss = -10907.273652878648
Iteration 1400: Loss = -10907.208533163745
Iteration 1500: Loss = -10907.153970991445
Iteration 1600: Loss = -10907.108838779324
Iteration 1700: Loss = -10907.070908601696
Iteration 1800: Loss = -10907.038588085006
Iteration 1900: Loss = -10907.011119640028
Iteration 2000: Loss = -10906.988360662759
Iteration 2100: Loss = -10906.969908865036
Iteration 2200: Loss = -10906.954802173432
Iteration 2300: Loss = -10906.942230990206
Iteration 2400: Loss = -10906.931637118054
Iteration 2500: Loss = -10906.922676172324
Iteration 2600: Loss = -10906.914979484378
Iteration 2700: Loss = -10906.908357328824
Iteration 2800: Loss = -10906.902617718291
Iteration 2900: Loss = -10906.89771098093
Iteration 3000: Loss = -10906.893515280726
Iteration 3100: Loss = -10906.889994267642
Iteration 3200: Loss = -10906.88706997324
Iteration 3300: Loss = -10906.884584729498
Iteration 3400: Loss = -10906.882477044315
Iteration 3500: Loss = -10906.880690455222
Iteration 3600: Loss = -10906.879105850234
Iteration 3700: Loss = -10906.877780220286
Iteration 3800: Loss = -10906.876620487028
Iteration 3900: Loss = -10906.875561822388
Iteration 4000: Loss = -10906.874711767558
Iteration 4100: Loss = -10906.873982293211
Iteration 4200: Loss = -10906.873378405182
Iteration 4300: Loss = -10906.872836194685
Iteration 4400: Loss = -10906.87239472844
Iteration 4500: Loss = -10906.871936066873
Iteration 4600: Loss = -10906.871608856729
Iteration 4700: Loss = -10906.871275270925
Iteration 4800: Loss = -10906.870985296217
Iteration 4900: Loss = -10906.87075373371
Iteration 5000: Loss = -10906.870517518666
Iteration 5100: Loss = -10906.870273863982
Iteration 5200: Loss = -10906.870108949946
Iteration 5300: Loss = -10906.869917533477
Iteration 5400: Loss = -10906.86973044578
Iteration 5500: Loss = -10906.869542868772
Iteration 5600: Loss = -10906.869407666367
Iteration 5700: Loss = -10906.869218697664
Iteration 5800: Loss = -10906.869079733942
Iteration 5900: Loss = -10906.868940521188
Iteration 6000: Loss = -10906.868762965149
Iteration 6100: Loss = -10906.868623465065
Iteration 6200: Loss = -10906.868526022363
Iteration 6300: Loss = -10906.868257694736
Iteration 6400: Loss = -10906.868048920862
Iteration 6500: Loss = -10906.867869833892
Iteration 6600: Loss = -10906.86751243673
Iteration 6700: Loss = -10906.867071941648
Iteration 6800: Loss = -10906.869186164968
1
Iteration 6900: Loss = -10906.864407423063
Iteration 7000: Loss = -10906.855689960254
Iteration 7100: Loss = -10904.616343080748
Iteration 7200: Loss = -10899.381403019544
Iteration 7300: Loss = -10899.325259276664
Iteration 7400: Loss = -10899.311766145727
Iteration 7500: Loss = -10899.304362959427
Iteration 7600: Loss = -10899.317124794256
1
Iteration 7700: Loss = -10899.297135190072
Iteration 7800: Loss = -10899.295145535149
Iteration 7900: Loss = -10899.354802622478
1
Iteration 8000: Loss = -10899.291880427505
Iteration 8100: Loss = -10899.290265340933
Iteration 8200: Loss = -10899.28930577353
Iteration 8300: Loss = -10899.288491056639
Iteration 8400: Loss = -10899.386003809164
1
Iteration 8500: Loss = -10899.287256355507
Iteration 8600: Loss = -10899.28673986561
Iteration 8700: Loss = -10899.319832663226
1
Iteration 8800: Loss = -10899.285885481617
Iteration 8900: Loss = -10899.28553355191
Iteration 9000: Loss = -10899.286549890243
1
Iteration 9100: Loss = -10899.284868986764
Iteration 9200: Loss = -10899.284693110189
Iteration 9300: Loss = -10899.28433514994
Iteration 9400: Loss = -10899.284044214255
Iteration 9500: Loss = -10899.28431915696
1
Iteration 9600: Loss = -10899.285044851722
2
Iteration 9700: Loss = -10899.285197441883
3
Iteration 9800: Loss = -10899.283302087846
Iteration 9900: Loss = -10899.28587222139
1
Iteration 10000: Loss = -10899.283019973234
Iteration 10100: Loss = -10899.298410795083
1
Iteration 10200: Loss = -10899.282724585375
Iteration 10300: Loss = -10899.287233357574
1
Iteration 10400: Loss = -10899.282522942764
Iteration 10500: Loss = -10899.282408520003
Iteration 10600: Loss = -10899.282634747176
1
Iteration 10700: Loss = -10899.28221496618
Iteration 10800: Loss = -10899.282104426753
Iteration 10900: Loss = -10899.287727776287
1
Iteration 11000: Loss = -10899.281965761922
Iteration 11100: Loss = -10899.281888257914
Iteration 11200: Loss = -10899.283241496596
1
Iteration 11300: Loss = -10899.281745511587
Iteration 11400: Loss = -10899.281766599548
1
Iteration 11500: Loss = -10899.282371687465
2
Iteration 11600: Loss = -10899.28324830745
3
Iteration 11700: Loss = -10899.284604005066
4
Iteration 11800: Loss = -10899.281497705142
Iteration 11900: Loss = -10899.300907303774
1
Iteration 12000: Loss = -10899.281628504952
2
Iteration 12100: Loss = -10899.28979797906
3
Iteration 12200: Loss = -10899.344606224087
4
Iteration 12300: Loss = -10899.284225621677
5
Stopping early at iteration 12300 due to no improvement.
pi: tensor([[9.9964e-01, 3.6273e-04],
        [4.6952e-02, 9.5305e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([6.4585e-06, 9.9999e-01], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4152, 0.1732],
         [0.6718, 0.1582]],

        [[0.6811, 0.1766],
         [0.7116, 0.5223]],

        [[0.5834, 0.1684],
         [0.5710, 0.7200]],

        [[0.7024, 0.1836],
         [0.6947, 0.5687]],

        [[0.6106, 0.1267],
         [0.5756, 0.5196]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 39
Adjusted Rand Index: -0.007726325420627255
time is 2
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.006837151998402187
time is 3
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.04410228386827175
time is 4
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 33
Adjusted Rand Index: 0.11060776050587955
Global Adjusted Rand Index: 0.03350251737288285
Average Adjusted Rand Index: 0.030764174190385248
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22606.07485484127
Iteration 100: Loss = -10911.543101001018
Iteration 200: Loss = -10910.43562795038
Iteration 300: Loss = -10909.919956937238
Iteration 400: Loss = -10909.167894113993
Iteration 500: Loss = -10908.547935802633
Iteration 600: Loss = -10908.130168035892
Iteration 700: Loss = -10907.812380667898
Iteration 800: Loss = -10907.542160252697
Iteration 900: Loss = -10907.368178895775
Iteration 1000: Loss = -10907.25888274816
Iteration 1100: Loss = -10907.177220986723
Iteration 1200: Loss = -10907.113383231062
Iteration 1300: Loss = -10907.063795049751
Iteration 1400: Loss = -10907.025351666229
Iteration 1500: Loss = -10906.995641823361
Iteration 1600: Loss = -10906.972748316948
Iteration 1700: Loss = -10906.954793015178
Iteration 1800: Loss = -10906.940400753094
Iteration 1900: Loss = -10906.928763519238
Iteration 2000: Loss = -10906.91925251531
Iteration 2100: Loss = -10906.911351880766
Iteration 2200: Loss = -10906.90472126442
Iteration 2300: Loss = -10906.899210846757
Iteration 2400: Loss = -10906.894602049384
Iteration 2500: Loss = -10906.890917826639
Iteration 2600: Loss = -10906.887943789414
Iteration 2700: Loss = -10906.885405772138
Iteration 2800: Loss = -10906.883300855603
Iteration 2900: Loss = -10906.88159271006
Iteration 3000: Loss = -10906.880124444157
Iteration 3100: Loss = -10906.878772443257
Iteration 3200: Loss = -10906.877654247692
Iteration 3300: Loss = -10906.87665647011
Iteration 3400: Loss = -10906.87581047311
Iteration 3500: Loss = -10906.875046583917
Iteration 3600: Loss = -10906.874411836889
Iteration 3700: Loss = -10906.873838910265
Iteration 3800: Loss = -10906.873337955341
Iteration 3900: Loss = -10906.872906815124
Iteration 4000: Loss = -10906.87250165953
Iteration 4100: Loss = -10906.872100586912
Iteration 4200: Loss = -10906.871769923611
Iteration 4300: Loss = -10906.871452247231
Iteration 4400: Loss = -10906.871148661889
Iteration 4500: Loss = -10906.870960683036
Iteration 4600: Loss = -10906.870679774258
Iteration 4700: Loss = -10906.870437378948
Iteration 4800: Loss = -10906.87022558836
Iteration 4900: Loss = -10906.870013536549
Iteration 5000: Loss = -10906.86985114558
Iteration 5100: Loss = -10906.86962293583
Iteration 5200: Loss = -10906.869449384212
Iteration 5300: Loss = -10906.869263968314
Iteration 5400: Loss = -10906.869149647866
Iteration 5500: Loss = -10906.868946555152
Iteration 5600: Loss = -10906.868730472801
Iteration 5700: Loss = -10906.8687755706
1
Iteration 5800: Loss = -10906.868327384604
Iteration 5900: Loss = -10906.868096678507
Iteration 6000: Loss = -10906.867819586867
Iteration 6100: Loss = -10906.867424305592
Iteration 6200: Loss = -10906.866754059549
Iteration 6300: Loss = -10906.86540139704
Iteration 6400: Loss = -10906.860778549155
Iteration 6500: Loss = -10906.722232407763
Iteration 6600: Loss = -10899.463322173708
Iteration 6700: Loss = -10899.346762423658
Iteration 6800: Loss = -10899.315037445027
Iteration 6900: Loss = -10899.306837246791
Iteration 7000: Loss = -10899.30201719774
Iteration 7100: Loss = -10899.298951729492
Iteration 7200: Loss = -10899.296639701568
Iteration 7300: Loss = -10899.294852394643
Iteration 7400: Loss = -10899.293374218503
Iteration 7500: Loss = -10899.291815363598
Iteration 7600: Loss = -10899.290096731862
Iteration 7700: Loss = -10899.289222865062
Iteration 7800: Loss = -10899.288479382461
Iteration 7900: Loss = -10899.287830932508
Iteration 8000: Loss = -10899.287348426018
Iteration 8100: Loss = -10899.286950457135
Iteration 8200: Loss = -10899.286991774597
1
Iteration 8300: Loss = -10899.285922812125
Iteration 8400: Loss = -10899.285724359788
Iteration 8500: Loss = -10899.285250987421
Iteration 8600: Loss = -10899.2849009863
Iteration 8700: Loss = -10899.284966362338
1
Iteration 8800: Loss = -10899.284327799225
Iteration 8900: Loss = -10899.284454977711
1
Iteration 9000: Loss = -10899.283894864517
Iteration 9100: Loss = -10899.283682320254
Iteration 9200: Loss = -10899.283472853991
Iteration 9300: Loss = -10899.28362300198
1
Iteration 9400: Loss = -10899.283179104052
Iteration 9500: Loss = -10899.283010520103
Iteration 9600: Loss = -10899.28308260692
1
Iteration 9700: Loss = -10899.282702194
Iteration 9800: Loss = -10899.28261310508
Iteration 9900: Loss = -10899.284232489706
1
Iteration 10000: Loss = -10899.282396089398
Iteration 10100: Loss = -10899.282345770798
Iteration 10200: Loss = -10899.282685290138
1
Iteration 10300: Loss = -10899.282286114929
Iteration 10400: Loss = -10899.349733874147
1
Iteration 10500: Loss = -10899.28239261495
2
Iteration 10600: Loss = -10899.302989467984
3
Iteration 10700: Loss = -10899.285381464457
4
Iteration 10800: Loss = -10899.28200377704
Iteration 10900: Loss = -10899.28183555179
Iteration 11000: Loss = -10899.283034982775
1
Iteration 11100: Loss = -10899.28179302035
Iteration 11200: Loss = -10899.346444617731
1
Iteration 11300: Loss = -10899.281483860887
Iteration 11400: Loss = -10899.282539937944
1
Iteration 11500: Loss = -10899.301100493602
2
Iteration 11600: Loss = -10899.282105139539
3
Iteration 11700: Loss = -10899.317092859155
4
Iteration 11800: Loss = -10899.281307342408
Iteration 11900: Loss = -10899.281389911024
1
Iteration 12000: Loss = -10899.28124405701
Iteration 12100: Loss = -10899.281244120837
1
Iteration 12200: Loss = -10899.281149801453
Iteration 12300: Loss = -10899.282457425563
1
Iteration 12400: Loss = -10899.281105378424
Iteration 12500: Loss = -10899.299835236023
1
Iteration 12600: Loss = -10899.281051772596
Iteration 12700: Loss = -10899.282609801849
1
Iteration 12800: Loss = -10899.281242557225
2
Iteration 12900: Loss = -10899.281232101468
3
Iteration 13000: Loss = -10899.280998115712
Iteration 13100: Loss = -10899.285731686596
1
Iteration 13200: Loss = -10899.282636313665
2
Iteration 13300: Loss = -10899.289993298018
3
Iteration 13400: Loss = -10899.280935592966
Iteration 13500: Loss = -10899.281253585312
1
Iteration 13600: Loss = -10899.280968112771
2
Iteration 13700: Loss = -10899.281203844519
3
Iteration 13800: Loss = -10899.281565589368
4
Iteration 13900: Loss = -10899.2816580269
5
Stopping early at iteration 13900 due to no improvement.
pi: tensor([[9.9989e-01, 1.0880e-04],
        [4.7175e-02, 9.5283e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([2.2560e-06, 1.0000e+00], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.4152, 0.1735],
         [0.6972, 0.1579]],

        [[0.7125, 0.1768],
         [0.5113, 0.5259]],

        [[0.5452, 0.1686],
         [0.5106, 0.5026]],

        [[0.6501, 0.1833],
         [0.6291, 0.7299]],

        [[0.7276, 0.1268],
         [0.5875, 0.5259]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 39
Adjusted Rand Index: -0.007726325420627255
time is 2
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 41
Adjusted Rand Index: 0.006837151998402187
time is 3
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 39
Adjusted Rand Index: 0.04410228386827175
time is 4
tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 33
Adjusted Rand Index: 0.11060776050587955
Global Adjusted Rand Index: 0.03350251737288285
Average Adjusted Rand Index: 0.030764174190385248
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21655.896420500758
Iteration 100: Loss = -10911.814140806706
Iteration 200: Loss = -10910.6491871981
Iteration 300: Loss = -10910.280199323095
Iteration 400: Loss = -10910.124508710309
Iteration 500: Loss = -10910.009893625098
Iteration 600: Loss = -10909.7863515702
Iteration 700: Loss = -10909.119696615344
Iteration 800: Loss = -10908.352858581524
Iteration 900: Loss = -10907.858458509596
Iteration 1000: Loss = -10907.587289068193
Iteration 1100: Loss = -10907.416922875433
Iteration 1200: Loss = -10907.304575946637
Iteration 1300: Loss = -10907.222265690414
Iteration 1400: Loss = -10907.153629328552
Iteration 1500: Loss = -10907.100005391727
Iteration 1600: Loss = -10907.05950007134
Iteration 1700: Loss = -10907.027281004028
Iteration 1800: Loss = -10907.001262293541
Iteration 1900: Loss = -10906.97985160977
Iteration 2000: Loss = -10906.962066211107
Iteration 2100: Loss = -10906.94747898005
Iteration 2200: Loss = -10906.93528234815
Iteration 2300: Loss = -10906.924931913061
Iteration 2400: Loss = -10906.916194962885
Iteration 2500: Loss = -10906.909051253488
Iteration 2600: Loss = -10906.902873510975
Iteration 2700: Loss = -10906.897383627167
Iteration 2800: Loss = -10906.892442872973
Iteration 2900: Loss = -10906.888182290817
Iteration 3000: Loss = -10906.884602377679
Iteration 3100: Loss = -10906.881735013241
Iteration 3200: Loss = -10906.879481992068
Iteration 3300: Loss = -10906.877689243107
Iteration 3400: Loss = -10906.876314839705
Iteration 3500: Loss = -10906.875198424674
Iteration 3600: Loss = -10906.874190296918
Iteration 3700: Loss = -10906.87342242478
Iteration 3800: Loss = -10906.87273324329
Iteration 3900: Loss = -10906.872131417114
Iteration 4000: Loss = -10906.87159622825
Iteration 4100: Loss = -10906.871118688576
Iteration 4200: Loss = -10906.870735505945
Iteration 4300: Loss = -10906.87039226169
Iteration 4400: Loss = -10906.870095774271
Iteration 4500: Loss = -10906.869842648897
Iteration 4600: Loss = -10906.869631844318
Iteration 4700: Loss = -10906.869451958968
Iteration 4800: Loss = -10906.869246803617
Iteration 4900: Loss = -10906.869109257057
Iteration 5000: Loss = -10906.86895059869
Iteration 5100: Loss = -10906.868811129347
Iteration 5200: Loss = -10906.86871132752
Iteration 5300: Loss = -10906.868558697619
Iteration 5400: Loss = -10906.868426845804
Iteration 5500: Loss = -10906.868302540359
Iteration 5600: Loss = -10906.868116018442
Iteration 5700: Loss = -10906.867921673489
Iteration 5800: Loss = -10906.867715988992
Iteration 5900: Loss = -10906.86741388548
Iteration 6000: Loss = -10906.866988888769
Iteration 6100: Loss = -10906.866355141809
Iteration 6200: Loss = -10906.865093656394
Iteration 6300: Loss = -10906.861100127351
Iteration 6400: Loss = -10906.782283753777
Iteration 6500: Loss = -10899.471015469404
Iteration 6600: Loss = -10899.329464658394
Iteration 6700: Loss = -10899.311741669384
Iteration 6800: Loss = -10899.30429572841
Iteration 6900: Loss = -10899.300126924987
Iteration 7000: Loss = -10899.297546963786
Iteration 7100: Loss = -10899.295588944215
Iteration 7200: Loss = -10899.29409760347
Iteration 7300: Loss = -10899.292849234924
Iteration 7400: Loss = -10899.291844492922
Iteration 7500: Loss = -10899.290920038862
Iteration 7600: Loss = -10899.290150728548
Iteration 7700: Loss = -10899.289682649603
Iteration 7800: Loss = -10899.28889345333
Iteration 7900: Loss = -10899.288322606442
Iteration 8000: Loss = -10899.291608498352
1
Iteration 8100: Loss = -10899.287434268532
Iteration 8200: Loss = -10899.287416368272
Iteration 8300: Loss = -10899.286651442104
Iteration 8400: Loss = -10899.28613676168
Iteration 8500: Loss = -10899.284993280788
Iteration 8600: Loss = -10899.284414113607
Iteration 8700: Loss = -10899.298515652807
1
Iteration 8800: Loss = -10899.283960345878
Iteration 8900: Loss = -10899.284198126634
1
Iteration 9000: Loss = -10899.28628858745
2
Iteration 9100: Loss = -10899.283450270172
Iteration 9200: Loss = -10899.29982227534
1
Iteration 9300: Loss = -10899.28303937271
Iteration 9400: Loss = -10899.285167155653
1
Iteration 9500: Loss = -10899.282796389689
Iteration 9600: Loss = -10899.283332461095
1
Iteration 9700: Loss = -10899.282510594065
Iteration 9800: Loss = -10899.30518829303
1
Iteration 9900: Loss = -10899.282314268381
Iteration 10000: Loss = -10899.312273425068
1
Iteration 10100: Loss = -10899.282105236045
Iteration 10200: Loss = -10899.283321568415
1
Iteration 10300: Loss = -10899.281939196957
Iteration 10400: Loss = -10899.282971795134
1
Iteration 10500: Loss = -10899.281810533748
Iteration 10600: Loss = -10899.292912005214
1
Iteration 10700: Loss = -10899.281745589036
Iteration 10800: Loss = -10899.281614931833
Iteration 10900: Loss = -10899.283949689956
1
Iteration 11000: Loss = -10899.289537975681
2
Iteration 11100: Loss = -10899.283327411442
3
Iteration 11200: Loss = -10899.28172355605
4
Iteration 11300: Loss = -10899.281707874867
5
Stopping early at iteration 11300 due to no improvement.
pi: tensor([[9.5276e-01, 4.7243e-02],
        [4.7602e-04, 9.9952e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([9.9999e-01, 6.7870e-06], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1581, 0.1732],
         [0.6030, 0.4151]],

        [[0.5445, 0.1767],
         [0.5655, 0.5658]],

        [[0.6641, 0.1684],
         [0.6752, 0.7053]],

        [[0.6880, 0.1830],
         [0.6057, 0.6660]],

        [[0.7001, 0.1267],
         [0.6647, 0.6049]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 61
Adjusted Rand Index: -0.007726325420627255
time is 2
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.006837151998402187
time is 3
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 61
Adjusted Rand Index: 0.04410228386827175
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 67
Adjusted Rand Index: 0.11060776050587955
Global Adjusted Rand Index: 0.03350251737288285
Average Adjusted Rand Index: 0.030764174190385248
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19455.025222567132
Iteration 100: Loss = -10910.181909329163
Iteration 200: Loss = -10909.395072814716
Iteration 300: Loss = -10909.088779260108
Iteration 400: Loss = -10908.753437372188
Iteration 500: Loss = -10908.140268145702
Iteration 600: Loss = -10907.632523572509
Iteration 700: Loss = -10907.379061566548
Iteration 800: Loss = -10907.228698503313
Iteration 900: Loss = -10907.123819296086
Iteration 1000: Loss = -10907.05559728009
Iteration 1100: Loss = -10907.007371949228
Iteration 1200: Loss = -10906.972482625575
Iteration 1300: Loss = -10906.947357936095
Iteration 1400: Loss = -10906.9293361574
Iteration 1500: Loss = -10906.915931916543
Iteration 1600: Loss = -10906.905683527484
Iteration 1700: Loss = -10906.897739718275
Iteration 1800: Loss = -10906.89173431304
Iteration 1900: Loss = -10906.887184530146
Iteration 2000: Loss = -10906.883798583913
Iteration 2100: Loss = -10906.881115569158
Iteration 2200: Loss = -10906.878893795427
Iteration 2300: Loss = -10906.877007962466
Iteration 2400: Loss = -10906.875487816307
Iteration 2500: Loss = -10906.874237560185
Iteration 2600: Loss = -10906.87313607856
Iteration 2700: Loss = -10906.87230796933
Iteration 2800: Loss = -10906.871568515133
Iteration 2900: Loss = -10906.871030121347
Iteration 3000: Loss = -10906.87062956487
Iteration 3100: Loss = -10906.870287307038
Iteration 3200: Loss = -10906.86999270319
Iteration 3300: Loss = -10906.869760449981
Iteration 3400: Loss = -10906.869582101983
Iteration 3500: Loss = -10906.869375203913
Iteration 3600: Loss = -10906.869159393735
Iteration 3700: Loss = -10906.869008620022
Iteration 3800: Loss = -10906.868882972043
Iteration 3900: Loss = -10906.868715092589
Iteration 4000: Loss = -10906.868572689265
Iteration 4100: Loss = -10906.868416614529
Iteration 4200: Loss = -10906.868250397985
Iteration 4300: Loss = -10906.86811973517
Iteration 4400: Loss = -10906.867907916774
Iteration 4500: Loss = -10906.867681263268
Iteration 4600: Loss = -10906.867423210324
Iteration 4700: Loss = -10906.867026934537
Iteration 4800: Loss = -10906.866458386638
Iteration 4900: Loss = -10906.865359421034
Iteration 5000: Loss = -10906.862262143586
Iteration 5100: Loss = -10906.833129478773
Iteration 5200: Loss = -10900.121420143734
Iteration 5300: Loss = -10899.332400003064
Iteration 5400: Loss = -10899.313233729867
Iteration 5500: Loss = -10899.304262247679
Iteration 5600: Loss = -10899.299190372387
Iteration 5700: Loss = -10899.297132368416
Iteration 5800: Loss = -10899.29435505115
Iteration 5900: Loss = -10899.292185963564
Iteration 6000: Loss = -10899.29011791649
Iteration 6100: Loss = -10899.289054247934
Iteration 6200: Loss = -10899.288238341498
Iteration 6300: Loss = -10899.287625064091
Iteration 6400: Loss = -10899.286962346074
Iteration 6500: Loss = -10899.286435876686
Iteration 6600: Loss = -10899.285973182781
Iteration 6700: Loss = -10899.285564340144
Iteration 6800: Loss = -10899.288542978438
1
Iteration 6900: Loss = -10899.28488730239
Iteration 7000: Loss = -10899.284543125386
Iteration 7100: Loss = -10899.284591855467
1
Iteration 7200: Loss = -10899.284017580776
Iteration 7300: Loss = -10899.283794017052
Iteration 7400: Loss = -10899.283635161772
Iteration 7500: Loss = -10899.283408617037
Iteration 7600: Loss = -10899.283578507062
1
Iteration 7700: Loss = -10899.283060901304
Iteration 7800: Loss = -10899.282915330361
Iteration 7900: Loss = -10899.392312585645
1
Iteration 8000: Loss = -10899.282660985147
Iteration 8100: Loss = -10899.282521164718
Iteration 8200: Loss = -10899.283228462786
1
Iteration 8300: Loss = -10899.282301951416
Iteration 8400: Loss = -10899.282201472597
Iteration 8500: Loss = -10899.365343564348
1
Iteration 8600: Loss = -10899.28202089072
Iteration 8700: Loss = -10899.281925092782
Iteration 8800: Loss = -10899.326456096183
1
Iteration 8900: Loss = -10899.281778429297
Iteration 9000: Loss = -10899.292033667034
1
Iteration 9100: Loss = -10899.2817478069
Iteration 9200: Loss = -10899.281669918422
Iteration 9300: Loss = -10899.282930280238
1
Iteration 9400: Loss = -10899.287608711591
2
Iteration 9500: Loss = -10899.281450438733
Iteration 9600: Loss = -10899.285549682356
1
Iteration 9700: Loss = -10899.281377401945
Iteration 9800: Loss = -10899.292133900994
1
Iteration 9900: Loss = -10899.281275105139
Iteration 10000: Loss = -10899.28129576061
1
Iteration 10100: Loss = -10899.281324897422
2
Iteration 10200: Loss = -10899.281191611975
Iteration 10300: Loss = -10899.305914319915
1
Iteration 10400: Loss = -10899.281143570719
Iteration 10500: Loss = -10899.281108173827
Iteration 10600: Loss = -10899.281839793734
1
Iteration 10700: Loss = -10899.281093673397
Iteration 10800: Loss = -10899.281585808205
1
Iteration 10900: Loss = -10899.281032876122
Iteration 11000: Loss = -10899.281243148553
1
Iteration 11100: Loss = -10899.281414038862
2
Iteration 11200: Loss = -10899.282357786262
3
Iteration 11300: Loss = -10899.31110195949
4
Iteration 11400: Loss = -10899.280921667452
Iteration 11500: Loss = -10899.287803506168
1
Iteration 11600: Loss = -10899.282273905807
2
Iteration 11700: Loss = -10899.291153602037
3
Iteration 11800: Loss = -10899.28088771899
Iteration 11900: Loss = -10899.294656486467
1
Iteration 12000: Loss = -10899.280840830299
Iteration 12100: Loss = -10899.28120737842
1
Iteration 12200: Loss = -10899.310275296506
2
Iteration 12300: Loss = -10899.281064412435
3
Iteration 12400: Loss = -10899.28530116357
4
Iteration 12500: Loss = -10899.28084139052
5
Stopping early at iteration 12500 due to no improvement.
pi: tensor([[9.5284e-01, 4.7160e-02],
        [1.1366e-04, 9.9989e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([1.0000e+00, 1.3169e-06], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1580, 0.1739],
         [0.5769, 0.4152]],

        [[0.5298, 0.1767],
         [0.5739, 0.7160]],

        [[0.6642, 0.1685],
         [0.6274, 0.5251]],

        [[0.6152, 0.1831],
         [0.6444, 0.7107]],

        [[0.6968, 0.1268],
         [0.5686, 0.6476]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 61
Adjusted Rand Index: -0.007726325420627255
time is 2
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.006837151998402187
time is 3
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 61
Adjusted Rand Index: 0.04410228386827175
time is 4
tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
Difference count: 67
Adjusted Rand Index: 0.11060776050587955
Global Adjusted Rand Index: 0.03350251737288285
Average Adjusted Rand Index: 0.030764174190385248
10804.705035829336
[0.03350251737288285, 0.03350251737288285, 0.03350251737288285, 0.03350251737288285] [0.030764174190385248, 0.030764174190385248, 0.030764174190385248, 0.030764174190385248] [10899.284225621677, 10899.2816580269, 10899.281707874867, 10899.28084139052]
-----------------------------------------------------------------------------------------
This iteration is 7
True Objective function: Loss = -11074.546360656203
Iteration 0: Loss = -11212.590619790297
Iteration 10: Loss = -11212.590620123216
1
Iteration 20: Loss = -11212.590643073496
2
Iteration 30: Loss = -11212.590623771743
3
Stopping early at iteration 30 due to no improvement.
pi: tensor([[6.7621e-04, 9.9932e-01],
        [1.6962e-07, 1.0000e+00]], dtype=torch.float64)
alpha: tensor([1.5342e-07, 1.0000e+00])
beta: tensor([[[0.3181, 0.2046],
         [0.5417, 0.1665]],

        [[0.4632, 0.2655],
         [0.6975, 0.6967]],

        [[0.1120, 0.2191],
         [0.4687, 0.0625]],

        [[0.3041, 0.2509],
         [0.1509, 0.3358]],

        [[0.1241, 0.2623],
         [0.1816, 0.7317]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -11212.590619787466
Iteration 10: Loss = -11212.590619787466
1
Iteration 20: Loss = -11212.59061978748
2
Iteration 30: Loss = -11212.590619789089
3
Stopping early at iteration 30 due to no improvement.
pi: tensor([[1.0000e+00, 6.2743e-14],
        [1.0000e+00, 7.1260e-33]], dtype=torch.float64)
alpha: tensor([1.0000e+00, 5.6722e-14])
beta: tensor([[[0.1665, 0.2047],
         [0.0622, 0.3180]],

        [[0.0267, 0.2655],
         [0.6285, 0.2174]],

        [[0.7260, 0.2191],
         [0.5798, 0.1411]],

        [[0.7347, 0.2509],
         [0.1325, 0.6223]],

        [[0.6376, 0.2624],
         [0.8202, 0.6289]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24354.683356541824
Iteration 100: Loss = -11213.564416313435
Iteration 200: Loss = -11212.215236032187
Iteration 300: Loss = -11211.95985155903
Iteration 400: Loss = -11211.840750638428
Iteration 500: Loss = -11211.750472265634
Iteration 600: Loss = -11211.646370612552
Iteration 700: Loss = -11211.51807635424
Iteration 800: Loss = -11211.393349226964
Iteration 900: Loss = -11211.292749522101
Iteration 1000: Loss = -11211.135429758233
Iteration 1100: Loss = -11210.256060966682
Iteration 1200: Loss = -11209.976047885888
Iteration 1300: Loss = -11209.744257988745
Iteration 1400: Loss = -11209.45199049561
Iteration 1500: Loss = -11209.128158202191
Iteration 1600: Loss = -11208.899154938137
Iteration 1700: Loss = -11208.736841305245
Iteration 1800: Loss = -11205.586925934802
Iteration 1900: Loss = -11192.476428489217
Iteration 2000: Loss = -11082.676029191127
Iteration 2100: Loss = -11058.423549362637
Iteration 2200: Loss = -11053.08301510926
Iteration 2300: Loss = -11052.796783580638
Iteration 2400: Loss = -11052.70274149258
Iteration 2500: Loss = -11050.098034626171
Iteration 2600: Loss = -11050.046794947595
Iteration 2700: Loss = -11047.131724685627
Iteration 2800: Loss = -11047.113223292039
Iteration 2900: Loss = -11042.299280154693
Iteration 3000: Loss = -11042.296527282111
Iteration 3100: Loss = -11042.288319236786
Iteration 3200: Loss = -11042.275166355827
Iteration 3300: Loss = -11042.272501207037
Iteration 3400: Loss = -11042.263557148286
Iteration 3500: Loss = -11042.093672095625
Iteration 3600: Loss = -11042.040683712212
Iteration 3700: Loss = -11042.039660386443
Iteration 3800: Loss = -11042.038811371744
Iteration 3900: Loss = -11042.03813688215
Iteration 4000: Loss = -11042.036968353113
Iteration 4100: Loss = -11041.984783656093
Iteration 4200: Loss = -11038.536850700739
Iteration 4300: Loss = -11034.19470306032
Iteration 4400: Loss = -11034.179552043986
Iteration 4500: Loss = -11034.173133511204
Iteration 4600: Loss = -11034.17269223748
Iteration 4700: Loss = -11034.172339566747
Iteration 4800: Loss = -11034.171922076752
Iteration 4900: Loss = -11034.171416995066
Iteration 5000: Loss = -11034.171396955275
Iteration 5100: Loss = -11034.170919732656
Iteration 5200: Loss = -11034.167379900971
Iteration 5300: Loss = -11034.166748325633
Iteration 5400: Loss = -11034.162424247788
Iteration 5500: Loss = -11034.16219249291
Iteration 5600: Loss = -11034.161513957437
Iteration 5700: Loss = -11034.163349523957
1
Iteration 5800: Loss = -11034.164159073805
2
Iteration 5900: Loss = -11034.160204258253
Iteration 6000: Loss = -11034.160061844195
Iteration 6100: Loss = -11034.152016705626
Iteration 6200: Loss = -11034.151901145839
Iteration 6300: Loss = -11034.152210105909
1
Iteration 6400: Loss = -11034.151578833074
Iteration 6500: Loss = -11034.13217811101
Iteration 6600: Loss = -11034.129176884608
Iteration 6700: Loss = -11034.13081305493
1
Iteration 6800: Loss = -11034.12765840691
Iteration 6900: Loss = -11034.127518245106
Iteration 7000: Loss = -11034.12765680547
1
Iteration 7100: Loss = -11034.132610833212
2
Iteration 7200: Loss = -11034.127218361926
Iteration 7300: Loss = -11034.127086873732
Iteration 7400: Loss = -11034.127096631728
1
Iteration 7500: Loss = -11034.126934324453
Iteration 7600: Loss = -11034.188268122798
1
Iteration 7700: Loss = -11034.126828081186
Iteration 7800: Loss = -11034.12674527915
Iteration 7900: Loss = -11034.12690105648
1
Iteration 8000: Loss = -11034.126596847052
Iteration 8100: Loss = -11034.126482966776
Iteration 8200: Loss = -11034.12588742739
Iteration 8300: Loss = -11034.12188990941
Iteration 8400: Loss = -11034.118783261334
Iteration 8500: Loss = -11034.112978618583
Iteration 8600: Loss = -11034.112884178809
Iteration 8700: Loss = -11034.112794826638
Iteration 8800: Loss = -11034.112808734426
1
Iteration 8900: Loss = -11034.112742907231
Iteration 9000: Loss = -11034.410355914506
1
Iteration 9100: Loss = -11034.112598604157
Iteration 9200: Loss = -11034.112138279888
Iteration 9300: Loss = -11034.117655026113
1
Iteration 9400: Loss = -11034.11055153722
Iteration 9500: Loss = -11034.125363022295
1
Iteration 9600: Loss = -11034.109876842978
Iteration 9700: Loss = -11034.108832660088
Iteration 9800: Loss = -11034.104351538517
Iteration 9900: Loss = -11034.10424339029
Iteration 10000: Loss = -11034.116281496359
1
Iteration 10100: Loss = -11034.104092178024
Iteration 10200: Loss = -11034.104059041278
Iteration 10300: Loss = -11034.06622183335
Iteration 10400: Loss = -11034.064413656388
Iteration 10500: Loss = -11034.064410900573
Iteration 10600: Loss = -11034.064561541414
1
Iteration 10700: Loss = -11034.05631008509
Iteration 10800: Loss = -11033.24440924798
Iteration 10900: Loss = -11033.375915584007
1
Iteration 11000: Loss = -11033.225397746655
Iteration 11100: Loss = -11033.28007480594
1
Iteration 11200: Loss = -11033.225160858365
Iteration 11300: Loss = -11033.22513302246
Iteration 11400: Loss = -11033.270405485719
1
Iteration 11500: Loss = -11033.225134301583
2
Iteration 11600: Loss = -11033.225110259287
Iteration 11700: Loss = -11033.226956723065
1
Iteration 11800: Loss = -11033.225115778796
2
Iteration 11900: Loss = -11033.224967981518
Iteration 12000: Loss = -11033.684869281034
1
Iteration 12100: Loss = -11033.224962500777
Iteration 12200: Loss = -11033.224955666215
Iteration 12300: Loss = -11033.238523961367
1
Iteration 12400: Loss = -11033.224790060955
Iteration 12500: Loss = -11033.22481253294
1
Iteration 12600: Loss = -11033.224895833033
2
Iteration 12700: Loss = -11033.224656959217
Iteration 12800: Loss = -11033.239892737622
1
Iteration 12900: Loss = -11033.224519558857
Iteration 13000: Loss = -11033.224530030138
1
Iteration 13100: Loss = -11033.228942594948
2
Iteration 13200: Loss = -11033.224516391632
Iteration 13300: Loss = -11033.224426487535
Iteration 13400: Loss = -11033.224946576322
1
Iteration 13500: Loss = -11033.224284186108
Iteration 13600: Loss = -11033.22392043512
Iteration 13700: Loss = -11033.223406129693
Iteration 13800: Loss = -11033.223414603934
1
Iteration 13900: Loss = -11033.440025058044
2
Iteration 14000: Loss = -11033.223365339007
Iteration 14100: Loss = -11033.223398404514
1
Iteration 14200: Loss = -11033.402663973007
2
Iteration 14300: Loss = -11033.223303347571
Iteration 14400: Loss = -11033.223042194526
Iteration 14500: Loss = -11033.232363186737
1
Iteration 14600: Loss = -11033.223058536436
2
Iteration 14700: Loss = -11033.222745999747
Iteration 14800: Loss = -11033.24235467351
1
Iteration 14900: Loss = -11033.222748441156
2
Iteration 15000: Loss = -11033.22270801983
Iteration 15100: Loss = -11033.222577988805
Iteration 15200: Loss = -11033.222768419111
1
Iteration 15300: Loss = -11033.222584707097
2
Iteration 15400: Loss = -11033.222592456988
3
Iteration 15500: Loss = -11033.222915462056
4
Iteration 15600: Loss = -11033.222562689625
Iteration 15700: Loss = -11033.241953638972
1
Iteration 15800: Loss = -11033.222569072317
2
Iteration 15900: Loss = -11033.222538367325
Iteration 16000: Loss = -11033.30131278726
1
Iteration 16100: Loss = -11033.222546898038
2
Iteration 16200: Loss = -11033.222531687054
Iteration 16300: Loss = -11033.242823964807
1
Iteration 16400: Loss = -11033.222529869545
Iteration 16500: Loss = -11033.222527732136
Iteration 16600: Loss = -11033.229873404496
1
Iteration 16700: Loss = -11033.222527577693
Iteration 16800: Loss = -11033.22244201278
Iteration 16900: Loss = -11033.223200294766
1
Iteration 17000: Loss = -11033.222474438036
2
Iteration 17100: Loss = -11033.22244670943
3
Iteration 17200: Loss = -11033.222563125664
4
Iteration 17300: Loss = -11033.22237757996
Iteration 17400: Loss = -11033.222734078752
1
Iteration 17500: Loss = -11033.222368129815
Iteration 17600: Loss = -11033.222379964385
1
Iteration 17700: Loss = -11033.222373898656
2
Iteration 17800: Loss = -11033.222747339632
3
Iteration 17900: Loss = -11033.222373441451
4
Iteration 18000: Loss = -11033.222371329279
5
Stopping early at iteration 18000 due to no improvement.
pi: tensor([[0.7584, 0.2416],
        [0.3522, 0.6478]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5488, 0.4512], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2514, 0.0936],
         [0.7155, 0.2040]],

        [[0.5899, 0.0959],
         [0.7295, 0.6150]],

        [[0.6552, 0.0947],
         [0.6562, 0.5733]],

        [[0.5065, 0.1022],
         [0.5409, 0.7199]],

        [[0.6981, 0.1036],
         [0.6728, 0.6366]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824191749678101
time is 1
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 5
Adjusted Rand Index: 0.8079491294919076
time is 2
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 2
Adjusted Rand Index: 0.9206925302859384
time is 3
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8447901065451248
time is 4
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 5
Adjusted Rand Index: 0.8080808080808081
Global Adjusted Rand Index: 0.8533871406228608
Average Adjusted Rand Index: 0.8527863498743178
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21191.652702741292
Iteration 100: Loss = -11212.529204282644
Iteration 200: Loss = -11212.054268123271
Iteration 300: Loss = -11211.808867312631
Iteration 400: Loss = -11211.596047363335
Iteration 500: Loss = -11211.387311231421
Iteration 600: Loss = -11211.143399272518
Iteration 700: Loss = -11209.840363250749
Iteration 800: Loss = -11208.99358469849
Iteration 900: Loss = -11208.819781982713
Iteration 1000: Loss = -11208.725774704202
Iteration 1100: Loss = -11208.674186926997
Iteration 1200: Loss = -11208.632762593159
Iteration 1300: Loss = -11208.547617832453
Iteration 1400: Loss = -11207.319821424198
Iteration 1500: Loss = -11206.889108986914
Iteration 1600: Loss = -11206.819061808244
Iteration 1700: Loss = -11206.79286281098
Iteration 1800: Loss = -11206.779592180023
Iteration 1900: Loss = -11206.77169173277
Iteration 2000: Loss = -11206.766471047602
Iteration 2100: Loss = -11206.762677641529
Iteration 2200: Loss = -11206.759889856428
Iteration 2300: Loss = -11206.757626033783
Iteration 2400: Loss = -11206.75584656595
Iteration 2500: Loss = -11206.754339090032
Iteration 2600: Loss = -11206.75310091078
Iteration 2700: Loss = -11206.752011470713
Iteration 2800: Loss = -11206.751050040377
Iteration 2900: Loss = -11206.750280576549
Iteration 3000: Loss = -11206.749567568299
Iteration 3100: Loss = -11206.748902700572
Iteration 3200: Loss = -11206.74835386727
Iteration 3300: Loss = -11206.74783021316
Iteration 3400: Loss = -11206.74738382407
Iteration 3500: Loss = -11206.74699890071
Iteration 3600: Loss = -11206.746660063161
Iteration 3700: Loss = -11206.746276163178
Iteration 3800: Loss = -11206.74596340056
Iteration 3900: Loss = -11206.745693931367
Iteration 4000: Loss = -11206.745469331932
Iteration 4100: Loss = -11206.745214670971
Iteration 4200: Loss = -11206.744960762418
Iteration 4300: Loss = -11206.744786366844
Iteration 4400: Loss = -11206.74457525842
Iteration 4500: Loss = -11206.746034187472
1
Iteration 4600: Loss = -11206.744305089656
Iteration 4700: Loss = -11206.744082032184
Iteration 4800: Loss = -11206.743993476544
Iteration 4900: Loss = -11206.743820517713
Iteration 5000: Loss = -11206.743705806673
Iteration 5100: Loss = -11206.743574993452
Iteration 5200: Loss = -11206.743484974282
Iteration 5300: Loss = -11206.743366953397
Iteration 5400: Loss = -11206.743276009413
Iteration 5500: Loss = -11206.74320809906
Iteration 5600: Loss = -11206.743153559437
Iteration 5700: Loss = -11206.74305281131
Iteration 5800: Loss = -11206.743458179053
1
Iteration 5900: Loss = -11206.742888097044
Iteration 6000: Loss = -11206.743034320385
1
Iteration 6100: Loss = -11206.742790189724
Iteration 6200: Loss = -11206.742712846397
Iteration 6300: Loss = -11206.742690733547
Iteration 6400: Loss = -11206.742610114557
Iteration 6500: Loss = -11206.742716885932
1
Iteration 6600: Loss = -11206.742512201075
Iteration 6700: Loss = -11206.742760540714
1
Iteration 6800: Loss = -11206.742430166498
Iteration 6900: Loss = -11206.742379382498
Iteration 7000: Loss = -11206.74234155516
Iteration 7100: Loss = -11206.746530028591
1
Iteration 7200: Loss = -11206.742288516507
Iteration 7300: Loss = -11206.751198739128
1
Iteration 7400: Loss = -11206.74224211252
Iteration 7500: Loss = -11206.742209566837
Iteration 7600: Loss = -11206.742147427374
Iteration 7700: Loss = -11206.742157193557
1
Iteration 7800: Loss = -11206.744441413392
2
Iteration 7900: Loss = -11206.742118330963
Iteration 8000: Loss = -11206.745022561205
1
Iteration 8100: Loss = -11206.742048232849
Iteration 8200: Loss = -11206.742111593341
1
Iteration 8300: Loss = -11206.742013860992
Iteration 8400: Loss = -11206.742109159275
1
Iteration 8500: Loss = -11206.742027366381
2
Iteration 8600: Loss = -11206.74377856583
3
Iteration 8700: Loss = -11206.741960809357
Iteration 8800: Loss = -11206.741937717068
Iteration 8900: Loss = -11206.742300446464
1
Iteration 9000: Loss = -11206.741906740846
Iteration 9100: Loss = -11206.741903715174
Iteration 9200: Loss = -11206.741896834183
Iteration 9300: Loss = -11206.741899264814
1
Iteration 9400: Loss = -11206.741875311802
Iteration 9500: Loss = -11206.755167849642
1
Iteration 9600: Loss = -11206.74190201884
2
Iteration 9700: Loss = -11206.74179730695
Iteration 9800: Loss = -11206.741826493557
1
Iteration 9900: Loss = -11206.749006667438
2
Iteration 10000: Loss = -11206.741806601964
3
Iteration 10100: Loss = -11206.74181302081
4
Iteration 10200: Loss = -11206.816410800951
5
Stopping early at iteration 10200 due to no improvement.
pi: tensor([[9.2854e-06, 9.9999e-01],
        [5.5754e-02, 9.4425e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0195, 0.9805], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3015, 0.0662],
         [0.5881, 0.1650]],

        [[0.6903, 0.2279],
         [0.6602, 0.5325]],

        [[0.6074, 0.2102],
         [0.5191, 0.5803]],

        [[0.5548, 0.0933],
         [0.5912, 0.6143]],

        [[0.6076, 0.2297],
         [0.7251, 0.6172]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.01382061954603653
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 59
Adjusted Rand Index: -0.006658343736995423
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 60
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 59
Adjusted Rand Index: 0.01382061954603653
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 53
Adjusted Rand Index: -0.010427427162149738
Global Adjusted Rand Index: 0.0015260052121688979
Average Adjusted Rand Index: 0.0021110936385855795
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23476.890306157562
Iteration 100: Loss = -11212.287246310327
Iteration 200: Loss = -11207.90200839834
Iteration 300: Loss = -11201.689060976805
Iteration 400: Loss = -11195.759950957201
Iteration 500: Loss = -11128.834650716028
Iteration 600: Loss = -11096.706751542424
Iteration 700: Loss = -11089.943894005106
Iteration 800: Loss = -11089.125370125128
Iteration 900: Loss = -11088.922225255521
Iteration 1000: Loss = -11088.788341144013
Iteration 1100: Loss = -11088.708786231795
Iteration 1200: Loss = -11088.645340907515
Iteration 1300: Loss = -11088.20853603191
Iteration 1400: Loss = -11086.551880292192
Iteration 1500: Loss = -11086.49540286035
Iteration 1600: Loss = -11086.458405295254
Iteration 1700: Loss = -11086.439083907337
Iteration 1800: Loss = -11086.424457292662
Iteration 1900: Loss = -11086.412687368298
Iteration 2000: Loss = -11086.401429874091
Iteration 2100: Loss = -11086.387124041812
Iteration 2200: Loss = -11086.36183112596
Iteration 2300: Loss = -11086.287091807642
Iteration 2400: Loss = -11084.90801513955
Iteration 2500: Loss = -11083.08581304823
Iteration 2600: Loss = -11082.989000962934
Iteration 2700: Loss = -11071.911408763981
Iteration 2800: Loss = -11065.632335530554
Iteration 2900: Loss = -11060.290897248335
Iteration 3000: Loss = -11060.26837510581
Iteration 3100: Loss = -11060.261852340058
Iteration 3200: Loss = -11060.25822742073
Iteration 3300: Loss = -11060.270382001077
1
Iteration 3400: Loss = -11060.253604790634
Iteration 3500: Loss = -11060.252236469105
Iteration 3600: Loss = -11060.249955348438
Iteration 3700: Loss = -11060.24832168618
Iteration 3800: Loss = -11060.247165338002
Iteration 3900: Loss = -11060.247599713768
1
Iteration 4000: Loss = -11060.246016269053
Iteration 4100: Loss = -11060.245626865937
Iteration 4200: Loss = -11060.247860336945
1
Iteration 4300: Loss = -11060.244883183355
Iteration 4400: Loss = -11060.24457507182
Iteration 4500: Loss = -11060.244059473383
Iteration 4600: Loss = -11060.243163991541
Iteration 4700: Loss = -11060.239097816242
Iteration 4800: Loss = -11060.231286013937
Iteration 4900: Loss = -11060.231193999662
Iteration 5000: Loss = -11060.231209105417
1
Iteration 5100: Loss = -11060.231026587084
Iteration 5200: Loss = -11060.230843681242
Iteration 5300: Loss = -11060.230513084378
Iteration 5400: Loss = -11060.230363786495
Iteration 5500: Loss = -11060.230394366796
1
Iteration 5600: Loss = -11060.231805805406
2
Iteration 5700: Loss = -11060.230021310634
Iteration 5800: Loss = -11060.230319827328
1
Iteration 5900: Loss = -11060.2310140427
2
Iteration 6000: Loss = -11060.231808449793
3
Iteration 6100: Loss = -11060.231484055987
4
Iteration 6200: Loss = -11060.22969649212
Iteration 6300: Loss = -11060.229647259537
Iteration 6400: Loss = -11060.229697298864
1
Iteration 6500: Loss = -11060.229576265889
Iteration 6600: Loss = -11060.229559846231
Iteration 6700: Loss = -11060.241585667083
1
Iteration 6800: Loss = -11060.229449077051
Iteration 6900: Loss = -11060.22969622493
1
Iteration 7000: Loss = -11060.229405875523
Iteration 7100: Loss = -11060.430487810641
1
Iteration 7200: Loss = -11060.229347536713
Iteration 7300: Loss = -11060.22932356841
Iteration 7400: Loss = -11060.230548397516
1
Iteration 7500: Loss = -11060.229270969454
Iteration 7600: Loss = -11060.229271149532
1
Iteration 7700: Loss = -11060.22941353826
2
Iteration 7800: Loss = -11060.229197169714
Iteration 7900: Loss = -11060.23171653794
1
Iteration 8000: Loss = -11060.22920006406
2
Iteration 8100: Loss = -11060.22914167476
Iteration 8200: Loss = -11060.229106059935
Iteration 8300: Loss = -11060.229143146636
1
Iteration 8400: Loss = -11060.229019537263
Iteration 8500: Loss = -11060.230381972118
1
Iteration 8600: Loss = -11060.228928038643
Iteration 8700: Loss = -11060.228826772815
Iteration 8800: Loss = -11060.22878010031
Iteration 8900: Loss = -11060.228706947966
Iteration 9000: Loss = -11060.22867548278
Iteration 9100: Loss = -11060.228895453636
1
Iteration 9200: Loss = -11060.228679382812
2
Iteration 9300: Loss = -11060.361280927038
3
Iteration 9400: Loss = -11060.228619521193
Iteration 9500: Loss = -11060.228623546174
1
Iteration 9600: Loss = -11060.231519310133
2
Iteration 9700: Loss = -11060.230404115655
3
Iteration 9800: Loss = -11060.240305881274
4
Iteration 9900: Loss = -11060.228308707503
Iteration 10000: Loss = -11060.228553941544
1
Iteration 10100: Loss = -11060.228220894061
Iteration 10200: Loss = -11060.228545318247
1
Iteration 10300: Loss = -11060.228209468482
Iteration 10400: Loss = -11060.330840729941
1
Iteration 10500: Loss = -11060.228258833653
2
Iteration 10600: Loss = -11060.228317775007
3
Iteration 10700: Loss = -11060.228685490394
4
Iteration 10800: Loss = -11060.239967831802
5
Stopping early at iteration 10800 due to no improvement.
pi: tensor([[0.6761, 0.3239],
        [0.5021, 0.4979]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.3964, 0.6036], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2473, 0.0913],
         [0.6212, 0.2150]],

        [[0.5599, 0.0953],
         [0.7150, 0.5973]],

        [[0.5643, 0.0932],
         [0.5265, 0.7252]],

        [[0.5588, 0.1013],
         [0.5038, 0.6745]],

        [[0.6971, 0.1031],
         [0.5568, 0.5562]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 95
Adjusted Rand Index: 0.8080243965168828
time is 1
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8447058823529412
time is 2
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8822858823962049
time is 3
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.8447901065451248
time is 4
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 4
Adjusted Rand Index: 0.844845420066659
Global Adjusted Rand Index: 0.3120525743504215
Average Adjusted Rand Index: 0.8449303375755626
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23392.276456938784
Iteration 100: Loss = -11212.767705224087
Iteration 200: Loss = -11211.451065582512
Iteration 300: Loss = -11211.09297487939
Iteration 400: Loss = -11210.96923705695
Iteration 500: Loss = -11210.839830069373
Iteration 600: Loss = -11210.62236945807
Iteration 700: Loss = -11210.403542252821
Iteration 800: Loss = -11210.30176928071
Iteration 900: Loss = -11210.2644015646
Iteration 1000: Loss = -11210.246338647656
Iteration 1100: Loss = -11210.235225942522
Iteration 1200: Loss = -11210.227457862344
Iteration 1300: Loss = -11210.221588213684
Iteration 1400: Loss = -11210.216927121231
Iteration 1500: Loss = -11210.213077226961
Iteration 1600: Loss = -11210.209847386204
Iteration 1700: Loss = -11210.207041258738
Iteration 1800: Loss = -11210.204571999962
Iteration 1900: Loss = -11210.202334838943
Iteration 2000: Loss = -11210.20035677888
Iteration 2100: Loss = -11210.198581240133
Iteration 2200: Loss = -11210.196905624833
Iteration 2300: Loss = -11210.195368081002
Iteration 2400: Loss = -11210.193960978431
Iteration 2500: Loss = -11210.192557334107
Iteration 2600: Loss = -11210.191298632117
Iteration 2700: Loss = -11210.190076962472
Iteration 2800: Loss = -11210.188857475086
Iteration 2900: Loss = -11210.18765679072
Iteration 3000: Loss = -11210.186514902138
Iteration 3100: Loss = -11210.185210608051
Iteration 3200: Loss = -11210.183851331047
Iteration 3300: Loss = -11210.18233559427
Iteration 3400: Loss = -11210.18049116355
Iteration 3500: Loss = -11210.17806495223
Iteration 3600: Loss = -11210.174326239798
Iteration 3700: Loss = -11210.16691284065
Iteration 3800: Loss = -11210.141314007644
Iteration 3900: Loss = -11207.81826753874
Iteration 4000: Loss = -11207.472342532852
Iteration 4100: Loss = -11206.325355909248
Iteration 4200: Loss = -11205.779968147815
Iteration 4300: Loss = -11205.402342953888
Iteration 4400: Loss = -11203.97492890774
Iteration 4500: Loss = -11049.747800529245
Iteration 4600: Loss = -11049.024682214147
Iteration 4700: Loss = -11044.13328563061
Iteration 4800: Loss = -11038.083733975785
Iteration 4900: Loss = -11037.312133017616
Iteration 5000: Loss = -11034.65087809553
Iteration 5100: Loss = -11034.341741257873
Iteration 5200: Loss = -11034.266813817738
Iteration 5300: Loss = -11034.249558359357
Iteration 5400: Loss = -11034.239105812612
Iteration 5500: Loss = -11034.22240726706
Iteration 5600: Loss = -11033.92100457673
Iteration 5700: Loss = -11033.875332605181
Iteration 5800: Loss = -11033.864886502935
Iteration 5900: Loss = -11033.824918564871
Iteration 6000: Loss = -11033.823018389508
Iteration 6100: Loss = -11033.822060557784
Iteration 6200: Loss = -11033.820943855771
Iteration 6300: Loss = -11033.802266841083
Iteration 6400: Loss = -11033.787670777005
Iteration 6500: Loss = -11033.779870281352
Iteration 6600: Loss = -11033.779003417922
Iteration 6700: Loss = -11033.779630791381
1
Iteration 6800: Loss = -11033.762862470814
Iteration 6900: Loss = -11033.757332233532
Iteration 7000: Loss = -11033.762403099629
1
Iteration 7100: Loss = -11033.75648976718
Iteration 7200: Loss = -11033.756069467478
Iteration 7300: Loss = -11033.74720118771
Iteration 7400: Loss = -11033.746002926235
Iteration 7500: Loss = -11033.746028077272
1
Iteration 7600: Loss = -11033.738110198614
Iteration 7700: Loss = -11033.695635378688
Iteration 7800: Loss = -11033.692067255315
Iteration 7900: Loss = -11033.691839100591
Iteration 8000: Loss = -11033.692632278535
1
Iteration 8100: Loss = -11033.684881867997
Iteration 8200: Loss = -11033.688994010994
1
Iteration 8300: Loss = -11033.677986878913
Iteration 8400: Loss = -11033.676908036192
Iteration 8500: Loss = -11033.672701985888
Iteration 8600: Loss = -11033.67287003839
1
Iteration 8700: Loss = -11033.673269114006
2
Iteration 8800: Loss = -11033.672069010818
Iteration 8900: Loss = -11033.675579517914
1
Iteration 9000: Loss = -11033.687961823829
2
Iteration 9100: Loss = -11033.634846173947
Iteration 9200: Loss = -11033.612843676514
Iteration 9300: Loss = -11033.611897496321
Iteration 9400: Loss = -11033.619490932984
1
Iteration 9500: Loss = -11033.678274495738
2
Iteration 9600: Loss = -11033.603334622618
Iteration 9700: Loss = -11033.601600697488
Iteration 9800: Loss = -11033.625212488954
1
Iteration 9900: Loss = -11033.600530748567
Iteration 10000: Loss = -11033.601445910781
1
Iteration 10100: Loss = -11033.598403509164
Iteration 10200: Loss = -11033.598276260489
Iteration 10300: Loss = -11033.604621582952
1
Iteration 10400: Loss = -11033.597633118688
Iteration 10500: Loss = -11033.597372658347
Iteration 10600: Loss = -11033.59778549696
1
Iteration 10700: Loss = -11033.595004330531
Iteration 10800: Loss = -11033.59407637825
Iteration 10900: Loss = -11033.603170882856
1
Iteration 11000: Loss = -11033.578159581915
Iteration 11100: Loss = -11033.577208869736
Iteration 11200: Loss = -11033.619328084498
1
Iteration 11300: Loss = -11033.576441407964
Iteration 11400: Loss = -11033.576135545418
Iteration 11500: Loss = -11033.96609974574
1
Iteration 11600: Loss = -11033.576142669646
2
Iteration 11700: Loss = -11033.576134944373
Iteration 11800: Loss = -11033.576116321969
Iteration 11900: Loss = -11033.576637384394
1
Iteration 12000: Loss = -11033.576116719785
2
Iteration 12100: Loss = -11033.630498530356
3
Iteration 12200: Loss = -11033.57431053699
Iteration 12300: Loss = -11033.574269878773
Iteration 12400: Loss = -11033.57621734435
1
Iteration 12500: Loss = -11033.574264756695
Iteration 12600: Loss = -11033.574260998135
Iteration 12700: Loss = -11033.621759070702
1
Iteration 12800: Loss = -11033.574290278826
2
Iteration 12900: Loss = -11033.574228977923
Iteration 13000: Loss = -11033.596306894637
1
Iteration 13100: Loss = -11033.571017499198
Iteration 13200: Loss = -11033.575586537536
1
Iteration 13300: Loss = -11033.571009372601
Iteration 13400: Loss = -11033.60339884822
1
Iteration 13500: Loss = -11033.570950603576
Iteration 13600: Loss = -11033.570922240808
Iteration 13700: Loss = -11033.57465680777
1
Iteration 13800: Loss = -11033.570956291142
2
Iteration 13900: Loss = -11033.570950556228
3
Iteration 14000: Loss = -11033.571112511516
4
Iteration 14100: Loss = -11033.570944456
5
Stopping early at iteration 14100 due to no improvement.
pi: tensor([[0.6442, 0.3558],
        [0.2424, 0.7576]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4506, 0.5494], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2045, 0.0935],
         [0.7062, 0.2509]],

        [[0.6486, 0.0958],
         [0.7033, 0.5752]],

        [[0.6901, 0.0940],
         [0.6776, 0.6908]],

        [[0.6668, 0.1022],
         [0.7249, 0.6893]],

        [[0.5430, 0.1036],
         [0.6432, 0.5651]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 97
Adjusted Rand Index: 0.8824191749678101
time is 1
tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 95
Adjusted Rand Index: 0.8079491294919076
time is 2
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 98
Adjusted Rand Index: 0.9206925302859384
time is 3
tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 96
Adjusted Rand Index: 0.8447901065451248
time is 4
tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        1, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        0, 1, 1, 1])
Difference count: 95
Adjusted Rand Index: 0.8080808080808081
Global Adjusted Rand Index: 0.8533871406228608
Average Adjusted Rand Index: 0.8527863498743178
11074.546360656203
[0.8533871406228608, 0.0015260052121688979, 0.3120525743504215, 0.8533871406228608] [0.8527863498743178, 0.0021110936385855795, 0.8449303375755626, 0.8527863498743178] [11033.222371329279, 11206.816410800951, 11060.239967831802, 11033.570944456]
-----------------------------------------------------------------------------------------
This iteration is 8
True Objective function: Loss = -10924.586851243004
Iteration 0: Loss = -11018.37986297891
Iteration 10: Loss = -11005.30214720763
Iteration 20: Loss = -11005.267230136898
Iteration 30: Loss = -11005.256118407055
Iteration 40: Loss = -11005.253043204511
Iteration 50: Loss = -11005.2527668186
Iteration 60: Loss = -11005.253313813313
1
Iteration 70: Loss = -11005.254035863383
2
Iteration 80: Loss = -11005.254659642796
3
Stopping early at iteration 80 due to no improvement.
pi: tensor([[3.2531e-09, 1.0000e+00],
        [2.1911e-02, 9.7809e-01]], dtype=torch.float64)
alpha: tensor([0.0213, 0.9787])
beta: tensor([[[0.2227, 0.1686],
         [0.5374, 0.1601]],

        [[0.7510, 0.2041],
         [0.4601, 0.3757]],

        [[0.1824, 0.1877],
         [0.4255, 0.7064]],

        [[0.8781, 0.1575],
         [0.4969, 0.8853]],

        [[0.8750, 0.2292],
         [0.4096, 0.1396]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -11213.570576025679
Iteration 10: Loss = -11005.44526822724
Iteration 20: Loss = -11005.365651417103
Iteration 30: Loss = -11005.347497828961
Iteration 40: Loss = -11005.340885049127
Iteration 50: Loss = -11005.337725967362
Iteration 60: Loss = -11005.336062450413
Iteration 70: Loss = -11005.33497131993
Iteration 80: Loss = -11005.334178953708
Iteration 90: Loss = -11005.333630576299
Iteration 100: Loss = -11005.333141309677
Iteration 110: Loss = -11005.332717343115
Iteration 120: Loss = -11005.332270641698
Iteration 130: Loss = -11005.331912531532
Iteration 140: Loss = -11005.33148685556
Iteration 150: Loss = -11005.330967657563
Iteration 160: Loss = -11005.330439673482
Iteration 170: Loss = -11005.329784554147
Iteration 180: Loss = -11005.328942531667
Iteration 190: Loss = -11005.327867952454
Iteration 200: Loss = -11005.326438224643
Iteration 210: Loss = -11005.324451608572
Iteration 220: Loss = -11005.321766976922
Iteration 230: Loss = -11005.31795564343
Iteration 240: Loss = -11005.312584041136
Iteration 250: Loss = -11005.304951127726
Iteration 260: Loss = -11005.294153991095
Iteration 270: Loss = -11005.27934677457
Iteration 280: Loss = -11005.260028530622
Iteration 290: Loss = -11005.236754369509
Iteration 300: Loss = -11005.211091346911
Iteration 310: Loss = -11005.1859030633
Iteration 320: Loss = -11005.163100528142
Iteration 330: Loss = -11005.143199823426
Iteration 340: Loss = -11005.125543871301
Iteration 350: Loss = -11005.108932533529
Iteration 360: Loss = -11005.092395792222
Iteration 370: Loss = -11005.07515199046
Iteration 380: Loss = -11005.056434172226
Iteration 390: Loss = -11005.035751448951
Iteration 400: Loss = -11005.012454929907
Iteration 410: Loss = -11004.985849710127
Iteration 420: Loss = -11004.95472667072
Iteration 430: Loss = -11004.917507700598
Iteration 440: Loss = -11004.871978870686
Iteration 450: Loss = -11004.81441518927
Iteration 460: Loss = -11004.739405595848
Iteration 470: Loss = -11004.63932624343
Iteration 480: Loss = -11004.505728685508
Iteration 490: Loss = -11004.33454815754
Iteration 500: Loss = -11004.136361624682
Iteration 510: Loss = -11003.938193272286
Iteration 520: Loss = -11003.768522298677
Iteration 530: Loss = -11003.641605473731
Iteration 540: Loss = -11003.55681063602
Iteration 550: Loss = -11003.505410195721
Iteration 560: Loss = -11003.476912059099
Iteration 570: Loss = -11003.462531132027
Iteration 580: Loss = -11003.456291681141
Iteration 590: Loss = -11003.454187326975
Iteration 600: Loss = -11003.454137081788
Iteration 610: Loss = -11003.454854194759
1
Iteration 620: Loss = -11003.45584557664
2
Iteration 630: Loss = -11003.456788226544
3
Stopping early at iteration 630 due to no improvement.
pi: tensor([[3.2458e-05, 9.9997e-01],
        [2.6840e-02, 9.7316e-01]], dtype=torch.float64)
alpha: tensor([0.0262, 0.9738])
beta: tensor([[[0.0993, 0.0760],
         [0.4046, 0.1636]],

        [[0.9544, 0.1244],
         [0.3848, 0.3280]],

        [[0.7720, 0.1536],
         [0.1594, 0.3987]],

        [[0.1772, 0.0912],
         [0.4530, 0.6346]],

        [[0.3274, 0.1618],
         [0.8879, 0.0097]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -9.428816943229404e-05
Average Adjusted Rand Index: -0.0005810948242862701
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23222.10767188489
Iteration 100: Loss = -11006.00536620917
Iteration 200: Loss = -11005.451831260107
Iteration 300: Loss = -11005.326801172678
Iteration 400: Loss = -11005.255167883462
Iteration 500: Loss = -11005.200843837221
Iteration 600: Loss = -11005.150886364021
Iteration 700: Loss = -11005.088015416888
Iteration 800: Loss = -11004.999266824087
Iteration 900: Loss = -11004.893280933711
Iteration 1000: Loss = -11004.804765038696
Iteration 1100: Loss = -11004.747783859326
Iteration 1200: Loss = -11004.711950338582
Iteration 1300: Loss = -11004.686029699766
Iteration 1400: Loss = -11004.662222785808
Iteration 1500: Loss = -11004.63038789213
Iteration 1600: Loss = -11004.543390342867
Iteration 1700: Loss = -11003.969816796945
Iteration 1800: Loss = -11003.541680201204
Iteration 1900: Loss = -11003.423760873175
Iteration 2000: Loss = -11003.377288954358
Iteration 2100: Loss = -11003.350933464972
Iteration 2200: Loss = -11003.332648683261
Iteration 2300: Loss = -11003.318681959383
Iteration 2400: Loss = -11003.307628411136
Iteration 2500: Loss = -11003.29866698384
Iteration 2600: Loss = -11003.291336702196
Iteration 2700: Loss = -11003.285353211042
Iteration 2800: Loss = -11003.280473407664
Iteration 2900: Loss = -11003.27644888691
Iteration 3000: Loss = -11003.27311181057
Iteration 3100: Loss = -11003.270466068549
Iteration 3200: Loss = -11003.268222543029
Iteration 3300: Loss = -11003.266389198581
Iteration 3400: Loss = -11003.26482892045
Iteration 3500: Loss = -11003.263575553576
Iteration 3600: Loss = -11003.262483513143
Iteration 3700: Loss = -11003.261565672028
Iteration 3800: Loss = -11003.26080952469
Iteration 3900: Loss = -11003.260117713242
Iteration 4000: Loss = -11003.259565833536
Iteration 4100: Loss = -11003.259063045345
Iteration 4200: Loss = -11003.258633699987
Iteration 4300: Loss = -11003.25829148212
Iteration 4400: Loss = -11003.25789989548
Iteration 4500: Loss = -11003.257656744438
Iteration 4600: Loss = -11003.257351565146
Iteration 4700: Loss = -11003.257091609374
Iteration 4800: Loss = -11003.256883509548
Iteration 4900: Loss = -11003.256684347505
Iteration 5000: Loss = -11003.256467941192
Iteration 5100: Loss = -11003.256300059937
Iteration 5200: Loss = -11003.256116267943
Iteration 5300: Loss = -11003.255941575375
Iteration 5400: Loss = -11003.255824067026
Iteration 5500: Loss = -11003.255662101754
Iteration 5600: Loss = -11003.255544526188
Iteration 5700: Loss = -11003.255401349932
Iteration 5800: Loss = -11003.25526672754
Iteration 5900: Loss = -11003.255162068028
Iteration 6000: Loss = -11003.255055859558
Iteration 6100: Loss = -11003.25498232237
Iteration 6200: Loss = -11003.254878226822
Iteration 6300: Loss = -11003.254778250637
Iteration 6400: Loss = -11003.254694633159
Iteration 6500: Loss = -11003.254648332273
Iteration 6600: Loss = -11003.254534292706
Iteration 6700: Loss = -11003.25499808704
1
Iteration 6800: Loss = -11003.254411825737
Iteration 6900: Loss = -11003.254350223331
Iteration 7000: Loss = -11003.257161896136
1
Iteration 7100: Loss = -11003.254235077535
Iteration 7200: Loss = -11003.254206331412
Iteration 7300: Loss = -11003.254162274088
Iteration 7400: Loss = -11003.254152895703
Iteration 7500: Loss = -11003.257242155722
1
Iteration 7600: Loss = -11003.253984015868
Iteration 7700: Loss = -11003.253960462742
Iteration 7800: Loss = -11003.260762393402
1
Iteration 7900: Loss = -11003.25388062591
Iteration 8000: Loss = -11003.254297551013
1
Iteration 8100: Loss = -11003.254056510417
2
Iteration 8200: Loss = -11003.253924851831
3
Iteration 8300: Loss = -11003.255278169958
4
Iteration 8400: Loss = -11003.253932005786
5
Stopping early at iteration 8400 due to no improvement.
pi: tensor([[9.8313e-01, 1.6868e-02],
        [9.9990e-01, 9.5152e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9739, 0.0261], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1644, 0.0751],
         [0.5998, 0.1161]],

        [[0.6882, 0.1247],
         [0.6585, 0.5796]],

        [[0.6155, 0.1656],
         [0.5558, 0.5874]],

        [[0.5890, 0.0835],
         [0.5155, 0.6658]],

        [[0.5952, 0.2327],
         [0.5300, 0.7168]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 51
Adjusted Rand Index: -0.0037221143673841777
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -9.428816943229404e-05
Average Adjusted Rand Index: -0.0005810948242862701
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22067.317885439457
Iteration 100: Loss = -11006.383233340463
Iteration 200: Loss = -11005.732669242818
Iteration 300: Loss = -11005.510763677536
Iteration 400: Loss = -11005.434469838714
Iteration 500: Loss = -11005.392835457526
Iteration 600: Loss = -11005.360065309398
Iteration 700: Loss = -11005.326849786747
Iteration 800: Loss = -11005.290068405931
Iteration 900: Loss = -11005.250746549305
Iteration 1000: Loss = -11005.212190620285
Iteration 1100: Loss = -11005.176024092521
Iteration 1200: Loss = -11005.142123691676
Iteration 1300: Loss = -11005.11061696116
Iteration 1400: Loss = -11005.082046796702
Iteration 1500: Loss = -11005.057579502589
Iteration 1600: Loss = -11005.0372929867
Iteration 1700: Loss = -11005.020438715588
Iteration 1800: Loss = -11005.005669060876
Iteration 1900: Loss = -11004.991329935741
Iteration 2000: Loss = -11004.974942044593
Iteration 2100: Loss = -11004.949534965206
Iteration 2200: Loss = -11004.867441979675
Iteration 2300: Loss = -11000.646904070274
Iteration 2400: Loss = -10998.103379661481
Iteration 2500: Loss = -10997.500155643595
Iteration 2600: Loss = -10997.259653073286
Iteration 2700: Loss = -10997.132318311118
Iteration 2800: Loss = -10997.053954961939
Iteration 2900: Loss = -10997.00128799486
Iteration 3000: Loss = -10996.96375874332
Iteration 3100: Loss = -10996.935880113559
Iteration 3200: Loss = -10996.914475779955
Iteration 3300: Loss = -10996.897630449033
Iteration 3400: Loss = -10996.884090464013
Iteration 3500: Loss = -10996.873006303791
Iteration 3600: Loss = -10996.863822583367
Iteration 3700: Loss = -10996.856133926885
Iteration 3800: Loss = -10996.849612023283
Iteration 3900: Loss = -10996.844018726284
Iteration 4000: Loss = -10996.839163480328
Iteration 4100: Loss = -10996.834933701914
Iteration 4200: Loss = -10996.8312255034
Iteration 4300: Loss = -10996.827941693311
Iteration 4400: Loss = -10996.825001489311
Iteration 4500: Loss = -10996.822425840128
Iteration 4600: Loss = -10996.820110880222
Iteration 4700: Loss = -10996.81798623329
Iteration 4800: Loss = -10996.816114983354
Iteration 4900: Loss = -10996.814397356618
Iteration 5000: Loss = -10996.812838611184
Iteration 5100: Loss = -10996.811423757212
Iteration 5200: Loss = -10996.810078619706
Iteration 5300: Loss = -10996.808879320446
Iteration 5400: Loss = -10996.807761505339
Iteration 5500: Loss = -10996.806775122206
Iteration 5600: Loss = -10996.805810244588
Iteration 5700: Loss = -10996.804931607692
Iteration 5800: Loss = -10996.804145425916
Iteration 5900: Loss = -10996.803379007599
Iteration 6000: Loss = -10996.802691948853
Iteration 6100: Loss = -10996.802078191473
Iteration 6200: Loss = -10996.801477190727
Iteration 6300: Loss = -10996.80092120914
Iteration 6400: Loss = -10996.800402617137
Iteration 6500: Loss = -10996.799907897976
Iteration 6600: Loss = -10996.799491683863
Iteration 6700: Loss = -10996.799028343601
Iteration 6800: Loss = -10996.798621700642
Iteration 6900: Loss = -10996.798248424595
Iteration 7000: Loss = -10996.797863305726
Iteration 7100: Loss = -10996.797504978658
Iteration 7200: Loss = -10996.797147808644
Iteration 7300: Loss = -10996.796868559135
Iteration 7400: Loss = -10996.796652152207
Iteration 7500: Loss = -10996.796271502475
Iteration 7600: Loss = -10996.796029890018
Iteration 7700: Loss = -10996.80159987174
1
Iteration 7800: Loss = -10996.795566362498
Iteration 7900: Loss = -10996.795333786633
Iteration 8000: Loss = -10996.795150003965
Iteration 8100: Loss = -10996.795228165958
1
Iteration 8200: Loss = -10996.794809527159
Iteration 8300: Loss = -10996.794635690205
Iteration 8400: Loss = -10996.794524573663
Iteration 8500: Loss = -10996.795080001282
1
Iteration 8600: Loss = -10996.794221039796
Iteration 8700: Loss = -10996.794115094506
Iteration 8800: Loss = -10997.451411867667
1
Iteration 8900: Loss = -10996.793898065318
Iteration 9000: Loss = -10996.793766160927
Iteration 9100: Loss = -10996.793700089394
Iteration 9200: Loss = -10996.812339635437
1
Iteration 9300: Loss = -10996.79351332236
Iteration 9400: Loss = -10996.793384114091
Iteration 9500: Loss = -10996.793478091206
1
Iteration 9600: Loss = -10996.793348311689
Iteration 9700: Loss = -10996.793184166358
Iteration 9800: Loss = -10996.793130966202
Iteration 9900: Loss = -10996.848222400296
1
Iteration 10000: Loss = -10996.792985977485
Iteration 10100: Loss = -10996.792900479388
Iteration 10200: Loss = -10996.792873135064
Iteration 10300: Loss = -10996.792914704165
1
Iteration 10400: Loss = -10996.792731400745
Iteration 10500: Loss = -10996.792701714394
Iteration 10600: Loss = -10996.79268839376
Iteration 10700: Loss = -10996.7926154902
Iteration 10800: Loss = -10996.79259188611
Iteration 10900: Loss = -10996.792526940024
Iteration 11000: Loss = -10996.793104514685
1
Iteration 11100: Loss = -10996.792453389868
Iteration 11200: Loss = -10996.792458673033
1
Iteration 11300: Loss = -10996.901307108681
2
Iteration 11400: Loss = -10996.792366200456
Iteration 11500: Loss = -10996.792364787712
Iteration 11600: Loss = -10996.792332474375
Iteration 11700: Loss = -10996.793249610262
1
Iteration 11800: Loss = -10996.79229957641
Iteration 11900: Loss = -10996.792242856758
Iteration 12000: Loss = -10996.792443545399
1
Iteration 12100: Loss = -10996.792231744543
Iteration 12200: Loss = -10996.792162017184
Iteration 12300: Loss = -10996.792185687706
1
Iteration 12400: Loss = -10996.793175379638
2
Iteration 12500: Loss = -10996.792149930956
Iteration 12600: Loss = -10996.792122128061
Iteration 12700: Loss = -10997.231102160864
1
Iteration 12800: Loss = -10996.792094967977
Iteration 12900: Loss = -10996.79205179321
Iteration 13000: Loss = -10996.792071939082
1
Iteration 13100: Loss = -10996.793896000438
2
Iteration 13200: Loss = -10996.792031733912
Iteration 13300: Loss = -10996.792054118307
1
Iteration 13400: Loss = -10996.82320409971
2
Iteration 13500: Loss = -10996.792015042189
Iteration 13600: Loss = -10996.821642629851
1
Iteration 13700: Loss = -10996.792008669421
Iteration 13800: Loss = -10996.792721664886
1
Iteration 13900: Loss = -10996.792012767166
2
Iteration 14000: Loss = -10996.792006970481
Iteration 14100: Loss = -10996.886750197396
1
Iteration 14200: Loss = -10996.791943936063
Iteration 14300: Loss = -10996.791940712099
Iteration 14400: Loss = -10997.014456730029
1
Iteration 14500: Loss = -10996.791945106635
2
Iteration 14600: Loss = -10996.791910715756
Iteration 14700: Loss = -10996.800677059067
1
Iteration 14800: Loss = -10996.791944055303
2
Iteration 14900: Loss = -10996.791916432132
3
Iteration 15000: Loss = -10996.791917086668
4
Iteration 15100: Loss = -10996.792139425874
5
Stopping early at iteration 15100 due to no improvement.
pi: tensor([[1.0000e+00, 1.8764e-07],
        [1.6210e-01, 8.3790e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9760, 0.0240], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1626, 0.0687],
         [0.5321, 0.1766]],

        [[0.6983, 0.2553],
         [0.5856, 0.6101]],

        [[0.5144, 0.1509],
         [0.6556, 0.7055]],

        [[0.7040, 0.1989],
         [0.5545, 0.6663]],

        [[0.5796, 0.2716],
         [0.5328, 0.5583]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.0005089183229907909
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.004144175314943718
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
Global Adjusted Rand Index: 0.0005829600081200244
Average Adjusted Rand Index: 0.00042877465266307326
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22890.008702488598
Iteration 100: Loss = -11006.69354360804
Iteration 200: Loss = -11005.707774094415
Iteration 300: Loss = -11005.470798805012
Iteration 400: Loss = -11005.367127324604
Iteration 500: Loss = -11005.31350089547
Iteration 600: Loss = -11005.277588999847
Iteration 700: Loss = -11005.250548405995
Iteration 800: Loss = -11005.22793469998
Iteration 900: Loss = -11005.208344374785
Iteration 1000: Loss = -11005.191061519952
Iteration 1100: Loss = -11005.175824379761
Iteration 1200: Loss = -11005.162458402154
Iteration 1300: Loss = -11005.151064918076
Iteration 1400: Loss = -11005.141464349299
Iteration 1500: Loss = -11005.133242514907
Iteration 1600: Loss = -11005.126068372832
Iteration 1700: Loss = -11005.119485096917
Iteration 1800: Loss = -11005.11328169057
Iteration 1900: Loss = -11005.107248065473
Iteration 2000: Loss = -11005.10118073895
Iteration 2100: Loss = -11005.094757454719
Iteration 2200: Loss = -11005.08745494225
Iteration 2300: Loss = -11005.077460307897
Iteration 2400: Loss = -11005.0561339555
Iteration 2500: Loss = -11004.88572621266
Iteration 2600: Loss = -11004.221072598204
Iteration 2700: Loss = -11004.069890784325
Iteration 2800: Loss = -10998.928849771135
Iteration 2900: Loss = -10997.145589554188
Iteration 3000: Loss = -10996.988717148322
Iteration 3100: Loss = -10996.92462273434
Iteration 3200: Loss = -10996.88997206847
Iteration 3300: Loss = -10996.869014158152
Iteration 3400: Loss = -10996.854884579825
Iteration 3500: Loss = -10996.844616967392
Iteration 3600: Loss = -10996.836825904886
Iteration 3700: Loss = -10996.830760405002
Iteration 3800: Loss = -10996.825843480927
Iteration 3900: Loss = -10996.821892008928
Iteration 4000: Loss = -10996.818550778244
Iteration 4100: Loss = -10996.815761147218
Iteration 4200: Loss = -10996.813442092382
Iteration 4300: Loss = -10996.811447639773
Iteration 4400: Loss = -10996.809628734049
Iteration 4500: Loss = -10996.80809875406
Iteration 4600: Loss = -10996.806720552417
Iteration 4700: Loss = -10996.805495892031
Iteration 4800: Loss = -10996.804333862014
Iteration 4900: Loss = -10996.803336675543
Iteration 5000: Loss = -10996.802474250722
Iteration 5100: Loss = -10996.801685450706
Iteration 5200: Loss = -10996.801014472989
Iteration 5300: Loss = -10996.80037856118
Iteration 5400: Loss = -10996.799746716017
Iteration 5500: Loss = -10996.79923387625
Iteration 5600: Loss = -10996.798773184666
Iteration 5700: Loss = -10996.798303674044
Iteration 5800: Loss = -10996.79852630343
1
Iteration 5900: Loss = -10996.797548193485
Iteration 6000: Loss = -10996.797156304805
Iteration 6100: Loss = -10996.796867975523
Iteration 6200: Loss = -10996.796547511702
Iteration 6300: Loss = -10996.796282021854
Iteration 6400: Loss = -10996.795985577979
Iteration 6500: Loss = -10996.795753388938
Iteration 6600: Loss = -10996.795517771336
Iteration 6700: Loss = -10996.795320928477
Iteration 6800: Loss = -10996.795111671583
Iteration 6900: Loss = -10996.794918505322
Iteration 7000: Loss = -10996.794745035664
Iteration 7100: Loss = -10996.794546160801
Iteration 7200: Loss = -10996.794617233521
1
Iteration 7300: Loss = -10996.794265989914
Iteration 7400: Loss = -10996.794127296826
Iteration 7500: Loss = -10996.793977318313
Iteration 7600: Loss = -10996.793837712605
Iteration 7700: Loss = -10996.793860823145
1
Iteration 7800: Loss = -10996.793622612051
Iteration 7900: Loss = -10996.7935358861
Iteration 8000: Loss = -10996.793451059526
Iteration 8100: Loss = -10996.793344608312
Iteration 8200: Loss = -10996.795377470205
1
Iteration 8300: Loss = -10996.794545480443
2
Iteration 8400: Loss = -10996.793989572721
3
Iteration 8500: Loss = -10996.793092361111
Iteration 8600: Loss = -10996.890400012047
1
Iteration 8700: Loss = -10996.792931615766
Iteration 8800: Loss = -10996.79285198375
Iteration 8900: Loss = -10996.810565193082
1
Iteration 9000: Loss = -10996.792764396447
Iteration 9100: Loss = -10996.792715155254
Iteration 9200: Loss = -10997.214617994032
1
Iteration 9300: Loss = -10996.792620150483
Iteration 9400: Loss = -10996.79260319599
Iteration 9500: Loss = -10996.792559950063
Iteration 9600: Loss = -10996.794950423096
1
Iteration 9700: Loss = -10996.792499001373
Iteration 9800: Loss = -10996.792452614796
Iteration 9900: Loss = -10996.792424369622
Iteration 10000: Loss = -10996.842156980536
1
Iteration 10100: Loss = -10996.79235735125
Iteration 10200: Loss = -10996.792333458141
Iteration 10300: Loss = -10996.792297975042
Iteration 10400: Loss = -10996.792281652737
Iteration 10500: Loss = -10996.792253039941
Iteration 10600: Loss = -10996.792216673783
Iteration 10700: Loss = -10996.926584065493
1
Iteration 10800: Loss = -10996.792199443536
Iteration 10900: Loss = -10996.792149135219
Iteration 11000: Loss = -10996.792157470041
1
Iteration 11100: Loss = -10996.793206754703
2
Iteration 11200: Loss = -10996.792120455348
Iteration 11300: Loss = -10996.79209590152
Iteration 11400: Loss = -10996.79210239562
1
Iteration 11500: Loss = -10996.792226938715
2
Iteration 11600: Loss = -10996.792057667957
Iteration 11700: Loss = -10996.792076314265
1
Iteration 11800: Loss = -10996.810375167375
2
Iteration 11900: Loss = -10996.792023582588
Iteration 12000: Loss = -10996.792054612666
1
Iteration 12100: Loss = -10996.7919933995
Iteration 12200: Loss = -10996.793580092526
1
Iteration 12300: Loss = -10996.791973556681
Iteration 12400: Loss = -10996.791985932652
1
Iteration 12500: Loss = -10996.79472933489
2
Iteration 12600: Loss = -10996.791971034945
Iteration 12700: Loss = -10996.79195265364
Iteration 12800: Loss = -10996.792727792126
1
Iteration 12900: Loss = -10996.79194837878
Iteration 13000: Loss = -10996.806354674585
1
Iteration 13100: Loss = -10996.791950096986
2
Iteration 13200: Loss = -10996.791926011067
Iteration 13300: Loss = -10997.301186204973
1
Iteration 13400: Loss = -10996.791926878957
2
Iteration 13500: Loss = -10996.791906867902
Iteration 13600: Loss = -10996.792230049205
1
Iteration 13700: Loss = -10996.79198975239
2
Iteration 13800: Loss = -10996.82240219493
3
Iteration 13900: Loss = -10996.791901502387
Iteration 14000: Loss = -10996.838700640274
1
Iteration 14100: Loss = -10996.791897692152
Iteration 14200: Loss = -10996.79193132974
1
Iteration 14300: Loss = -10996.791883018323
Iteration 14400: Loss = -10996.791883818283
1
Iteration 14500: Loss = -10996.862977936247
2
Iteration 14600: Loss = -10996.79184745374
Iteration 14700: Loss = -10996.791855725418
1
Iteration 14800: Loss = -10996.946593345247
2
Iteration 14900: Loss = -10996.79187727211
3
Iteration 15000: Loss = -10996.791852647604
4
Iteration 15100: Loss = -10996.791845161186
Iteration 15200: Loss = -10996.792867779042
1
Iteration 15300: Loss = -10996.791851438751
2
Iteration 15400: Loss = -10996.954673003796
3
Iteration 15500: Loss = -10996.791857495658
4
Iteration 15600: Loss = -10996.791851321444
5
Stopping early at iteration 15600 due to no improvement.
pi: tensor([[8.3802e-01, 1.6198e-01],
        [7.4227e-08, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0240, 0.9760], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1766, 0.0687],
         [0.7220, 0.1625]],

        [[0.6604, 0.2549],
         [0.6326, 0.5989]],

        [[0.6412, 0.1509],
         [0.6999, 0.5365]],

        [[0.6785, 0.1989],
         [0.6835, 0.6396]],

        [[0.7134, 0.2715],
         [0.5975, 0.6077]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.0005089183229907909
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.004144175314943718
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
Global Adjusted Rand Index: 0.0005829600081200244
Average Adjusted Rand Index: 0.00042877465266307326
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22673.010355317878
Iteration 100: Loss = -11006.37231435476
Iteration 200: Loss = -11005.645567073318
Iteration 300: Loss = -11005.435409872955
Iteration 400: Loss = -11005.349031599591
Iteration 500: Loss = -11005.299030170767
Iteration 600: Loss = -11005.265073969102
Iteration 700: Loss = -11005.236499620327
Iteration 800: Loss = -11005.208570749119
Iteration 900: Loss = -11005.17992593416
Iteration 1000: Loss = -11005.150696111406
Iteration 1100: Loss = -11005.121754682219
Iteration 1200: Loss = -11005.094542623527
Iteration 1300: Loss = -11005.07003894907
Iteration 1400: Loss = -11005.048578050157
Iteration 1500: Loss = -11005.029171707138
Iteration 1600: Loss = -11005.009643715704
Iteration 1700: Loss = -11004.98369284889
Iteration 1800: Loss = -11004.907310169478
Iteration 1900: Loss = -11000.01368921817
Iteration 2000: Loss = -10997.561966825007
Iteration 2100: Loss = -10997.198166365935
Iteration 2200: Loss = -10997.055053266564
Iteration 2300: Loss = -10996.986625090882
Iteration 2400: Loss = -10996.944245044491
Iteration 2500: Loss = -10996.915183276633
Iteration 2600: Loss = -10996.894017297294
Iteration 2700: Loss = -10996.877967897333
Iteration 2800: Loss = -10996.86548934367
Iteration 2900: Loss = -10996.855509252371
Iteration 3000: Loss = -10996.847446691978
Iteration 3100: Loss = -10996.840819648212
Iteration 3200: Loss = -10996.835301262123
Iteration 3300: Loss = -10996.830708189418
Iteration 3400: Loss = -10996.826800808763
Iteration 3500: Loss = -10996.823357349787
Iteration 3600: Loss = -10996.820438040815
Iteration 3700: Loss = -10996.8178408817
Iteration 3800: Loss = -10996.815602914974
Iteration 3900: Loss = -10996.813690034836
Iteration 4000: Loss = -10996.811962510164
Iteration 4100: Loss = -10996.810445954014
Iteration 4200: Loss = -10996.809096105077
Iteration 4300: Loss = -10996.807839217396
Iteration 4400: Loss = -10996.806753831754
Iteration 4500: Loss = -10996.805703227179
Iteration 4600: Loss = -10996.804762821648
Iteration 4700: Loss = -10996.803876052176
Iteration 4800: Loss = -10996.803079538091
Iteration 4900: Loss = -10996.80232878859
Iteration 5000: Loss = -10996.801692563871
Iteration 5100: Loss = -10996.801062899618
Iteration 5200: Loss = -10996.800519058725
Iteration 5300: Loss = -10996.79997920326
Iteration 5400: Loss = -10996.799493350343
Iteration 5500: Loss = -10996.79903565757
Iteration 5600: Loss = -10996.798624537469
Iteration 5700: Loss = -10996.798246804814
Iteration 5800: Loss = -10996.79783545688
Iteration 5900: Loss = -10996.797483239605
Iteration 6000: Loss = -10996.79712628984
Iteration 6100: Loss = -10996.796803661746
Iteration 6200: Loss = -10996.796474394607
Iteration 6300: Loss = -10996.796184911103
Iteration 6400: Loss = -10996.795912059633
Iteration 6500: Loss = -10996.795659133344
Iteration 6600: Loss = -10996.795473822105
Iteration 6700: Loss = -10996.795257089516
Iteration 6800: Loss = -10996.795093040611
Iteration 6900: Loss = -10996.794918651656
Iteration 7000: Loss = -10996.794764507278
Iteration 7100: Loss = -10996.794594507763
Iteration 7200: Loss = -10996.794450952699
Iteration 7300: Loss = -10996.794310932599
Iteration 7400: Loss = -10996.794181834472
Iteration 7500: Loss = -10996.794069346532
Iteration 7600: Loss = -10996.793942469529
Iteration 7700: Loss = -10996.793832371299
Iteration 7800: Loss = -10996.79371547159
Iteration 7900: Loss = -10996.793600591245
Iteration 8000: Loss = -10996.793921943523
1
Iteration 8100: Loss = -10996.795392613
2
Iteration 8200: Loss = -10996.809267610204
3
Iteration 8300: Loss = -10996.793296530455
Iteration 8400: Loss = -10996.793999412132
1
Iteration 8500: Loss = -10996.793157746479
Iteration 8600: Loss = -10996.802447917911
1
Iteration 8700: Loss = -10996.792999202808
Iteration 8800: Loss = -10996.792956450696
Iteration 8900: Loss = -10997.20646541362
1
Iteration 9000: Loss = -10996.792855751697
Iteration 9100: Loss = -10996.792798573046
Iteration 9200: Loss = -10996.792813151038
1
Iteration 9300: Loss = -10996.792684738333
Iteration 9400: Loss = -10996.792639668509
Iteration 9500: Loss = -10996.792618919713
Iteration 9600: Loss = -10996.79271736052
1
Iteration 9700: Loss = -10996.792516836467
Iteration 9800: Loss = -10996.792466119607
Iteration 9900: Loss = -10996.792463738693
Iteration 10000: Loss = -10996.792538741676
1
Iteration 10100: Loss = -10996.792405931845
Iteration 10200: Loss = -10996.792354636784
Iteration 10300: Loss = -10996.804225676407
1
Iteration 10400: Loss = -10996.792335579652
Iteration 10500: Loss = -10996.792309618211
Iteration 10600: Loss = -10997.00602143724
1
Iteration 10700: Loss = -10996.792244317887
Iteration 10800: Loss = -10996.792203163955
Iteration 10900: Loss = -10996.792197821653
Iteration 11000: Loss = -10996.792280764632
1
Iteration 11100: Loss = -10996.792127987963
Iteration 11200: Loss = -10996.792135465175
1
Iteration 11300: Loss = -10997.220718503086
2
Iteration 11400: Loss = -10996.792131172688
3
Iteration 11500: Loss = -10996.792104405138
Iteration 11600: Loss = -10996.792066923359
Iteration 11700: Loss = -10996.792078233359
1
Iteration 11800: Loss = -10996.79205295747
Iteration 11900: Loss = -10996.792015196814
Iteration 12000: Loss = -10996.801079714214
1
Iteration 12100: Loss = -10996.792046356793
2
Iteration 12200: Loss = -10996.792014202554
Iteration 12300: Loss = -10996.79197780415
Iteration 12400: Loss = -10996.795084865793
1
Iteration 12500: Loss = -10996.79197703988
Iteration 12600: Loss = -10996.791990660227
1
Iteration 12700: Loss = -10996.791991025031
2
Iteration 12800: Loss = -10996.794142578614
3
Iteration 12900: Loss = -10996.791960592498
Iteration 13000: Loss = -10996.791952602156
Iteration 13100: Loss = -10996.824863309608
1
Iteration 13200: Loss = -10996.791955773868
2
Iteration 13300: Loss = -10996.79190762209
Iteration 13400: Loss = -10996.842733915797
1
Iteration 13500: Loss = -10996.792317172543
2
Iteration 13600: Loss = -10996.792916834056
3
Iteration 13700: Loss = -10997.047396136726
4
Iteration 13800: Loss = -10996.791908155172
5
Stopping early at iteration 13800 due to no improvement.
pi: tensor([[1.0000e+00, 1.8336e-07],
        [1.6199e-01, 8.3801e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9760, 0.0240], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1625, 0.0687],
         [0.5505, 0.1766]],

        [[0.6000, 0.2549],
         [0.6441, 0.6706]],

        [[0.6503, 0.1509],
         [0.5411, 0.7099]],

        [[0.7062, 0.1989],
         [0.6864, 0.6324]],

        [[0.6477, 0.2716],
         [0.5547, 0.5703]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.0005089183229907909
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.004144175314943718
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
Global Adjusted Rand Index: 0.0005829600081200244
Average Adjusted Rand Index: 0.00042877465266307326
10924.586851243004
[-9.428816943229404e-05, 0.0005829600081200244, 0.0005829600081200244, 0.0005829600081200244] [-0.0005810948242862701, 0.00042877465266307326, 0.00042877465266307326, 0.00042877465266307326] [11003.253932005786, 10996.792139425874, 10996.791851321444, 10996.791908155172]
-----------------------------------------------------------------------------------------
This iteration is 9
True Objective function: Loss = -10864.111769819872
Iteration 0: Loss = -10967.832177857057
Iteration 10: Loss = -10949.804843545695
Iteration 20: Loss = -10949.804759816421
Iteration 30: Loss = -10949.804740857448
Iteration 40: Loss = -10949.804740211232
Iteration 50: Loss = -10949.800351778791
Iteration 60: Loss = -10948.799199525376
Iteration 70: Loss = -10947.986031888515
Iteration 80: Loss = -10947.82835551102
Iteration 90: Loss = -10947.76366001393
Iteration 100: Loss = -10947.731165030878
Iteration 110: Loss = -10947.71421747526
Iteration 120: Loss = -10947.705301535017
Iteration 130: Loss = -10947.700544867692
Iteration 140: Loss = -10947.697915654733
Iteration 150: Loss = -10947.696454070301
Iteration 160: Loss = -10947.695591951247
Iteration 170: Loss = -10947.695068043347
Iteration 180: Loss = -10947.694748601032
Iteration 190: Loss = -10947.694577069213
Iteration 200: Loss = -10947.694453123371
Iteration 210: Loss = -10947.694379060571
Iteration 220: Loss = -10947.694317659809
Iteration 230: Loss = -10947.694309909475
Iteration 240: Loss = -10947.694277180379
Iteration 250: Loss = -10947.694263948431
Iteration 260: Loss = -10947.694248599524
Iteration 270: Loss = -10947.694260705046
1
Iteration 280: Loss = -10947.694254979926
2
Iteration 290: Loss = -10947.694277588678
3
Stopping early at iteration 290 due to no improvement.
pi: tensor([[0.0349, 0.9651],
        [0.0415, 0.9585]], dtype=torch.float64)
alpha: tensor([0.0412, 0.9588])
beta: tensor([[[0.1805, 0.1641],
         [0.1165, 0.1605]],

        [[0.2207, 0.1920],
         [0.7132, 0.7628]],

        [[0.7378, 0.1800],
         [0.0414, 0.8152]],

        [[0.3026, 0.1454],
         [0.8436, 0.6098]],

        [[0.2704, 0.0827],
         [0.4221, 0.2119]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
Global Adjusted Rand Index: -0.0006422660312947975
Average Adjusted Rand Index: 0.0001617442499919128
Iteration 0: Loss = -10985.397685617343
Iteration 10: Loss = -10949.27191413348
Iteration 20: Loss = -10949.17061190419
Iteration 30: Loss = -10949.086097663876
Iteration 40: Loss = -10949.011303029514
Iteration 50: Loss = -10948.896514356418
Iteration 60: Loss = -10948.31429918718
Iteration 70: Loss = -10947.890876710413
Iteration 80: Loss = -10947.824975744643
Iteration 90: Loss = -10947.8021500909
Iteration 100: Loss = -10947.792810024306
Iteration 110: Loss = -10947.78868854404
Iteration 120: Loss = -10947.786780377917
Iteration 130: Loss = -10947.785633423888
Iteration 140: Loss = -10947.776808292048
Iteration 150: Loss = -10947.725262686437
Iteration 160: Loss = -10947.702029835198
Iteration 170: Loss = -10947.697092949666
Iteration 180: Loss = -10947.695685960927
Iteration 190: Loss = -10947.695104076649
Iteration 200: Loss = -10947.694745789133
Iteration 210: Loss = -10947.694573812521
Iteration 220: Loss = -10947.694435825179
Iteration 230: Loss = -10947.694362192813
Iteration 240: Loss = -10947.694364911637
1
Iteration 250: Loss = -10947.694284027652
Iteration 260: Loss = -10947.694279588673
Iteration 270: Loss = -10947.694281751765
1
Iteration 280: Loss = -10947.694220693336
Iteration 290: Loss = -10947.6942577019
1
Iteration 300: Loss = -10947.694283358489
2
Iteration 310: Loss = -10947.69423741024
3
Stopping early at iteration 310 due to no improvement.
pi: tensor([[0.9585, 0.0415],
        [0.9651, 0.0349]], dtype=torch.float64)
alpha: tensor([0.9588, 0.0412])
beta: tensor([[[0.1605, 0.1641],
         [0.2391, 0.1805]],

        [[0.0729, 0.1920],
         [0.8706, 0.8700]],

        [[0.7697, 0.1800],
         [0.6960, 0.9236]],

        [[0.8992, 0.1454],
         [0.0639, 0.5571]],

        [[0.8026, 0.0827],
         [0.9631, 0.7984]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
Global Adjusted Rand Index: -0.0006422660312947975
Average Adjusted Rand Index: 0.0001617442499919128
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23259.63752553002
Iteration 100: Loss = -10951.24963196597
Iteration 200: Loss = -10950.146299807311
Iteration 300: Loss = -10949.790353695113
Iteration 400: Loss = -10949.667261425142
Iteration 500: Loss = -10949.612728324799
Iteration 600: Loss = -10949.579944130355
Iteration 700: Loss = -10949.555874223452
Iteration 800: Loss = -10949.535737432123
Iteration 900: Loss = -10949.517637875548
Iteration 1000: Loss = -10949.500673324736
Iteration 1100: Loss = -10949.484409886469
Iteration 1200: Loss = -10949.468883586776
Iteration 1300: Loss = -10949.453893075137
Iteration 1400: Loss = -10949.439226870325
Iteration 1500: Loss = -10949.424652764572
Iteration 1600: Loss = -10949.409718841729
Iteration 1700: Loss = -10949.393924396083
Iteration 1800: Loss = -10949.376772701438
Iteration 1900: Loss = -10949.357404650624
Iteration 2000: Loss = -10949.335058745972
Iteration 2100: Loss = -10949.30862601302
Iteration 2200: Loss = -10949.276973674905
Iteration 2300: Loss = -10949.238873985407
Iteration 2400: Loss = -10949.193243763795
Iteration 2500: Loss = -10949.139322137655
Iteration 2600: Loss = -10949.07708991467
Iteration 2700: Loss = -10949.007735837149
Iteration 2800: Loss = -10948.933784707033
Iteration 2900: Loss = -10948.85880505415
Iteration 3000: Loss = -10948.786925829781
Iteration 3100: Loss = -10948.72441323055
Iteration 3200: Loss = -10948.67919818305
Iteration 3300: Loss = -10948.649197904022
Iteration 3400: Loss = -10948.641366226288
Iteration 3500: Loss = -10948.616423875743
Iteration 3600: Loss = -10948.606626234174
Iteration 3700: Loss = -10948.599686234302
Iteration 3800: Loss = -10948.593309492579
Iteration 3900: Loss = -10948.588190609278
Iteration 4000: Loss = -10948.583872030069
Iteration 4100: Loss = -10948.57970693826
Iteration 4200: Loss = -10948.575777016096
Iteration 4300: Loss = -10948.572177491309
Iteration 4400: Loss = -10948.568386660529
Iteration 4500: Loss = -10948.564561233083
Iteration 4600: Loss = -10948.560945997797
Iteration 4700: Loss = -10948.557048345396
Iteration 4800: Loss = -10948.55319180722
Iteration 4900: Loss = -10948.549771684058
Iteration 5000: Loss = -10948.545930403465
Iteration 5100: Loss = -10948.542459184926
Iteration 5200: Loss = -10948.539293229354
Iteration 5300: Loss = -10948.53620575901
Iteration 5400: Loss = -10948.533282414197
Iteration 5500: Loss = -10948.530534589554
Iteration 5600: Loss = -10948.527879154046
Iteration 5700: Loss = -10948.525178912494
Iteration 5800: Loss = -10948.522382356643
Iteration 5900: Loss = -10948.51982411691
Iteration 6000: Loss = -10948.516287468792
Iteration 6100: Loss = -10948.51258268621
Iteration 6200: Loss = -10948.50851825395
Iteration 6300: Loss = -10948.503528059871
Iteration 6400: Loss = -10948.498351841195
Iteration 6500: Loss = -10948.493144546257
Iteration 6600: Loss = -10948.48858051998
Iteration 6700: Loss = -10948.484914585071
Iteration 6800: Loss = -10948.48225082883
Iteration 6900: Loss = -10948.480440016778
Iteration 7000: Loss = -10948.479284367162
Iteration 7100: Loss = -10948.478516287287
Iteration 7200: Loss = -10948.47800374742
Iteration 7300: Loss = -10948.477470864733
Iteration 7400: Loss = -10948.477026592034
Iteration 7500: Loss = -10948.476623200673
Iteration 7600: Loss = -10948.47617005256
Iteration 7700: Loss = -10948.47568131811
Iteration 7800: Loss = -10948.47519441916
Iteration 7900: Loss = -10948.474734582269
Iteration 8000: Loss = -10948.47424042597
Iteration 8100: Loss = -10948.473697496449
Iteration 8200: Loss = -10948.473207104329
Iteration 8300: Loss = -10948.474853252232
1
Iteration 8400: Loss = -10948.472138799705
Iteration 8500: Loss = -10948.48047559851
1
Iteration 8600: Loss = -10948.471831686431
Iteration 8700: Loss = -10948.47046000879
Iteration 8800: Loss = -10948.469545342658
Iteration 8900: Loss = -10948.470915855234
1
Iteration 9000: Loss = -10948.46590625943
Iteration 9100: Loss = -10947.645757152426
Iteration 9200: Loss = -10947.583817707875
Iteration 9300: Loss = -10947.570218691779
Iteration 9400: Loss = -10947.566320313696
Iteration 9500: Loss = -10947.57777136774
1
Iteration 9600: Loss = -10947.557598149084
Iteration 9700: Loss = -10947.618593924559
1
Iteration 9800: Loss = -10947.554451270285
Iteration 9900: Loss = -10947.616088653544
1
Iteration 10000: Loss = -10947.552555212997
Iteration 10100: Loss = -10947.555111992242
1
Iteration 10200: Loss = -10947.551343659343
Iteration 10300: Loss = -10947.550800309245
Iteration 10400: Loss = -10947.598302655846
1
Iteration 10500: Loss = -10947.558469837506
2
Iteration 10600: Loss = -10947.561985011054
3
Iteration 10700: Loss = -10947.553154847128
4
Iteration 10800: Loss = -10947.554508388188
5
Stopping early at iteration 10800 due to no improvement.
pi: tensor([[7.9923e-05, 9.9992e-01],
        [9.9994e-01, 5.5941e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8822, 0.1178], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1554, 0.1893],
         [0.6847, 0.1563]],

        [[0.5889, 0.2057],
         [0.5518, 0.7297]],

        [[0.5748, 0.2056],
         [0.5092, 0.6995]],

        [[0.7277, 0.1512],
         [0.5993, 0.6366]],

        [[0.6368, 0.1704],
         [0.5279, 0.5043]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.012100586122140291
time is 1
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.003997142393814948
time is 2
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0056450870649491815
time is 3
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.02397039968412133
time is 4
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.002918749315918129
Global Adjusted Rand Index: -0.0020024050761642386
Average Adjusted Rand Index: 0.006960036232295545
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23505.83828701321
Iteration 100: Loss = -10952.126882411156
Iteration 200: Loss = -10950.575124576575
Iteration 300: Loss = -10950.10089226842
Iteration 400: Loss = -10949.893933002968
Iteration 500: Loss = -10949.785507921071
Iteration 600: Loss = -10949.719582782433
Iteration 700: Loss = -10949.674792113432
Iteration 800: Loss = -10949.641148022149
Iteration 900: Loss = -10949.613803574752
Iteration 1000: Loss = -10949.590427285973
Iteration 1100: Loss = -10949.569637572926
Iteration 1200: Loss = -10949.55050713336
Iteration 1300: Loss = -10949.532651529122
Iteration 1400: Loss = -10949.515666827549
Iteration 1500: Loss = -10949.499531521347
Iteration 1600: Loss = -10949.484187816697
Iteration 1700: Loss = -10949.469499768595
Iteration 1800: Loss = -10949.455370496216
Iteration 1900: Loss = -10949.441626751624
Iteration 2000: Loss = -10949.42793235338
Iteration 2100: Loss = -10949.41394280877
Iteration 2200: Loss = -10949.399210406995
Iteration 2300: Loss = -10949.383292267643
Iteration 2400: Loss = -10949.365412833953
Iteration 2500: Loss = -10949.345003879009
Iteration 2600: Loss = -10949.321070732483
Iteration 2700: Loss = -10949.292537223871
Iteration 2800: Loss = -10949.258150953176
Iteration 2900: Loss = -10949.216446991059
Iteration 3000: Loss = -10949.166189548669
Iteration 3100: Loss = -10949.106806827765
Iteration 3200: Loss = -10949.038824719242
Iteration 3300: Loss = -10948.963838612954
Iteration 3400: Loss = -10948.883682481877
Iteration 3500: Loss = -10948.802883380245
Iteration 3600: Loss = -10948.732885236612
Iteration 3700: Loss = -10948.681606461694
Iteration 3800: Loss = -10948.64916058053
Iteration 3900: Loss = -10948.628582924754
Iteration 4000: Loss = -10948.616724285177
Iteration 4100: Loss = -10948.605757891984
Iteration 4200: Loss = -10948.598422091718
Iteration 4300: Loss = -10948.59258171638
Iteration 4400: Loss = -10948.587428232087
Iteration 4500: Loss = -10948.584155774006
Iteration 4600: Loss = -10948.578719298843
Iteration 4700: Loss = -10948.595763691348
1
Iteration 4800: Loss = -10948.570717711098
Iteration 4900: Loss = -10948.56667930515
Iteration 5000: Loss = -10948.562877986868
Iteration 5100: Loss = -10948.558811622752
Iteration 5200: Loss = -10948.564975030911
1
Iteration 5300: Loss = -10948.550999067398
Iteration 5400: Loss = -10948.547196680065
Iteration 5500: Loss = -10948.543767716013
Iteration 5600: Loss = -10948.540428103404
Iteration 5700: Loss = -10948.537307873346
Iteration 5800: Loss = -10948.534464078419
Iteration 5900: Loss = -10948.531724995977
Iteration 6000: Loss = -10948.52912970949
Iteration 6100: Loss = -10948.526675867433
Iteration 6200: Loss = -10948.52411997828
Iteration 6300: Loss = -10948.521864930859
Iteration 6400: Loss = -10948.51868229952
Iteration 6500: Loss = -10948.515554895086
Iteration 6600: Loss = -10948.51200273097
Iteration 6700: Loss = -10948.507820414148
Iteration 6800: Loss = -10948.503323077173
Iteration 6900: Loss = -10948.49813280315
Iteration 7000: Loss = -10948.493130810071
Iteration 7100: Loss = -10948.4886826607
Iteration 7200: Loss = -10948.485427554944
Iteration 7300: Loss = -10948.482593199831
Iteration 7400: Loss = -10948.48088168462
Iteration 7500: Loss = -10948.479798636215
Iteration 7600: Loss = -10948.479053940853
Iteration 7700: Loss = -10948.47851743741
Iteration 7800: Loss = -10948.478008686416
Iteration 7900: Loss = -10948.477512653755
Iteration 8000: Loss = -10948.477059141624
Iteration 8100: Loss = -10948.476536320972
Iteration 8200: Loss = -10948.476021185694
Iteration 8300: Loss = -10948.475491447936
Iteration 8400: Loss = -10948.475430710698
Iteration 8500: Loss = -10948.474411744342
Iteration 8600: Loss = -10948.474946400966
1
Iteration 8700: Loss = -10948.473363625992
Iteration 8800: Loss = -10948.47293325024
Iteration 8900: Loss = -10948.501383561075
1
Iteration 9000: Loss = -10948.472301784408
Iteration 9100: Loss = -10948.47133375778
Iteration 9200: Loss = -10948.470593644955
Iteration 9300: Loss = -10948.56041576164
1
Iteration 9400: Loss = -10948.468475146265
Iteration 9500: Loss = -10948.462088834482
Iteration 9600: Loss = -10947.679729769614
Iteration 9700: Loss = -10947.586581816193
Iteration 9800: Loss = -10947.587596516127
1
Iteration 9900: Loss = -10947.569642003898
Iteration 10000: Loss = -10947.562379150102
Iteration 10100: Loss = -10947.570857443761
1
Iteration 10200: Loss = -10947.684799998877
2
Iteration 10300: Loss = -10947.554846637864
Iteration 10400: Loss = -10947.553600474354
Iteration 10500: Loss = -10947.555431819397
1
Iteration 10600: Loss = -10947.552030451876
Iteration 10700: Loss = -10947.552680288027
1
Iteration 10800: Loss = -10947.630425902555
2
Iteration 10900: Loss = -10947.551584376375
Iteration 11000: Loss = -10947.550113503947
Iteration 11100: Loss = -10947.55772781626
1
Iteration 11200: Loss = -10947.55016299674
2
Iteration 11300: Loss = -10947.550982622466
3
Iteration 11400: Loss = -10947.594839501855
4
Iteration 11500: Loss = -10947.577157552809
5
Stopping early at iteration 11500 due to no improvement.
pi: tensor([[7.0638e-05, 9.9993e-01],
        [9.9995e-01, 4.6478e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8816, 0.1184], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1552, 0.1900],
         [0.6865, 0.1557]],

        [[0.5254, 0.2060],
         [0.6460, 0.5270]],

        [[0.6423, 0.2054],
         [0.5220, 0.5712]],

        [[0.5048, 0.1515],
         [0.5229, 0.5093]],

        [[0.5737, 0.1706],
         [0.6252, 0.5293]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 57
Adjusted Rand Index: 0.012100586122140291
time is 1
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.003997142393814948
time is 2
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0056450870649491815
time is 3
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 40
Adjusted Rand Index: 0.02397039968412133
time is 4
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.002918749315918129
Global Adjusted Rand Index: -0.0020024050761642386
Average Adjusted Rand Index: 0.006960036232295545
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23096.229142137363
Iteration 100: Loss = -10951.109685506593
Iteration 200: Loss = -10949.733678241606
Iteration 300: Loss = -10949.490592109927
Iteration 400: Loss = -10949.364605433317
Iteration 500: Loss = -10949.213685412604
Iteration 600: Loss = -10948.887441134399
Iteration 700: Loss = -10948.2516712617
Iteration 800: Loss = -10947.8459903665
Iteration 900: Loss = -10947.687345381933
Iteration 1000: Loss = -10947.60224184541
Iteration 1100: Loss = -10947.54540849975
Iteration 1200: Loss = -10947.504408933131
Iteration 1300: Loss = -10947.473450013826
Iteration 1400: Loss = -10947.449641864709
Iteration 1500: Loss = -10947.431130409523
Iteration 1600: Loss = -10947.416234148628
Iteration 1700: Loss = -10947.403566820518
Iteration 1800: Loss = -10947.391974371641
Iteration 1900: Loss = -10947.380775721755
Iteration 2000: Loss = -10947.369598118763
Iteration 2100: Loss = -10947.358089916212
Iteration 2200: Loss = -10947.346197286288
Iteration 2300: Loss = -10947.335070640138
Iteration 2400: Loss = -10947.325131261989
Iteration 2500: Loss = -10947.316349250777
Iteration 2600: Loss = -10947.308767427496
Iteration 2700: Loss = -10947.30215546084
Iteration 2800: Loss = -10947.296002667235
Iteration 2900: Loss = -10947.289010158054
Iteration 3000: Loss = -10947.28025859889
Iteration 3100: Loss = -10947.273026370767
Iteration 3200: Loss = -10947.266842527706
Iteration 3300: Loss = -10947.26083460651
Iteration 3400: Loss = -10947.254996547514
Iteration 3500: Loss = -10947.248993354711
Iteration 3600: Loss = -10947.24113028671
Iteration 3700: Loss = -10947.222208159275
Iteration 3800: Loss = -10947.075048045175
Iteration 3900: Loss = -10946.571885851115
Iteration 4000: Loss = -10946.450523506035
Iteration 4100: Loss = -10946.408862388364
Iteration 4200: Loss = -10946.387502558862
Iteration 4300: Loss = -10946.374500394804
Iteration 4400: Loss = -10946.36584190236
Iteration 4500: Loss = -10946.360464313995
Iteration 4600: Loss = -10946.355129860493
Iteration 4700: Loss = -10946.35198960441
Iteration 4800: Loss = -10946.34877080896
Iteration 4900: Loss = -10946.350042012573
1
Iteration 5000: Loss = -10946.34455367329
Iteration 5100: Loss = -10946.343493553717
Iteration 5200: Loss = -10946.3416145577
Iteration 5300: Loss = -10946.3404522276
Iteration 5400: Loss = -10946.33943118333
Iteration 5500: Loss = -10946.338518142473
Iteration 5600: Loss = -10946.343069157185
1
Iteration 5700: Loss = -10946.337099604805
Iteration 5800: Loss = -10946.336465877928
Iteration 5900: Loss = -10946.336099949533
Iteration 6000: Loss = -10946.335422625838
Iteration 6100: Loss = -10946.33557909754
1
Iteration 6200: Loss = -10946.334563042077
Iteration 6300: Loss = -10946.334191451762
Iteration 6400: Loss = -10946.33384581239
Iteration 6500: Loss = -10946.333553882723
Iteration 6600: Loss = -10946.33341717168
Iteration 6700: Loss = -10946.333019226216
Iteration 6800: Loss = -10946.332759125497
Iteration 6900: Loss = -10946.332575040547
Iteration 7000: Loss = -10946.33234239943
Iteration 7100: Loss = -10946.332276762056
Iteration 7200: Loss = -10946.33194453552
Iteration 7300: Loss = -10946.332023664092
1
Iteration 7400: Loss = -10946.331651650267
Iteration 7500: Loss = -10946.33148981928
Iteration 7600: Loss = -10946.332049497903
1
Iteration 7700: Loss = -10946.331256462232
Iteration 7800: Loss = -10946.331115208506
Iteration 7900: Loss = -10946.331013328152
Iteration 8000: Loss = -10946.33422615425
1
Iteration 8100: Loss = -10946.33080337698
Iteration 8200: Loss = -10946.330721079054
Iteration 8300: Loss = -10946.330883466673
1
Iteration 8400: Loss = -10946.330516285143
Iteration 8500: Loss = -10946.330495901446
Iteration 8600: Loss = -10946.334176996537
1
Iteration 8700: Loss = -10946.33034995153
Iteration 8800: Loss = -10946.333333354956
1
Iteration 8900: Loss = -10946.33020623946
Iteration 9000: Loss = -10946.330375224074
1
Iteration 9100: Loss = -10946.330518726802
2
Iteration 9200: Loss = -10946.330163460436
Iteration 9300: Loss = -10946.330117245085
Iteration 9400: Loss = -10946.358814029672
1
Iteration 9500: Loss = -10946.333196009145
2
Iteration 9600: Loss = -10946.333913625329
3
Iteration 9700: Loss = -10946.329893968834
Iteration 9800: Loss = -10946.334857742617
1
Iteration 9900: Loss = -10946.329781186
Iteration 10000: Loss = -10946.332310786818
1
Iteration 10100: Loss = -10946.329741902226
Iteration 10200: Loss = -10946.35448517252
1
Iteration 10300: Loss = -10946.329724753292
Iteration 10400: Loss = -10946.329678002148
Iteration 10500: Loss = -10946.347514628951
1
Iteration 10600: Loss = -10946.432933279399
2
Iteration 10700: Loss = -10946.329701877223
3
Iteration 10800: Loss = -10946.329802862596
4
Iteration 10900: Loss = -10946.340802582477
5
Stopping early at iteration 10900 due to no improvement.
pi: tensor([[5.3876e-01, 4.6124e-01],
        [5.1799e-06, 9.9999e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2375, 0.7625], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1813, 0.1680],
         [0.5207, 0.1591]],

        [[0.6291, 0.1835],
         [0.5007, 0.6863]],

        [[0.7081, 0.1965],
         [0.7034, 0.5765]],

        [[0.5845, 0.1640],
         [0.6701, 0.6256]],

        [[0.6376, 0.0623],
         [0.7146, 0.6026]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0031985518644306096
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.0006667223888488251
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.0006667223888488251
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 45
Adjusted Rand Index: -0.009009494263563287
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
Global Adjusted Rand Index: -0.0009193039805369588
Average Adjusted Rand Index: -0.001267133185374153
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22013.771185366455
Iteration 100: Loss = -10950.04250481797
Iteration 200: Loss = -10949.233851046452
Iteration 300: Loss = -10948.790754116424
Iteration 400: Loss = -10948.092409634151
Iteration 500: Loss = -10947.526443442423
Iteration 600: Loss = -10947.41281156667
Iteration 700: Loss = -10947.376125677532
Iteration 800: Loss = -10947.342989791054
Iteration 900: Loss = -10947.301573574483
Iteration 1000: Loss = -10947.249703618296
Iteration 1100: Loss = -10947.189402594475
Iteration 1200: Loss = -10947.109693409093
Iteration 1300: Loss = -10946.973281169638
Iteration 1400: Loss = -10946.782452244359
Iteration 1500: Loss = -10946.62190185507
Iteration 1600: Loss = -10946.525264199767
Iteration 1700: Loss = -10946.469900583992
Iteration 1800: Loss = -10946.436117802887
Iteration 1900: Loss = -10946.41392825608
Iteration 2000: Loss = -10946.398368226432
Iteration 2100: Loss = -10946.387005363435
Iteration 2200: Loss = -10946.378330431398
Iteration 2300: Loss = -10946.371611768825
Iteration 2400: Loss = -10946.366207535682
Iteration 2500: Loss = -10946.361738808058
Iteration 2600: Loss = -10946.35811911279
Iteration 2700: Loss = -10946.35500567058
Iteration 2800: Loss = -10946.352415373443
Iteration 2900: Loss = -10946.350168749705
Iteration 3000: Loss = -10946.348233345758
Iteration 3100: Loss = -10946.346562582556
Iteration 3200: Loss = -10946.345094626075
Iteration 3300: Loss = -10946.343718882548
Iteration 3400: Loss = -10946.342563043558
Iteration 3500: Loss = -10946.341516508104
Iteration 3600: Loss = -10946.340573019048
Iteration 3700: Loss = -10946.339722448773
Iteration 3800: Loss = -10946.338974575174
Iteration 3900: Loss = -10946.338275499567
Iteration 4000: Loss = -10946.337596252979
Iteration 4100: Loss = -10946.337053102516
Iteration 4200: Loss = -10946.336541501314
Iteration 4300: Loss = -10946.347861620285
1
Iteration 4400: Loss = -10946.335577429107
Iteration 4500: Loss = -10946.335209644889
Iteration 4600: Loss = -10946.334865222861
Iteration 4700: Loss = -10946.334437319434
Iteration 4800: Loss = -10946.3341857454
Iteration 4900: Loss = -10946.33383432278
Iteration 5000: Loss = -10946.333595475733
Iteration 5100: Loss = -10946.333295751981
Iteration 5200: Loss = -10946.333053174754
Iteration 5300: Loss = -10946.332845131867
Iteration 5400: Loss = -10946.332622983367
Iteration 5500: Loss = -10946.332533229504
Iteration 5600: Loss = -10946.332246280657
Iteration 5700: Loss = -10946.332102806564
Iteration 5800: Loss = -10946.331920343135
Iteration 5900: Loss = -10946.331760652263
Iteration 6000: Loss = -10946.331608626684
Iteration 6100: Loss = -10946.33147785356
Iteration 6200: Loss = -10946.33168763291
1
Iteration 6300: Loss = -10946.331235210115
Iteration 6400: Loss = -10946.331111968419
Iteration 6500: Loss = -10946.33121462498
1
Iteration 6600: Loss = -10946.330932312614
Iteration 6700: Loss = -10946.334159012864
1
Iteration 6800: Loss = -10946.330745831285
Iteration 6900: Loss = -10946.330622143832
Iteration 7000: Loss = -10946.330585473406
Iteration 7100: Loss = -10946.330482049312
Iteration 7200: Loss = -10946.33042650316
Iteration 7300: Loss = -10946.330398581169
Iteration 7400: Loss = -10946.330322257458
Iteration 7500: Loss = -10946.330473708276
1
Iteration 7600: Loss = -10946.330174685516
Iteration 7700: Loss = -10946.330149306537
Iteration 7800: Loss = -10946.33010492121
Iteration 7900: Loss = -10946.330005972548
Iteration 8000: Loss = -10946.334588184103
1
Iteration 8100: Loss = -10946.329976594152
Iteration 8200: Loss = -10946.330115373637
1
Iteration 8300: Loss = -10946.399140126192
2
Iteration 8400: Loss = -10946.329844912534
Iteration 8500: Loss = -10946.336251700965
1
Iteration 8600: Loss = -10946.329773570282
Iteration 8700: Loss = -10946.381490917012
1
Iteration 8800: Loss = -10946.329712457766
Iteration 8900: Loss = -10946.334482939
1
Iteration 9000: Loss = -10946.33349883735
2
Iteration 9100: Loss = -10946.349254855493
3
Iteration 9200: Loss = -10946.340455076197
4
Iteration 9300: Loss = -10946.352245690956
5
Stopping early at iteration 9300 due to no improvement.
pi: tensor([[9.9999e-01, 6.0516e-06],
        [4.6237e-01, 5.3763e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7624, 0.2376], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1598, 0.1674],
         [0.6951, 0.1812]],

        [[0.5377, 0.1831],
         [0.7203, 0.6079]],

        [[0.5618, 0.1961],
         [0.6019, 0.5645]],

        [[0.5052, 0.1639],
         [0.6409, 0.5674]],

        [[0.6717, 0.0623],
         [0.6117, 0.6272]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0031985518644306096
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,
        1, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: -0.0006667223888488251
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 1])
Difference count: 52
Adjusted Rand Index: -0.0006667223888488251
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 55
Adjusted Rand Index: -0.009009494263563287
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
Global Adjusted Rand Index: -0.0009193039805369588
Average Adjusted Rand Index: -0.001267133185374153
10864.111769819872
[-0.0020024050761642386, -0.0020024050761642386, -0.0009193039805369588, -0.0009193039805369588] [0.006960036232295545, 0.006960036232295545, -0.001267133185374153, -0.001267133185374153] [10947.554508388188, 10947.577157552809, 10946.340802582477, 10946.352245690956]
-----------------------------------------------------------------------------------------
This iteration is 10
True Objective function: Loss = -10833.813229028518
Iteration 0: Loss = -10961.51360866852
Iteration 10: Loss = -10957.507024676954
Iteration 20: Loss = -10956.746000354658
Iteration 30: Loss = -10956.625100143965
Iteration 40: Loss = -10956.615165953299
Iteration 50: Loss = -10956.609484766921
Iteration 60: Loss = -10956.60491147655
Iteration 70: Loss = -10956.601501768448
Iteration 80: Loss = -10956.59920391932
Iteration 90: Loss = -10956.597844532798
Iteration 100: Loss = -10956.596994917054
Iteration 110: Loss = -10956.596505870702
Iteration 120: Loss = -10956.59623060275
Iteration 130: Loss = -10956.596091895473
Iteration 140: Loss = -10956.595988999075
Iteration 150: Loss = -10956.595956447934
Iteration 160: Loss = -10956.595916038495
Iteration 170: Loss = -10956.595915585502
Iteration 180: Loss = -10956.595934258989
1
Iteration 190: Loss = -10956.595877443277
Iteration 200: Loss = -10956.595894003989
1
Iteration 210: Loss = -10956.595892544958
2
Iteration 220: Loss = -10956.595876389993
Iteration 230: Loss = -10956.59589999333
1
Iteration 240: Loss = -10956.595876731677
2
Iteration 250: Loss = -10956.595891885403
3
Stopping early at iteration 250 due to no improvement.
pi: tensor([[5.1066e-07, 1.0000e+00],
        [1.7998e-02, 9.8200e-01]], dtype=torch.float64)
alpha: tensor([0.0177, 0.9823])
beta: tensor([[[0.1970, 0.1750],
         [0.2875, 0.1598]],

        [[0.3462, 0.1962],
         [0.7966, 0.4227]],

        [[0.9363, 0.2389],
         [0.7442, 0.9493]],

        [[0.4202, 0.0718],
         [0.6585, 0.3900]],

        [[0.7039, 0.1422],
         [0.2679, 0.9256]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.002963392440128199
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -2.2751963154028812e-05
Average Adjusted Rand Index: -0.0004293504388350744
Iteration 0: Loss = -11048.615368527713
Iteration 10: Loss = -10957.308012047803
Iteration 20: Loss = -10956.769262730193
Iteration 30: Loss = -10956.6609713334
Iteration 40: Loss = -10956.652359190412
Iteration 50: Loss = -10956.651725411823
Iteration 60: Loss = -10956.652115116973
1
Iteration 70: Loss = -10956.652642539226
2
Iteration 80: Loss = -10956.653292716055
3
Stopping early at iteration 80 due to no improvement.
pi: tensor([[0.9708, 0.0292],
        [0.9790, 0.0210]], dtype=torch.float64)
alpha: tensor([0.9710, 0.0290])
beta: tensor([[[0.1596, 0.1743],
         [0.8879, 0.1927]],

        [[0.9770, 0.1937],
         [0.8891, 0.5566]],

        [[0.8294, 0.2284],
         [0.8732, 0.1251]],

        [[0.4088, 0.1039],
         [0.2023, 0.2539]],

        [[0.4989, 0.1448],
         [0.9596, 0.1901]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.002963392440128199
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -2.2751963154028812e-05
Average Adjusted Rand Index: -0.0004293504388350744
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23438.742885888052
Iteration 100: Loss = -10960.603321765286
Iteration 200: Loss = -10958.415234066668
Iteration 300: Loss = -10957.88534915494
Iteration 400: Loss = -10957.696295276328
Iteration 500: Loss = -10957.585024821597
Iteration 600: Loss = -10957.496306526169
Iteration 700: Loss = -10957.416635370992
Iteration 800: Loss = -10957.333987118904
Iteration 900: Loss = -10957.24570449456
Iteration 1000: Loss = -10957.153031196072
Iteration 1100: Loss = -10957.064053778608
Iteration 1200: Loss = -10956.985385158465
Iteration 1300: Loss = -10956.916068232289
Iteration 1400: Loss = -10956.851831897104
Iteration 1500: Loss = -10956.790573752181
Iteration 1600: Loss = -10956.732757287184
Iteration 1700: Loss = -10956.679515681308
Iteration 1800: Loss = -10956.63238441828
Iteration 1900: Loss = -10956.592362546067
Iteration 2000: Loss = -10956.559695674086
Iteration 2100: Loss = -10956.533947990201
Iteration 2200: Loss = -10956.513838824661
Iteration 2300: Loss = -10956.498006292755
Iteration 2400: Loss = -10956.485138168127
Iteration 2500: Loss = -10956.474421304862
Iteration 2600: Loss = -10956.465255874953
Iteration 2700: Loss = -10956.457351077084
Iteration 2800: Loss = -10956.450252821895
Iteration 2900: Loss = -10956.443557695546
Iteration 3000: Loss = -10956.43682525081
Iteration 3100: Loss = -10956.42931912284
Iteration 3200: Loss = -10956.41974368779
Iteration 3300: Loss = -10956.404544384308
Iteration 3400: Loss = -10956.372105004879
Iteration 3500: Loss = -10956.274522977634
Iteration 3600: Loss = -10955.945894538017
Iteration 3700: Loss = -10954.653434519067
Iteration 3800: Loss = -10833.002448964333
Iteration 3900: Loss = -10795.066749085421
Iteration 4000: Loss = -10793.065620648547
Iteration 4100: Loss = -10792.374984454607
Iteration 4200: Loss = -10790.72477300389
Iteration 4300: Loss = -10790.674107637358
Iteration 4400: Loss = -10790.653814955534
Iteration 4500: Loss = -10790.61742636755
Iteration 4600: Loss = -10790.56514012119
Iteration 4700: Loss = -10790.509816363849
Iteration 4800: Loss = -10790.49680464137
Iteration 4900: Loss = -10790.490029421777
Iteration 5000: Loss = -10790.469880297473
Iteration 5100: Loss = -10790.401955570016
Iteration 5200: Loss = -10790.403319883098
1
Iteration 5300: Loss = -10790.3894002856
Iteration 5400: Loss = -10790.343254613841
Iteration 5500: Loss = -10790.342071507326
Iteration 5600: Loss = -10790.326375981433
Iteration 5700: Loss = -10790.32382756401
Iteration 5800: Loss = -10790.321040724732
Iteration 5900: Loss = -10790.315191556445
Iteration 6000: Loss = -10790.306120210671
Iteration 6100: Loss = -10790.307851267411
1
Iteration 6200: Loss = -10790.29903982911
Iteration 6300: Loss = -10790.29502339005
Iteration 6400: Loss = -10790.293138096906
Iteration 6500: Loss = -10790.294227392165
1
Iteration 6600: Loss = -10790.291351681873
Iteration 6700: Loss = -10790.290625088908
Iteration 6800: Loss = -10790.290106917448
Iteration 6900: Loss = -10790.289625767424
Iteration 7000: Loss = -10790.292449461482
1
Iteration 7100: Loss = -10790.28974709003
2
Iteration 7200: Loss = -10790.28775431148
Iteration 7300: Loss = -10790.287113722803
Iteration 7400: Loss = -10790.283653584107
Iteration 7500: Loss = -10790.27864647399
Iteration 7600: Loss = -10790.278112317439
Iteration 7700: Loss = -10790.269451761204
Iteration 7800: Loss = -10790.270916974796
1
Iteration 7900: Loss = -10790.272214329438
2
Iteration 8000: Loss = -10790.272141282441
3
Iteration 8100: Loss = -10790.26827339767
Iteration 8200: Loss = -10790.2680418517
Iteration 8300: Loss = -10790.333066338144
1
Iteration 8400: Loss = -10790.269742239025
2
Iteration 8500: Loss = -10790.267224801059
Iteration 8600: Loss = -10790.26655218218
Iteration 8700: Loss = -10790.265495590573
Iteration 8800: Loss = -10790.2646691631
Iteration 8900: Loss = -10790.048213747212
Iteration 9000: Loss = -10790.038625028374
Iteration 9100: Loss = -10790.057366642168
1
Iteration 9200: Loss = -10790.038664271491
2
Iteration 9300: Loss = -10790.043807547863
3
Iteration 9400: Loss = -10790.018005090942
Iteration 9500: Loss = -10789.95781129073
Iteration 9600: Loss = -10789.957953310928
1
Iteration 9700: Loss = -10789.955469800536
Iteration 9800: Loss = -10789.94838349335
Iteration 9900: Loss = -10789.949248586636
1
Iteration 10000: Loss = -10790.020535707217
2
Iteration 10100: Loss = -10789.943555826952
Iteration 10200: Loss = -10789.944711579941
1
Iteration 10300: Loss = -10789.954571633623
2
Iteration 10400: Loss = -10790.095834528407
3
Iteration 10500: Loss = -10789.943229232107
Iteration 10600: Loss = -10789.942983064831
Iteration 10700: Loss = -10789.95573905455
1
Iteration 10800: Loss = -10789.934887932684
Iteration 10900: Loss = -10789.934342298662
Iteration 11000: Loss = -10789.937856842465
1
Iteration 11100: Loss = -10789.940436903697
2
Iteration 11200: Loss = -10790.001490517025
3
Iteration 11300: Loss = -10789.933092609466
Iteration 11400: Loss = -10789.931269717776
Iteration 11500: Loss = -10789.930691043935
Iteration 11600: Loss = -10789.930755990816
1
Iteration 11700: Loss = -10789.930667015542
Iteration 11800: Loss = -10789.931251534581
1
Iteration 11900: Loss = -10789.935911444863
2
Iteration 12000: Loss = -10789.930562978334
Iteration 12100: Loss = -10789.9306891489
1
Iteration 12200: Loss = -10789.937586970456
2
Iteration 12300: Loss = -10789.929420143673
Iteration 12400: Loss = -10789.945300418829
1
Iteration 12500: Loss = -10789.967700925117
2
Iteration 12600: Loss = -10789.929725497857
3
Iteration 12700: Loss = -10789.929117185431
Iteration 12800: Loss = -10789.934493332512
1
Iteration 12900: Loss = -10789.940100310647
2
Iteration 13000: Loss = -10789.950707816897
3
Iteration 13100: Loss = -10789.928018268438
Iteration 13200: Loss = -10789.9352234827
1
Iteration 13300: Loss = -10789.926546198287
Iteration 13400: Loss = -10789.961659687495
1
Iteration 13500: Loss = -10789.926459986817
Iteration 13600: Loss = -10789.93333128745
1
Iteration 13700: Loss = -10789.996036239405
2
Iteration 13800: Loss = -10789.926270807802
Iteration 13900: Loss = -10789.923508169137
Iteration 14000: Loss = -10789.95348708039
1
Iteration 14100: Loss = -10789.923353594448
Iteration 14200: Loss = -10789.925142435859
1
Iteration 14300: Loss = -10789.923291971183
Iteration 14400: Loss = -10789.933168879275
1
Iteration 14500: Loss = -10789.927811148662
2
Iteration 14600: Loss = -10789.923230569058
Iteration 14700: Loss = -10789.933804556
1
Iteration 14800: Loss = -10789.923154789512
Iteration 14900: Loss = -10789.924228312026
1
Iteration 15000: Loss = -10789.922974582829
Iteration 15100: Loss = -10789.91920295479
Iteration 15200: Loss = -10789.904518639798
Iteration 15300: Loss = -10789.900969785765
Iteration 15400: Loss = -10789.900902671996
Iteration 15500: Loss = -10789.936258613738
1
Iteration 15600: Loss = -10789.9024800001
2
Iteration 15700: Loss = -10789.90031064694
Iteration 15800: Loss = -10789.899873486
Iteration 15900: Loss = -10789.899625386623
Iteration 16000: Loss = -10789.900272853583
1
Iteration 16100: Loss = -10789.901941286022
2
Iteration 16200: Loss = -10789.899868206983
3
Iteration 16300: Loss = -10789.898032478784
Iteration 16400: Loss = -10789.899660658191
1
Iteration 16500: Loss = -10789.898160406621
2
Iteration 16600: Loss = -10789.89823957186
3
Iteration 16700: Loss = -10789.897584279892
Iteration 16800: Loss = -10789.897413274362
Iteration 16900: Loss = -10789.89736804106
Iteration 17000: Loss = -10789.898089800743
1
Iteration 17100: Loss = -10789.903443523239
2
Iteration 17200: Loss = -10789.89758797628
3
Iteration 17300: Loss = -10789.945324542918
4
Iteration 17400: Loss = -10789.94526313197
5
Stopping early at iteration 17400 due to no improvement.
pi: tensor([[0.7792, 0.2208],
        [0.2722, 0.7278]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4790, 0.5210], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1965, 0.1065],
         [0.5561, 0.2588]],

        [[0.7021, 0.0995],
         [0.6610, 0.7181]],

        [[0.7295, 0.0939],
         [0.5084, 0.5517]],

        [[0.5950, 0.0943],
         [0.6642, 0.5792]],

        [[0.6824, 0.0904],
         [0.5733, 0.5150]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 96
Adjusted Rand Index: 0.8448436976567872
time is 1
tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824200963560147
time is 3
tensor([1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 0, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 97
Adjusted Rand Index: 0.8824127248460211
time is 4
tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 95
Adjusted Rand Index: 0.8079046942578025
Global Adjusted Rand Index: 0.8683604925834989
Average Adjusted Rand Index: 0.8676778587849412
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23019.124337036355
Iteration 100: Loss = -10958.935521014426
Iteration 200: Loss = -10958.209400382646
Iteration 300: Loss = -10957.979220564386
Iteration 400: Loss = -10957.859608690975
Iteration 500: Loss = -10957.781659990434
Iteration 600: Loss = -10957.720894460643
Iteration 700: Loss = -10957.669922443012
Iteration 800: Loss = -10957.617932774945
Iteration 900: Loss = -10957.559786179354
Iteration 1000: Loss = -10957.488876498777
Iteration 1100: Loss = -10957.397197350881
Iteration 1200: Loss = -10957.276637606421
Iteration 1300: Loss = -10957.122947888387
Iteration 1400: Loss = -10956.948050191662
Iteration 1500: Loss = -10956.777581992705
Iteration 1600: Loss = -10956.676557372004
Iteration 1700: Loss = -10956.606460703337
Iteration 1800: Loss = -10956.547965310121
Iteration 1900: Loss = -10956.502551393269
Iteration 2000: Loss = -10956.470130676116
Iteration 2100: Loss = -10956.44796529526
Iteration 2200: Loss = -10956.433350536256
Iteration 2300: Loss = -10956.422891356135
Iteration 2400: Loss = -10956.414417484166
Iteration 2500: Loss = -10956.406979604084
Iteration 2600: Loss = -10956.399693701554
Iteration 2700: Loss = -10956.39232050802
Iteration 2800: Loss = -10956.384555876659
Iteration 2900: Loss = -10956.376525739326
Iteration 3000: Loss = -10956.36853908683
Iteration 3100: Loss = -10956.361255084083
Iteration 3200: Loss = -10956.35529045212
Iteration 3300: Loss = -10956.350928530117
Iteration 3400: Loss = -10956.347931841374
Iteration 3500: Loss = -10956.345996646647
Iteration 3600: Loss = -10956.344698617946
Iteration 3700: Loss = -10956.343864007009
Iteration 3800: Loss = -10956.343236677576
Iteration 3900: Loss = -10956.34269529215
Iteration 4000: Loss = -10956.342250377322
Iteration 4100: Loss = -10956.341788209933
Iteration 4200: Loss = -10956.341378440973
Iteration 4300: Loss = -10956.340936986282
Iteration 4400: Loss = -10956.340453057583
Iteration 4500: Loss = -10956.33995357541
Iteration 4600: Loss = -10956.339436420525
Iteration 4700: Loss = -10956.338889470611
Iteration 4800: Loss = -10956.338308268794
Iteration 4900: Loss = -10956.33770476909
Iteration 5000: Loss = -10956.337112037148
Iteration 5100: Loss = -10956.336543327652
Iteration 5200: Loss = -10956.336026663068
Iteration 5300: Loss = -10956.335488075303
Iteration 5400: Loss = -10956.334996027115
Iteration 5500: Loss = -10956.334508186743
Iteration 5600: Loss = -10956.334100627128
Iteration 5700: Loss = -10956.33370504488
Iteration 5800: Loss = -10956.33336236216
Iteration 5900: Loss = -10956.333011597017
Iteration 6000: Loss = -10956.332785857112
Iteration 6100: Loss = -10956.332505631985
Iteration 6200: Loss = -10956.332280737732
Iteration 6300: Loss = -10956.332011202594
Iteration 6400: Loss = -10956.331857478815
Iteration 6500: Loss = -10956.331655859565
Iteration 6600: Loss = -10956.331546723826
Iteration 6700: Loss = -10956.331338572238
Iteration 6800: Loss = -10956.331303831817
Iteration 6900: Loss = -10956.331078592164
Iteration 7000: Loss = -10956.336186509177
1
Iteration 7100: Loss = -10956.330822575399
Iteration 7200: Loss = -10956.330738020974
Iteration 7300: Loss = -10956.330657456356
Iteration 7400: Loss = -10956.330517026268
Iteration 7500: Loss = -10956.330611087038
1
Iteration 7600: Loss = -10956.330398109716
Iteration 7700: Loss = -10956.330328256314
Iteration 7800: Loss = -10956.33021177167
Iteration 7900: Loss = -10956.330194905497
Iteration 8000: Loss = -10956.330184362288
Iteration 8100: Loss = -10956.33151655832
1
Iteration 8200: Loss = -10956.377763006823
2
Iteration 8300: Loss = -10956.329979075603
Iteration 8400: Loss = -10956.329973603026
Iteration 8500: Loss = -10956.32986928843
Iteration 8600: Loss = -10956.33049394934
1
Iteration 8700: Loss = -10956.329768923786
Iteration 8800: Loss = -10956.331037987431
1
Iteration 8900: Loss = -10956.329726696775
Iteration 9000: Loss = -10956.3296769688
Iteration 9100: Loss = -10956.332967266042
1
Iteration 9200: Loss = -10956.329657873155
Iteration 9300: Loss = -10956.329591607015
Iteration 9400: Loss = -10956.34123710349
1
Iteration 9500: Loss = -10956.329547199
Iteration 9600: Loss = -10956.329570323074
1
Iteration 9700: Loss = -10956.394364349153
2
Iteration 9800: Loss = -10956.329474875822
Iteration 9900: Loss = -10956.329470029643
Iteration 10000: Loss = -10956.530470070262
1
Iteration 10100: Loss = -10956.32946441508
Iteration 10200: Loss = -10956.329406572057
Iteration 10300: Loss = -10956.329395787228
Iteration 10400: Loss = -10956.329797291819
1
Iteration 10500: Loss = -10956.32935756503
Iteration 10600: Loss = -10956.329361054022
1
Iteration 10700: Loss = -10956.33145761581
2
Iteration 10800: Loss = -10956.329333313928
Iteration 10900: Loss = -10956.329289840682
Iteration 11000: Loss = -10956.338828632797
1
Iteration 11100: Loss = -10956.32930190416
2
Iteration 11200: Loss = -10956.32928225159
Iteration 11300: Loss = -10956.864666094349
1
Iteration 11400: Loss = -10956.329261298153
Iteration 11500: Loss = -10956.329263214899
1
Iteration 11600: Loss = -10956.329252647667
Iteration 11700: Loss = -10956.329323643402
1
Iteration 11800: Loss = -10956.329225157288
Iteration 11900: Loss = -10956.329224672736
Iteration 12000: Loss = -10956.329536463345
1
Iteration 12100: Loss = -10956.329212226856
Iteration 12200: Loss = -10956.481248935781
1
Iteration 12300: Loss = -10956.329199258586
Iteration 12400: Loss = -10956.329169842178
Iteration 12500: Loss = -10956.411029182258
1
Iteration 12600: Loss = -10956.329190341805
2
Iteration 12700: Loss = -10956.329209587973
3
Iteration 12800: Loss = -10956.368364264108
4
Iteration 12900: Loss = -10956.329222511711
5
Stopping early at iteration 12900 due to no improvement.
pi: tensor([[9.3529e-05, 9.9991e-01],
        [1.7866e-02, 9.8213e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0177, 0.9823], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2220, 0.1780],
         [0.6742, 0.1614]],

        [[0.5933, 0.1987],
         [0.5485, 0.7118]],

        [[0.5089, 0.2387],
         [0.6523, 0.6882]],

        [[0.6738, 0.0682],
         [0.5523, 0.5203]],

        [[0.6214, 0.1440],
         [0.5421, 0.6032]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.000816640245952827
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 42
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0001967235538417382
Average Adjusted Rand Index: 0.0001633280491905654
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23680.269425679668
Iteration 100: Loss = -10959.470971539338
Iteration 200: Loss = -10957.572609735496
Iteration 300: Loss = -10957.34268800004
Iteration 400: Loss = -10957.19422533803
Iteration 500: Loss = -10957.048065759527
Iteration 600: Loss = -10956.849984523144
Iteration 700: Loss = -10956.365890096518
Iteration 800: Loss = -10954.943238549253
Iteration 900: Loss = -10954.333622041797
Iteration 1000: Loss = -10954.17665161159
Iteration 1100: Loss = -10954.12726391644
Iteration 1200: Loss = -10954.105237762054
Iteration 1300: Loss = -10954.091433019417
Iteration 1400: Loss = -10954.080583754097
Iteration 1500: Loss = -10954.071017220092
Iteration 1600: Loss = -10954.06222289842
Iteration 1700: Loss = -10954.05400225744
Iteration 1800: Loss = -10954.04637565205
Iteration 1900: Loss = -10954.039545026071
Iteration 2000: Loss = -10954.033599691758
Iteration 2100: Loss = -10954.028676870135
Iteration 2200: Loss = -10954.02462340246
Iteration 2300: Loss = -10954.02151232779
Iteration 2400: Loss = -10954.019166470116
Iteration 2500: Loss = -10954.01733457747
Iteration 2600: Loss = -10954.016007567072
Iteration 2700: Loss = -10954.01494668524
Iteration 2800: Loss = -10954.014148301632
Iteration 2900: Loss = -10954.013481042308
Iteration 3000: Loss = -10954.012882755487
Iteration 3100: Loss = -10954.012406646176
Iteration 3200: Loss = -10954.012025857965
Iteration 3300: Loss = -10954.011621305664
Iteration 3400: Loss = -10954.011357369063
Iteration 3500: Loss = -10954.011032032871
Iteration 3600: Loss = -10954.010741225498
Iteration 3700: Loss = -10954.01052927556
Iteration 3800: Loss = -10954.010303974483
Iteration 3900: Loss = -10954.010126103396
Iteration 4000: Loss = -10954.009934016674
Iteration 4100: Loss = -10954.009713725092
Iteration 4200: Loss = -10954.009593720291
Iteration 4300: Loss = -10954.009433028285
Iteration 4400: Loss = -10954.009323633203
Iteration 4500: Loss = -10954.009204665806
Iteration 4600: Loss = -10954.009091188967
Iteration 4700: Loss = -10954.00896070695
Iteration 4800: Loss = -10954.008881024567
Iteration 4900: Loss = -10954.008780653496
Iteration 5000: Loss = -10954.00871107765
Iteration 5100: Loss = -10954.008602087204
Iteration 5200: Loss = -10954.00853126112
Iteration 5300: Loss = -10954.008433701512
Iteration 5400: Loss = -10954.008392114074
Iteration 5500: Loss = -10954.00832618438
Iteration 5600: Loss = -10954.00827402101
Iteration 5700: Loss = -10954.008219094207
Iteration 5800: Loss = -10954.00813943863
Iteration 5900: Loss = -10954.008107155361
Iteration 6000: Loss = -10954.00808164637
Iteration 6100: Loss = -10954.008006073791
Iteration 6200: Loss = -10954.007980138349
Iteration 6300: Loss = -10954.010863860169
1
Iteration 6400: Loss = -10954.007904602378
Iteration 6500: Loss = -10954.009392560414
1
Iteration 6600: Loss = -10954.00782363244
Iteration 6700: Loss = -10954.00781764339
Iteration 6800: Loss = -10954.007759131753
Iteration 6900: Loss = -10954.007736450125
Iteration 7000: Loss = -10954.007719472786
Iteration 7100: Loss = -10954.00799746532
1
Iteration 7200: Loss = -10954.007695577087
Iteration 7300: Loss = -10954.013230377112
1
Iteration 7400: Loss = -10954.00763229255
Iteration 7500: Loss = -10954.007851771472
1
Iteration 7600: Loss = -10954.009196503632
2
Iteration 7700: Loss = -10954.012861418641
3
Iteration 7800: Loss = -10954.007553881818
Iteration 7900: Loss = -10954.010387509277
1
Iteration 8000: Loss = -10954.007499567724
Iteration 8100: Loss = -10954.106239240598
1
Iteration 8200: Loss = -10954.007523692619
2
Iteration 8300: Loss = -10954.007488415104
Iteration 8400: Loss = -10954.017950584912
1
Iteration 8500: Loss = -10954.007433675675
Iteration 8600: Loss = -10954.007448395381
1
Iteration 8700: Loss = -10954.012159402653
2
Iteration 8800: Loss = -10954.007426053699
Iteration 8900: Loss = -10954.007412011877
Iteration 9000: Loss = -10954.007381641914
Iteration 9100: Loss = -10954.01024437033
1
Iteration 9200: Loss = -10954.007404039294
2
Iteration 9300: Loss = -10954.007380526002
Iteration 9400: Loss = -10954.012395879554
1
Iteration 9500: Loss = -10954.009562967476
2
Iteration 9600: Loss = -10954.007376343712
Iteration 9700: Loss = -10954.0601412817
1
Iteration 9800: Loss = -10954.007335259685
Iteration 9900: Loss = -10954.013223705986
1
Iteration 10000: Loss = -10954.007367130716
2
Iteration 10100: Loss = -10954.007324083163
Iteration 10200: Loss = -10954.058134054767
1
Iteration 10300: Loss = -10954.007298497863
Iteration 10400: Loss = -10954.007285036982
Iteration 10500: Loss = -10954.008422819858
1
Iteration 10600: Loss = -10954.007285061536
2
Iteration 10700: Loss = -10954.007305319761
3
Iteration 10800: Loss = -10954.008622234756
4
Iteration 10900: Loss = -10954.007350737149
5
Stopping early at iteration 10900 due to no improvement.
pi: tensor([[9.8360e-01, 1.6404e-02],
        [9.9996e-01, 3.9191e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9792, 0.0208], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1626, 0.0574],
         [0.5629, 0.1779]],

        [[0.5385, 0.1957],
         [0.5013, 0.6133]],

        [[0.5481, 0.2408],
         [0.5354, 0.7228]],

        [[0.5252, 0.0681],
         [0.5938, 0.6894]],

        [[0.6258, 0.1406],
         [0.5888, 0.6894]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 44
Adjusted Rand Index: 0.00776683106263838
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.000816640245952827
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 58
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0006863796963546269
Average Adjusted Rand Index: 0.0017166942617182414
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21774.152324902523
Iteration 100: Loss = -10960.219151432522
Iteration 200: Loss = -10959.048060763005
Iteration 300: Loss = -10958.518450613368
Iteration 400: Loss = -10958.236051470578
Iteration 500: Loss = -10958.072137824047
Iteration 600: Loss = -10957.966895018399
Iteration 700: Loss = -10957.890395884442
Iteration 800: Loss = -10957.824998770304
Iteration 900: Loss = -10957.756305220979
Iteration 1000: Loss = -10957.663963295217
Iteration 1100: Loss = -10957.524728568975
Iteration 1200: Loss = -10957.366424836255
Iteration 1300: Loss = -10957.213839581502
Iteration 1400: Loss = -10957.068805586736
Iteration 1500: Loss = -10956.935614410877
Iteration 1600: Loss = -10956.822203288551
Iteration 1700: Loss = -10956.733403572694
Iteration 1800: Loss = -10956.65979812042
Iteration 1900: Loss = -10956.59412294199
Iteration 2000: Loss = -10956.531804208957
Iteration 2100: Loss = -10956.465343708303
Iteration 2200: Loss = -10956.37872414731
Iteration 2300: Loss = -10956.24172847903
Iteration 2400: Loss = -10956.01985300313
Iteration 2500: Loss = -10955.570634033895
Iteration 2600: Loss = -10954.642958094535
Iteration 2700: Loss = -10950.974903616427
Iteration 2800: Loss = -10798.195898069996
Iteration 2900: Loss = -10794.204253677013
Iteration 3000: Loss = -10793.42432692447
Iteration 3100: Loss = -10792.471787841803
Iteration 3200: Loss = -10792.420565330534
Iteration 3300: Loss = -10792.374494602465
Iteration 3400: Loss = -10792.286803325447
Iteration 3500: Loss = -10791.585630586835
Iteration 3600: Loss = -10790.651721318789
Iteration 3700: Loss = -10790.648518971057
Iteration 3800: Loss = -10790.635111025631
Iteration 3900: Loss = -10790.629477602479
Iteration 4000: Loss = -10790.624898737204
Iteration 4100: Loss = -10790.622833066924
Iteration 4200: Loss = -10790.623008899891
1
Iteration 4300: Loss = -10790.614647646289
Iteration 4400: Loss = -10790.611115264786
Iteration 4500: Loss = -10790.607764135586
Iteration 4600: Loss = -10790.5998822341
Iteration 4700: Loss = -10790.550794593457
Iteration 4800: Loss = -10790.398580874178
Iteration 4900: Loss = -10790.393532184884
Iteration 5000: Loss = -10790.391463466829
Iteration 5100: Loss = -10790.38673923029
Iteration 5200: Loss = -10790.383559933998
Iteration 5300: Loss = -10790.30484719381
Iteration 5400: Loss = -10790.300674116143
Iteration 5500: Loss = -10790.299130537915
Iteration 5600: Loss = -10790.29721190369
Iteration 5700: Loss = -10790.294409376302
Iteration 5800: Loss = -10790.288387346534
Iteration 5900: Loss = -10790.254460607679
Iteration 6000: Loss = -10790.200036730783
Iteration 6100: Loss = -10790.20251254752
1
Iteration 6200: Loss = -10790.157264725862
Iteration 6300: Loss = -10789.966645382703
Iteration 6400: Loss = -10789.965427952133
Iteration 6500: Loss = -10789.96448338523
Iteration 6600: Loss = -10789.97339688252
1
Iteration 6700: Loss = -10789.956401567055
Iteration 6800: Loss = -10789.955987817875
Iteration 6900: Loss = -10789.958609454154
1
Iteration 7000: Loss = -10789.963568531195
2
Iteration 7100: Loss = -10789.956752655022
3
Iteration 7200: Loss = -10789.953325752378
Iteration 7300: Loss = -10789.95305986712
Iteration 7400: Loss = -10789.954388119751
1
Iteration 7500: Loss = -10789.963370793392
2
Iteration 7600: Loss = -10789.951965806553
Iteration 7700: Loss = -10789.951576536148
Iteration 7800: Loss = -10789.951611096787
1
Iteration 7900: Loss = -10789.952149030732
2
Iteration 8000: Loss = -10789.952216814803
3
Iteration 8100: Loss = -10789.958462008732
4
Iteration 8200: Loss = -10790.006538091282
5
Stopping early at iteration 8200 due to no improvement.
pi: tensor([[0.7235, 0.2765],
        [0.2213, 0.7787]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5184, 0.4816], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2598, 0.1060],
         [0.6731, 0.1971]],

        [[0.5028, 0.0990],
         [0.5895, 0.6928]],

        [[0.5811, 0.0936],
         [0.6481, 0.5866]],

        [[0.5261, 0.0940],
         [0.6690, 0.5347]],

        [[0.5866, 0.0909],
         [0.5611, 0.6732]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
tensor([0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 1])
Difference count: 4
Adjusted Rand Index: 0.8448436976567872
time is 1
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208080808080809
time is 2
tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824200963560147
time is 3
tensor([0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 1, 0])
tensor([0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824127248460211
time is 4
tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0])
tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,
        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 5
Adjusted Rand Index: 0.8079046942578025
Global Adjusted Rand Index: 0.8683604925834989
Average Adjusted Rand Index: 0.8676778587849412
10833.813229028518
[0.8683604925834989, 0.0001967235538417382, 0.0006863796963546269, 0.8683604925834989] [0.8676778587849412, 0.0001633280491905654, 0.0017166942617182414, 0.8676778587849412] [10789.94526313197, 10956.329222511711, 10954.007350737149, 10790.006538091282]
-----------------------------------------------------------------------------------------
This iteration is 11
True Objective function: Loss = -10723.002435867535
Iteration 0: Loss = -10786.858558277527
Iteration 10: Loss = -10772.095828521182
Iteration 20: Loss = -10771.151869449375
Iteration 30: Loss = -10770.590329077213
Iteration 40: Loss = -10770.393949674402
Iteration 50: Loss = -10770.31962964464
Iteration 60: Loss = -10770.283064322763
Iteration 70: Loss = -10770.262637041025
Iteration 80: Loss = -10770.250466009164
Iteration 90: Loss = -10770.243090984064
Iteration 100: Loss = -10770.23852823987
Iteration 110: Loss = -10770.235698202312
Iteration 120: Loss = -10770.23394847648
Iteration 130: Loss = -10770.23281609807
Iteration 140: Loss = -10770.232102129768
Iteration 150: Loss = -10770.231624913384
Iteration 160: Loss = -10770.231384287772
Iteration 170: Loss = -10770.23120037497
Iteration 180: Loss = -10770.231125179344
Iteration 190: Loss = -10770.231057064744
Iteration 200: Loss = -10770.230995836728
Iteration 210: Loss = -10770.230985594373
Iteration 220: Loss = -10770.230949058561
Iteration 230: Loss = -10770.230936456062
Iteration 240: Loss = -10770.230962925047
1
Iteration 250: Loss = -10770.230947803906
2
Iteration 260: Loss = -10770.230923330906
Iteration 270: Loss = -10770.230944933444
1
Iteration 280: Loss = -10770.230899556225
Iteration 290: Loss = -10770.23094471604
1
Iteration 300: Loss = -10770.23093594232
2
Iteration 310: Loss = -10770.230893242933
Iteration 320: Loss = -10770.230881304235
Iteration 330: Loss = -10770.230860269126
Iteration 340: Loss = -10770.230825572206
Iteration 350: Loss = -10770.230770959148
Iteration 360: Loss = -10770.230623734098
Iteration 370: Loss = -10770.230418170679
Iteration 380: Loss = -10770.230119810794
Iteration 390: Loss = -10770.229517120395
Iteration 400: Loss = -10770.228659849627
Iteration 410: Loss = -10770.22746108138
Iteration 420: Loss = -10770.225881163855
Iteration 430: Loss = -10770.224185623498
Iteration 440: Loss = -10770.22251782192
Iteration 450: Loss = -10770.221083415261
Iteration 460: Loss = -10770.219907022765
Iteration 470: Loss = -10770.219006529927
Iteration 480: Loss = -10770.218320727256
Iteration 490: Loss = -10770.217842454904
Iteration 500: Loss = -10770.217455770395
Iteration 510: Loss = -10770.217216351624
Iteration 520: Loss = -10770.216975842723
Iteration 530: Loss = -10770.216852939944
Iteration 540: Loss = -10770.216724257238
Iteration 550: Loss = -10770.216674767464
Iteration 560: Loss = -10770.216637983493
Iteration 570: Loss = -10770.21658140639
Iteration 580: Loss = -10770.216540168474
Iteration 590: Loss = -10770.216539949068
Iteration 600: Loss = -10770.216548698208
1
Iteration 610: Loss = -10770.21655068884
2
Iteration 620: Loss = -10770.216533023522
Iteration 630: Loss = -10770.216507872137
Iteration 640: Loss = -10770.216526163369
1
Iteration 650: Loss = -10770.21648321151
Iteration 660: Loss = -10770.216520118878
1
Iteration 670: Loss = -10770.21649887374
2
Iteration 680: Loss = -10770.216514832193
3
Stopping early at iteration 680 due to no improvement.
pi: tensor([[0.9692, 0.0308],
        [0.9857, 0.0143]], dtype=torch.float64)
alpha: tensor([0.9654, 0.0346])
beta: tensor([[[0.1528, 0.2388],
         [0.3463, 0.2686]],

        [[0.9014, 0.1459],
         [0.8822, 0.8481]],

        [[0.2149, 0.2078],
         [0.0668, 0.8883]],

        [[0.1028, 0.2193],
         [0.3400, 0.5708]],

        [[0.9097, 0.1209],
         [0.0333, 0.1421]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.006464646464646465
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.00018707291620490558
Average Adjusted Rand Index: -0.0011311850429373803
Iteration 0: Loss = -10994.308217920176
Iteration 10: Loss = -10768.238965711886
Iteration 20: Loss = -10768.166368287031
Iteration 30: Loss = -10768.222149164476
1
Iteration 40: Loss = -10768.340021516547
2
Iteration 50: Loss = -10768.486892965622
3
Stopping early at iteration 50 due to no improvement.
pi: tensor([[0.9106, 0.0894],
        [0.9528, 0.0472]], dtype=torch.float64)
alpha: tensor([0.9095, 0.0905])
beta: tensor([[[0.1479, 0.2207],
         [0.4866, 0.2460]],

        [[0.5869, 0.1589],
         [0.0792, 0.7949]],

        [[0.3994, 0.1949],
         [0.7193, 0.6172]],

        [[0.6966, 0.1957],
         [0.2371, 0.5073]],

        [[0.5115, 0.1728],
         [0.3060, 0.8032]]], dtype=torch.float64)
time is 0
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.009696969696969697
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: 0.0016672277529296718
time is 3
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.000272027817601613
Average Adjusted Rand Index: -0.0014442041388160922
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20835.038411257647
Iteration 100: Loss = -10776.858933633792
Iteration 200: Loss = -10775.189144446973
Iteration 300: Loss = -10771.377084106973
Iteration 400: Loss = -10769.159564673335
Iteration 500: Loss = -10768.745435495455
Iteration 600: Loss = -10768.441064592713
Iteration 700: Loss = -10768.186297237888
Iteration 800: Loss = -10767.973095776966
Iteration 900: Loss = -10767.802407109042
Iteration 1000: Loss = -10767.67234659058
Iteration 1100: Loss = -10767.567197234772
Iteration 1200: Loss = -10767.475671839717
Iteration 1300: Loss = -10767.393823994093
Iteration 1400: Loss = -10767.319687782194
Iteration 1500: Loss = -10767.252259687833
Iteration 1600: Loss = -10767.19022367563
Iteration 1700: Loss = -10767.133207418785
Iteration 1800: Loss = -10767.08136878103
Iteration 1900: Loss = -10767.034728788642
Iteration 2000: Loss = -10766.993576073119
Iteration 2100: Loss = -10766.958295237186
Iteration 2200: Loss = -10766.929428229481
Iteration 2300: Loss = -10766.906733886884
Iteration 2400: Loss = -10766.889089328228
Iteration 2500: Loss = -10766.875512723856
Iteration 2600: Loss = -10766.865393382941
Iteration 2700: Loss = -10766.857814417586
Iteration 2800: Loss = -10766.852121340766
Iteration 2900: Loss = -10766.847647159151
Iteration 3000: Loss = -10766.844145024153
Iteration 3100: Loss = -10766.841339703718
Iteration 3200: Loss = -10766.839121546556
Iteration 3300: Loss = -10766.837290289906
Iteration 3400: Loss = -10766.835710652676
Iteration 3500: Loss = -10766.834253996692
Iteration 3600: Loss = -10766.832911313413
Iteration 3700: Loss = -10766.831700243156
Iteration 3800: Loss = -10766.830499744323
Iteration 3900: Loss = -10766.829502954588
Iteration 4000: Loss = -10766.82843723199
Iteration 4100: Loss = -10766.827506137088
Iteration 4200: Loss = -10766.826767984876
Iteration 4300: Loss = -10766.825781375625
Iteration 4400: Loss = -10766.825093947995
Iteration 4500: Loss = -10766.824278105452
Iteration 4600: Loss = -10766.82361051791
Iteration 4700: Loss = -10766.823047842045
Iteration 4800: Loss = -10766.822551111441
Iteration 4900: Loss = -10766.82183087135
Iteration 5000: Loss = -10766.82164434745
Iteration 5100: Loss = -10766.820733853832
Iteration 5200: Loss = -10766.820299623558
Iteration 5300: Loss = -10766.819789214891
Iteration 5400: Loss = -10766.81940432999
Iteration 5500: Loss = -10766.818987821716
Iteration 5600: Loss = -10766.818855698406
Iteration 5700: Loss = -10766.818251550692
Iteration 5800: Loss = -10766.818034665062
Iteration 5900: Loss = -10766.818009568122
Iteration 6000: Loss = -10766.81876224064
1
Iteration 6100: Loss = -10766.817175466482
Iteration 6200: Loss = -10766.816814620955
Iteration 6300: Loss = -10766.816473180841
Iteration 6400: Loss = -10766.816423654142
Iteration 6500: Loss = -10766.816356815727
Iteration 6600: Loss = -10766.81599745565
Iteration 6700: Loss = -10766.815620184407
Iteration 6800: Loss = -10766.815404650655
Iteration 6900: Loss = -10766.815231777791
Iteration 7000: Loss = -10766.81516978847
Iteration 7100: Loss = -10766.81756752632
1
Iteration 7200: Loss = -10766.814889841926
Iteration 7300: Loss = -10766.81453975591
Iteration 7400: Loss = -10766.815236492652
1
Iteration 7500: Loss = -10766.814268302385
Iteration 7600: Loss = -10766.814589872038
1
Iteration 7700: Loss = -10766.814053149052
Iteration 7800: Loss = -10766.813914265304
Iteration 7900: Loss = -10766.814321987134
1
Iteration 8000: Loss = -10766.813732501398
Iteration 8100: Loss = -10766.813579547124
Iteration 8200: Loss = -10766.813655921154
1
Iteration 8300: Loss = -10766.8134246407
Iteration 8400: Loss = -10766.830353379626
1
Iteration 8500: Loss = -10766.813232602339
Iteration 8600: Loss = -10766.813192529084
Iteration 8700: Loss = -10766.831506344675
1
Iteration 8800: Loss = -10766.813096048692
Iteration 8900: Loss = -10766.81297973507
Iteration 9000: Loss = -10766.813522476301
1
Iteration 9100: Loss = -10766.812831567793
Iteration 9200: Loss = -10766.814858536853
1
Iteration 9300: Loss = -10766.81275531501
Iteration 9400: Loss = -10766.812677257725
Iteration 9500: Loss = -10766.816675805889
1
Iteration 9600: Loss = -10766.812594811745
Iteration 9700: Loss = -10766.812557648638
Iteration 9800: Loss = -10766.813026711163
1
Iteration 9900: Loss = -10766.812461250009
Iteration 10000: Loss = -10766.829114591856
1
Iteration 10100: Loss = -10766.812402989099
Iteration 10200: Loss = -10766.81237676446
Iteration 10300: Loss = -10766.846184940716
1
Iteration 10400: Loss = -10766.812323496062
Iteration 10500: Loss = -10766.81227060151
Iteration 10600: Loss = -10766.867685825246
1
Iteration 10700: Loss = -10766.8122153986
Iteration 10800: Loss = -10766.812208682637
Iteration 10900: Loss = -10766.813294576756
1
Iteration 11000: Loss = -10766.819693686017
2
Iteration 11100: Loss = -10766.81234565783
3
Iteration 11200: Loss = -10766.812886218991
4
Iteration 11300: Loss = -10766.81218244312
Iteration 11400: Loss = -10766.819470774804
1
Iteration 11500: Loss = -10766.819252953854
2
Iteration 11600: Loss = -10766.816652170264
3
Iteration 11700: Loss = -10766.885850990202
4
Iteration 11800: Loss = -10766.813114662471
5
Stopping early at iteration 11800 due to no improvement.
pi: tensor([[7.5813e-05, 9.9992e-01],
        [1.0448e-01, 8.9552e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2107, 0.7893], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2650, 0.1972],
         [0.6005, 0.1469]],

        [[0.6352, 0.1653],
         [0.6828, 0.7290]],

        [[0.6182, 0.1977],
         [0.7052, 0.5370]],

        [[0.5759, 0.1963],
         [0.5480, 0.7110]],

        [[0.5104, 0.1797],
         [0.7194, 0.5532]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.009696969696969697
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.003506908785360844
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0022753246940548423
Average Adjusted Rand Index: 0.0028025199464580207
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23478.96870123904
Iteration 100: Loss = -10777.299161934485
Iteration 200: Loss = -10775.485478322138
Iteration 300: Loss = -10770.451210473651
Iteration 400: Loss = -10769.224189703305
Iteration 500: Loss = -10768.94230809714
Iteration 600: Loss = -10768.625830708701
Iteration 700: Loss = -10768.342574876613
Iteration 800: Loss = -10768.125544331657
Iteration 900: Loss = -10767.95262965359
Iteration 1000: Loss = -10767.812908583819
Iteration 1100: Loss = -10767.695137780534
Iteration 1200: Loss = -10767.59279626832
Iteration 1300: Loss = -10767.50201041152
Iteration 1400: Loss = -10767.419669613482
Iteration 1500: Loss = -10767.343769520723
Iteration 1600: Loss = -10767.27282099986
Iteration 1700: Loss = -10767.206038600652
Iteration 1800: Loss = -10767.14390504331
Iteration 1900: Loss = -10767.087024140179
Iteration 2000: Loss = -10767.036112427875
Iteration 2100: Loss = -10766.991442877028
Iteration 2200: Loss = -10766.954066388693
Iteration 2300: Loss = -10766.92433965043
Iteration 2400: Loss = -10766.901829199927
Iteration 2500: Loss = -10766.885329066607
Iteration 2600: Loss = -10766.873314578697
Iteration 2700: Loss = -10766.864578919603
Iteration 2800: Loss = -10766.85822319984
Iteration 2900: Loss = -10766.853648802824
Iteration 3000: Loss = -10766.850168742221
Iteration 3100: Loss = -10766.847336975301
Iteration 3200: Loss = -10766.84487733694
Iteration 3300: Loss = -10766.842787190624
Iteration 3400: Loss = -10766.840869865444
Iteration 3500: Loss = -10766.839133617039
Iteration 3600: Loss = -10766.837477033592
Iteration 3700: Loss = -10766.836017940228
Iteration 3800: Loss = -10766.834619138675
Iteration 3900: Loss = -10766.833282376287
Iteration 4000: Loss = -10766.832103298306
Iteration 4100: Loss = -10766.830972044041
Iteration 4200: Loss = -10766.829839796066
Iteration 4300: Loss = -10766.83799935681
1
Iteration 4400: Loss = -10766.827917712833
Iteration 4500: Loss = -10766.827040965627
Iteration 4600: Loss = -10766.826234280144
Iteration 4700: Loss = -10766.82542507654
Iteration 4800: Loss = -10766.824707963582
Iteration 4900: Loss = -10766.823976099793
Iteration 5000: Loss = -10766.823333186088
Iteration 5100: Loss = -10766.823353664196
1
Iteration 5200: Loss = -10766.82213802572
Iteration 5300: Loss = -10766.82160241881
Iteration 5400: Loss = -10766.821065377228
Iteration 5500: Loss = -10766.82062427303
Iteration 5600: Loss = -10766.820670474875
1
Iteration 5700: Loss = -10766.81966703226
Iteration 5800: Loss = -10766.81934967615
Iteration 5900: Loss = -10766.818888565715
Iteration 6000: Loss = -10766.818518655558
Iteration 6100: Loss = -10766.818268248338
Iteration 6200: Loss = -10766.820498758163
1
Iteration 6300: Loss = -10766.81767858575
Iteration 6400: Loss = -10766.817543193409
Iteration 6500: Loss = -10766.81701709605
Iteration 6600: Loss = -10766.818547597739
1
Iteration 6700: Loss = -10766.817596644236
2
Iteration 6800: Loss = -10766.816236461204
Iteration 6900: Loss = -10766.816020412549
Iteration 7000: Loss = -10766.815795435485
Iteration 7100: Loss = -10766.816163979644
1
Iteration 7200: Loss = -10766.815347651802
Iteration 7300: Loss = -10766.815183728468
Iteration 7400: Loss = -10766.815008026038
Iteration 7500: Loss = -10766.814840076582
Iteration 7600: Loss = -10766.814695013423
Iteration 7700: Loss = -10766.814612376542
Iteration 7800: Loss = -10766.814421457164
Iteration 7900: Loss = -10766.814260255398
Iteration 8000: Loss = -10766.816292100173
1
Iteration 8100: Loss = -10766.814009977325
Iteration 8200: Loss = -10766.81554754441
1
Iteration 8300: Loss = -10766.813857221956
Iteration 8400: Loss = -10766.813707233858
Iteration 8500: Loss = -10766.81358947229
Iteration 8600: Loss = -10766.813833968781
1
Iteration 8700: Loss = -10766.813397269083
Iteration 8800: Loss = -10766.8133926001
Iteration 8900: Loss = -10766.813241539156
Iteration 9000: Loss = -10766.813157665716
Iteration 9100: Loss = -10766.814276781957
1
Iteration 9200: Loss = -10766.813040648252
Iteration 9300: Loss = -10766.81297394932
Iteration 9400: Loss = -10766.812912806994
Iteration 9500: Loss = -10766.812893622875
Iteration 9600: Loss = -10766.812765498755
Iteration 9700: Loss = -10766.876525268883
1
Iteration 9800: Loss = -10766.812681750143
Iteration 9900: Loss = -10766.812696018953
1
Iteration 10000: Loss = -10766.813806068963
2
Iteration 10100: Loss = -10766.812536635653
Iteration 10200: Loss = -10766.985416754696
1
Iteration 10300: Loss = -10766.812449521305
Iteration 10400: Loss = -10766.812436091266
Iteration 10500: Loss = -10766.816265960058
1
Iteration 10600: Loss = -10766.812351323024
Iteration 10700: Loss = -10766.812347100178
Iteration 10800: Loss = -10766.812692258725
1
Iteration 10900: Loss = -10766.812286633394
Iteration 11000: Loss = -10767.046776043044
1
Iteration 11100: Loss = -10766.812262810023
Iteration 11200: Loss = -10766.816773001186
1
Iteration 11300: Loss = -10766.812206605853
Iteration 11400: Loss = -10766.81385506424
1
Iteration 11500: Loss = -10766.82277648718
2
Iteration 11600: Loss = -10766.81265208031
3
Iteration 11700: Loss = -10766.8124975585
4
Iteration 11800: Loss = -10766.838847006216
5
Stopping early at iteration 11800 due to no improvement.
pi: tensor([[8.9506e-01, 1.0494e-01],
        [9.9991e-01, 9.2793e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7881, 0.2119], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1466, 0.1965],
         [0.5659, 0.2650]],

        [[0.5578, 0.1655],
         [0.5386, 0.6004]],

        [[0.7177, 0.1974],
         [0.6088, 0.6204]],

        [[0.6419, 0.1955],
         [0.5543, 0.6135]],

        [[0.7163, 0.1798],
         [0.7251, 0.7147]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.009696969696969697
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.003506908785360844
time is 3
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0022753246940548423
Average Adjusted Rand Index: 0.0028025199464580207
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21285.966941593175
Iteration 100: Loss = -10776.96213723499
Iteration 200: Loss = -10775.034430398564
Iteration 300: Loss = -10769.795903576367
Iteration 400: Loss = -10769.049941951112
Iteration 500: Loss = -10768.69328603996
Iteration 600: Loss = -10768.389411021893
Iteration 700: Loss = -10768.140925351538
Iteration 800: Loss = -10767.945854912663
Iteration 900: Loss = -10767.79028561531
Iteration 1000: Loss = -10767.666072067183
Iteration 1100: Loss = -10767.561563765583
Iteration 1200: Loss = -10767.470264794381
Iteration 1300: Loss = -10767.389977004237
Iteration 1400: Loss = -10767.318190968903
Iteration 1500: Loss = -10767.25259755438
Iteration 1600: Loss = -10767.191677138295
Iteration 1700: Loss = -10767.135243927918
Iteration 1800: Loss = -10767.083866390123
Iteration 1900: Loss = -10767.037614114228
Iteration 2000: Loss = -10766.99669657991
Iteration 2100: Loss = -10766.961640048434
Iteration 2200: Loss = -10766.932878143984
Iteration 2300: Loss = -10766.91008879294
Iteration 2400: Loss = -10766.89232585553
Iteration 2500: Loss = -10766.878741780207
Iteration 2600: Loss = -10766.868655593751
Iteration 2700: Loss = -10766.861132508973
Iteration 2800: Loss = -10766.855339781765
Iteration 2900: Loss = -10766.8508175411
Iteration 3000: Loss = -10766.847279344574
Iteration 3100: Loss = -10766.84445167738
Iteration 3200: Loss = -10766.84221399879
Iteration 3300: Loss = -10766.840256888125
Iteration 3400: Loss = -10766.838547077648
Iteration 3500: Loss = -10766.836962213132
Iteration 3600: Loss = -10766.835475690752
Iteration 3700: Loss = -10766.834061800553
Iteration 3800: Loss = -10766.832898539691
Iteration 3900: Loss = -10766.831645803088
Iteration 4000: Loss = -10766.830517315275
Iteration 4100: Loss = -10766.829480974295
Iteration 4200: Loss = -10766.828500467991
Iteration 4300: Loss = -10766.827574624
Iteration 4400: Loss = -10766.826665939065
Iteration 4500: Loss = -10766.825890552433
Iteration 4600: Loss = -10766.826313343667
1
Iteration 4700: Loss = -10766.824384893454
Iteration 4800: Loss = -10766.823708592989
Iteration 4900: Loss = -10766.8236157496
Iteration 5000: Loss = -10766.822472415246
Iteration 5100: Loss = -10766.821903879969
Iteration 5200: Loss = -10766.821462155736
Iteration 5300: Loss = -10766.820859561394
Iteration 5400: Loss = -10766.820393116164
Iteration 5500: Loss = -10766.821175357327
1
Iteration 5600: Loss = -10766.819610982328
Iteration 5700: Loss = -10766.819124880722
Iteration 5800: Loss = -10766.818762040251
Iteration 5900: Loss = -10766.818434673778
Iteration 6000: Loss = -10766.818141614907
Iteration 6100: Loss = -10766.818699699617
1
Iteration 6200: Loss = -10766.826716428846
2
Iteration 6300: Loss = -10766.817135005138
Iteration 6400: Loss = -10766.816862176713
Iteration 6500: Loss = -10766.816626734544
Iteration 6600: Loss = -10766.816351917541
Iteration 6700: Loss = -10766.816402798899
1
Iteration 6800: Loss = -10766.815890492988
Iteration 6900: Loss = -10766.81568051759
Iteration 7000: Loss = -10766.824218972979
1
Iteration 7100: Loss = -10766.815287302805
Iteration 7200: Loss = -10766.815898634664
1
Iteration 7300: Loss = -10766.814933311598
Iteration 7400: Loss = -10766.814895263557
Iteration 7500: Loss = -10766.814583532581
Iteration 7600: Loss = -10766.814678006296
1
Iteration 7700: Loss = -10766.932941867219
2
Iteration 7800: Loss = -10766.814189514616
Iteration 7900: Loss = -10766.81605445473
1
Iteration 8000: Loss = -10766.813922521562
Iteration 8100: Loss = -10766.814150300628
1
Iteration 8200: Loss = -10766.813728170373
Iteration 8300: Loss = -10766.81646865809
1
Iteration 8400: Loss = -10766.813545588735
Iteration 8500: Loss = -10766.813658730907
1
Iteration 8600: Loss = -10766.81339219847
Iteration 8700: Loss = -10766.813274503598
Iteration 8800: Loss = -10766.813240655954
Iteration 8900: Loss = -10766.813217201148
Iteration 9000: Loss = -10766.813093586063
Iteration 9100: Loss = -10766.813276955754
1
Iteration 9200: Loss = -10766.812923532076
Iteration 9300: Loss = -10766.81287947161
Iteration 9400: Loss = -10766.832489058452
1
Iteration 9500: Loss = -10766.812791854736
Iteration 9600: Loss = -10766.812704046577
Iteration 9700: Loss = -10766.813934802793
1
Iteration 9800: Loss = -10766.812644563946
Iteration 9900: Loss = -10766.816757572955
1
Iteration 10000: Loss = -10766.812548817334
Iteration 10100: Loss = -10766.812469680077
Iteration 10200: Loss = -10766.8292597668
1
Iteration 10300: Loss = -10766.81240838255
Iteration 10400: Loss = -10766.812382572782
Iteration 10500: Loss = -10766.812774612918
1
Iteration 10600: Loss = -10766.812314377808
Iteration 10700: Loss = -10766.979120603404
1
Iteration 10800: Loss = -10766.81225780613
Iteration 10900: Loss = -10766.822012150076
1
Iteration 11000: Loss = -10766.841955106145
2
Iteration 11100: Loss = -10766.830386774263
3
Iteration 11200: Loss = -10766.844417219838
4
Iteration 11300: Loss = -10766.812179606588
Iteration 11400: Loss = -10766.812211115588
1
Iteration 11500: Loss = -10766.861633048304
2
Iteration 11600: Loss = -10766.813541113737
3
Iteration 11700: Loss = -10766.813511437835
4
Iteration 11800: Loss = -10766.81405990926
5
Stopping early at iteration 11800 due to no improvement.
pi: tensor([[8.9567e-01, 1.0433e-01],
        [9.9991e-01, 8.5621e-05]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.7899, 0.2101], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1470, 0.1976],
         [0.6311, 0.2650]],

        [[0.7056, 0.1651],
         [0.6056, 0.6853]],

        [[0.5235, 0.1979],
         [0.6460, 0.6127]],

        [[0.6690, 0.1965],
         [0.7116, 0.6771]],

        [[0.6567, 0.1794],
         [0.5893, 0.5264]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 57
Adjusted Rand Index: 0.009696969696969697
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.003506908785360844
time is 3
tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.000808721249959564
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0022753246940548423
Average Adjusted Rand Index: 0.0028025199464580207
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23158.780884098393
Iteration 100: Loss = -10776.325515165532
Iteration 200: Loss = -10774.287037847476
Iteration 300: Loss = -10769.739358914694
Iteration 400: Loss = -10769.172654435313
Iteration 500: Loss = -10768.867753181328
Iteration 600: Loss = -10768.521349539553
Iteration 700: Loss = -10768.221868380933
Iteration 800: Loss = -10767.993924767552
Iteration 900: Loss = -10767.813035799412
Iteration 1000: Loss = -10767.672940281804
Iteration 1100: Loss = -10767.55990464573
Iteration 1200: Loss = -10767.463222706376
Iteration 1300: Loss = -10767.376990668388
Iteration 1400: Loss = -10767.298142340926
Iteration 1500: Loss = -10767.225217987874
Iteration 1600: Loss = -10767.15800533862
Iteration 1700: Loss = -10767.096926427714
Iteration 1800: Loss = -10767.042510053112
Iteration 1900: Loss = -10766.99535599928
Iteration 2000: Loss = -10766.956066444205
Iteration 2100: Loss = -10766.924791429761
Iteration 2200: Loss = -10766.900799316501
Iteration 2300: Loss = -10766.883165353376
Iteration 2400: Loss = -10766.87054983093
Iteration 2500: Loss = -10766.861404280102
Iteration 2600: Loss = -10766.854768882711
Iteration 2700: Loss = -10766.84990314294
Iteration 2800: Loss = -10766.84631220802
Iteration 2900: Loss = -10766.843539233767
Iteration 3000: Loss = -10766.841182084147
Iteration 3100: Loss = -10766.839223501254
Iteration 3200: Loss = -10766.83744108007
Iteration 3300: Loss = -10766.835829168162
Iteration 3400: Loss = -10766.834317653507
Iteration 3500: Loss = -10766.833128553004
Iteration 3600: Loss = -10766.831716934163
Iteration 3700: Loss = -10766.830485341969
Iteration 3800: Loss = -10766.830536618221
1
Iteration 3900: Loss = -10766.828359019282
Iteration 4000: Loss = -10766.827455738661
Iteration 4100: Loss = -10766.826560279893
Iteration 4200: Loss = -10766.827899364955
1
Iteration 4300: Loss = -10766.824918400529
Iteration 4400: Loss = -10766.824290744444
Iteration 4500: Loss = -10766.823985085486
Iteration 4600: Loss = -10766.822858404252
Iteration 4700: Loss = -10766.82219570602
Iteration 4800: Loss = -10766.82219395831
Iteration 4900: Loss = -10766.82107620803
Iteration 5000: Loss = -10766.82068234289
Iteration 5100: Loss = -10766.820110557996
Iteration 5200: Loss = -10766.819654620149
Iteration 5300: Loss = -10766.819355283735
Iteration 5400: Loss = -10766.818837052428
Iteration 5500: Loss = -10766.818506364629
Iteration 5600: Loss = -10766.818087338137
Iteration 5700: Loss = -10766.818023914637
Iteration 5800: Loss = -10766.817442399306
Iteration 5900: Loss = -10766.817149887798
Iteration 6000: Loss = -10766.81717663022
1
Iteration 6100: Loss = -10766.816578661334
Iteration 6200: Loss = -10766.817797438627
1
Iteration 6300: Loss = -10766.816108131941
Iteration 6400: Loss = -10766.815903563089
Iteration 6500: Loss = -10766.816838354933
1
Iteration 6600: Loss = -10766.81574595332
Iteration 6700: Loss = -10766.817957068311
1
Iteration 6800: Loss = -10766.816214764573
2
Iteration 6900: Loss = -10766.814975575764
Iteration 7000: Loss = -10766.814789367296
Iteration 7100: Loss = -10766.814941960976
1
Iteration 7200: Loss = -10766.815053633558
2
Iteration 7300: Loss = -10766.814598970861
Iteration 7400: Loss = -10766.815098808087
1
Iteration 7500: Loss = -10766.814103030249
Iteration 7600: Loss = -10766.814285328439
1
Iteration 7700: Loss = -10766.813846878182
Iteration 7800: Loss = -10766.813724424925
Iteration 7900: Loss = -10766.813635920736
Iteration 8000: Loss = -10766.813573185578
Iteration 8100: Loss = -10766.813482836846
Iteration 8200: Loss = -10766.81337398862
Iteration 8300: Loss = -10766.813571980429
1
Iteration 8400: Loss = -10766.813179843759
Iteration 8500: Loss = -10766.813441842705
1
Iteration 8600: Loss = -10766.813070741815
Iteration 8700: Loss = -10766.813205065066
1
Iteration 8800: Loss = -10766.812947687013
Iteration 8900: Loss = -10766.826881479916
1
Iteration 9000: Loss = -10766.812806988872
Iteration 9100: Loss = -10766.81275726721
Iteration 9200: Loss = -10766.813271520074
1
Iteration 9300: Loss = -10766.81261451813
Iteration 9400: Loss = -10766.812752213202
1
Iteration 9500: Loss = -10766.812635327717
2
Iteration 9600: Loss = -10766.81252380342
Iteration 9700: Loss = -10767.22402673047
1
Iteration 9800: Loss = -10766.812460409068
Iteration 9900: Loss = -10766.812395094135
Iteration 10000: Loss = -10766.813473472914
1
Iteration 10100: Loss = -10766.812345241613
Iteration 10200: Loss = -10766.840499754591
1
Iteration 10300: Loss = -10766.812292329672
Iteration 10400: Loss = -10766.812232925367
Iteration 10500: Loss = -10766.825143758888
1
Iteration 10600: Loss = -10766.812220463886
Iteration 10700: Loss = -10766.812158386707
Iteration 10800: Loss = -10766.812879598769
1
Iteration 10900: Loss = -10766.812166686525
2
Iteration 11000: Loss = -10766.81212882667
Iteration 11100: Loss = -10766.819783492938
1
Iteration 11200: Loss = -10766.814759602701
2
Iteration 11300: Loss = -10766.814164666277
3
Iteration 11400: Loss = -10766.817244529482
4
Iteration 11500: Loss = -10766.812057203655
Iteration 11600: Loss = -10766.813045387502
1
Iteration 11700: Loss = -10766.81333052507
2
Iteration 11800: Loss = -10766.813725043085
3
Iteration 11900: Loss = -10766.813068867415
4
Iteration 12000: Loss = -10766.812058677533
5
Stopping early at iteration 12000 due to no improvement.
pi: tensor([[6.3450e-05, 9.9994e-01],
        [1.0440e-01, 8.9560e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.2104, 0.7896], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2650, 0.1974],
         [0.5976, 0.1469]],

        [[0.6904, 0.1652],
         [0.6461, 0.5824]],

        [[0.7008, 0.1976],
         [0.5638, 0.5373]],

        [[0.6973, 0.1965],
         [0.6503, 0.6113]],

        [[0.5561, 0.1796],
         [0.6380, 0.7184]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.009696969696969697
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.003506908785360844
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        0, 0, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.000808721249959564
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0022753246940548423
Average Adjusted Rand Index: 0.0028025199464580207
10723.002435867535
[0.0022753246940548423, 0.0022753246940548423, 0.0022753246940548423, 0.0022753246940548423] [0.0028025199464580207, 0.0028025199464580207, 0.0028025199464580207, 0.0028025199464580207] [10766.813114662471, 10766.838847006216, 10766.81405990926, 10766.812058677533]
-----------------------------------------------------------------------------------------
This iteration is 12
True Objective function: Loss = -10970.693152888682
pi: tensor([[1.0000e+00, 1.8094e-35],
        [1.0000e+00, 2.2780e-36]], dtype=torch.float64)
alpha: tensor([1.0000e+00, 1.4475e-35])
beta: tensor([[[0.1639, 0.2796],
         [0.7059,    nan]],

        [[0.9483, 0.3000],
         [0.0663, 0.6970]],

        [[0.7656, 0.2642],
         [0.0245, 0.5175]],

        [[0.5692, 0.2624],
         [0.9080, 0.7234]],

        [[0.5716, 0.2500],
         [0.9996, 0.3206]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -11334.62312170616
Iteration 10: Loss = -11102.825965407908
Iteration 20: Loss = -11102.524320786653
Iteration 30: Loss = -11102.507864364152
Iteration 40: Loss = -11102.503537506314
Iteration 50: Loss = -11102.499098160766
Iteration 60: Loss = -11102.493940353912
Iteration 70: Loss = -11102.487958197242
Iteration 80: Loss = -11102.480872070675
Iteration 90: Loss = -11102.472669028804
Iteration 100: Loss = -11102.462985803586
Iteration 110: Loss = -11102.451487466607
Iteration 120: Loss = -11102.437871939013
Iteration 130: Loss = -11102.421607844366
Iteration 140: Loss = -11102.401830578592
Iteration 150: Loss = -11102.377722441457
Iteration 160: Loss = -11102.34761109426
Iteration 170: Loss = -11102.309550854816
Iteration 180: Loss = -11102.260016223145
Iteration 190: Loss = -11102.193779703135
Iteration 200: Loss = -11102.101629477607
Iteration 210: Loss = -11101.968234934191
Iteration 220: Loss = -11101.770803343508
Iteration 230: Loss = -11101.498986413551
Iteration 240: Loss = -11101.219988352832
Iteration 250: Loss = -11101.046172735987
Iteration 260: Loss = -11100.981084643036
Iteration 270: Loss = -11100.969573197299
Iteration 280: Loss = -11100.977380163546
1
Iteration 290: Loss = -11100.990570516926
2
Iteration 300: Loss = -11101.00399823511
3
Stopping early at iteration 300 due to no improvement.
pi: tensor([[0.8405, 0.1595],
        [0.7315, 0.2685]], dtype=torch.float64)
alpha: tensor([0.8199, 0.1801])
beta: tensor([[[0.1801, 0.1503],
         [0.4688, 0.1200]],

        [[0.5916, 0.1413],
         [0.4549, 0.6196]],

        [[0.0873, 0.1302],
         [0.5868, 0.6943]],

        [[0.1483, 0.1086],
         [0.1385, 0.1364]],

        [[0.4868, 0.1280],
         [0.9797, 0.6387]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 43
Adjusted Rand Index: 0.015076275178066338
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.006767238045575733
time is 3
tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 41
Adjusted Rand Index: 0.027469209953202733
time is 4
tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0056450870649491815
Global Adjusted Rand Index: 0.007214181046460957
Average Adjusted Rand Index: 0.010991562048358796
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22939.338397242194
Iteration 100: Loss = -11105.643947975803
Iteration 200: Loss = -11101.103081291245
Iteration 300: Loss = -11100.634064819244
Iteration 400: Loss = -11100.493017328074
Iteration 500: Loss = -11100.434406906883
Iteration 600: Loss = -11100.395445076807
Iteration 700: Loss = -11100.358991119556
Iteration 800: Loss = -11100.31446903985
Iteration 900: Loss = -11100.240833576061
Iteration 1000: Loss = -11099.946281445897
Iteration 1100: Loss = -10996.656312423445
Iteration 1200: Loss = -10948.489068246818
Iteration 1300: Loss = -10943.059620479758
Iteration 1400: Loss = -10942.927937001179
Iteration 1500: Loss = -10942.899399726777
Iteration 1600: Loss = -10942.883199368322
Iteration 1700: Loss = -10942.864550189195
Iteration 1800: Loss = -10942.859655161625
Iteration 1900: Loss = -10942.855323927657
Iteration 2000: Loss = -10942.847174963377
Iteration 2100: Loss = -10942.60066536438
Iteration 2200: Loss = -10942.582959593703
Iteration 2300: Loss = -10942.475596133505
Iteration 2400: Loss = -10942.474367075203
Iteration 2500: Loss = -10942.473607582328
Iteration 2600: Loss = -10942.472901719608
Iteration 2700: Loss = -10942.472303869661
Iteration 2800: Loss = -10942.471696193092
Iteration 2900: Loss = -10942.471174261163
Iteration 3000: Loss = -10942.470784055215
Iteration 3100: Loss = -10942.470675796612
Iteration 3200: Loss = -10942.470124512225
Iteration 3300: Loss = -10942.470129463105
1
Iteration 3400: Loss = -10942.469595946744
Iteration 3500: Loss = -10942.469632324153
1
Iteration 3600: Loss = -10942.473103842793
2
Iteration 3700: Loss = -10942.468878904721
Iteration 3800: Loss = -10942.468692755205
Iteration 3900: Loss = -10942.469116663475
1
Iteration 4000: Loss = -10942.467901449612
Iteration 4100: Loss = -10942.46713984009
Iteration 4200: Loss = -10942.434029534013
Iteration 4300: Loss = -10942.43318981269
Iteration 4400: Loss = -10942.433228521733
1
Iteration 4500: Loss = -10942.456555058658
2
Iteration 4600: Loss = -10942.43299056148
Iteration 4700: Loss = -10942.433517709356
1
Iteration 4800: Loss = -10942.432833933024
Iteration 4900: Loss = -10942.433286655783
1
Iteration 5000: Loss = -10942.433030087375
2
Iteration 5100: Loss = -10942.43641008291
3
Iteration 5200: Loss = -10942.431949463393
Iteration 5300: Loss = -10942.432000760244
1
Iteration 5400: Loss = -10942.43180795065
Iteration 5500: Loss = -10942.431731661789
Iteration 5600: Loss = -10942.432327810326
1
Iteration 5700: Loss = -10942.431367303116
Iteration 5800: Loss = -10942.429984191964
Iteration 5900: Loss = -10942.428596777743
Iteration 6000: Loss = -10942.428042750444
Iteration 6100: Loss = -10942.428560821207
1
Iteration 6200: Loss = -10942.427885456475
Iteration 6300: Loss = -10942.43358401478
1
Iteration 6400: Loss = -10942.427822323458
Iteration 6500: Loss = -10942.451031891273
1
Iteration 6600: Loss = -10942.427799598705
Iteration 6700: Loss = -10942.427729945146
Iteration 6800: Loss = -10942.427790129885
1
Iteration 6900: Loss = -10942.427727241151
Iteration 7000: Loss = -10942.427905847799
1
Iteration 7100: Loss = -10942.427669200972
Iteration 7200: Loss = -10942.446217774359
1
Iteration 7300: Loss = -10942.42764852748
Iteration 7400: Loss = -10942.427623419004
Iteration 7500: Loss = -10942.427837807802
1
Iteration 7600: Loss = -10942.427537369758
Iteration 7700: Loss = -10942.427582594024
1
Iteration 7800: Loss = -10942.427514237079
Iteration 7900: Loss = -10942.428178462218
1
Iteration 8000: Loss = -10942.42747837334
Iteration 8100: Loss = -10942.427505606356
1
Iteration 8200: Loss = -10942.427463423337
Iteration 8300: Loss = -10942.42739188393
Iteration 8400: Loss = -10942.429891468484
1
Iteration 8500: Loss = -10942.42494804086
Iteration 8600: Loss = -10942.426231525073
1
Iteration 8700: Loss = -10942.426123510737
2
Iteration 8800: Loss = -10942.424852742262
Iteration 8900: Loss = -10942.424783568578
Iteration 9000: Loss = -10942.424867517431
1
Iteration 9100: Loss = -10942.43115654161
2
Iteration 9200: Loss = -10942.422604693807
Iteration 9300: Loss = -10942.425758820724
1
Iteration 9400: Loss = -10942.427375910434
2
Iteration 9500: Loss = -10942.431923796075
3
Iteration 9600: Loss = -10942.424949076305
4
Iteration 9700: Loss = -10942.462832490946
5
Stopping early at iteration 9700 due to no improvement.
pi: tensor([[0.7450, 0.2550],
        [0.2668, 0.7332]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4398, 0.5602], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2662, 0.1075],
         [0.5007, 0.2017]],

        [[0.6275, 0.1000],
         [0.5634, 0.5833]],

        [[0.5704, 0.1012],
         [0.5429, 0.5275]],

        [[0.5139, 0.0969],
         [0.6794, 0.6295]],

        [[0.5503, 0.0930],
         [0.5598, 0.6841]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 3
Adjusted Rand Index: 0.8824283882000855
time is 1
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 1
Adjusted Rand Index: 0.9599990596656818
time is 2
tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 6
Adjusted Rand Index: 0.7720997823854144
time is 3
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 3
Adjusted Rand Index: 0.8824145678101706
time is 4
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 0
Adjusted Rand Index: 1.0
Global Adjusted Rand Index: 0.8985008046740597
Average Adjusted Rand Index: 0.8993883596122705
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21225.234127987478
Iteration 100: Loss = -11108.40258602478
Iteration 200: Loss = -11106.752122083593
Iteration 300: Loss = -11105.896311459503
Iteration 400: Loss = -11105.242214289428
Iteration 500: Loss = -11103.955630063494
Iteration 600: Loss = -11103.446257268319
Iteration 700: Loss = -11103.07222129071
Iteration 800: Loss = -11102.78819503596
Iteration 900: Loss = -11102.586058715433
Iteration 1000: Loss = -11102.424862924037
Iteration 1100: Loss = -11102.287677679644
Iteration 1200: Loss = -11102.169525938712
Iteration 1300: Loss = -11102.068838214505
Iteration 1400: Loss = -11101.983680640906
Iteration 1500: Loss = -11101.907918825373
Iteration 1600: Loss = -11101.83912367693
Iteration 1700: Loss = -11101.777002395362
Iteration 1800: Loss = -11101.725133968928
Iteration 1900: Loss = -11101.685214837391
Iteration 2000: Loss = -11101.655732182402
Iteration 2100: Loss = -11101.63354023912
Iteration 2200: Loss = -11101.616285152186
Iteration 2300: Loss = -11101.603223726495
Iteration 2400: Loss = -11101.594513644046
Iteration 2500: Loss = -11101.5884876717
Iteration 2600: Loss = -11101.584283182576
Iteration 2700: Loss = -11101.581375850397
Iteration 2800: Loss = -11101.579246736588
Iteration 2900: Loss = -11101.577507321803
Iteration 3000: Loss = -11101.57597947703
Iteration 3100: Loss = -11101.574819184138
Iteration 3200: Loss = -11101.578520202085
1
Iteration 3300: Loss = -11101.572516270273
Iteration 3400: Loss = -11101.571984791419
Iteration 3500: Loss = -11101.571689876044
Iteration 3600: Loss = -11101.57327723106
1
Iteration 3700: Loss = -11101.571361173168
Iteration 3800: Loss = -11101.570737590791
Iteration 3900: Loss = -11101.585762852535
1
Iteration 4000: Loss = -11101.57068799829
Iteration 4100: Loss = -11101.57066704909
Iteration 4200: Loss = -11101.57075184626
1
Iteration 4300: Loss = -11101.570328358805
Iteration 4400: Loss = -11101.571067484065
1
Iteration 4500: Loss = -11101.570302090076
Iteration 4600: Loss = -11101.570352504428
1
Iteration 4700: Loss = -11101.57025332705
Iteration 4800: Loss = -11101.57026911264
1
Iteration 4900: Loss = -11101.570246815638
Iteration 5000: Loss = -11101.571316588426
1
Iteration 5100: Loss = -11101.570224410467
Iteration 5200: Loss = -11101.570452748436
1
Iteration 5300: Loss = -11101.570240253755
2
Iteration 5400: Loss = -11101.570227086942
3
Iteration 5500: Loss = -11101.570341781486
4
Iteration 5600: Loss = -11101.570253118247
5
Stopping early at iteration 5600 due to no improvement.
pi: tensor([[0.3826, 0.6174],
        [0.1464, 0.8536]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.1116, 0.8884], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2538, 0.2211],
         [0.7284, 0.1507]],

        [[0.5989, 0.1890],
         [0.5972, 0.5181]],

        [[0.5480, 0.1956],
         [0.6528, 0.6506]],

        [[0.6327, 0.1870],
         [0.6205, 0.5657]],

        [[0.6415, 0.1778],
         [0.5844, 0.6996]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.0008263300397341679
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.008732110438009916
time is 2
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0022119509621718893
time is 3
tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0035895661719517516
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.004144175314943718
Global Adjusted Rand Index: 0.004625698280336174
Average Adjusted Rand Index: 0.002134468100687921
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21057.753988337525
Iteration 100: Loss = -11108.672927625365
Iteration 200: Loss = -11107.821592641494
Iteration 300: Loss = -11107.42681768208
Iteration 400: Loss = -11106.819202401432
Iteration 500: Loss = -11106.211666003028
Iteration 600: Loss = -11104.486040451258
Iteration 700: Loss = -11103.260920390903
Iteration 800: Loss = -11102.88880780554
Iteration 900: Loss = -11102.647862850665
Iteration 1000: Loss = -11102.474431789882
Iteration 1100: Loss = -11102.332335088526
Iteration 1200: Loss = -11102.209927943622
Iteration 1300: Loss = -11102.103835602546
Iteration 1400: Loss = -11102.012632044924
Iteration 1500: Loss = -11101.9359224085
Iteration 1600: Loss = -11101.869401731119
Iteration 1700: Loss = -11101.810221701999
Iteration 1800: Loss = -11101.760130523646
Iteration 1900: Loss = -11101.718863385084
Iteration 2000: Loss = -11101.685936227774
Iteration 2100: Loss = -11101.660379660645
Iteration 2200: Loss = -11101.63946985755
Iteration 2300: Loss = -11101.622084331197
Iteration 2400: Loss = -11101.608957844399
Iteration 2500: Loss = -11101.599696281532
Iteration 2600: Loss = -11101.593077715837
Iteration 2700: Loss = -11101.588314838102
Iteration 2800: Loss = -11101.584809973008
Iteration 2900: Loss = -11101.582108390114
Iteration 3000: Loss = -11101.579934174672
Iteration 3100: Loss = -11101.578085452911
Iteration 3200: Loss = -11101.576521080038
Iteration 3300: Loss = -11101.575081490237
Iteration 3400: Loss = -11101.573855587869
Iteration 3500: Loss = -11101.572740713373
Iteration 3600: Loss = -11101.571957877512
Iteration 3700: Loss = -11101.572722088777
1
Iteration 3800: Loss = -11101.57106861767
Iteration 3900: Loss = -11101.570840264658
Iteration 4000: Loss = -11101.580883195764
1
Iteration 4100: Loss = -11101.570559857642
Iteration 4200: Loss = -11101.571019426498
1
Iteration 4300: Loss = -11101.570511259637
Iteration 4400: Loss = -11101.570519679197
1
Iteration 4500: Loss = -11101.570548679078
2
Iteration 4600: Loss = -11101.571093980721
3
Iteration 4700: Loss = -11101.570310840327
Iteration 4800: Loss = -11101.580987351872
1
Iteration 4900: Loss = -11101.57027499544
Iteration 5000: Loss = -11101.57036848193
1
Iteration 5100: Loss = -11101.570279014017
2
Iteration 5200: Loss = -11101.570299925284
3
Iteration 5300: Loss = -11101.570241159423
Iteration 5400: Loss = -11101.570285674574
1
Iteration 5500: Loss = -11101.57026988417
2
Iteration 5600: Loss = -11101.570249826214
3
Iteration 5700: Loss = -11101.57045573231
4
Iteration 5800: Loss = -11101.57028931603
5
Stopping early at iteration 5800 due to no improvement.
pi: tensor([[0.8535, 0.1465],
        [0.6173, 0.3827]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8884, 0.1116], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1507, 0.2211],
         [0.6920, 0.2538]],

        [[0.6434, 0.1890],
         [0.6462, 0.6837]],

        [[0.5698, 0.1956],
         [0.5295, 0.6724]],

        [[0.5212, 0.1870],
         [0.5207, 0.6857]],

        [[0.7164, 0.1778],
         [0.6961, 0.6595]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.0008263300397341679
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.008732110438009916
time is 2
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0022119509621718893
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0035895661719517516
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.004144175314943718
Global Adjusted Rand Index: 0.004625698280336174
Average Adjusted Rand Index: 0.002134468100687921
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19935.19494449896
Iteration 100: Loss = -11108.485193285236
Iteration 200: Loss = -11107.562045051258
Iteration 300: Loss = -11106.944323464348
Iteration 400: Loss = -11106.368803243899
Iteration 500: Loss = -11106.1070050207
Iteration 600: Loss = -11105.5531627012
Iteration 700: Loss = -11103.536286334956
Iteration 800: Loss = -11103.016848412455
Iteration 900: Loss = -11102.691715140181
Iteration 1000: Loss = -11102.497981019493
Iteration 1100: Loss = -11102.352007778565
Iteration 1200: Loss = -11102.224129709128
Iteration 1300: Loss = -11102.112186996146
Iteration 1400: Loss = -11102.013173431962
Iteration 1500: Loss = -11101.92812564044
Iteration 1600: Loss = -11101.854547957964
Iteration 1700: Loss = -11101.791549937601
Iteration 1800: Loss = -11101.74060880334
Iteration 1900: Loss = -11101.700284411332
Iteration 2000: Loss = -11101.670122380383
Iteration 2100: Loss = -11101.647367033727
Iteration 2200: Loss = -11101.628269726096
Iteration 2300: Loss = -11101.612775818296
Iteration 2400: Loss = -11101.601679038951
Iteration 2500: Loss = -11101.594310550025
Iteration 2600: Loss = -11101.589121688212
Iteration 2700: Loss = -11101.585480303509
Iteration 2800: Loss = -11101.582770071134
Iteration 2900: Loss = -11101.580523672934
Iteration 3000: Loss = -11101.579371118856
Iteration 3100: Loss = -11101.576882838644
Iteration 3200: Loss = -11101.575453934944
Iteration 3300: Loss = -11101.574115179907
Iteration 3400: Loss = -11101.572868470772
Iteration 3500: Loss = -11101.5720269931
Iteration 3600: Loss = -11101.571460212437
Iteration 3700: Loss = -11101.571075277801
Iteration 3800: Loss = -11101.570916173576
Iteration 3900: Loss = -11101.570666977235
Iteration 4000: Loss = -11101.570846701148
1
Iteration 4100: Loss = -11101.571220217085
2
Iteration 4200: Loss = -11101.571029553186
3
Iteration 4300: Loss = -11101.570451058837
Iteration 4400: Loss = -11101.57043855067
Iteration 4500: Loss = -11101.574983213362
1
Iteration 4600: Loss = -11101.57036348917
Iteration 4700: Loss = -11101.57045674034
1
Iteration 4800: Loss = -11101.571687985665
2
Iteration 4900: Loss = -11101.572364505131
3
Iteration 5000: Loss = -11101.572143566696
4
Iteration 5100: Loss = -11101.57161189847
5
Stopping early at iteration 5100 due to no improvement.
pi: tensor([[0.8541, 0.1459],
        [0.6164, 0.3836]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.8878, 0.1122], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1508, 0.2211],
         [0.5468, 0.2539]],

        [[0.6421, 0.1891],
         [0.7095, 0.6389]],

        [[0.6274, 0.1956],
         [0.6143, 0.6277]],

        [[0.6273, 0.1870],
         [0.5223, 0.6518]],

        [[0.7069, 0.1779],
         [0.6607, 0.5911]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.0008263300397341679
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.008732110438009916
time is 2
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0])
tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0022119509621718893
time is 3
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0035895661719517516
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.004144175314943718
Global Adjusted Rand Index: 0.004625698280336174
Average Adjusted Rand Index: 0.002134468100687921
10970.693152888682
[0.8985008046740597, 0.004625698280336174, 0.004625698280336174, 0.004625698280336174] [0.8993883596122705, 0.002134468100687921, 0.002134468100687921, 0.002134468100687921] [10942.462832490946, 11101.570253118247, 11101.57028931603, 11101.57161189847]
-----------------------------------------------------------------------------------------
This iteration is 13
True Objective function: Loss = -10551.181700948497
Iteration 0: Loss = -10675.65907964762
Iteration 10: Loss = -10648.287059603768
Iteration 20: Loss = -10648.035084850751
Iteration 30: Loss = -10647.989333790523
Iteration 40: Loss = -10647.977397043369
Iteration 50: Loss = -10647.973543766902
Iteration 60: Loss = -10647.97226218581
Iteration 70: Loss = -10647.971826526209
Iteration 80: Loss = -10647.971664588173
Iteration 90: Loss = -10647.9716388751
Iteration 100: Loss = -10647.971599973334
Iteration 110: Loss = -10647.971616255956
1
Iteration 120: Loss = -10647.971586692922
Iteration 130: Loss = -10647.971585520885
Iteration 140: Loss = -10647.971573301116
Iteration 150: Loss = -10647.9715744619
1
Iteration 160: Loss = -10647.971570922085
Iteration 170: Loss = -10647.971574048506
1
Iteration 180: Loss = -10647.971599862958
2
Iteration 190: Loss = -10647.971571561031
3
Stopping early at iteration 190 due to no improvement.
pi: tensor([[0.9776, 0.0224],
        [0.9507, 0.0493]], dtype=torch.float64)
alpha: tensor([0.9762, 0.0238])
beta: tensor([[[0.1498, 0.1979],
         [0.4589, 0.3151]],

        [[0.3028, 0.2247],
         [0.6148, 0.5707]],

        [[0.1452, 0.2119],
         [0.7654, 0.5935]],

        [[0.8788, 0.2626],
         [0.6849, 0.0875]],

        [[0.4342, 0.1851],
         [0.7342, 0.3927]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 57
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.006509078451524495
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0008495080255643008
Average Adjusted Rand Index: 0.001301815690304899
Iteration 0: Loss = -10888.653134491722
Iteration 10: Loss = -10648.703644503228
Iteration 20: Loss = -10647.887063883252
Iteration 30: Loss = -10646.894602402761
Iteration 40: Loss = -10646.608094308836
Iteration 50: Loss = -10646.499158185308
Iteration 60: Loss = -10646.275878675797
Iteration 70: Loss = -10645.779359847402
Iteration 80: Loss = -10644.89899729888
Iteration 90: Loss = -10637.532352407792
Iteration 100: Loss = -10619.879051324642
Iteration 110: Loss = -10598.434608573121
Iteration 120: Loss = -10579.391067228744
Iteration 130: Loss = -10577.784784340438
Iteration 140: Loss = -10577.219196647475
Iteration 150: Loss = -10575.78894886973
Iteration 160: Loss = -10575.815490521856
1
Iteration 170: Loss = -10574.897857740321
Iteration 180: Loss = -10569.873095535484
Iteration 190: Loss = -10566.714279979515
Iteration 200: Loss = -10566.681410363759
Iteration 210: Loss = -10566.677247811362
Iteration 220: Loss = -10566.676569452082
Iteration 230: Loss = -10566.676441649504
Iteration 240: Loss = -10566.67641124619
Iteration 250: Loss = -10566.676401348948
Iteration 260: Loss = -10566.676408636977
1
Iteration 270: Loss = -10566.676400965685
Iteration 280: Loss = -10566.67640955898
1
Iteration 290: Loss = -10566.67641366367
2
Iteration 300: Loss = -10566.676412502937
3
Stopping early at iteration 300 due to no improvement.
pi: tensor([[0.4521, 0.5479],
        [0.5945, 0.4055]], dtype=torch.float64)
alpha: tensor([0.5328, 0.4672])
beta: tensor([[[0.1822, 0.1080],
         [0.7509, 0.2429]],

        [[0.2957, 0.1017],
         [0.1107, 0.5819]],

        [[0.7781, 0.0868],
         [0.9739, 0.4516]],

        [[0.6827, 0.0927],
         [0.4979, 0.1439]],

        [[0.1093, 0.0917],
         [0.3875, 0.9141]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 97
Adjusted Rand Index: 0.8824062740165256
time is 1
tensor([1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 91
Adjusted Rand Index: 0.6690909090909091
time is 2
tensor([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 94
Adjusted Rand Index: 0.7721069260785004
time is 3
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 6
Adjusted Rand Index: 0.7721426378272603
time is 4
tensor([1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 95
Adjusted Rand Index: 0.8080702804390127
Global Adjusted Rand Index: 0.28157884646721276
Average Adjusted Rand Index: 0.7807634054904417
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22877.78639073192
Iteration 100: Loss = -10649.905277474776
Iteration 200: Loss = -10648.623015867355
Iteration 300: Loss = -10647.653977636443
Iteration 400: Loss = -10647.266622751726
Iteration 500: Loss = -10647.053020098494
Iteration 600: Loss = -10646.885875979067
Iteration 700: Loss = -10646.67069701808
Iteration 800: Loss = -10646.310784344389
Iteration 900: Loss = -10645.709471097382
Iteration 1000: Loss = -10645.260178651291
Iteration 1100: Loss = -10645.144954620537
Iteration 1200: Loss = -10645.101063092918
Iteration 1300: Loss = -10645.07388675113
Iteration 1400: Loss = -10645.05449848374
Iteration 1500: Loss = -10645.040283729755
Iteration 1600: Loss = -10645.028371520826
Iteration 1700: Loss = -10645.019186101443
Iteration 1800: Loss = -10645.011735301858
Iteration 1900: Loss = -10645.005585147623
Iteration 2000: Loss = -10645.00197941374
Iteration 2100: Loss = -10644.995993388646
Iteration 2200: Loss = -10644.992195103443
Iteration 2300: Loss = -10644.989385716084
Iteration 2400: Loss = -10644.986115742826
Iteration 2500: Loss = -10644.983612496238
Iteration 2600: Loss = -10644.981392134656
Iteration 2700: Loss = -10644.979448146823
Iteration 2800: Loss = -10644.97771212724
Iteration 2900: Loss = -10644.976162633135
Iteration 3000: Loss = -10644.97476791865
Iteration 3100: Loss = -10644.973812594873
Iteration 3200: Loss = -10644.97239561084
Iteration 3300: Loss = -10644.971307948312
Iteration 3400: Loss = -10644.970318066256
Iteration 3500: Loss = -10644.969499327211
Iteration 3600: Loss = -10644.968676737552
Iteration 3700: Loss = -10644.967947928984
Iteration 3800: Loss = -10644.970074121813
1
Iteration 3900: Loss = -10644.966632647132
Iteration 4000: Loss = -10644.967428087552
1
Iteration 4100: Loss = -10644.96554516248
Iteration 4200: Loss = -10644.965038564414
Iteration 4300: Loss = -10644.964634992664
Iteration 4400: Loss = -10644.964210933615
Iteration 4500: Loss = -10644.964375740374
1
Iteration 4600: Loss = -10644.96344530069
Iteration 4700: Loss = -10644.969241332705
1
Iteration 4800: Loss = -10644.962725716281
Iteration 4900: Loss = -10644.962469731037
Iteration 5000: Loss = -10644.963168989465
1
Iteration 5100: Loss = -10644.961920088519
Iteration 5200: Loss = -10644.961719084335
Iteration 5300: Loss = -10644.961463829737
Iteration 5400: Loss = -10644.961269161204
Iteration 5500: Loss = -10644.96235665765
1
Iteration 5600: Loss = -10644.96085125209
Iteration 5700: Loss = -10644.960677963343
Iteration 5800: Loss = -10644.960485621023
Iteration 5900: Loss = -10644.960334092855
Iteration 6000: Loss = -10644.962469807726
1
Iteration 6100: Loss = -10644.960034191496
Iteration 6200: Loss = -10644.959938387487
Iteration 6300: Loss = -10644.959830422531
Iteration 6400: Loss = -10644.959716861478
Iteration 6500: Loss = -10644.95984740616
1
Iteration 6600: Loss = -10644.959445292241
Iteration 6700: Loss = -10644.971530529327
1
Iteration 6800: Loss = -10644.959278842578
Iteration 6900: Loss = -10644.959163302461
Iteration 7000: Loss = -10644.959113910647
Iteration 7100: Loss = -10644.95903888015
Iteration 7200: Loss = -10644.97111246282
1
Iteration 7300: Loss = -10644.95893495097
Iteration 7400: Loss = -10644.958805850372
Iteration 7500: Loss = -10644.958744729787
Iteration 7600: Loss = -10644.958688632529
Iteration 7700: Loss = -10644.958848456376
1
Iteration 7800: Loss = -10644.958614380348
Iteration 7900: Loss = -10644.958588045372
Iteration 8000: Loss = -10644.958471065234
Iteration 8100: Loss = -10644.958499083421
1
Iteration 8200: Loss = -10644.958396820608
Iteration 8300: Loss = -10644.961232913665
1
Iteration 8400: Loss = -10644.95831638741
Iteration 8500: Loss = -10644.960298340084
1
Iteration 8600: Loss = -10644.958210010305
Iteration 8700: Loss = -10644.958274296083
1
Iteration 8800: Loss = -10644.95816679313
Iteration 8900: Loss = -10644.958143249405
Iteration 9000: Loss = -10644.958150804465
1
Iteration 9100: Loss = -10644.958066459287
Iteration 9200: Loss = -10644.95804726276
Iteration 9300: Loss = -10644.958050849858
1
Iteration 9400: Loss = -10644.961028654383
2
Iteration 9500: Loss = -10644.96090399124
3
Iteration 9600: Loss = -10644.957991003272
Iteration 9700: Loss = -10644.959889836486
1
Iteration 9800: Loss = -10644.965618018603
2
Iteration 9900: Loss = -10645.014363842358
3
Iteration 10000: Loss = -10644.958949092255
4
Iteration 10100: Loss = -10644.960628699828
5
Stopping early at iteration 10100 due to no improvement.
pi: tensor([[1.0000e+00, 4.5895e-06],
        [6.5822e-01, 3.4178e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4713, 0.5287], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1474, 0.1614],
         [0.6472, 0.1788]],

        [[0.5365, 0.1740],
         [0.6383, 0.6461]],

        [[0.7046, 0.1980],
         [0.5770, 0.5259]],

        [[0.5110, 0.2594],
         [0.6579, 0.6524]],

        [[0.5717, 0.1336],
         [0.6848, 0.7302]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.009740487132105357
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.01575757575757576
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.010091437982433116
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: -0.0041478895259491316
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.002619556489131975
Average Adjusted Rand Index: 0.002392127416390877
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21916.812568759862
Iteration 100: Loss = -10650.200658698337
Iteration 200: Loss = -10649.706360285025
Iteration 300: Loss = -10649.175016848636
Iteration 400: Loss = -10648.553773068532
Iteration 500: Loss = -10648.115605003191
Iteration 600: Loss = -10647.909597874528
Iteration 700: Loss = -10647.724519639181
Iteration 800: Loss = -10647.366768165153
Iteration 900: Loss = -10646.836125566646
Iteration 1000: Loss = -10646.596284404468
Iteration 1100: Loss = -10646.341712129617
Iteration 1200: Loss = -10646.095197830244
Iteration 1300: Loss = -10645.887481962553
Iteration 1400: Loss = -10645.775925574464
Iteration 1500: Loss = -10645.722278160187
Iteration 1600: Loss = -10645.687210073425
Iteration 1700: Loss = -10645.663954577667
Iteration 1800: Loss = -10645.648592917129
Iteration 1900: Loss = -10645.63811403762
Iteration 2000: Loss = -10645.630519426664
Iteration 2100: Loss = -10645.624900725963
Iteration 2200: Loss = -10645.620573249893
Iteration 2300: Loss = -10645.617129687349
Iteration 2400: Loss = -10645.61437714265
Iteration 2500: Loss = -10645.61205374881
Iteration 2600: Loss = -10645.611319976784
Iteration 2700: Loss = -10645.60853152481
Iteration 2800: Loss = -10645.607170068068
Iteration 2900: Loss = -10645.60591658634
Iteration 3000: Loss = -10645.605445731566
Iteration 3100: Loss = -10645.604106003962
Iteration 3200: Loss = -10645.603142614149
Iteration 3300: Loss = -10645.602439116488
Iteration 3400: Loss = -10645.601784594357
Iteration 3500: Loss = -10645.60141287137
Iteration 3600: Loss = -10645.600645580893
Iteration 3700: Loss = -10645.60052343282
Iteration 3800: Loss = -10645.599791440836
Iteration 3900: Loss = -10645.599398437995
Iteration 4000: Loss = -10645.599125563664
Iteration 4100: Loss = -10645.598815289186
Iteration 4200: Loss = -10645.599264263989
1
Iteration 4300: Loss = -10645.598257955015
Iteration 4400: Loss = -10645.598702994708
1
Iteration 4500: Loss = -10645.597794960648
Iteration 4600: Loss = -10645.598229170802
1
Iteration 4700: Loss = -10645.597346871786
Iteration 4800: Loss = -10645.597192421228
Iteration 4900: Loss = -10645.597102903617
Iteration 5000: Loss = -10645.596854834577
Iteration 5100: Loss = -10645.601381513328
1
Iteration 5200: Loss = -10645.596819895693
Iteration 5300: Loss = -10645.596528901067
Iteration 5400: Loss = -10645.596395462793
Iteration 5500: Loss = -10645.596933265846
1
Iteration 5600: Loss = -10645.596197143444
Iteration 5700: Loss = -10645.596127599554
Iteration 5800: Loss = -10645.596069363923
Iteration 5900: Loss = -10645.59600822501
Iteration 6000: Loss = -10645.595946942942
Iteration 6100: Loss = -10645.59588324624
Iteration 6200: Loss = -10645.598588214407
1
Iteration 6300: Loss = -10645.595746980385
Iteration 6400: Loss = -10645.595649157252
Iteration 6500: Loss = -10645.595633880217
Iteration 6600: Loss = -10645.595584963705
Iteration 6700: Loss = -10645.597564266513
1
Iteration 6800: Loss = -10645.595520568639
Iteration 6900: Loss = -10645.595453549018
Iteration 7000: Loss = -10645.599207348025
1
Iteration 7100: Loss = -10645.595415292695
Iteration 7200: Loss = -10645.598329583012
1
Iteration 7300: Loss = -10645.595344495987
Iteration 7400: Loss = -10645.595329626882
Iteration 7500: Loss = -10645.595314280252
Iteration 7600: Loss = -10645.59530807947
Iteration 7700: Loss = -10645.595238984482
Iteration 7800: Loss = -10645.595249243874
1
Iteration 7900: Loss = -10645.595214026187
Iteration 8000: Loss = -10645.595202564667
Iteration 8100: Loss = -10645.595662939886
1
Iteration 8200: Loss = -10645.595149531082
Iteration 8300: Loss = -10645.595449541523
1
Iteration 8400: Loss = -10645.601326679678
2
Iteration 8500: Loss = -10645.596791603919
3
Iteration 8600: Loss = -10645.59512896098
Iteration 8700: Loss = -10645.600200556655
1
Iteration 8800: Loss = -10645.595124294668
Iteration 8900: Loss = -10645.596599409606
1
Iteration 9000: Loss = -10645.595519849567
2
Iteration 9100: Loss = -10645.597263977173
3
Iteration 9200: Loss = -10645.595316629617
4
Iteration 9300: Loss = -10645.614790991696
5
Stopping early at iteration 9300 due to no improvement.
pi: tensor([[0.9777, 0.0223],
        [0.9769, 0.0231]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0070, 0.9930], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1519, 0.1615],
         [0.6803, 0.1625]],

        [[0.5812, 0.2227],
         [0.5209, 0.7263]],

        [[0.5654, 0.0815],
         [0.6563, 0.5765]],

        [[0.6699, 0.2679],
         [0.5665, 0.6466]],

        [[0.7279, 0.1755],
         [0.5552, 0.6061]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.006509078451524495
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0026298907826991626
Average Adjusted Rand Index: 0.001301815690304899
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20145.22212071445
Iteration 100: Loss = -10650.309406976916
Iteration 200: Loss = -10649.96675267236
Iteration 300: Loss = -10649.69352469383
Iteration 400: Loss = -10649.350464890354
Iteration 500: Loss = -10648.96409990253
Iteration 600: Loss = -10648.139824130518
Iteration 700: Loss = -10647.689919541317
Iteration 800: Loss = -10647.561708467487
Iteration 900: Loss = -10647.493044986208
Iteration 1000: Loss = -10647.44530922722
Iteration 1100: Loss = -10647.399623213116
Iteration 1200: Loss = -10647.152994218854
Iteration 1300: Loss = -10645.97639672172
Iteration 1400: Loss = -10645.544979526128
Iteration 1500: Loss = -10645.24631187946
Iteration 1600: Loss = -10645.125725727581
Iteration 1700: Loss = -10645.075787644762
Iteration 1800: Loss = -10645.047819471953
Iteration 1900: Loss = -10645.02965484649
Iteration 2000: Loss = -10645.017028400815
Iteration 2100: Loss = -10645.00782148621
Iteration 2200: Loss = -10645.000779407894
Iteration 2300: Loss = -10644.995533613459
Iteration 2400: Loss = -10644.990789466878
Iteration 2500: Loss = -10644.987147269827
Iteration 2600: Loss = -10644.9846118005
Iteration 2700: Loss = -10644.981408597894
Iteration 2800: Loss = -10644.97910715289
Iteration 2900: Loss = -10644.97710652517
Iteration 3000: Loss = -10644.975411870162
Iteration 3100: Loss = -10644.974044853188
Iteration 3200: Loss = -10644.972460336501
Iteration 3300: Loss = -10644.97126849812
Iteration 3400: Loss = -10644.970213355415
Iteration 3500: Loss = -10644.969277699209
Iteration 3600: Loss = -10644.97005013989
1
Iteration 3700: Loss = -10644.967663489288
Iteration 3800: Loss = -10644.967438023396
Iteration 3900: Loss = -10644.96635548749
Iteration 4000: Loss = -10644.965737043443
Iteration 4100: Loss = -10644.965298800238
Iteration 4200: Loss = -10644.964702244764
Iteration 4300: Loss = -10644.964618461378
Iteration 4400: Loss = -10644.963833260263
Iteration 4500: Loss = -10644.96342602772
Iteration 4600: Loss = -10644.965167304068
1
Iteration 4700: Loss = -10644.962725713629
Iteration 4800: Loss = -10644.96239570012
Iteration 4900: Loss = -10644.963846148119
1
Iteration 5000: Loss = -10644.961829413838
Iteration 5100: Loss = -10644.961571435051
Iteration 5200: Loss = -10644.961489037956
Iteration 5300: Loss = -10644.961113883794
Iteration 5400: Loss = -10644.961010868119
Iteration 5500: Loss = -10644.960741699932
Iteration 5600: Loss = -10644.960540345919
Iteration 5700: Loss = -10644.960670398628
1
Iteration 5800: Loss = -10644.960229241631
Iteration 5900: Loss = -10644.960131187665
Iteration 6000: Loss = -10644.96096128239
1
Iteration 6100: Loss = -10644.959822565323
Iteration 6200: Loss = -10644.959701381684
Iteration 6300: Loss = -10644.967990223304
1
Iteration 6400: Loss = -10644.959500355117
Iteration 6500: Loss = -10644.959420933752
Iteration 6600: Loss = -10644.959658411626
1
Iteration 6700: Loss = -10644.959205014748
Iteration 6800: Loss = -10644.959749255477
1
Iteration 6900: Loss = -10644.959031039962
Iteration 7000: Loss = -10644.958954426253
Iteration 7100: Loss = -10644.95893421813
Iteration 7200: Loss = -10644.95880166651
Iteration 7300: Loss = -10644.983844476037
1
Iteration 7400: Loss = -10644.958666687979
Iteration 7500: Loss = -10644.958639476565
Iteration 7600: Loss = -10644.958610844787
Iteration 7700: Loss = -10644.958518929732
Iteration 7800: Loss = -10644.958551684349
1
Iteration 7900: Loss = -10644.958400548516
Iteration 8000: Loss = -10644.95839960343
Iteration 8100: Loss = -10644.958310365244
Iteration 8200: Loss = -10645.10359045859
1
Iteration 8300: Loss = -10644.958254010866
Iteration 8400: Loss = -10644.958241559694
Iteration 8500: Loss = -10644.958545399528
1
Iteration 8600: Loss = -10644.958135941813
Iteration 8700: Loss = -10644.986301478668
1
Iteration 8800: Loss = -10644.958062737724
Iteration 8900: Loss = -10644.958102732735
1
Iteration 9000: Loss = -10644.959015611359
2
Iteration 9100: Loss = -10644.9580188305
Iteration 9200: Loss = -10645.003132579086
1
Iteration 9300: Loss = -10644.957975699292
Iteration 9400: Loss = -10644.9579858646
1
Iteration 9500: Loss = -10644.958073572729
2
Iteration 9600: Loss = -10644.984412292231
3
Iteration 9700: Loss = -10644.957930740951
Iteration 9800: Loss = -10644.992793770476
1
Iteration 9900: Loss = -10644.959721507057
2
Iteration 10000: Loss = -10644.957893353776
Iteration 10100: Loss = -10644.959515515582
1
Iteration 10200: Loss = -10644.958469378975
2
Iteration 10300: Loss = -10644.958293794705
3
Iteration 10400: Loss = -10644.959208449776
4
Iteration 10500: Loss = -10644.957815889971
Iteration 10600: Loss = -10644.977890592261
1
Iteration 10700: Loss = -10644.958888869724
2
Iteration 10800: Loss = -10644.957833887742
3
Iteration 10900: Loss = -10644.983757684833
4
Iteration 11000: Loss = -10645.163227269808
5
Stopping early at iteration 11000 due to no improvement.
pi: tensor([[3.3739e-01, 6.6261e-01],
        [2.5224e-06, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.5274, 0.4726], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1773, 0.1622],
         [0.5324, 0.1489]],

        [[0.5237, 0.1734],
         [0.6037, 0.7295]],

        [[0.6993, 0.1982],
         [0.5181, 0.7080]],

        [[0.7163, 0.2595],
         [0.5566, 0.5901]],

        [[0.5327, 0.1336],
         [0.5534, 0.7191]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 0])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.010196000064652544
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 42
Adjusted Rand Index: 0.01575757575757576
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.010091437982433116
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 44
Adjusted Rand Index: -0.0041478895259491316
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 48
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.001937705050917631
Average Adjusted Rand Index: 0.0023010248298814396
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22100.10571522174
Iteration 100: Loss = -10650.197011316195
Iteration 200: Loss = -10649.608147920879
Iteration 300: Loss = -10649.190438712925
Iteration 400: Loss = -10648.699612733059
Iteration 500: Loss = -10648.124889088102
Iteration 600: Loss = -10647.547043325148
Iteration 700: Loss = -10647.366210967883
Iteration 800: Loss = -10647.269586566881
Iteration 900: Loss = -10647.19836991142
Iteration 1000: Loss = -10647.130290772931
Iteration 1100: Loss = -10647.05545510922
Iteration 1200: Loss = -10646.961934950956
Iteration 1300: Loss = -10646.835975924758
Iteration 1400: Loss = -10646.660592010188
Iteration 1500: Loss = -10646.4128646219
Iteration 1600: Loss = -10646.055228235964
Iteration 1700: Loss = -10645.561717484426
Iteration 1800: Loss = -10645.27540755541
Iteration 1900: Loss = -10645.1760422558
Iteration 2000: Loss = -10645.126114077744
Iteration 2100: Loss = -10645.094190388189
Iteration 2200: Loss = -10645.071820634746
Iteration 2300: Loss = -10645.055061195835
Iteration 2400: Loss = -10645.041844451434
Iteration 2500: Loss = -10645.03124222464
Iteration 2600: Loss = -10645.02263382901
Iteration 2700: Loss = -10645.015338639294
Iteration 2800: Loss = -10645.00933099455
Iteration 2900: Loss = -10645.004181505989
Iteration 3000: Loss = -10644.999728091601
Iteration 3100: Loss = -10644.995914164461
Iteration 3200: Loss = -10644.992566364375
Iteration 3300: Loss = -10644.989618698402
Iteration 3400: Loss = -10644.996652243357
1
Iteration 3500: Loss = -10644.98465562746
Iteration 3600: Loss = -10644.982551628456
Iteration 3700: Loss = -10644.981649403931
Iteration 3800: Loss = -10644.978967174855
Iteration 3900: Loss = -10644.977452015191
Iteration 4000: Loss = -10644.976678774472
Iteration 4100: Loss = -10644.97477465706
Iteration 4200: Loss = -10644.97359309119
Iteration 4300: Loss = -10644.972536826277
Iteration 4400: Loss = -10644.971604099625
Iteration 4500: Loss = -10644.974022524508
1
Iteration 4600: Loss = -10644.969820836186
Iteration 4700: Loss = -10644.969097470424
Iteration 4800: Loss = -10644.968380836153
Iteration 4900: Loss = -10644.967697496551
Iteration 5000: Loss = -10644.98648608877
1
Iteration 5100: Loss = -10644.966484899855
Iteration 5200: Loss = -10644.965945224603
Iteration 5300: Loss = -10644.965494950133
Iteration 5400: Loss = -10644.964998713007
Iteration 5500: Loss = -10644.964830014998
Iteration 5600: Loss = -10644.964191338835
Iteration 5700: Loss = -10644.963815465351
Iteration 5800: Loss = -10644.963496236423
Iteration 5900: Loss = -10644.963128526213
Iteration 6000: Loss = -10644.963282772878
1
Iteration 6100: Loss = -10644.96250943137
Iteration 6200: Loss = -10644.971139088642
1
Iteration 6300: Loss = -10644.962010318508
Iteration 6400: Loss = -10644.9617823766
Iteration 6500: Loss = -10644.961564866353
Iteration 6600: Loss = -10644.9613538355
Iteration 6700: Loss = -10644.961371945721
1
Iteration 6800: Loss = -10644.960931605812
Iteration 6900: Loss = -10644.960760330812
Iteration 7000: Loss = -10644.96058353828
Iteration 7100: Loss = -10644.960424970472
Iteration 7200: Loss = -10644.962395072647
1
Iteration 7300: Loss = -10644.960161020528
Iteration 7400: Loss = -10644.960003151973
Iteration 7500: Loss = -10644.961921215245
1
Iteration 7600: Loss = -10644.959755200329
Iteration 7700: Loss = -10644.95964634622
Iteration 7800: Loss = -10644.9595697923
Iteration 7900: Loss = -10644.959433989696
Iteration 8000: Loss = -10644.960451236619
1
Iteration 8100: Loss = -10644.959246739772
Iteration 8200: Loss = -10644.959168671916
Iteration 8300: Loss = -10644.959125895637
Iteration 8400: Loss = -10644.95904328599
Iteration 8500: Loss = -10644.959076801983
1
Iteration 8600: Loss = -10645.18576844496
2
Iteration 8700: Loss = -10644.958832707789
Iteration 8800: Loss = -10644.970929157447
1
Iteration 8900: Loss = -10644.958674658592
Iteration 9000: Loss = -10644.963045405375
1
Iteration 9100: Loss = -10644.958625275405
Iteration 9200: Loss = -10644.958544376825
Iteration 9300: Loss = -10644.9592719965
1
Iteration 9400: Loss = -10644.95843124235
Iteration 9500: Loss = -10644.958370273234
Iteration 9600: Loss = -10644.958688257915
1
Iteration 9700: Loss = -10644.958350208106
Iteration 9800: Loss = -10644.958297580997
Iteration 9900: Loss = -10644.972075167185
1
Iteration 10000: Loss = -10644.959335884698
2
Iteration 10100: Loss = -10644.958557933978
3
Iteration 10200: Loss = -10644.977214112645
4
Iteration 10300: Loss = -10644.986419699368
5
Stopping early at iteration 10300 due to no improvement.
pi: tensor([[9.9999e-01, 7.3347e-06],
        [6.5547e-01, 3.4453e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4706, 0.5294], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1469, 0.1609],
         [0.7216, 0.1793]],

        [[0.5388, 0.1739],
         [0.5876, 0.6018]],

        [[0.5200, 0.1979],
         [0.7302, 0.6461]],

        [[0.6909, 0.2593],
         [0.7133, 0.6987]],

        [[0.6294, 0.1336],
         [0.6908, 0.5522]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 0])
Difference count: 51
Adjusted Rand Index: -0.009740487132105357
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 58
Adjusted Rand Index: 0.01575757575757576
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.010091437982433116
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 56
Adjusted Rand Index: -0.0041478895259491316
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.002619556489131975
Average Adjusted Rand Index: 0.002392127416390877
10551.181700948497
[0.002619556489131975, -0.0026298907826991626, 0.001937705050917631, 0.002619556489131975] [0.002392127416390877, 0.001301815690304899, 0.0023010248298814396, 0.002392127416390877] [10644.960628699828, 10645.614790991696, 10645.163227269808, 10644.986419699368]
-----------------------------------------------------------------------------------------
This iteration is 14
True Objective function: Loss = -10996.106887007372
Iteration 0: Loss = -11268.673385518625
Iteration 10: Loss = -11077.147396201184
Iteration 20: Loss = -11076.573403694221
Iteration 30: Loss = -11076.358693876758
Iteration 40: Loss = -11076.320273457366
Iteration 50: Loss = -11076.312032697962
Iteration 60: Loss = -11076.308140976198
Iteration 70: Loss = -11076.304883034232
Iteration 80: Loss = -11076.301838330179
Iteration 90: Loss = -11076.298705255795
Iteration 100: Loss = -11076.295506808068
Iteration 110: Loss = -11076.292337367327
Iteration 120: Loss = -11076.289032700752
Iteration 130: Loss = -11076.285672744187
Iteration 140: Loss = -11076.28225344376
Iteration 150: Loss = -11076.278773052496
Iteration 160: Loss = -11076.275199747688
Iteration 170: Loss = -11076.271610062835
Iteration 180: Loss = -11076.267973044929
Iteration 190: Loss = -11076.264198432445
Iteration 200: Loss = -11076.26040329983
Iteration 210: Loss = -11076.256538156409
Iteration 220: Loss = -11076.252581232748
Iteration 230: Loss = -11076.248575214078
Iteration 240: Loss = -11076.244565293528
Iteration 250: Loss = -11076.240454667892
Iteration 260: Loss = -11076.236341645428
Iteration 270: Loss = -11076.232108478052
Iteration 280: Loss = -11076.227781627662
Iteration 290: Loss = -11076.223485565733
Iteration 300: Loss = -11076.219059753594
Iteration 310: Loss = -11076.214669659745
Iteration 320: Loss = -11076.210241308348
Iteration 330: Loss = -11076.205719717795
Iteration 340: Loss = -11076.201154800681
Iteration 350: Loss = -11076.196641060313
Iteration 360: Loss = -11076.191979758207
Iteration 370: Loss = -11076.187343512183
Iteration 380: Loss = -11076.182710922703
Iteration 390: Loss = -11076.1780284805
Iteration 400: Loss = -11076.173332919952
Iteration 410: Loss = -11076.16862849326
Iteration 420: Loss = -11076.16395052294
Iteration 430: Loss = -11076.159266484861
Iteration 440: Loss = -11076.154585422835
Iteration 450: Loss = -11076.14985246885
Iteration 460: Loss = -11076.145168584808
Iteration 470: Loss = -11076.140573460376
Iteration 480: Loss = -11076.135949038595
Iteration 490: Loss = -11076.131357400964
Iteration 500: Loss = -11076.126804363515
Iteration 510: Loss = -11076.12225298817
Iteration 520: Loss = -11076.117850006047
Iteration 530: Loss = -11076.113394831296
Iteration 540: Loss = -11076.109049909594
Iteration 550: Loss = -11076.104799835222
Iteration 560: Loss = -11076.100588134854
Iteration 570: Loss = -11076.096447731232
Iteration 580: Loss = -11076.092372516305
Iteration 590: Loss = -11076.088414580905
Iteration 600: Loss = -11076.084518406411
Iteration 610: Loss = -11076.08070859237
Iteration 620: Loss = -11076.077015541472
Iteration 630: Loss = -11076.073382656838
Iteration 640: Loss = -11076.06985797552
Iteration 650: Loss = -11076.066430043276
Iteration 660: Loss = -11076.063123156824
Iteration 670: Loss = -11076.059879140485
Iteration 680: Loss = -11076.056798973816
Iteration 690: Loss = -11076.053763637796
Iteration 700: Loss = -11076.050885412436
Iteration 710: Loss = -11076.04808035414
Iteration 720: Loss = -11076.045370828268
Iteration 730: Loss = -11076.042754534083
Iteration 740: Loss = -11076.040288307995
Iteration 750: Loss = -11076.037851485586
Iteration 760: Loss = -11076.035579398333
Iteration 770: Loss = -11076.033396546392
Iteration 780: Loss = -11076.03127093115
Iteration 790: Loss = -11076.029245625263
Iteration 800: Loss = -11076.027324611598
Iteration 810: Loss = -11076.025504666346
Iteration 820: Loss = -11076.023725699837
Iteration 830: Loss = -11076.022067157792
Iteration 840: Loss = -11076.020442941031
Iteration 850: Loss = -11076.018956503212
Iteration 860: Loss = -11076.017514484245
Iteration 870: Loss = -11076.016126920422
Iteration 880: Loss = -11076.014863869017
Iteration 890: Loss = -11076.013614426698
Iteration 900: Loss = -11076.012432751817
Iteration 910: Loss = -11076.011334822559
Iteration 920: Loss = -11076.010292918701
Iteration 930: Loss = -11076.009334403254
Iteration 940: Loss = -11076.008364571097
Iteration 950: Loss = -11076.007461090809
Iteration 960: Loss = -11076.006673327342
Iteration 970: Loss = -11076.005824573878
Iteration 980: Loss = -11076.005088104286
Iteration 990: Loss = -11076.004369118984
Iteration 1000: Loss = -11076.003718902033
Iteration 1010: Loss = -11076.003081101779
Iteration 1020: Loss = -11076.002508702837
Iteration 1030: Loss = -11076.001916724721
Iteration 1040: Loss = -11076.001398731365
Iteration 1050: Loss = -11076.00087147637
Iteration 1060: Loss = -11076.000421625231
Iteration 1070: Loss = -11075.999961743233
Iteration 1080: Loss = -11075.999597250637
Iteration 1090: Loss = -11075.999134668358
Iteration 1100: Loss = -11075.99877774917
Iteration 1110: Loss = -11075.998412711835
Iteration 1120: Loss = -11075.998114387796
Iteration 1130: Loss = -11075.997782640598
Iteration 1140: Loss = -11075.997523704407
Iteration 1150: Loss = -11075.997230226285
Iteration 1160: Loss = -11075.9969999571
Iteration 1170: Loss = -11075.996768139155
Iteration 1180: Loss = -11075.996530354603
Iteration 1190: Loss = -11075.996302604697
Iteration 1200: Loss = -11075.99608437947
Iteration 1210: Loss = -11075.99588856786
Iteration 1220: Loss = -11075.99572351327
Iteration 1230: Loss = -11075.99553576513
Iteration 1240: Loss = -11075.995394491863
Iteration 1250: Loss = -11075.995230880832
Iteration 1260: Loss = -11075.99510934499
Iteration 1270: Loss = -11075.99498965085
Iteration 1280: Loss = -11075.994865869678
Iteration 1290: Loss = -11075.99472973121
Iteration 1300: Loss = -11075.994636959978
Iteration 1310: Loss = -11075.994535248486
Iteration 1320: Loss = -11075.994421571013
Iteration 1330: Loss = -11075.994338309241
Iteration 1340: Loss = -11075.994249669777
Iteration 1350: Loss = -11075.994165692147
Iteration 1360: Loss = -11075.994112128874
Iteration 1370: Loss = -11075.994007584823
Iteration 1380: Loss = -11075.993973800598
Iteration 1390: Loss = -11075.993887905013
Iteration 1400: Loss = -11075.99384105785
Iteration 1410: Loss = -11075.993771560547
Iteration 1420: Loss = -11075.993720612809
Iteration 1430: Loss = -11075.993683674045
Iteration 1440: Loss = -11075.993642173702
Iteration 1450: Loss = -11075.993609562844
Iteration 1460: Loss = -11075.993534031471
Iteration 1470: Loss = -11075.993506248511
Iteration 1480: Loss = -11075.993473576436
Iteration 1490: Loss = -11075.99344125302
Iteration 1500: Loss = -11075.993404724228
Iteration 1510: Loss = -11075.993385145679
Iteration 1520: Loss = -11075.993371572562
Iteration 1530: Loss = -11075.993355795108
Iteration 1540: Loss = -11075.993323556986
Iteration 1550: Loss = -11075.993323857034
1
Iteration 1560: Loss = -11075.993251680897
Iteration 1570: Loss = -11075.993222676925
Iteration 1580: Loss = -11075.993228033289
1
Iteration 1590: Loss = -11075.993206053843
Iteration 1600: Loss = -11075.993151503584
Iteration 1610: Loss = -11075.99318348874
1
Iteration 1620: Loss = -11075.993155010177
2
Iteration 1630: Loss = -11075.993162690103
3
Stopping early at iteration 1630 due to no improvement.
pi: tensor([[0.1967, 0.8033],
        [0.1255, 0.8745]], dtype=torch.float64)
alpha: tensor([0.1339, 0.8661])
beta: tensor([[[0.2270, 0.1786],
         [0.8675, 0.1546]],

        [[0.4984, 0.1669],
         [0.6321, 0.7176]],

        [[0.9718, 0.2016],
         [0.1688, 0.4616]],

        [[0.1735, 0.1865],
         [0.6101, 0.3570]],

        [[0.1474, 0.1950],
         [0.5619, 0.3989]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.009696969696969697
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 55
Adjusted Rand Index: -0.004284949116229245
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
Global Adjusted Rand Index: 9.316029599505335e-06
Average Adjusted Rand Index: -0.002948608865510659
Iteration 0: Loss = -11142.715297218403
Iteration 10: Loss = -11076.308982135612
Iteration 20: Loss = -11076.265551304454
Iteration 30: Loss = -11076.216531305749
Iteration 40: Loss = -11076.043947578199
Iteration 50: Loss = -11075.930743853749
Iteration 60: Loss = -11075.90746803429
Iteration 70: Loss = -11075.904089225593
Iteration 80: Loss = -11075.905718764398
1
Iteration 90: Loss = -11075.90939169083
2
Iteration 100: Loss = -11075.913859719587
3
Stopping early at iteration 100 due to no improvement.
pi: tensor([[0.1607, 0.8393],
        [0.0970, 0.9030]], dtype=torch.float64)
alpha: tensor([0.1025, 0.8975])
beta: tensor([[[0.2344, 0.1804],
         [0.9861, 0.1560]],

        [[0.9378, 0.1663],
         [0.3160, 0.6794]],

        [[0.1882, 0.2104],
         [0.5171, 0.6719]],

        [[0.2033, 0.1905],
         [0.2543, 0.2460]],

        [[0.2676, 0.2016],
         [0.3777, 0.8008]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 47
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.009696969696969697
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: -0.0007611255143543515
Global Adjusted Rand Index: -8.821581797071552e-05
Average Adjusted Rand Index: -0.00209161904226481
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21606.466274785274
Iteration 100: Loss = -11079.632820385319
Iteration 200: Loss = -11078.437557176432
Iteration 300: Loss = -11078.004334703579
Iteration 400: Loss = -11077.645067993726
Iteration 500: Loss = -11077.292394940203
Iteration 600: Loss = -11076.930167171653
Iteration 700: Loss = -11076.555896581443
Iteration 800: Loss = -11076.230546555404
Iteration 900: Loss = -11076.002718677348
Iteration 1000: Loss = -11075.829080564388
Iteration 1100: Loss = -11075.65271416629
Iteration 1200: Loss = -11075.43265857752
Iteration 1300: Loss = -11075.128048293464
Iteration 1400: Loss = -11074.67876551654
Iteration 1500: Loss = -11073.84940251092
Iteration 1600: Loss = -11011.26898863728
Iteration 1700: Loss = -10979.790830964035
Iteration 1800: Loss = -10975.882880285604
Iteration 1900: Loss = -10975.704510325657
Iteration 2000: Loss = -10973.991841186671
Iteration 2100: Loss = -10973.263248501777
Iteration 2200: Loss = -10973.222281071463
Iteration 2300: Loss = -10973.062483125379
Iteration 2400: Loss = -10972.98320538977
Iteration 2500: Loss = -10972.886325272324
Iteration 2600: Loss = -10972.875619143157
Iteration 2700: Loss = -10972.854165449009
Iteration 2800: Loss = -10972.837310089795
Iteration 2900: Loss = -10972.823641355997
Iteration 3000: Loss = -10972.814868523314
Iteration 3100: Loss = -10972.810335605796
Iteration 3200: Loss = -10972.06542120439
Iteration 3300: Loss = -10971.250414425507
Iteration 3400: Loss = -10971.241098289449
Iteration 3500: Loss = -10971.234927883539
Iteration 3600: Loss = -10971.231153319175
Iteration 3700: Loss = -10971.233942335584
1
Iteration 3800: Loss = -10971.217177782495
Iteration 3900: Loss = -10971.215049900376
Iteration 4000: Loss = -10971.21416560045
Iteration 4100: Loss = -10971.21343348325
Iteration 4200: Loss = -10971.212940581881
Iteration 4300: Loss = -10971.212392528172
Iteration 4400: Loss = -10971.211105306233
Iteration 4500: Loss = -10971.203659042343
Iteration 4600: Loss = -10971.186390862835
Iteration 4700: Loss = -10971.185855243995
Iteration 4800: Loss = -10971.185586424008
Iteration 4900: Loss = -10971.185317084657
Iteration 5000: Loss = -10971.185009396475
Iteration 5100: Loss = -10971.18634325207
1
Iteration 5200: Loss = -10971.184218877488
Iteration 5300: Loss = -10971.183256740549
Iteration 5400: Loss = -10971.174616094864
Iteration 5500: Loss = -10971.172408326873
Iteration 5600: Loss = -10971.175922790177
1
Iteration 5700: Loss = -10971.171226908495
Iteration 5800: Loss = -10971.170679035415
Iteration 5900: Loss = -10971.170938967844
1
Iteration 6000: Loss = -10971.169916511135
Iteration 6100: Loss = -10971.16946785665
Iteration 6200: Loss = -10971.170454575122
1
Iteration 6300: Loss = -10971.168908874706
Iteration 6400: Loss = -10971.168663741752
Iteration 6500: Loss = -10971.168316585938
Iteration 6600: Loss = -10971.167870962763
Iteration 6700: Loss = -10971.16724977243
Iteration 6800: Loss = -10971.164714168981
Iteration 6900: Loss = -10971.164218936285
Iteration 7000: Loss = -10971.160397164605
Iteration 7100: Loss = -10971.154339599118
Iteration 7200: Loss = -10971.156199229164
1
Iteration 7300: Loss = -10971.153880025655
Iteration 7400: Loss = -10971.15382733275
Iteration 7500: Loss = -10971.153869091275
1
Iteration 7600: Loss = -10971.170321154254
2
Iteration 7700: Loss = -10971.153651630335
Iteration 7800: Loss = -10971.15485070196
1
Iteration 7900: Loss = -10971.153577757681
Iteration 8000: Loss = -10971.153603047469
1
Iteration 8100: Loss = -10971.15354925293
Iteration 8200: Loss = -10971.153532926735
Iteration 8300: Loss = -10971.154735645683
1
Iteration 8400: Loss = -10971.153446399043
Iteration 8500: Loss = -10971.153399121875
Iteration 8600: Loss = -10971.153384070944
Iteration 8700: Loss = -10971.154886472686
1
Iteration 8800: Loss = -10971.153293839387
Iteration 8900: Loss = -10971.153398471297
1
Iteration 9000: Loss = -10971.152944377303
Iteration 9100: Loss = -10971.151469267143
Iteration 9200: Loss = -10971.152268186694
1
Iteration 9300: Loss = -10971.151006521439
Iteration 9400: Loss = -10971.177015627058
1
Iteration 9500: Loss = -10971.150781759594
Iteration 9600: Loss = -10971.15077455251
Iteration 9700: Loss = -10971.168807599994
1
Iteration 9800: Loss = -10971.15072774902
Iteration 9900: Loss = -10971.150705451319
Iteration 10000: Loss = -10971.15150528897
1
Iteration 10100: Loss = -10971.150662678738
Iteration 10200: Loss = -10971.150667350223
1
Iteration 10300: Loss = -10971.150734017425
2
Iteration 10400: Loss = -10971.150567018623
Iteration 10500: Loss = -10971.14834547092
Iteration 10600: Loss = -10971.148646435395
1
Iteration 10700: Loss = -10971.148341799868
Iteration 10800: Loss = -10971.382392313915
1
Iteration 10900: Loss = -10971.148349402847
2
Iteration 11000: Loss = -10971.14834997386
3
Iteration 11100: Loss = -10971.174024549227
4
Iteration 11200: Loss = -10971.148374979688
5
Stopping early at iteration 11200 due to no improvement.
pi: tensor([[0.6402, 0.3598],
        [0.2768, 0.7232]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9493, 0.0507], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1817, 0.0984],
         [0.6384, 0.2639]],

        [[0.5867, 0.1067],
         [0.6155, 0.7084]],

        [[0.5538, 0.1010],
         [0.7125, 0.6703]],

        [[0.7106, 0.0912],
         [0.7229, 0.5927]],

        [[0.6043, 0.1097],
         [0.7242, 0.6388]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.002056465888134684
time is 1
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 93
Adjusted Rand Index: 0.7369432436752338
time is 2
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 95
Adjusted Rand Index: 0.8080808080808081
time is 3
tensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 4
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448338943900694
Global Adjusted Rand Index: 0.5586499157610304
Average Adjusted Rand Index: 0.6703828824068492
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23343.04960067158
Iteration 100: Loss = -11080.173677886674
Iteration 200: Loss = -11078.996199742058
Iteration 300: Loss = -11078.66223925144
Iteration 400: Loss = -11078.469479012481
Iteration 500: Loss = -11078.18858152834
Iteration 600: Loss = -11077.233812745448
Iteration 700: Loss = -11076.523435916764
Iteration 800: Loss = -11076.13098817199
Iteration 900: Loss = -11075.86067372528
Iteration 1000: Loss = -11075.609954126912
Iteration 1100: Loss = -11075.342536752709
Iteration 1200: Loss = -11075.028841494803
Iteration 1300: Loss = -11074.622726100371
Iteration 1400: Loss = -11073.973301636634
Iteration 1500: Loss = -11036.861519954045
Iteration 1600: Loss = -10976.42194104012
Iteration 1700: Loss = -10975.940970496631
Iteration 1800: Loss = -10975.799967047442
Iteration 1900: Loss = -10975.383278106034
Iteration 2000: Loss = -10973.368508374388
Iteration 2100: Loss = -10972.964763872586
Iteration 2200: Loss = -10972.894538075981
Iteration 2300: Loss = -10971.368504928982
Iteration 2400: Loss = -10971.321554747272
Iteration 2500: Loss = -10971.29370981752
Iteration 2600: Loss = -10971.272376413279
Iteration 2700: Loss = -10971.248914636688
Iteration 2800: Loss = -10971.231309517516
Iteration 2900: Loss = -10971.216411990981
Iteration 3000: Loss = -10971.208549362098
Iteration 3100: Loss = -10971.205396694148
Iteration 3200: Loss = -10971.200892598623
Iteration 3300: Loss = -10971.198731407203
Iteration 3400: Loss = -10971.197086460683
Iteration 3500: Loss = -10971.198402726137
1
Iteration 3600: Loss = -10971.194021398964
Iteration 3700: Loss = -10971.192738805023
Iteration 3800: Loss = -10971.19182298198
Iteration 3900: Loss = -10971.190888537993
Iteration 4000: Loss = -10971.190151231109
Iteration 4100: Loss = -10971.189621884023
Iteration 4200: Loss = -10971.18890706103
Iteration 4300: Loss = -10971.188318165392
Iteration 4400: Loss = -10971.189625290699
1
Iteration 4500: Loss = -10971.187010467193
Iteration 4600: Loss = -10971.185835280523
Iteration 4700: Loss = -10971.184731583173
Iteration 4800: Loss = -10971.183885222688
Iteration 4900: Loss = -10971.18307046897
Iteration 5000: Loss = -10971.182115445194
Iteration 5100: Loss = -10971.180979626153
Iteration 5200: Loss = -10971.17978242346
Iteration 5300: Loss = -10971.20481585148
1
Iteration 5400: Loss = -10971.17897742232
Iteration 5500: Loss = -10971.17958696145
1
Iteration 5600: Loss = -10971.178618189682
Iteration 5700: Loss = -10971.178516926524
Iteration 5800: Loss = -10971.178358877218
Iteration 5900: Loss = -10971.178234900344
Iteration 6000: Loss = -10971.179074421541
1
Iteration 6100: Loss = -10971.17803609707
Iteration 6200: Loss = -10971.201394839098
1
Iteration 6300: Loss = -10971.177835695895
Iteration 6400: Loss = -10971.177712455474
Iteration 6500: Loss = -10971.177787139708
1
Iteration 6600: Loss = -10971.17681268699
Iteration 6700: Loss = -10971.152103194647
Iteration 6800: Loss = -10971.152062937932
Iteration 6900: Loss = -10971.151939639698
Iteration 7000: Loss = -10971.157064943714
1
Iteration 7100: Loss = -10971.15186498507
Iteration 7200: Loss = -10971.152106049578
1
Iteration 7300: Loss = -10971.151763196674
Iteration 7400: Loss = -10971.15194872126
1
Iteration 7500: Loss = -10971.151689569138
Iteration 7600: Loss = -10971.151694050683
1
Iteration 7700: Loss = -10971.151740555122
2
Iteration 7800: Loss = -10971.151587689017
Iteration 7900: Loss = -10971.1515308753
Iteration 8000: Loss = -10971.151501119197
Iteration 8100: Loss = -10971.151088160013
Iteration 8200: Loss = -10971.149855343258
Iteration 8300: Loss = -10971.150099408718
1
Iteration 8400: Loss = -10971.149714687255
Iteration 8500: Loss = -10971.14969452948
Iteration 8600: Loss = -10971.14965780156
Iteration 8700: Loss = -10971.149635233904
Iteration 8800: Loss = -10971.154520190103
1
Iteration 8900: Loss = -10971.149568433948
Iteration 9000: Loss = -10971.149572774068
1
Iteration 9100: Loss = -10971.156952673973
2
Iteration 9200: Loss = -10971.14952002639
Iteration 9300: Loss = -10971.149428848981
Iteration 9400: Loss = -10971.149428942083
1
Iteration 9500: Loss = -10971.149342771523
Iteration 9600: Loss = -10971.150995190708
1
Iteration 9700: Loss = -10971.1490371103
Iteration 9800: Loss = -10971.176050733391
1
Iteration 9900: Loss = -10971.149031809611
Iteration 10000: Loss = -10971.14896594736
Iteration 10100: Loss = -10971.213145186222
1
Iteration 10200: Loss = -10971.148922169046
Iteration 10300: Loss = -10971.148792242611
Iteration 10400: Loss = -10971.153305200425
1
Iteration 10500: Loss = -10971.148506648704
Iteration 10600: Loss = -10971.148235762199
Iteration 10700: Loss = -10971.147993921482
Iteration 10800: Loss = -10971.14729795908
Iteration 10900: Loss = -10971.147200104868
Iteration 11000: Loss = -10971.147265483829
1
Iteration 11100: Loss = -10971.147162192263
Iteration 11200: Loss = -10971.147160756516
Iteration 11300: Loss = -10971.147604105407
1
Iteration 11400: Loss = -10971.147165300043
2
Iteration 11500: Loss = -10971.147133118877
Iteration 11600: Loss = -10971.147394075962
1
Iteration 11700: Loss = -10971.147144447556
2
Iteration 11800: Loss = -10971.147149914872
3
Iteration 11900: Loss = -10971.147265435799
4
Iteration 12000: Loss = -10971.14713224371
Iteration 12100: Loss = -10971.165256431066
1
Iteration 12200: Loss = -10971.147115189659
Iteration 12300: Loss = -10971.147122425178
1
Iteration 12400: Loss = -10971.147483331852
2
Iteration 12500: Loss = -10971.14713577251
3
Iteration 12600: Loss = -10971.157994559402
4
Iteration 12700: Loss = -10971.147130545514
5
Stopping early at iteration 12700 due to no improvement.
pi: tensor([[0.6403, 0.3597],
        [0.2768, 0.7232]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9493, 0.0507], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1818, 0.0984],
         [0.6578, 0.2639]],

        [[0.5832, 0.1067],
         [0.7188, 0.6013]],

        [[0.5297, 0.1010],
         [0.5711, 0.5043]],

        [[0.6795, 0.0911],
         [0.7204, 0.5786]],

        [[0.6482, 0.1097],
         [0.5162, 0.6377]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.002056465888134684
time is 1
tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 93
Adjusted Rand Index: 0.7369432436752338
time is 2
tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        0, 1, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 95
Adjusted Rand Index: 0.8080808080808081
time is 3
tensor([1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 99
Adjusted Rand Index: 0.96
time is 4
tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 96
Adjusted Rand Index: 0.8448338943900694
Global Adjusted Rand Index: 0.5586499157610304
Average Adjusted Rand Index: 0.6703828824068492
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22658.941024419157
Iteration 100: Loss = -11080.260402280566
Iteration 200: Loss = -11079.172095768155
Iteration 300: Loss = -11078.838791496064
Iteration 400: Loss = -11078.025873238055
Iteration 500: Loss = -11075.892251026364
Iteration 600: Loss = -11074.91452924222
Iteration 700: Loss = -11074.41559765565
Iteration 800: Loss = -11074.105384010387
Iteration 900: Loss = -11073.87412587665
Iteration 1000: Loss = -11073.16044517469
Iteration 1100: Loss = -11070.962255367145
Iteration 1200: Loss = -11070.481500939472
Iteration 1300: Loss = -11070.285856861474
Iteration 1400: Loss = -11070.168034661734
Iteration 1500: Loss = -11070.086219280654
Iteration 1600: Loss = -11070.025805341485
Iteration 1700: Loss = -11069.980055652082
Iteration 1800: Loss = -11069.944530883227
Iteration 1900: Loss = -11069.916209356277
Iteration 2000: Loss = -11069.893294108186
Iteration 2100: Loss = -11069.87436780746
Iteration 2200: Loss = -11069.858628676242
Iteration 2300: Loss = -11069.845251904451
Iteration 2400: Loss = -11069.833733751182
Iteration 2500: Loss = -11069.823825514812
Iteration 2600: Loss = -11069.815117007587
Iteration 2700: Loss = -11069.807468944366
Iteration 2800: Loss = -11069.800671109948
Iteration 2900: Loss = -11069.794679245713
Iteration 3000: Loss = -11069.789336350796
Iteration 3100: Loss = -11069.784499914707
Iteration 3200: Loss = -11069.78014041038
Iteration 3300: Loss = -11069.776265295466
Iteration 3400: Loss = -11069.772687175187
Iteration 3500: Loss = -11069.769489286547
Iteration 3600: Loss = -11069.766528166563
Iteration 3700: Loss = -11069.763844910354
Iteration 3800: Loss = -11069.761435624243
Iteration 3900: Loss = -11069.759139348318
Iteration 4000: Loss = -11069.75707219011
Iteration 4100: Loss = -11069.755183090001
Iteration 4200: Loss = -11069.753435605935
Iteration 4300: Loss = -11069.75183597098
Iteration 4400: Loss = -11069.750286083463
Iteration 4500: Loss = -11069.748897865726
Iteration 4600: Loss = -11069.747581009511
Iteration 4700: Loss = -11069.746366277426
Iteration 4800: Loss = -11069.74524222479
Iteration 4900: Loss = -11069.744242642952
Iteration 5000: Loss = -11069.743259779292
Iteration 5100: Loss = -11069.742331980517
Iteration 5200: Loss = -11069.74148871078
Iteration 5300: Loss = -11069.740673337288
Iteration 5400: Loss = -11069.739949051978
Iteration 5500: Loss = -11069.73926046157
Iteration 5600: Loss = -11069.738593808142
Iteration 5700: Loss = -11069.738001909498
Iteration 5800: Loss = -11069.737421102063
Iteration 5900: Loss = -11069.736895095359
Iteration 6000: Loss = -11069.73640708574
Iteration 6100: Loss = -11069.735920095898
Iteration 6200: Loss = -11069.735451993442
Iteration 6300: Loss = -11069.735031950217
Iteration 6400: Loss = -11069.73463087681
Iteration 6500: Loss = -11069.73424128854
Iteration 6600: Loss = -11069.73390154081
Iteration 6700: Loss = -11069.73355460554
Iteration 6800: Loss = -11069.733257520682
Iteration 6900: Loss = -11069.732961776002
Iteration 7000: Loss = -11069.732663490884
Iteration 7100: Loss = -11069.732392147402
Iteration 7200: Loss = -11069.732156086919
Iteration 7300: Loss = -11069.731939148349
Iteration 7400: Loss = -11069.73171645573
Iteration 7500: Loss = -11069.73146522709
Iteration 7600: Loss = -11069.731270952565
Iteration 7700: Loss = -11069.731093037655
Iteration 7800: Loss = -11069.730962569593
Iteration 7900: Loss = -11069.730818192736
Iteration 8000: Loss = -11069.73057892346
Iteration 8100: Loss = -11069.730470615696
Iteration 8200: Loss = -11069.730313126487
Iteration 8300: Loss = -11069.730156390531
Iteration 8400: Loss = -11069.730817774083
1
Iteration 8500: Loss = -11069.729940607589
Iteration 8600: Loss = -11069.729824756689
Iteration 8700: Loss = -11069.729671156714
Iteration 8800: Loss = -11069.731150887825
1
Iteration 8900: Loss = -11069.729487978102
Iteration 9000: Loss = -11069.729424937146
Iteration 9100: Loss = -11069.729290610278
Iteration 9200: Loss = -11069.729594001503
1
Iteration 9300: Loss = -11069.72917459607
Iteration 9400: Loss = -11069.729055365187
Iteration 9500: Loss = -11069.902871549562
1
Iteration 9600: Loss = -11069.728951417714
Iteration 9700: Loss = -11069.728901406856
Iteration 9800: Loss = -11069.728799947174
Iteration 9900: Loss = -11069.730362260227
1
Iteration 10000: Loss = -11069.728692106697
Iteration 10100: Loss = -11069.728621255792
Iteration 10200: Loss = -11069.728580945934
Iteration 10300: Loss = -11069.72864815745
1
Iteration 10400: Loss = -11069.728463583315
Iteration 10500: Loss = -11069.728444825458
Iteration 10600: Loss = -11069.728500707903
1
Iteration 10700: Loss = -11069.728425252339
Iteration 10800: Loss = -11069.728309809856
Iteration 10900: Loss = -11069.728271578513
Iteration 11000: Loss = -11069.782148349765
1
Iteration 11100: Loss = -11069.7282138679
Iteration 11200: Loss = -11069.728192006272
Iteration 11300: Loss = -11069.728162112982
Iteration 11400: Loss = -11069.749504743168
1
Iteration 11500: Loss = -11069.728122982155
Iteration 11600: Loss = -11069.728096473218
Iteration 11700: Loss = -11069.728060944939
Iteration 11800: Loss = -11069.72923222997
1
Iteration 11900: Loss = -11069.728084240409
2
Iteration 12000: Loss = -11069.72806617541
3
Iteration 12100: Loss = -11069.728054940719
Iteration 12200: Loss = -11069.728045573902
Iteration 12300: Loss = -11069.728014221017
Iteration 12400: Loss = -11069.728000217345
Iteration 12500: Loss = -11069.72798548004
Iteration 12600: Loss = -11069.728108547319
1
Iteration 12700: Loss = -11069.727965425374
Iteration 12800: Loss = -11069.727918729352
Iteration 12900: Loss = -11069.730024284496
1
Iteration 13000: Loss = -11069.727924188115
2
Iteration 13100: Loss = -11069.727880015662
Iteration 13200: Loss = -11069.72783534588
Iteration 13300: Loss = -11069.72784941494
1
Iteration 13400: Loss = -11069.727904494475
2
Iteration 13500: Loss = -11069.727820993405
Iteration 13600: Loss = -11069.727789037483
Iteration 13700: Loss = -11069.738802414031
1
Iteration 13800: Loss = -11069.727795589604
2
Iteration 13900: Loss = -11069.727804804528
3
Iteration 14000: Loss = -11069.727794032646
4
Iteration 14100: Loss = -11069.728285149122
5
Stopping early at iteration 14100 due to no improvement.
pi: tensor([[1.0000e+00, 3.6430e-08],
        [5.6908e-05, 9.9994e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9900, 0.0100], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1651, 0.0808],
         [0.5866, 0.2027]],

        [[0.5337, 0.1818],
         [0.6171, 0.5466]],

        [[0.6559, 0.1919],
         [0.5469, 0.6229]],

        [[0.6571, 0.0505],
         [0.7180, 0.5752]],

        [[0.5705, 0.2929],
         [0.5626, 0.6928]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: -0.002264732809714258
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 55
Adjusted Rand Index: -0.004284949116229245
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 44
Adjusted Rand Index: 0.0044444444444444444
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 43
Adjusted Rand Index: 0.005453571602473584
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
Global Adjusted Rand Index: 0.00035843312135763213
Average Adjusted Rand Index: 0.0005127437965412247
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23421.70340586271
Iteration 100: Loss = -11078.832981353635
Iteration 200: Loss = -11078.014187201095
Iteration 300: Loss = -11076.950919600955
Iteration 400: Loss = -11076.000506036013
Iteration 500: Loss = -11075.454118197067
Iteration 600: Loss = -11074.935900563458
Iteration 700: Loss = -11074.26750808481
Iteration 800: Loss = -11069.50321402882
Iteration 900: Loss = -10978.115123697558
Iteration 1000: Loss = -10974.891584026616
Iteration 1100: Loss = -10974.735195036805
Iteration 1200: Loss = -10974.66262814597
Iteration 1300: Loss = -10974.556924357597
Iteration 1400: Loss = -10974.525359693003
Iteration 1500: Loss = -10974.503762754348
Iteration 1600: Loss = -10974.482907844962
Iteration 1700: Loss = -10974.458182187966
Iteration 1800: Loss = -10974.443397174633
Iteration 1900: Loss = -10974.425232049589
Iteration 2000: Loss = -10974.393063199952
Iteration 2100: Loss = -10974.195841855408
Iteration 2200: Loss = -10971.415864929622
Iteration 2300: Loss = -10971.209365220313
Iteration 2400: Loss = -10971.17768985256
Iteration 2500: Loss = -10971.168055608521
Iteration 2600: Loss = -10971.162306048258
Iteration 2700: Loss = -10971.160147284629
Iteration 2800: Loss = -10971.158275230842
Iteration 2900: Loss = -10971.157418071047
Iteration 3000: Loss = -10971.159701298768
1
Iteration 3100: Loss = -10971.156192269074
Iteration 3200: Loss = -10971.162163274972
1
Iteration 3300: Loss = -10971.161720665526
2
Iteration 3400: Loss = -10971.154375597518
Iteration 3500: Loss = -10971.153708210764
Iteration 3600: Loss = -10971.15341275495
Iteration 3700: Loss = -10971.152874768206
Iteration 3800: Loss = -10971.15260954856
Iteration 3900: Loss = -10971.152385767064
Iteration 4000: Loss = -10971.152470850353
1
Iteration 4100: Loss = -10971.152286476938
Iteration 4200: Loss = -10971.151806233262
Iteration 4300: Loss = -10971.15213085399
1
Iteration 4400: Loss = -10971.15155724491
Iteration 4500: Loss = -10971.163980814077
1
Iteration 4600: Loss = -10971.151317814989
Iteration 4700: Loss = -10971.151224763064
Iteration 4800: Loss = -10971.151211832457
Iteration 4900: Loss = -10971.151053063906
Iteration 5000: Loss = -10971.15097601281
Iteration 5100: Loss = -10971.150925089441
Iteration 5200: Loss = -10971.150842753581
Iteration 5300: Loss = -10971.150911022143
1
Iteration 5400: Loss = -10971.151744124121
2
Iteration 5500: Loss = -10971.15127416779
3
Iteration 5600: Loss = -10971.15095362234
4
Iteration 5700: Loss = -10971.150602041038
Iteration 5800: Loss = -10971.15055610227
Iteration 5900: Loss = -10971.150526460251
Iteration 6000: Loss = -10971.150554221405
1
Iteration 6100: Loss = -10971.1504126945
Iteration 6200: Loss = -10971.150915894304
1
Iteration 6300: Loss = -10971.150394868644
Iteration 6400: Loss = -10971.167392271724
1
Iteration 6500: Loss = -10971.150303979115
Iteration 6600: Loss = -10971.152555684752
1
Iteration 6700: Loss = -10971.149050856542
Iteration 6800: Loss = -10971.153710090688
1
Iteration 6900: Loss = -10971.148507656399
Iteration 7000: Loss = -10971.14870484628
1
Iteration 7100: Loss = -10971.148481426178
Iteration 7200: Loss = -10971.14852544776
1
Iteration 7300: Loss = -10971.148428985607
Iteration 7400: Loss = -10971.148465507116
1
Iteration 7500: Loss = -10971.148587032714
2
Iteration 7600: Loss = -10971.148573282531
3
Iteration 7700: Loss = -10971.148725698506
4
Iteration 7800: Loss = -10971.157342186269
5
Stopping early at iteration 7800 due to no improvement.
pi: tensor([[0.7226, 0.2774],
        [0.3591, 0.6409]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0499, 0.9501], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2641, 0.0984],
         [0.6518, 0.1817]],

        [[0.5208, 0.1067],
         [0.6118, 0.6545]],

        [[0.6731, 0.1010],
         [0.6016, 0.6632]],

        [[0.6920, 0.0911],
         [0.5675, 0.6938]],

        [[0.5318, 0.1097],
         [0.6958, 0.6132]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,
        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.002056465888134684
time is 1
tensor([0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1])
Difference count: 7
Adjusted Rand Index: 0.7369432436752338
time is 2
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 1, 1])
Difference count: 5
Adjusted Rand Index: 0.8080808080808081
time is 3
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
tensor([0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 0, 1, 0])
Difference count: 1
Adjusted Rand Index: 0.96
time is 4
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 0, 0])
tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0])
Difference count: 4
Adjusted Rand Index: 0.8448338943900694
Global Adjusted Rand Index: 0.5586499157610304
Average Adjusted Rand Index: 0.6703828824068492
10996.106887007372
[0.5586499157610304, 0.5586499157610304, 0.00035843312135763213, 0.5586499157610304] [0.6703828824068492, 0.6703828824068492, 0.0005127437965412247, 0.6703828824068492] [10971.148374979688, 10971.147130545514, 11069.728285149122, 10971.157342186269]
-----------------------------------------------------------------------------------------
This iteration is 15
True Objective function: Loss = -10659.615696593703
Iteration 0: Loss = -10839.826543706167
Iteration 10: Loss = -10735.475171415894
Iteration 20: Loss = -10735.263796361454
Iteration 30: Loss = -10735.203580763427
Iteration 40: Loss = -10735.172872792167
Iteration 50: Loss = -10735.151448240074
Iteration 60: Loss = -10735.13340417317
Iteration 70: Loss = -10735.116437621782
Iteration 80: Loss = -10735.09933072914
Iteration 90: Loss = -10735.081726433289
Iteration 100: Loss = -10735.063496518911
Iteration 110: Loss = -10735.044870268954
Iteration 120: Loss = -10735.026401252799
Iteration 130: Loss = -10735.008513556535
Iteration 140: Loss = -10734.991751697376
Iteration 150: Loss = -10734.976423405551
Iteration 160: Loss = -10734.962563863462
Iteration 170: Loss = -10734.950009021752
Iteration 180: Loss = -10734.938601470612
Iteration 190: Loss = -10734.928212031333
Iteration 200: Loss = -10734.918717138176
Iteration 210: Loss = -10734.910134623653
Iteration 220: Loss = -10734.902457115895
Iteration 230: Loss = -10734.895797795303
Iteration 240: Loss = -10734.89007904772
Iteration 250: Loss = -10734.885282329404
Iteration 260: Loss = -10734.881342582637
Iteration 270: Loss = -10734.878143622422
Iteration 280: Loss = -10734.875601459924
Iteration 290: Loss = -10734.8735749632
Iteration 300: Loss = -10734.871969829506
Iteration 310: Loss = -10734.870747067745
Iteration 320: Loss = -10734.869723318261
Iteration 330: Loss = -10734.868962937791
Iteration 340: Loss = -10734.868354733138
Iteration 350: Loss = -10734.867925904153
Iteration 360: Loss = -10734.867569481272
Iteration 370: Loss = -10734.86726712638
Iteration 380: Loss = -10734.867038276561
Iteration 390: Loss = -10734.866861300445
Iteration 400: Loss = -10734.86672071656
Iteration 410: Loss = -10734.86660549575
Iteration 420: Loss = -10734.866539230274
Iteration 430: Loss = -10734.86648205564
Iteration 440: Loss = -10734.866431984736
Iteration 450: Loss = -10734.866387837219
Iteration 460: Loss = -10734.86637997187
Iteration 470: Loss = -10734.86631612127
Iteration 480: Loss = -10734.866314261475
Iteration 490: Loss = -10734.86630478848
Iteration 500: Loss = -10734.866260623266
Iteration 510: Loss = -10734.86624141719
Iteration 520: Loss = -10734.866263675536
1
Iteration 530: Loss = -10734.866254045457
2
Iteration 540: Loss = -10734.866236132324
Iteration 550: Loss = -10734.866234629768
Iteration 560: Loss = -10734.866240460484
1
Iteration 570: Loss = -10734.86621193885
Iteration 580: Loss = -10734.866194192442
Iteration 590: Loss = -10734.866182663889
Iteration 600: Loss = -10734.866230059424
1
Iteration 610: Loss = -10734.866241055257
2
Iteration 620: Loss = -10734.866216710643
3
Stopping early at iteration 620 due to no improvement.
pi: tensor([[0.0172, 0.9828],
        [0.0586, 0.9414]], dtype=torch.float64)
alpha: tensor([0.0557, 0.9443])
beta: tensor([[[0.1396, 0.1161],
         [0.0894, 0.1562]],

        [[0.6399, 0.1619],
         [0.1284, 0.3212]],

        [[0.1290, 0.1608],
         [0.2345, 0.1233]],

        [[0.9410, 0.1063],
         [0.5780, 0.2343]],

        [[0.7783, 0.1796],
         [0.6138, 0.7337]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -10745.877593403178
Iteration 10: Loss = -10736.650054237449
Iteration 20: Loss = -10736.650042816056
Iteration 30: Loss = -10736.649942762759
Iteration 40: Loss = -10736.649437193326
Iteration 50: Loss = -10736.64513618276
Iteration 60: Loss = -10736.60510396204
Iteration 70: Loss = -10736.320915175014
Iteration 80: Loss = -10735.579705004942
Iteration 90: Loss = -10735.060577371793
Iteration 100: Loss = -10734.88574016452
Iteration 110: Loss = -10734.842813196588
Iteration 120: Loss = -10734.837183415499
Iteration 130: Loss = -10734.841068079546
1
Iteration 140: Loss = -10734.846633882275
2
Iteration 150: Loss = -10734.851854129663
3
Stopping early at iteration 150 due to no improvement.
pi: tensor([[5.1092e-10, 1.0000e+00],
        [5.4839e-02, 9.4516e-01]], dtype=torch.float64)
alpha: tensor([0.0514, 0.9486])
beta: tensor([[[0.1402, 0.1124],
         [0.4560, 0.1561]],

        [[0.6445, 0.1626],
         [0.9383, 0.1454]],

        [[0.0855, 0.1614],
         [0.3964, 0.3240]],

        [[0.9955, 0.1051],
         [0.1877, 0.9343]],

        [[0.0753, 0.1829],
         [0.9163, 0.0360]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 43
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22716.842692833885
Iteration 100: Loss = -10738.637775955922
Iteration 200: Loss = -10737.366976568932
Iteration 300: Loss = -10736.90594351925
Iteration 400: Loss = -10736.68555669343
Iteration 500: Loss = -10736.571631625522
Iteration 600: Loss = -10736.506623050727
Iteration 700: Loss = -10736.466302737557
Iteration 800: Loss = -10736.43897464773
Iteration 900: Loss = -10736.418754102031
Iteration 1000: Loss = -10736.402518200319
Iteration 1100: Loss = -10736.388308491078
Iteration 1200: Loss = -10736.375086486747
Iteration 1300: Loss = -10736.362226808527
Iteration 1400: Loss = -10736.34897730097
Iteration 1500: Loss = -10736.334482026568
Iteration 1600: Loss = -10736.31763566625
Iteration 1700: Loss = -10736.296902547052
Iteration 1800: Loss = -10736.269708690308
Iteration 1900: Loss = -10736.231623415604
Iteration 2000: Loss = -10736.175439236311
Iteration 2100: Loss = -10736.09390563698
Iteration 2200: Loss = -10735.978698988569
Iteration 2300: Loss = -10735.781444885537
Iteration 2400: Loss = -10735.447104521363
Iteration 2500: Loss = -10735.109749108276
Iteration 2600: Loss = -10734.841111541213
Iteration 2700: Loss = -10734.68513668716
Iteration 2800: Loss = -10734.616510125104
Iteration 2900: Loss = -10734.578100315572
Iteration 3000: Loss = -10734.549084833907
Iteration 3100: Loss = -10734.526850160466
Iteration 3200: Loss = -10734.513704364253
Iteration 3300: Loss = -10734.506837868246
Iteration 3400: Loss = -10734.503819048155
Iteration 3500: Loss = -10734.501726227541
Iteration 3600: Loss = -10734.499957438222
Iteration 3700: Loss = -10734.498511435959
Iteration 3800: Loss = -10734.497125474345
Iteration 3900: Loss = -10734.495913862249
Iteration 4000: Loss = -10734.49475931386
Iteration 4100: Loss = -10734.493683616236
Iteration 4200: Loss = -10734.492763868138
Iteration 4300: Loss = -10734.491703235788
Iteration 4400: Loss = -10734.49082475143
Iteration 4500: Loss = -10734.490179826451
Iteration 4600: Loss = -10734.489224028452
Iteration 4700: Loss = -10734.488444579123
Iteration 4800: Loss = -10734.487767456125
Iteration 4900: Loss = -10734.48708648815
Iteration 5000: Loss = -10734.48648501384
Iteration 5100: Loss = -10734.485904012998
Iteration 5200: Loss = -10734.485348563037
Iteration 5300: Loss = -10734.484967545965
Iteration 5400: Loss = -10734.484345113842
Iteration 5500: Loss = -10734.483918964275
Iteration 5600: Loss = -10734.483699647884
Iteration 5700: Loss = -10734.483031624446
Iteration 5800: Loss = -10734.482790857324
Iteration 5900: Loss = -10734.48229468416
Iteration 6000: Loss = -10734.481962863756
Iteration 6100: Loss = -10734.48204631925
1
Iteration 6200: Loss = -10734.481328118723
Iteration 6300: Loss = -10734.48104194291
Iteration 6400: Loss = -10734.480841765899
Iteration 6500: Loss = -10734.480495398328
Iteration 6600: Loss = -10734.480205139826
Iteration 6700: Loss = -10734.480004513676
Iteration 6800: Loss = -10734.47980783112
Iteration 6900: Loss = -10734.485635580228
1
Iteration 7000: Loss = -10734.479368159646
Iteration 7100: Loss = -10734.479200771197
Iteration 7200: Loss = -10734.478993309454
Iteration 7300: Loss = -10734.478858931392
Iteration 7400: Loss = -10734.479538198553
1
Iteration 7500: Loss = -10734.478533081825
Iteration 7600: Loss = -10734.478415706239
Iteration 7700: Loss = -10734.478301212614
Iteration 7800: Loss = -10734.478125519245
Iteration 7900: Loss = -10734.479044832442
1
Iteration 8000: Loss = -10734.478222665059
2
Iteration 8100: Loss = -10734.479795163212
3
Iteration 8200: Loss = -10734.477699558804
Iteration 8300: Loss = -10734.497654856661
1
Iteration 8400: Loss = -10734.477509578419
Iteration 8500: Loss = -10734.477461087505
Iteration 8600: Loss = -10734.478283079632
1
Iteration 8700: Loss = -10734.477263472858
Iteration 8800: Loss = -10734.477182422905
Iteration 8900: Loss = -10734.4774137724
1
Iteration 9000: Loss = -10734.477063469834
Iteration 9100: Loss = -10734.481737749276
1
Iteration 9200: Loss = -10734.477552582734
2
Iteration 9300: Loss = -10734.477031983919
Iteration 9400: Loss = -10734.480694887949
1
Iteration 9500: Loss = -10734.478969468146
2
Iteration 9600: Loss = -10734.476945635479
Iteration 9700: Loss = -10734.727772624412
1
Iteration 9800: Loss = -10734.476619607674
Iteration 9900: Loss = -10734.553445892514
1
Iteration 10000: Loss = -10734.476553066586
Iteration 10100: Loss = -10734.4809886789
1
Iteration 10200: Loss = -10734.476499592274
Iteration 10300: Loss = -10734.478829805666
1
Iteration 10400: Loss = -10734.476427701647
Iteration 10500: Loss = -10734.489681604904
1
Iteration 10600: Loss = -10734.477212749229
2
Iteration 10700: Loss = -10734.477489104413
3
Iteration 10800: Loss = -10734.47866558924
4
Iteration 10900: Loss = -10734.533414077569
5
Stopping early at iteration 10900 due to no improvement.
pi: tensor([[3.9073e-04, 9.9961e-01],
        [6.4069e-02, 9.3593e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0251, 0.9749], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1597, 0.0985],
         [0.5109, 0.1563]],

        [[0.5975, 0.1671],
         [0.6638, 0.6226]],

        [[0.5674, 0.1666],
         [0.7292, 0.6541]],

        [[0.6110, 0.1060],
         [0.5422, 0.6315]],

        [[0.6067, 0.1857],
         [0.6275, 0.6866]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: -0.00491035742834304
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0004242385566336003
Average Adjusted Rand Index: -0.000982071485668608
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25793.778718811474
Iteration 100: Loss = -10738.657758663963
Iteration 200: Loss = -10737.164465269061
Iteration 300: Loss = -10736.7821199006
Iteration 400: Loss = -10736.622380336927
Iteration 500: Loss = -10736.540691450398
Iteration 600: Loss = -10736.493737678786
Iteration 700: Loss = -10736.46401915786
Iteration 800: Loss = -10736.44358413301
Iteration 900: Loss = -10736.428466874255
Iteration 1000: Loss = -10736.416613013589
Iteration 1100: Loss = -10736.406681510025
Iteration 1200: Loss = -10736.397738179177
Iteration 1300: Loss = -10736.389218602237
Iteration 1400: Loss = -10736.380753623225
Iteration 1500: Loss = -10736.371781037325
Iteration 1600: Loss = -10736.36173835447
Iteration 1700: Loss = -10736.349811140699
Iteration 1800: Loss = -10736.334922846632
Iteration 1900: Loss = -10736.315135445306
Iteration 2000: Loss = -10736.28665164779
Iteration 2100: Loss = -10736.241933054736
Iteration 2200: Loss = -10736.166898961379
Iteration 2300: Loss = -10736.042014514836
Iteration 2400: Loss = -10735.863681483781
Iteration 2500: Loss = -10735.641234954339
Iteration 2600: Loss = -10735.3915233911
Iteration 2700: Loss = -10735.135054829805
Iteration 2800: Loss = -10734.950334466927
Iteration 2900: Loss = -10734.86379999073
Iteration 3000: Loss = -10734.82309507957
Iteration 3100: Loss = -10734.802829682709
Iteration 3200: Loss = -10734.791394465867
Iteration 3300: Loss = -10734.778583670746
Iteration 3400: Loss = -10734.751439014271
Iteration 3500: Loss = -10734.616804424615
Iteration 3600: Loss = -10734.495272198006
Iteration 3700: Loss = -10734.490362889985
Iteration 3800: Loss = -10734.488742807303
Iteration 3900: Loss = -10734.4877548287
Iteration 4000: Loss = -10734.48691624609
Iteration 4100: Loss = -10734.486178114323
Iteration 4200: Loss = -10734.485482578495
Iteration 4300: Loss = -10734.48486128875
Iteration 4400: Loss = -10734.484291145896
Iteration 4500: Loss = -10734.483808310546
Iteration 4600: Loss = -10734.483327657468
Iteration 4700: Loss = -10734.483034075012
Iteration 4800: Loss = -10734.482485590786
Iteration 4900: Loss = -10734.482047191703
Iteration 5000: Loss = -10734.481860630403
Iteration 5100: Loss = -10734.481350756274
Iteration 5200: Loss = -10734.481023744016
Iteration 5300: Loss = -10734.480777089599
Iteration 5400: Loss = -10734.480439513578
Iteration 5500: Loss = -10734.480697991836
1
Iteration 5600: Loss = -10734.479973958065
Iteration 5700: Loss = -10734.479747610974
Iteration 5800: Loss = -10734.479489390087
Iteration 5900: Loss = -10734.479318138625
Iteration 6000: Loss = -10734.480789001544
1
Iteration 6100: Loss = -10734.478944679671
Iteration 6200: Loss = -10734.478747141693
Iteration 6300: Loss = -10734.478624070114
Iteration 6400: Loss = -10734.47843466362
Iteration 6500: Loss = -10734.47855597871
1
Iteration 6600: Loss = -10734.478202568589
Iteration 6700: Loss = -10734.479257859626
1
Iteration 6800: Loss = -10734.47794686562
Iteration 6900: Loss = -10734.477831063346
Iteration 7000: Loss = -10734.47775723727
Iteration 7100: Loss = -10734.477612235494
Iteration 7200: Loss = -10734.47751441548
Iteration 7300: Loss = -10734.477516014418
1
Iteration 7400: Loss = -10734.477352925041
Iteration 7500: Loss = -10734.478531510671
1
Iteration 7600: Loss = -10734.477224720693
Iteration 7700: Loss = -10734.477570042818
1
Iteration 7800: Loss = -10734.484965900598
2
Iteration 7900: Loss = -10734.47698702231
Iteration 8000: Loss = -10734.476943645312
Iteration 8100: Loss = -10734.477386560558
1
Iteration 8200: Loss = -10734.476819204288
Iteration 8300: Loss = -10734.476769924802
Iteration 8400: Loss = -10734.477008491574
1
Iteration 8500: Loss = -10734.47669309789
Iteration 8600: Loss = -10734.4955801043
1
Iteration 8700: Loss = -10734.476567431846
Iteration 8800: Loss = -10734.476647944924
1
Iteration 8900: Loss = -10734.476575224526
2
Iteration 9000: Loss = -10734.480152502167
3
Iteration 9100: Loss = -10734.476807501755
4
Iteration 9200: Loss = -10734.476537895007
Iteration 9300: Loss = -10734.504095529579
1
Iteration 9400: Loss = -10734.482028268856
2
Iteration 9500: Loss = -10734.495739109763
3
Iteration 9600: Loss = -10734.47871049426
4
Iteration 9700: Loss = -10734.676866542253
5
Stopping early at iteration 9700 due to no improvement.
pi: tensor([[9.3629e-01, 6.3706e-02],
        [9.9961e-01, 3.9138e-04]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9740, 0.0260], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1588, 0.0985],
         [0.5606, 0.1595]],

        [[0.7285, 0.1663],
         [0.6226, 0.5748]],

        [[0.5209, 0.1658],
         [0.7160, 0.7033]],

        [[0.6314, 0.1060],
         [0.6359, 0.5520]],

        [[0.6316, 0.1852],
         [0.7298, 0.5162]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 56
Adjusted Rand Index: -0.00491035742834304
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0004242385566336003
Average Adjusted Rand Index: -0.000982071485668608
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -25348.152251429277
Iteration 100: Loss = -10737.621840748858
Iteration 200: Loss = -10736.605533945185
Iteration 300: Loss = -10736.403950327693
Iteration 400: Loss = -10736.287309550871
Iteration 500: Loss = -10736.193462309175
Iteration 600: Loss = -10736.095453396236
Iteration 700: Loss = -10735.991416153258
Iteration 800: Loss = -10735.889285094165
Iteration 900: Loss = -10735.792902746038
Iteration 1000: Loss = -10735.698847180542
Iteration 1100: Loss = -10735.602945120745
Iteration 1200: Loss = -10735.50194887165
Iteration 1300: Loss = -10735.393398177248
Iteration 1400: Loss = -10735.277025478155
Iteration 1500: Loss = -10735.156921162421
Iteration 1600: Loss = -10735.042462075575
Iteration 1700: Loss = -10734.945303002864
Iteration 1800: Loss = -10734.872641989507
Iteration 1900: Loss = -10734.823079945167
Iteration 2000: Loss = -10734.786339063849
Iteration 2100: Loss = -10734.742743907667
Iteration 2200: Loss = -10734.6387367552
Iteration 2300: Loss = -10734.525311583953
Iteration 2400: Loss = -10734.50496564336
Iteration 2500: Loss = -10734.497908192776
Iteration 2600: Loss = -10734.494716081652
Iteration 2700: Loss = -10734.492739172316
Iteration 2800: Loss = -10734.491301082375
Iteration 2900: Loss = -10734.490169148596
Iteration 3000: Loss = -10734.489227088952
Iteration 3100: Loss = -10734.488393172925
Iteration 3200: Loss = -10734.487628119614
Iteration 3300: Loss = -10734.48698693135
Iteration 3400: Loss = -10734.486320564849
Iteration 3500: Loss = -10734.485753025469
Iteration 3600: Loss = -10734.485228050406
Iteration 3700: Loss = -10734.484728225327
Iteration 3800: Loss = -10734.484255795387
Iteration 3900: Loss = -10734.4838054327
Iteration 4000: Loss = -10734.483435398788
Iteration 4100: Loss = -10734.483000292257
Iteration 4200: Loss = -10734.482641123495
Iteration 4300: Loss = -10734.482325390944
Iteration 4400: Loss = -10734.481969094924
Iteration 4500: Loss = -10734.481660266152
Iteration 4600: Loss = -10734.481370408752
Iteration 4700: Loss = -10734.481097558639
Iteration 4800: Loss = -10734.480803077604
Iteration 4900: Loss = -10734.480660549421
Iteration 5000: Loss = -10734.480332424493
Iteration 5100: Loss = -10734.480077966731
Iteration 5200: Loss = -10734.481271970244
1
Iteration 5300: Loss = -10734.479671283098
Iteration 5400: Loss = -10734.479456052246
Iteration 5500: Loss = -10734.479299887153
Iteration 5600: Loss = -10734.47907897502
Iteration 5700: Loss = -10734.479590437586
1
Iteration 5800: Loss = -10734.47879143705
Iteration 5900: Loss = -10734.478618877121
Iteration 6000: Loss = -10734.478487278719
Iteration 6100: Loss = -10734.478336879916
Iteration 6200: Loss = -10734.47836181664
1
Iteration 6300: Loss = -10734.478124196348
Iteration 6400: Loss = -10734.478007728136
Iteration 6500: Loss = -10734.477985060777
Iteration 6600: Loss = -10734.477772956387
Iteration 6700: Loss = -10734.47776113686
Iteration 6800: Loss = -10734.477569442659
Iteration 6900: Loss = -10734.477522662419
Iteration 7000: Loss = -10734.477413223627
Iteration 7100: Loss = -10734.477327343291
Iteration 7200: Loss = -10734.477313278547
Iteration 7300: Loss = -10734.477203244873
Iteration 7400: Loss = -10734.477173305037
Iteration 7500: Loss = -10734.477046835676
Iteration 7600: Loss = -10734.476981412015
Iteration 7700: Loss = -10734.476917115402
Iteration 7800: Loss = -10734.476866845658
Iteration 7900: Loss = -10734.501059799826
1
Iteration 8000: Loss = -10734.476733477866
Iteration 8100: Loss = -10734.476708260927
Iteration 8200: Loss = -10734.50195818511
1
Iteration 8300: Loss = -10734.47664800803
Iteration 8400: Loss = -10734.4765921117
Iteration 8500: Loss = -10734.476555090276
Iteration 8600: Loss = -10734.476981009082
1
Iteration 8700: Loss = -10734.721348345845
2
Iteration 8800: Loss = -10734.476471228016
Iteration 8900: Loss = -10734.485353436668
1
Iteration 9000: Loss = -10734.476407645594
Iteration 9100: Loss = -10734.476383377667
Iteration 9200: Loss = -10734.476337340062
Iteration 9300: Loss = -10734.476760673004
1
Iteration 9400: Loss = -10734.476284606191
Iteration 9500: Loss = -10734.4792012623
1
Iteration 9600: Loss = -10734.476230876136
Iteration 9700: Loss = -10734.476200723822
Iteration 9800: Loss = -10734.47775675193
1
Iteration 9900: Loss = -10734.47757009478
2
Iteration 10000: Loss = -10734.48202962931
3
Iteration 10100: Loss = -10734.476873871028
4
Iteration 10200: Loss = -10734.498121163897
5
Stopping early at iteration 10200 due to no improvement.
pi: tensor([[2.5918e-04, 9.9974e-01],
        [6.4044e-02, 9.3596e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0256, 0.9744], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1596, 0.0985],
         [0.6093, 0.1578]],

        [[0.7286, 0.1664],
         [0.6299, 0.7253]],

        [[0.6667, 0.1659],
         [0.5085, 0.5537]],

        [[0.6456, 0.1060],
         [0.6438, 0.6283]],

        [[0.6017, 0.1853],
         [0.6385, 0.5905]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: -0.00491035742834304
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0004242385566336003
Average Adjusted Rand Index: -0.000982071485668608
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21419.885563604304
Iteration 100: Loss = -10736.731454744484
Iteration 200: Loss = -10736.336302092504
Iteration 300: Loss = -10736.224193508258
Iteration 400: Loss = -10736.110327610748
Iteration 500: Loss = -10735.918845945422
Iteration 600: Loss = -10735.543349697838
Iteration 700: Loss = -10735.053151086544
Iteration 800: Loss = -10734.75463902085
Iteration 900: Loss = -10734.635517012191
Iteration 1000: Loss = -10734.568012674245
Iteration 1100: Loss = -10734.528609588579
Iteration 1200: Loss = -10734.507220107416
Iteration 1300: Loss = -10734.496374557564
Iteration 1400: Loss = -10734.491093032726
Iteration 1500: Loss = -10734.488382854688
Iteration 1600: Loss = -10734.486889700744
Iteration 1700: Loss = -10734.485980207608
Iteration 1800: Loss = -10734.485218506958
Iteration 1900: Loss = -10734.484613966433
Iteration 2000: Loss = -10734.484052843974
Iteration 2100: Loss = -10734.483588570576
Iteration 2200: Loss = -10734.483137388883
Iteration 2300: Loss = -10734.4826825312
Iteration 2400: Loss = -10734.482304192274
Iteration 2500: Loss = -10734.481949623405
Iteration 2600: Loss = -10734.481561879371
Iteration 2700: Loss = -10734.481254593393
Iteration 2800: Loss = -10734.480981258255
Iteration 2900: Loss = -10734.480653389248
Iteration 3000: Loss = -10734.48040152533
Iteration 3100: Loss = -10734.480155332996
Iteration 3200: Loss = -10734.479896378001
Iteration 3300: Loss = -10734.47970042383
Iteration 3400: Loss = -10734.479490075591
Iteration 3500: Loss = -10734.4793262186
Iteration 3600: Loss = -10734.479101852989
Iteration 3700: Loss = -10734.478963342872
Iteration 3800: Loss = -10734.47900549803
1
Iteration 3900: Loss = -10734.478628139148
Iteration 4000: Loss = -10734.478433192626
Iteration 4100: Loss = -10734.478443532618
1
Iteration 4200: Loss = -10734.478181405184
Iteration 4300: Loss = -10734.478066010404
Iteration 4400: Loss = -10734.477945551409
Iteration 4500: Loss = -10734.477873760023
Iteration 4600: Loss = -10734.477722158197
Iteration 4700: Loss = -10734.477651618427
Iteration 4800: Loss = -10734.477538733063
Iteration 4900: Loss = -10734.479202756263
1
Iteration 5000: Loss = -10734.477344268305
Iteration 5100: Loss = -10734.477284417888
Iteration 5200: Loss = -10734.477191836753
Iteration 5300: Loss = -10734.477118870267
Iteration 5400: Loss = -10734.4897659655
1
Iteration 5500: Loss = -10734.477003785358
Iteration 5600: Loss = -10734.476939844006
Iteration 5700: Loss = -10734.476973985242
1
Iteration 5800: Loss = -10734.476834983927
Iteration 5900: Loss = -10734.478618551822
1
Iteration 6000: Loss = -10734.47673251405
Iteration 6100: Loss = -10734.476679456058
Iteration 6200: Loss = -10734.476791477882
1
Iteration 6300: Loss = -10734.476589461785
Iteration 6400: Loss = -10734.476589723068
1
Iteration 6500: Loss = -10734.476498666207
Iteration 6600: Loss = -10734.476485091524
Iteration 6700: Loss = -10734.476457479424
Iteration 6800: Loss = -10734.476435210907
Iteration 6900: Loss = -10734.478144101562
1
Iteration 7000: Loss = -10734.47634307062
Iteration 7100: Loss = -10734.47666945824
1
Iteration 7200: Loss = -10734.476298558062
Iteration 7300: Loss = -10734.47660319912
1
Iteration 7400: Loss = -10734.47624701214
Iteration 7500: Loss = -10734.476321175993
1
Iteration 7600: Loss = -10734.476201629208
Iteration 7700: Loss = -10734.476238191473
1
Iteration 7800: Loss = -10734.477266827906
2
Iteration 7900: Loss = -10734.487873341719
3
Iteration 8000: Loss = -10734.476132699312
Iteration 8100: Loss = -10734.496370970643
1
Iteration 8200: Loss = -10734.476120623005
Iteration 8300: Loss = -10734.479388053329
1
Iteration 8400: Loss = -10734.476060801844
Iteration 8500: Loss = -10734.479589274792
1
Iteration 8600: Loss = -10734.476040142514
Iteration 8700: Loss = -10734.479599525688
1
Iteration 8800: Loss = -10734.476044025387
2
Iteration 8900: Loss = -10734.48945834794
3
Iteration 9000: Loss = -10734.479182372266
4
Iteration 9100: Loss = -10734.489374752318
5
Stopping early at iteration 9100 due to no improvement.
pi: tensor([[1.5084e-04, 9.9985e-01],
        [6.4273e-02, 9.3573e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0251, 0.9749], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1597, 0.0984],
         [0.6736, 0.1563]],

        [[0.6865, 0.1667],
         [0.7192, 0.6616]],

        [[0.5629, 0.1661],
         [0.5303, 0.5009]],

        [[0.5459, 0.1061],
         [0.6198, 0.6339]],

        [[0.6632, 0.1854],
         [0.5586, 0.6713]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1])
Difference count: 44
Adjusted Rand Index: -0.00491035742834304
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0004242385566336003
Average Adjusted Rand Index: -0.000982071485668608
10659.615696593703
[-0.0004242385566336003, -0.0004242385566336003, -0.0004242385566336003, -0.0004242385566336003] [-0.000982071485668608, -0.000982071485668608, -0.000982071485668608, -0.000982071485668608] [10734.533414077569, 10734.676866542253, 10734.498121163897, 10734.489374752318]
-----------------------------------------------------------------------------------------
This iteration is 16
True Objective function: Loss = -10918.961502337863
Iteration 0: Loss = -11013.77366118452
Iteration 10: Loss = -11013.773660952553
Iteration 20: Loss = -11013.773661116995
1
Iteration 30: Loss = -11013.773661568497
2
Iteration 40: Loss = -11013.77366253425
3
Stopping early at iteration 40 due to no improvement.
pi: tensor([[1.0000e+00, 8.5695e-11],
        [1.0000e+00, 9.6456e-18]], dtype=torch.float64)
alpha: tensor([1.0000e+00, 8.6577e-11])
beta: tensor([[[0.1616, 0.1997],
         [0.8922, 0.1660]],

        [[0.6535, 0.0892],
         [0.4193, 0.9917]],

        [[0.8993, 0.2015],
         [0.7678, 0.3538]],

        [[0.2839, 0.1578],
         [0.7210, 0.2952]],

        [[0.9140, 0.1654],
         [0.0667, 0.4836]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 53
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -11221.768142360428
Iteration 10: Loss = -11012.910532574171
Iteration 20: Loss = -11012.90947936228
Iteration 30: Loss = -11012.909405173741
Iteration 40: Loss = -11012.909370503758
Iteration 50: Loss = -11012.909373428693
1
Iteration 60: Loss = -11012.909367099237
Iteration 70: Loss = -11012.90933218777
Iteration 80: Loss = -11012.909330001863
Iteration 90: Loss = -11012.909362928602
1
Iteration 100: Loss = -11012.909286619148
Iteration 110: Loss = -11012.909294735675
1
Iteration 120: Loss = -11012.909286799792
2
Iteration 130: Loss = -11012.909324732698
3
Stopping early at iteration 130 due to no improvement.
pi: tensor([[0.1735, 0.8265],
        [0.5858, 0.4142]], dtype=torch.float64)
alpha: tensor([0.4148, 0.5852])
beta: tensor([[[0.1619, 0.1688],
         [0.6046, 0.1614]],

        [[0.7730, 0.1549],
         [0.6823, 0.3516]],

        [[0.6676, 0.1614],
         [0.9263, 0.4785]],

        [[0.1180, 0.1606],
         [0.9046, 0.9684]],

        [[0.1143, 0.1624],
         [0.0160, 0.5905]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 47
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20983.601775409275
Iteration 100: Loss = -11014.822478643473
Iteration 200: Loss = -11013.945865185051
Iteration 300: Loss = -11013.560381754778
Iteration 400: Loss = -11013.318488312661
Iteration 500: Loss = -11013.181103363584
Iteration 600: Loss = -11013.101605373457
Iteration 700: Loss = -11013.034641033379
Iteration 800: Loss = -11012.964152377146
Iteration 900: Loss = -11012.882886099076
Iteration 1000: Loss = -11012.792467809815
Iteration 1100: Loss = -11012.706648940204
Iteration 1200: Loss = -11012.63581719853
Iteration 1300: Loss = -11012.577952855501
Iteration 1400: Loss = -11012.529529601874
Iteration 1500: Loss = -11012.488597478898
Iteration 1600: Loss = -11012.453847230512
Iteration 1700: Loss = -11012.42423773084
Iteration 1800: Loss = -11012.398867061158
Iteration 1900: Loss = -11012.376906659278
Iteration 2000: Loss = -11012.357669586325
Iteration 2100: Loss = -11012.340624183154
Iteration 2200: Loss = -11012.325371304953
Iteration 2300: Loss = -11012.311775267772
Iteration 2400: Loss = -11012.299607546509
Iteration 2500: Loss = -11012.28871341511
Iteration 2600: Loss = -11012.27909312298
Iteration 2700: Loss = -11012.270683168885
Iteration 2800: Loss = -11012.263429376295
Iteration 2900: Loss = -11012.257222793498
Iteration 3000: Loss = -11012.25196734075
Iteration 3100: Loss = -11012.247548318332
Iteration 3200: Loss = -11012.243796432187
Iteration 3300: Loss = -11012.240612773612
Iteration 3400: Loss = -11012.237829528334
Iteration 3500: Loss = -11012.235411445292
Iteration 3600: Loss = -11012.233302008895
Iteration 3700: Loss = -11012.231350065356
Iteration 3800: Loss = -11012.229557658951
Iteration 3900: Loss = -11012.22793511998
Iteration 4000: Loss = -11012.226321191732
Iteration 4100: Loss = -11012.22483275148
Iteration 4200: Loss = -11012.223305308768
Iteration 4300: Loss = -11012.221770365764
Iteration 4400: Loss = -11012.220208604187
Iteration 4500: Loss = -11012.218537607074
Iteration 4600: Loss = -11012.216722936364
Iteration 4700: Loss = -11012.214794955307
Iteration 4800: Loss = -11012.21254384936
Iteration 4900: Loss = -11012.210040257685
Iteration 5000: Loss = -11012.207112568154
Iteration 5100: Loss = -11012.203696966308
Iteration 5200: Loss = -11012.199785393772
Iteration 5300: Loss = -11012.195324664368
Iteration 5400: Loss = -11012.190435827322
Iteration 5500: Loss = -11012.18539618541
Iteration 5600: Loss = -11012.18053623622
Iteration 5700: Loss = -11012.176207443734
Iteration 5800: Loss = -11012.172505857478
Iteration 5900: Loss = -11012.169420510327
Iteration 6000: Loss = -11012.16690712514
Iteration 6100: Loss = -11012.164380980563
Iteration 6200: Loss = -11012.16193760521
Iteration 6300: Loss = -11012.15849706946
Iteration 6400: Loss = -11012.153616165704
Iteration 6500: Loss = -11012.144642325939
Iteration 6600: Loss = -11012.11933936804
Iteration 6700: Loss = -11012.064611180613
Iteration 6800: Loss = -11012.006412743802
Iteration 6900: Loss = -11011.980643348415
Iteration 7000: Loss = -11011.976753287287
Iteration 7100: Loss = -11011.975753121984
Iteration 7200: Loss = -11012.036695605017
1
Iteration 7300: Loss = -11011.974879583826
Iteration 7400: Loss = -11011.97461049578
Iteration 7500: Loss = -11011.97450173561
Iteration 7600: Loss = -11011.974353022424
Iteration 7700: Loss = -11011.986214658478
1
Iteration 7800: Loss = -11011.97424361595
Iteration 7900: Loss = -11011.974197959742
Iteration 8000: Loss = -11011.974303602263
1
Iteration 8100: Loss = -11012.006948420994
2
Iteration 8200: Loss = -11011.974028568877
Iteration 8300: Loss = -11011.981362476581
1
Iteration 8400: Loss = -11011.978884449254
2
Iteration 8500: Loss = -11011.989261905908
3
Iteration 8600: Loss = -11011.974306000284
4
Iteration 8700: Loss = -11011.973561818402
Iteration 8800: Loss = -11011.97313549157
Iteration 8900: Loss = -11012.003993495533
1
Iteration 9000: Loss = -11011.967137101896
Iteration 9100: Loss = -11011.86354035017
Iteration 9200: Loss = -11011.773228366468
Iteration 9300: Loss = -11002.817992311586
Iteration 9400: Loss = -11002.726348612008
Iteration 9500: Loss = -11002.711328663001
Iteration 9600: Loss = -11002.748347241542
1
Iteration 9700: Loss = -11002.705660083162
Iteration 9800: Loss = -11002.70243235612
Iteration 9900: Loss = -11002.66451254979
Iteration 10000: Loss = -11002.66375685626
Iteration 10100: Loss = -11002.691726239522
1
Iteration 10200: Loss = -11002.662248741475
Iteration 10300: Loss = -11002.662847053709
1
Iteration 10400: Loss = -11002.672999860506
2
Iteration 10500: Loss = -11002.662685594554
3
Iteration 10600: Loss = -11002.662079677655
Iteration 10700: Loss = -11002.664318032173
1
Iteration 10800: Loss = -11002.661976512136
Iteration 10900: Loss = -11002.662510664617
1
Iteration 11000: Loss = -11002.66450046277
2
Iteration 11100: Loss = -11002.661339356117
Iteration 11200: Loss = -11002.706355776694
1
Iteration 11300: Loss = -11002.666447608968
2
Iteration 11400: Loss = -11002.661320049232
Iteration 11500: Loss = -11002.660903540165
Iteration 11600: Loss = -11002.664525520546
1
Iteration 11700: Loss = -11002.793530821107
2
Iteration 11800: Loss = -11002.759225845231
3
Iteration 11900: Loss = -11002.661453001749
4
Iteration 12000: Loss = -11002.663339982786
5
Stopping early at iteration 12000 due to no improvement.
pi: tensor([[3.4846e-01, 6.5154e-01],
        [1.0000e+00, 4.0813e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0217, 0.9783], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1678, 0.2187],
         [0.5370, 0.1844]],

        [[0.6178, 0.0880],
         [0.7291, 0.5052]],

        [[0.5001, 0.0936],
         [0.5177, 0.5485]],

        [[0.7138, 0.1324],
         [0.6859, 0.5666]],

        [[0.6564, 0.1617],
         [0.6581, 0.5474]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.00028700959432072443
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.0037221143673841777
time is 2
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 88
Adjusted Rand Index: 0.5735833098393008
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 39
Adjusted Rand Index: 0.03993889419626938
time is 4
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.01609150833656456
Global Adjusted Rand Index: 0.014414207777435718
Average Adjusted Rand Index: 0.12512091768208597
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21478.3792931809
Iteration 100: Loss = -11015.403251659123
Iteration 200: Loss = -11014.157065256675
Iteration 300: Loss = -11013.780635097104
Iteration 400: Loss = -11013.631116729657
Iteration 500: Loss = -11013.55574578914
Iteration 600: Loss = -11013.51026766666
Iteration 700: Loss = -11013.477911902759
Iteration 800: Loss = -11013.45125261463
Iteration 900: Loss = -11013.426702235174
Iteration 1000: Loss = -11013.40254000488
Iteration 1100: Loss = -11013.377753436154
Iteration 1200: Loss = -11013.351185116955
Iteration 1300: Loss = -11013.321434411097
Iteration 1400: Loss = -11013.287068931308
Iteration 1500: Loss = -11013.246652163327
Iteration 1600: Loss = -11013.199110679136
Iteration 1700: Loss = -11013.143402338099
Iteration 1800: Loss = -11013.077388239262
Iteration 1900: Loss = -11012.996031157581
Iteration 2000: Loss = -11012.89008699631
Iteration 2100: Loss = -11012.74088869041
Iteration 2200: Loss = -11012.566902246088
Iteration 2300: Loss = -11012.470167735517
Iteration 2400: Loss = -11012.41808725885
Iteration 2500: Loss = -11012.38257505362
Iteration 2600: Loss = -11012.356031986008
Iteration 2700: Loss = -11012.335215313064
Iteration 2800: Loss = -11012.318246424897
Iteration 2900: Loss = -11012.30409006092
Iteration 3000: Loss = -11012.292143663877
Iteration 3100: Loss = -11012.282022418032
Iteration 3200: Loss = -11012.273427302222
Iteration 3300: Loss = -11012.266137667275
Iteration 3400: Loss = -11012.260025038371
Iteration 3500: Loss = -11012.25488748459
Iteration 3600: Loss = -11012.250470389812
Iteration 3700: Loss = -11012.246754979109
Iteration 3800: Loss = -11012.24359764303
Iteration 3900: Loss = -11012.240765678513
Iteration 4000: Loss = -11012.238314953673
Iteration 4100: Loss = -11012.236140138959
Iteration 4200: Loss = -11012.234154231412
Iteration 4300: Loss = -11012.23229707039
Iteration 4400: Loss = -11012.230549770442
Iteration 4500: Loss = -11012.22884403556
Iteration 4600: Loss = -11012.227209704717
Iteration 4700: Loss = -11012.225573427042
Iteration 4800: Loss = -11012.223814918061
Iteration 4900: Loss = -11012.2220198233
Iteration 5000: Loss = -11012.220083151884
Iteration 5100: Loss = -11012.21782709338
Iteration 5200: Loss = -11012.21524352884
Iteration 5300: Loss = -11012.212169380711
Iteration 5400: Loss = -11012.208522916533
Iteration 5500: Loss = -11012.20493212946
Iteration 5600: Loss = -11012.198731893463
Iteration 5700: Loss = -11012.192682257037
Iteration 5800: Loss = -11012.186269910275
Iteration 5900: Loss = -11012.180234369333
Iteration 6000: Loss = -11012.175089853625
Iteration 6100: Loss = -11012.171203062278
Iteration 6200: Loss = -11012.16784251584
Iteration 6300: Loss = -11012.164749272475
Iteration 6400: Loss = -11012.16162445867
Iteration 6500: Loss = -11012.157373983697
Iteration 6600: Loss = -11012.1495933707
Iteration 6700: Loss = -11012.151336660601
1
Iteration 6800: Loss = -11012.085732377447
Iteration 6900: Loss = -11012.01933563922
Iteration 7000: Loss = -11011.982983089852
Iteration 7100: Loss = -11011.977748460788
Iteration 7200: Loss = -11012.023264447267
1
Iteration 7300: Loss = -11011.975171479908
Iteration 7400: Loss = -11012.002216891904
1
Iteration 7500: Loss = -11012.00247897025
2
Iteration 7600: Loss = -11011.977150106753
3
Iteration 7700: Loss = -11011.973907285523
Iteration 7800: Loss = -11011.973684476989
Iteration 7900: Loss = -11011.974415988432
1
Iteration 8000: Loss = -11011.972874624431
Iteration 8100: Loss = -11011.973204758
1
Iteration 8200: Loss = -11011.996569387318
2
Iteration 8300: Loss = -11012.230390603056
3
Iteration 8400: Loss = -11011.976212282781
4
Iteration 8500: Loss = -11003.986153971764
Iteration 8600: Loss = -11002.753145602172
Iteration 8700: Loss = -11002.730838404868
Iteration 8800: Loss = -11002.714970194664
Iteration 8900: Loss = -11002.734912047606
1
Iteration 9000: Loss = -11002.708942661993
Iteration 9100: Loss = -11002.709354701676
1
Iteration 9200: Loss = -11002.667094288669
Iteration 9300: Loss = -11002.665891079216
Iteration 9400: Loss = -11002.681712226284
1
Iteration 9500: Loss = -11002.67577475183
2
Iteration 9600: Loss = -11002.669260976863
3
Iteration 9700: Loss = -11002.705358486146
4
Iteration 9800: Loss = -11002.692314269578
5
Stopping early at iteration 9800 due to no improvement.
pi: tensor([[9.4194e-06, 9.9999e-01],
        [6.5274e-01, 3.4726e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9781, 0.0219], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1834, 0.2187],
         [0.6375, 0.1680]],

        [[0.5945, 0.0880],
         [0.5965, 0.5658]],

        [[0.6755, 0.0937],
         [0.5556, 0.7230]],

        [[0.6729, 0.1329],
         [0.6961, 0.5778]],

        [[0.6933, 0.1624],
         [0.6481, 0.6863]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.00028700959432072443
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
time is 2
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 12
Adjusted Rand Index: 0.5735833098393008
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 61
Adjusted Rand Index: 0.03993889419626938
time is 4
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 41
Adjusted Rand Index: 0.023005218177999644
Global Adjusted Rand Index: 0.01545625708128396
Average Adjusted Rand Index: 0.12650365965037297
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22476.6393024725
Iteration 100: Loss = -11015.037329637948
Iteration 200: Loss = -11014.111240754819
Iteration 300: Loss = -11013.817207144912
Iteration 400: Loss = -11013.664905425177
Iteration 500: Loss = -11013.562265338851
Iteration 600: Loss = -11013.481390393457
Iteration 700: Loss = -11013.414738943573
Iteration 800: Loss = -11013.360991285543
Iteration 900: Loss = -11013.318546198148
Iteration 1000: Loss = -11013.283473930407
Iteration 1100: Loss = -11013.251605758733
Iteration 1200: Loss = -11013.219956050454
Iteration 1300: Loss = -11013.18675903518
Iteration 1400: Loss = -11013.150582896345
Iteration 1500: Loss = -11013.110160267644
Iteration 1600: Loss = -11013.064263345274
Iteration 1700: Loss = -11013.011891886246
Iteration 1800: Loss = -11012.952135102043
Iteration 1900: Loss = -11012.884267702846
Iteration 2000: Loss = -11012.806742401648
Iteration 2100: Loss = -11012.72954948887
Iteration 2200: Loss = -11012.660808828161
Iteration 2300: Loss = -11012.602577121137
Iteration 2400: Loss = -11012.554030607113
Iteration 2500: Loss = -11012.513261051266
Iteration 2600: Loss = -11012.478879777831
Iteration 2700: Loss = -11012.44935630077
Iteration 2800: Loss = -11012.423628634413
Iteration 2900: Loss = -11012.400869349081
Iteration 3000: Loss = -11012.380456759502
Iteration 3100: Loss = -11012.361969456912
Iteration 3200: Loss = -11012.345167410938
Iteration 3300: Loss = -11012.329772508225
Iteration 3400: Loss = -11012.31574633867
Iteration 3500: Loss = -11012.302999775715
Iteration 3600: Loss = -11012.291533102587
Iteration 3700: Loss = -11012.28130510812
Iteration 3800: Loss = -11012.272252603707
Iteration 3900: Loss = -11012.264306439221
Iteration 4000: Loss = -11012.257268249185
Iteration 4100: Loss = -11012.251130945702
Iteration 4200: Loss = -11012.245688294608
Iteration 4300: Loss = -11012.240828253845
Iteration 4400: Loss = -11012.236306996158
Iteration 4500: Loss = -11012.232251973632
Iteration 4600: Loss = -11012.228397664949
Iteration 4700: Loss = -11012.22474621401
Iteration 4800: Loss = -11012.22114986801
Iteration 4900: Loss = -11012.217574659648
Iteration 5000: Loss = -11012.213934543226
Iteration 5100: Loss = -11012.210328979729
Iteration 5200: Loss = -11012.206612948163
Iteration 5300: Loss = -11012.20284434134
Iteration 5400: Loss = -11012.198965070353
Iteration 5500: Loss = -11012.195092997952
Iteration 5600: Loss = -11012.191286066538
Iteration 5700: Loss = -11012.187614621747
Iteration 5800: Loss = -11012.184135871012
Iteration 5900: Loss = -11012.180966942065
Iteration 6000: Loss = -11012.177994113546
Iteration 6100: Loss = -11012.175513387921
Iteration 6200: Loss = -11012.173085111453
Iteration 6300: Loss = -11012.171060334116
Iteration 6400: Loss = -11012.168813688337
Iteration 6500: Loss = -11012.17501023385
1
Iteration 6600: Loss = -11012.164487265569
Iteration 6700: Loss = -11012.163228192027
Iteration 6800: Loss = -11012.158149358862
Iteration 6900: Loss = -11012.153472852568
Iteration 7000: Loss = -11012.14540002395
Iteration 7100: Loss = -11012.122212212897
Iteration 7200: Loss = -11012.078837089717
Iteration 7300: Loss = -11012.025515247524
Iteration 7400: Loss = -11011.99244502042
Iteration 7500: Loss = -11011.982402366933
Iteration 7600: Loss = -11011.979446491807
Iteration 7700: Loss = -11011.977892931161
Iteration 7800: Loss = -11011.976919833856
Iteration 7900: Loss = -11011.976151082354
Iteration 8000: Loss = -11011.975585184895
Iteration 8100: Loss = -11011.975255363903
Iteration 8200: Loss = -11011.974904727726
Iteration 8300: Loss = -11011.979483331346
1
Iteration 8400: Loss = -11011.989226952013
2
Iteration 8500: Loss = -11011.974402019769
Iteration 8600: Loss = -11011.975214832726
1
Iteration 8700: Loss = -11011.976606399383
2
Iteration 8800: Loss = -11011.97550976236
3
Iteration 8900: Loss = -11011.977631287606
4
Iteration 9000: Loss = -11011.974245010948
Iteration 9100: Loss = -11012.019370256601
1
Iteration 9200: Loss = -11011.973879329089
Iteration 9300: Loss = -11011.973878634446
Iteration 9400: Loss = -11011.977083236668
1
Iteration 9500: Loss = -11011.984672825522
2
Iteration 9600: Loss = -11011.973437917559
Iteration 9700: Loss = -11011.984755937008
1
Iteration 9800: Loss = -11011.990206206654
2
Iteration 9900: Loss = -11011.98364954255
3
Iteration 10000: Loss = -11011.952622061392
Iteration 10100: Loss = -11011.840900269373
Iteration 10200: Loss = -11011.702242043413
Iteration 10300: Loss = -11002.774083106053
Iteration 10400: Loss = -11002.740605337785
Iteration 10500: Loss = -11002.717295767494
Iteration 10600: Loss = -11002.723720726419
1
Iteration 10700: Loss = -11002.706478229164
Iteration 10800: Loss = -11002.704939191744
Iteration 10900: Loss = -11002.671006020417
Iteration 11000: Loss = -11002.665719649925
Iteration 11100: Loss = -11002.663430128048
Iteration 11200: Loss = -11002.675587773347
1
Iteration 11300: Loss = -11002.662764524814
Iteration 11400: Loss = -11002.66226921979
Iteration 11500: Loss = -11002.673024263193
1
Iteration 11600: Loss = -11002.665927026435
2
Iteration 11700: Loss = -11002.675391985891
3
Iteration 11800: Loss = -11002.664549895242
4
Iteration 11900: Loss = -11002.661417739357
Iteration 12000: Loss = -11002.66596696193
1
Iteration 12100: Loss = -11002.686311075353
2
Iteration 12200: Loss = -11002.682036037762
3
Iteration 12300: Loss = -11002.672393339473
4
Iteration 12400: Loss = -11002.665951074956
5
Stopping early at iteration 12400 due to no improvement.
pi: tensor([[3.4900e-01, 6.5100e-01],
        [9.9999e-01, 7.1339e-06]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0217, 0.9783], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1678, 0.2187],
         [0.5978, 0.1841]],

        [[0.7056, 0.0880],
         [0.7135, 0.5219]],

        [[0.5133, 0.0938],
         [0.5948, 0.6654]],

        [[0.5720, 0.1325],
         [0.6956, 0.5076]],

        [[0.6804, 0.1622],
         [0.5573, 0.5704]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: -0.00028700959432072443
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.0037221143673841777
time is 2
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 88
Adjusted Rand Index: 0.5735833098393008
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 39
Adjusted Rand Index: 0.03993889419626938
time is 4
tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,
        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 58
Adjusted Rand Index: 0.01609150833656456
Global Adjusted Rand Index: 0.014414207777435718
Average Adjusted Rand Index: 0.12512091768208597
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21839.827182672158
Iteration 100: Loss = -11014.997444989167
Iteration 200: Loss = -11014.100374908268
Iteration 300: Loss = -11013.763374410868
Iteration 400: Loss = -11013.621028494901
Iteration 500: Loss = -11013.54459595399
Iteration 600: Loss = -11013.49063052031
Iteration 700: Loss = -11013.445695143479
Iteration 800: Loss = -11013.404965258609
Iteration 900: Loss = -11013.366298832638
Iteration 1000: Loss = -11013.328360937485
Iteration 1100: Loss = -11013.28856619869
Iteration 1200: Loss = -11013.243184311099
Iteration 1300: Loss = -11013.187240966106
Iteration 1400: Loss = -11013.114898086003
Iteration 1500: Loss = -11013.02479901732
Iteration 1600: Loss = -11012.929456873944
Iteration 1700: Loss = -11012.842541864442
Iteration 1800: Loss = -11012.767236785789
Iteration 1900: Loss = -11012.70267246802
Iteration 2000: Loss = -11012.648072816006
Iteration 2100: Loss = -11012.602917046594
Iteration 2200: Loss = -11012.565608042174
Iteration 2300: Loss = -11012.535225891024
Iteration 2400: Loss = -11012.511032299482
Iteration 2500: Loss = -11012.491797594319
Iteration 2600: Loss = -11012.475768239125
Iteration 2700: Loss = -11012.461306958316
Iteration 2800: Loss = -11012.447273129284
Iteration 2900: Loss = -11012.432956373737
Iteration 3000: Loss = -11012.417981101687
Iteration 3100: Loss = -11012.402560013608
Iteration 3200: Loss = -11012.386884023304
Iteration 3300: Loss = -11012.371354822873
Iteration 3400: Loss = -11012.356410750173
Iteration 3500: Loss = -11012.342633158032
Iteration 3600: Loss = -11012.330149015695
Iteration 3700: Loss = -11012.319231553762
Iteration 3800: Loss = -11012.30981316145
Iteration 3900: Loss = -11012.301685178525
Iteration 4000: Loss = -11012.294769006274
Iteration 4100: Loss = -11012.288770677444
Iteration 4200: Loss = -11012.28341401992
Iteration 4300: Loss = -11012.278421043095
Iteration 4400: Loss = -11012.273230242145
Iteration 4500: Loss = -11012.267159761615
Iteration 4600: Loss = -11012.259314500418
Iteration 4700: Loss = -11012.249940994398
Iteration 4800: Loss = -11012.242346776298
Iteration 4900: Loss = -11012.23807340739
Iteration 5000: Loss = -11012.235952071473
Iteration 5100: Loss = -11012.233431290364
Iteration 5200: Loss = -11012.23180955108
Iteration 5300: Loss = -11012.231545341281
Iteration 5400: Loss = -11012.22899133942
Iteration 5500: Loss = -11012.227750035201
Iteration 5600: Loss = -11012.226644336535
Iteration 5700: Loss = -11012.225445334396
Iteration 5800: Loss = -11012.22425497509
Iteration 5900: Loss = -11012.223093459403
Iteration 6000: Loss = -11012.221771435197
Iteration 6100: Loss = -11012.220431205875
Iteration 6200: Loss = -11012.218831705948
Iteration 6300: Loss = -11012.217043147775
Iteration 6400: Loss = -11012.214957305478
Iteration 6500: Loss = -11012.212313297823
Iteration 6600: Loss = -11012.209050564277
Iteration 6700: Loss = -11012.205987204374
Iteration 6800: Loss = -11012.19988615336
Iteration 6900: Loss = -11012.193734334733
Iteration 7000: Loss = -11012.186899827539
Iteration 7100: Loss = -11012.198159683061
1
Iteration 7200: Loss = -11012.17460874701
Iteration 7300: Loss = -11012.181580149148
1
Iteration 7400: Loss = -11012.166638993382
Iteration 7500: Loss = -11012.16389626161
Iteration 7600: Loss = -11012.159995503114
Iteration 7700: Loss = -11012.154860486959
Iteration 7800: Loss = -11012.144490459843
Iteration 7900: Loss = -11012.11508282603
Iteration 8000: Loss = -11012.062409411166
Iteration 8100: Loss = -11011.993168283881
Iteration 8200: Loss = -11011.9776316392
Iteration 8300: Loss = -11011.985650751069
1
Iteration 8400: Loss = -11011.974942547411
Iteration 8500: Loss = -11011.97484568291
Iteration 8600: Loss = -11011.976189242792
1
Iteration 8700: Loss = -11011.9749346834
2
Iteration 8800: Loss = -11011.978453034364
3
Iteration 8900: Loss = -11011.978461546752
4
Iteration 9000: Loss = -11011.974020656013
Iteration 9100: Loss = -11011.980327231031
1
Iteration 9200: Loss = -11011.991378234701
2
Iteration 9300: Loss = -11011.974171955857
3
Iteration 9400: Loss = -11011.97252911261
Iteration 9500: Loss = -11011.991886022854
1
Iteration 9600: Loss = -11011.938449709178
Iteration 9700: Loss = -11011.811459342889
Iteration 9800: Loss = -11003.181857042406
Iteration 9900: Loss = -11002.749603501527
Iteration 10000: Loss = -11002.7173628799
Iteration 10100: Loss = -11002.711622411427
Iteration 10200: Loss = -11002.70873251845
Iteration 10300: Loss = -11002.707881903547
Iteration 10400: Loss = -11002.70455633844
Iteration 10500: Loss = -11002.664360898578
Iteration 10600: Loss = -11002.662778835647
Iteration 10700: Loss = -11002.662573972066
Iteration 10800: Loss = -11002.662879612477
1
Iteration 10900: Loss = -11002.662513396204
Iteration 11000: Loss = -11002.662208148322
Iteration 11100: Loss = -11002.662123418431
Iteration 11200: Loss = -11002.674160722732
1
Iteration 11300: Loss = -11002.675833812462
2
Iteration 11400: Loss = -11002.662505050655
3
Iteration 11500: Loss = -11002.694817818254
4
Iteration 11600: Loss = -11002.6764955453
5
Stopping early at iteration 11600 due to no improvement.
pi: tensor([[6.9813e-06, 9.9999e-01],
        [6.5032e-01, 3.4968e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9784, 0.0216], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1848, 0.2187],
         [0.6588, 0.1676]],

        [[0.6866, 0.0880],
         [0.5694, 0.5446]],

        [[0.5008, 0.0938],
         [0.6974, 0.6591]],

        [[0.5698, 0.1325],
         [0.6196, 0.5994]],

        [[0.7101, 0.1615],
         [0.6332, 0.5431]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: -0.00028700959432072443
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1])
Difference count: 49
Adjusted Rand Index: -0.0037221143673841777
time is 2
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 12
Adjusted Rand Index: 0.5735833098393008
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0])
tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 0, 1])
Difference count: 61
Adjusted Rand Index: 0.03993889419626938
time is 4
tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 1])
tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1])
Difference count: 42
Adjusted Rand Index: 0.01609150833656456
Global Adjusted Rand Index: 0.014414207777435718
Average Adjusted Rand Index: 0.12512091768208597
10918.961502337863
[0.014414207777435718, 0.01545625708128396, 0.014414207777435718, 0.014414207777435718] [0.12512091768208597, 0.12650365965037297, 0.12512091768208597, 0.12512091768208597] [11002.663339982786, 11002.692314269578, 11002.665951074956, 11002.6764955453]
-----------------------------------------------------------------------------------------
This iteration is 17
True Objective function: Loss = -10816.159260534829
Iteration 0: Loss = -11112.709962649846
Iteration 10: Loss = -10938.370641526237
Iteration 20: Loss = -10937.715862130071
Iteration 30: Loss = -10937.726314438722
1
Iteration 40: Loss = -10937.827453548116
2
Iteration 50: Loss = -10937.924869860579
3
Stopping early at iteration 50 due to no improvement.
pi: tensor([[0.2724, 0.7276],
        [0.1315, 0.8685]], dtype=torch.float64)
alpha: tensor([0.1602, 0.8398])
beta: tensor([[[0.1158, 0.1267],
         [0.6534, 0.1735]],

        [[0.2021, 0.1328],
         [0.7716, 0.3199]],

        [[0.6812, 0.1493],
         [0.9325, 0.5117]],

        [[0.8390, 0.1243],
         [0.9760, 0.7792]],

        [[0.0585, 0.1093],
         [0.7096, 0.2592]]], dtype=torch.float64)
time is 0
tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0070073061126442035
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 50
Adjusted Rand Index: -0.0007846151382684024
time is 3
tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 60
Adjusted Rand Index: 0.030303030303030304
time is 4
tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 66
Adjusted Rand Index: 0.07550561797752808
Global Adjusted Rand Index: 0.008829034450914782
Average Adjusted Rand Index: 0.0192955495130806
pi: tensor([[7.4526e-05, 9.9993e-01],
        [9.9993e-01, 6.8891e-05]], dtype=torch.float64)
alpha: tensor([0.4000, 0.6000])
beta: tensor([[[0.1627, 0.1818],
         [0.7551, 0.1581]],

        [[0.4004, 0.1670],
         [0.0304, 0.1057]],

        [[0.2301, 0.1859],
         [0.7099, 0.6837]],

        [[0.3259, 0.2177],
         [0.8558, 0.6078]],

        [[0.1624,    nan],
         [0.8255, 0.1465]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 60
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0019025721155792321
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -23703.55106302837
Iteration 100: Loss = -10945.943700387288
Iteration 200: Loss = -10944.500993675108
Iteration 300: Loss = -10943.992933344942
Iteration 400: Loss = -10943.123741055653
Iteration 500: Loss = -10938.799495935787
Iteration 600: Loss = -10883.00834406288
Iteration 700: Loss = -10809.648516667492
Iteration 800: Loss = -10808.1505177588
Iteration 900: Loss = -10808.04287037862
Iteration 1000: Loss = -10807.97318750323
Iteration 1100: Loss = -10807.939713448086
Iteration 1200: Loss = -10807.915500183386
Iteration 1300: Loss = -10807.893072974864
Iteration 1400: Loss = -10807.879134554436
Iteration 1500: Loss = -10807.86874333603
Iteration 1600: Loss = -10807.85827942048
Iteration 1700: Loss = -10807.850716229319
Iteration 1800: Loss = -10807.843241833212
Iteration 1900: Loss = -10807.840280593082
Iteration 2000: Loss = -10807.837808947661
Iteration 2100: Loss = -10807.835500128609
Iteration 2200: Loss = -10807.833202654285
Iteration 2300: Loss = -10807.829718557827
Iteration 2400: Loss = -10807.825719680577
Iteration 2500: Loss = -10807.824052625127
Iteration 2600: Loss = -10807.822435099693
Iteration 2700: Loss = -10807.82059666206
Iteration 2800: Loss = -10807.818006687374
Iteration 2900: Loss = -10807.814426934441
Iteration 3000: Loss = -10807.80910968147
Iteration 3100: Loss = -10807.791177185876
Iteration 3200: Loss = -10807.42666448783
Iteration 3300: Loss = -10807.086879120554
Iteration 3400: Loss = -10806.973267467361
Iteration 3500: Loss = -10806.678243446659
Iteration 3600: Loss = -10805.831315445246
Iteration 3700: Loss = -10805.377934135091
Iteration 3800: Loss = -10805.371648021382
Iteration 3900: Loss = -10805.321116734473
Iteration 4000: Loss = -10805.278808716052
Iteration 4100: Loss = -10805.277008974528
Iteration 4200: Loss = -10805.27635378781
Iteration 4300: Loss = -10805.271819750196
Iteration 4400: Loss = -10805.207156350749
Iteration 4500: Loss = -10805.206759650073
Iteration 4600: Loss = -10805.206463209875
Iteration 4700: Loss = -10805.20595917845
Iteration 4800: Loss = -10805.202504363724
Iteration 4900: Loss = -10805.201819363501
Iteration 5000: Loss = -10805.201706211796
Iteration 5100: Loss = -10805.20353929426
1
Iteration 5200: Loss = -10805.201301868672
Iteration 5300: Loss = -10805.20096280156
Iteration 5400: Loss = -10805.216658703892
1
Iteration 5500: Loss = -10805.177752234209
Iteration 5600: Loss = -10805.164727385198
Iteration 5700: Loss = -10805.164385360826
Iteration 5800: Loss = -10805.1641680989
Iteration 5900: Loss = -10805.163878922225
Iteration 6000: Loss = -10805.169142245513
1
Iteration 6100: Loss = -10805.057825898932
Iteration 6200: Loss = -10765.985206281492
Iteration 6300: Loss = -10765.848779703285
Iteration 6400: Loss = -10764.964557534488
Iteration 6500: Loss = -10764.835545300532
Iteration 6600: Loss = -10764.833357091533
Iteration 6700: Loss = -10764.82645630335
Iteration 6800: Loss = -10764.81062514157
Iteration 6900: Loss = -10764.549007542488
Iteration 7000: Loss = -10764.55339151534
1
Iteration 7100: Loss = -10764.546914343255
Iteration 7200: Loss = -10764.546137561054
Iteration 7300: Loss = -10764.48688248528
Iteration 7400: Loss = -10764.458238193918
Iteration 7500: Loss = -10764.463056634135
1
Iteration 7600: Loss = -10764.457990091858
Iteration 7700: Loss = -10764.45779627175
Iteration 7800: Loss = -10764.457543563705
Iteration 7900: Loss = -10764.461594215476
1
Iteration 8000: Loss = -10764.456819048519
Iteration 8100: Loss = -10764.456615810315
Iteration 8200: Loss = -10764.45708114146
1
Iteration 8300: Loss = -10764.455885417321
Iteration 8400: Loss = -10764.452799054014
Iteration 8500: Loss = -10764.452822649575
1
Iteration 8600: Loss = -10764.452737491465
Iteration 8700: Loss = -10764.468271591431
1
Iteration 8800: Loss = -10764.452728544033
Iteration 8900: Loss = -10764.452700408172
Iteration 9000: Loss = -10764.499026728556
1
Iteration 9100: Loss = -10764.452549751675
Iteration 9200: Loss = -10764.454580534852
1
Iteration 9300: Loss = -10764.452398101128
Iteration 9400: Loss = -10764.452325297218
Iteration 9500: Loss = -10764.452429765613
1
Iteration 9600: Loss = -10764.45424518548
2
Iteration 9700: Loss = -10764.45193443213
Iteration 9800: Loss = -10764.453231570751
1
Iteration 9900: Loss = -10764.451878085594
Iteration 10000: Loss = -10764.451884097474
1
Iteration 10100: Loss = -10764.462406986406
2
Iteration 10200: Loss = -10764.451834397125
Iteration 10300: Loss = -10764.45229530776
1
Iteration 10400: Loss = -10764.451778486957
Iteration 10500: Loss = -10764.452370176728
1
Iteration 10600: Loss = -10764.451654774412
Iteration 10700: Loss = -10764.45159387059
Iteration 10800: Loss = -10764.451716417057
1
Iteration 10900: Loss = -10764.451204408453
Iteration 11000: Loss = -10764.451840473548
1
Iteration 11100: Loss = -10764.451146595986
Iteration 11200: Loss = -10764.451129864425
Iteration 11300: Loss = -10764.452899170472
1
Iteration 11400: Loss = -10764.451149620143
2
Iteration 11500: Loss = -10764.451129054562
Iteration 11600: Loss = -10764.465692678958
1
Iteration 11700: Loss = -10764.451153147314
2
Iteration 11800: Loss = -10764.451152548136
3
Iteration 11900: Loss = -10764.451483227194
4
Iteration 12000: Loss = -10764.45115390448
5
Stopping early at iteration 12000 due to no improvement.
pi: tensor([[0.8175, 0.1825],
        [0.2251, 0.7749]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4369, 0.5631], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2615, 0.0942],
         [0.7168, 0.1889]],

        [[0.6721, 0.0973],
         [0.5871, 0.5479]],

        [[0.6325, 0.1025],
         [0.7144, 0.6977]],

        [[0.5513, 0.1036],
         [0.5734, 0.5942]],

        [[0.5671, 0.0934],
         [0.5459, 0.6254]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 6
Adjusted Rand Index: 0.7721248949511927
time is 1
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448543354594036
time is 2
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 3
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 7
Adjusted Rand Index: 0.7369696969696969
Global Adjusted Rand Index: 0.8314064937856682
Average Adjusted Rand Index: 0.831435257191895
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22686.086664956965
Iteration 100: Loss = -10943.752047688118
Iteration 200: Loss = -10941.129442970809
Iteration 300: Loss = -10940.091326453497
Iteration 400: Loss = -10939.321449376701
Iteration 500: Loss = -10938.39644431202
Iteration 600: Loss = -10918.055145181437
Iteration 700: Loss = -10830.297738460831
Iteration 800: Loss = -10812.176112366526
Iteration 900: Loss = -10807.622672857495
Iteration 1000: Loss = -10807.283023299316
Iteration 1100: Loss = -10806.251027971563
Iteration 1200: Loss = -10806.173188880874
Iteration 1300: Loss = -10806.13297053004
Iteration 1400: Loss = -10806.028825801466
Iteration 1500: Loss = -10806.009047885203
Iteration 1600: Loss = -10805.989250324523
Iteration 1700: Loss = -10805.979399896394
Iteration 1800: Loss = -10805.968389962363
Iteration 1900: Loss = -10805.961112638302
Iteration 2000: Loss = -10805.952499378865
Iteration 2100: Loss = -10805.93845035459
Iteration 2200: Loss = -10805.926120003793
Iteration 2300: Loss = -10805.923402488372
Iteration 2400: Loss = -10805.920859564338
Iteration 2500: Loss = -10805.910837064264
Iteration 2600: Loss = -10805.903086208646
Iteration 2700: Loss = -10805.894664908692
Iteration 2800: Loss = -10805.864776681283
Iteration 2900: Loss = -10805.850226668004
Iteration 3000: Loss = -10805.849126561843
Iteration 3100: Loss = -10805.848568358651
Iteration 3200: Loss = -10805.848140553648
Iteration 3300: Loss = -10805.848330453991
1
Iteration 3400: Loss = -10805.847350883927
Iteration 3500: Loss = -10805.847990078475
1
Iteration 3600: Loss = -10805.846421954075
Iteration 3700: Loss = -10805.844671960636
Iteration 3800: Loss = -10805.848920032025
1
Iteration 3900: Loss = -10805.841440284015
Iteration 4000: Loss = -10805.84066856754
Iteration 4100: Loss = -10805.46503106444
Iteration 4200: Loss = -10805.464053047413
Iteration 4300: Loss = -10805.463742683922
Iteration 4400: Loss = -10805.462904268263
Iteration 4500: Loss = -10805.46116630501
Iteration 4600: Loss = -10805.442681739016
Iteration 4700: Loss = -10805.443069468784
1
Iteration 4800: Loss = -10805.44231830024
Iteration 4900: Loss = -10805.44219319957
Iteration 5000: Loss = -10805.442228195292
1
Iteration 5100: Loss = -10805.441593694806
Iteration 5200: Loss = -10805.440660836737
Iteration 5300: Loss = -10805.440453797519
Iteration 5400: Loss = -10805.440372025665
Iteration 5500: Loss = -10805.440580483193
1
Iteration 5600: Loss = -10805.439996667383
Iteration 5700: Loss = -10805.436328797952
Iteration 5800: Loss = -10805.39489133435
Iteration 5900: Loss = -10805.392314156343
Iteration 6000: Loss = -10805.392248225256
Iteration 6100: Loss = -10805.392126925752
Iteration 6200: Loss = -10805.392010155216
Iteration 6300: Loss = -10805.442466616847
1
Iteration 6400: Loss = -10805.39166751165
Iteration 6500: Loss = -10805.391413816908
Iteration 6600: Loss = -10805.391317201067
Iteration 6700: Loss = -10805.393372129953
1
Iteration 6800: Loss = -10805.391093954808
Iteration 6900: Loss = -10805.391270858234
1
Iteration 7000: Loss = -10805.374375996038
Iteration 7100: Loss = -10805.383870191105
1
Iteration 7200: Loss = -10805.374251063833
Iteration 7300: Loss = -10805.374217393362
Iteration 7400: Loss = -10805.376264444421
1
Iteration 7500: Loss = -10805.374086550677
Iteration 7600: Loss = -10805.374855576094
1
Iteration 7700: Loss = -10805.374022491194
Iteration 7800: Loss = -10805.373957100233
Iteration 7900: Loss = -10805.374823017488
1
Iteration 8000: Loss = -10805.373992594148
2
Iteration 8100: Loss = -10805.37393175148
Iteration 8200: Loss = -10805.374172600621
1
Iteration 8300: Loss = -10805.373986993292
2
Iteration 8400: Loss = -10805.37395248032
3
Iteration 8500: Loss = -10805.374029766697
4
Iteration 8600: Loss = -10805.37389123589
Iteration 8700: Loss = -10805.368961585418
Iteration 8800: Loss = -10805.369020226875
1
Iteration 8900: Loss = -10805.370576590432
2
Iteration 9000: Loss = -10805.369189010466
3
Iteration 9100: Loss = -10805.369420112664
4
Iteration 9200: Loss = -10805.392902290036
5
Stopping early at iteration 9200 due to no improvement.
pi: tensor([[0.8329, 0.1671],
        [0.2909, 0.7091]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0741, 0.9259], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2656, 0.1049],
         [0.5589, 0.1747]],

        [[0.6187, 0.0998],
         [0.6478, 0.5762]],

        [[0.6777, 0.1029],
         [0.6652, 0.7153]],

        [[0.6457, 0.1040],
         [0.5759, 0.6469]],

        [[0.5845, 0.0937],
         [0.6076, 0.5138]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.0008263300397341679
time is 1
tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 9
Adjusted Rand Index: 0.6691997785118287
time is 2
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 3
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 7
Adjusted Rand Index: 0.7369696969696969
Global Adjusted Rand Index: 0.5349076424153784
Average Adjusted Rand Index: 0.6417141008041947
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22280.758863640775
Iteration 100: Loss = -10945.838276775658
Iteration 200: Loss = -10944.756532978872
Iteration 300: Loss = -10943.506647107992
Iteration 400: Loss = -10940.835043274074
Iteration 500: Loss = -10939.152636948478
Iteration 600: Loss = -10931.605636544286
Iteration 700: Loss = -10887.394494739021
Iteration 800: Loss = -10830.292493274155
Iteration 900: Loss = -10813.588875072084
Iteration 1000: Loss = -10812.451531935667
Iteration 1100: Loss = -10811.823814910027
Iteration 1200: Loss = -10811.657411603184
Iteration 1300: Loss = -10808.169270346054
Iteration 1400: Loss = -10808.04694138187
Iteration 1500: Loss = -10807.95833472877
Iteration 1600: Loss = -10807.790134920664
Iteration 1700: Loss = -10807.501320943753
Iteration 1800: Loss = -10807.351098621022
Iteration 1900: Loss = -10807.136881310975
Iteration 2000: Loss = -10806.93075798231
Iteration 2100: Loss = -10806.873422413479
Iteration 2200: Loss = -10806.791454201873
Iteration 2300: Loss = -10805.770977661048
Iteration 2400: Loss = -10805.723703569489
Iteration 2500: Loss = -10805.69385978739
Iteration 2600: Loss = -10805.696942735185
1
Iteration 2700: Loss = -10805.522491248657
Iteration 2800: Loss = -10805.477686993485
Iteration 2900: Loss = -10805.463923106929
Iteration 3000: Loss = -10805.44264620511
Iteration 3100: Loss = -10805.421461219785
Iteration 3200: Loss = -10805.40949926333
Iteration 3300: Loss = -10805.38112377221
Iteration 3400: Loss = -10805.311301110125
Iteration 3500: Loss = -10805.271821706581
Iteration 3600: Loss = -10805.234940892551
Iteration 3700: Loss = -10805.242503844105
1
Iteration 3800: Loss = -10805.229807144304
Iteration 3900: Loss = -10805.2281937607
Iteration 4000: Loss = -10805.226591553299
Iteration 4100: Loss = -10805.224917045907
Iteration 4200: Loss = -10805.22219131051
Iteration 4300: Loss = -10805.21790655059
Iteration 4400: Loss = -10805.218289665314
1
Iteration 4500: Loss = -10805.213246284404
Iteration 4600: Loss = -10805.212634081769
Iteration 4700: Loss = -10805.211881228537
Iteration 4800: Loss = -10805.211947569669
1
Iteration 4900: Loss = -10805.21034273086
Iteration 5000: Loss = -10805.20929668163
Iteration 5100: Loss = -10805.207983799928
Iteration 5200: Loss = -10805.201762891258
Iteration 5300: Loss = -10805.119885979508
Iteration 5400: Loss = -10778.852476909708
Iteration 5500: Loss = -10769.942441293742
Iteration 5600: Loss = -10769.869211691032
Iteration 5700: Loss = -10765.120516068913
Iteration 5800: Loss = -10764.723283986586
Iteration 5900: Loss = -10764.720147198257
Iteration 6000: Loss = -10764.718664291488
Iteration 6100: Loss = -10764.717733544316
Iteration 6200: Loss = -10764.717068763974
Iteration 6300: Loss = -10764.721684057564
1
Iteration 6400: Loss = -10764.715907204616
Iteration 6500: Loss = -10764.715234246243
Iteration 6600: Loss = -10764.71149844796
Iteration 6700: Loss = -10764.475337477275
Iteration 6800: Loss = -10764.458274425744
Iteration 6900: Loss = -10764.461240406461
1
Iteration 7000: Loss = -10764.459194875564
2
Iteration 7100: Loss = -10764.45770338165
Iteration 7200: Loss = -10764.458991374358
1
Iteration 7300: Loss = -10764.460445146027
2
Iteration 7400: Loss = -10764.464625190085
3
Iteration 7500: Loss = -10764.458329572573
4
Iteration 7600: Loss = -10764.456961011985
Iteration 7700: Loss = -10764.619259172707
1
Iteration 7800: Loss = -10764.45680419457
Iteration 7900: Loss = -10764.456748318784
Iteration 8000: Loss = -10764.456776484158
1
Iteration 8100: Loss = -10764.456680339303
Iteration 8200: Loss = -10764.463049627553
1
Iteration 8300: Loss = -10764.456585192027
Iteration 8400: Loss = -10764.456576473856
Iteration 8500: Loss = -10764.465868525895
1
Iteration 8600: Loss = -10764.456485597713
Iteration 8700: Loss = -10764.456453223607
Iteration 8800: Loss = -10764.46384734808
1
Iteration 8900: Loss = -10764.456345237266
Iteration 9000: Loss = -10764.472672106049
1
Iteration 9100: Loss = -10764.520777632279
2
Iteration 9200: Loss = -10764.455863659965
Iteration 9300: Loss = -10764.45502935176
Iteration 9400: Loss = -10764.456549031973
1
Iteration 9500: Loss = -10764.454748926426
Iteration 9600: Loss = -10764.456226947348
1
Iteration 9700: Loss = -10764.454711860404
Iteration 9800: Loss = -10764.459551380585
1
Iteration 9900: Loss = -10764.454679506758
Iteration 10000: Loss = -10764.456941614315
1
Iteration 10100: Loss = -10764.468433321876
2
Iteration 10200: Loss = -10764.54413912828
3
Iteration 10300: Loss = -10764.49818177978
4
Iteration 10400: Loss = -10764.454534369588
Iteration 10500: Loss = -10764.454213972458
Iteration 10600: Loss = -10764.460574492232
1
Iteration 10700: Loss = -10764.451564759052
Iteration 10800: Loss = -10764.45302599216
1
Iteration 10900: Loss = -10764.451385012228
Iteration 11000: Loss = -10764.468132447091
1
Iteration 11100: Loss = -10764.451302195293
Iteration 11200: Loss = -10764.451299136552
Iteration 11300: Loss = -10764.452026047442
1
Iteration 11400: Loss = -10764.451262345172
Iteration 11500: Loss = -10764.521908897888
1
Iteration 11600: Loss = -10764.451280538762
2
Iteration 11700: Loss = -10764.472973307556
3
Iteration 11800: Loss = -10764.451272660757
4
Iteration 11900: Loss = -10764.451230177321
Iteration 12000: Loss = -10764.451297378298
1
Iteration 12100: Loss = -10764.451247598621
2
Iteration 12200: Loss = -10764.453661986165
3
Iteration 12300: Loss = -10764.45124632773
4
Iteration 12400: Loss = -10764.497316371951
5
Stopping early at iteration 12400 due to no improvement.
pi: tensor([[0.8171, 0.1829],
        [0.2230, 0.7770]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4379, 0.5621], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2622, 0.0948],
         [0.6390, 0.1883]],

        [[0.5444, 0.0978],
         [0.6777, 0.6088]],

        [[0.6682, 0.1029],
         [0.5530, 0.5921]],

        [[0.5198, 0.1041],
         [0.7291, 0.6242]],

        [[0.6095, 0.0938],
         [0.6919, 0.5611]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 6
Adjusted Rand Index: 0.7721248949511927
time is 1
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448543354594036
time is 2
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 3
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 7
Adjusted Rand Index: 0.7369696969696969
Global Adjusted Rand Index: 0.8314064937856682
Average Adjusted Rand Index: 0.831435257191895
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -19845.96612436666
Iteration 100: Loss = -10945.05194202664
Iteration 200: Loss = -10943.640577155616
Iteration 300: Loss = -10943.132028960292
Iteration 400: Loss = -10941.857554159917
Iteration 500: Loss = -10939.831161311457
Iteration 600: Loss = -10925.62495044979
Iteration 700: Loss = -10838.425436564368
Iteration 800: Loss = -10817.823946056413
Iteration 900: Loss = -10811.843565173096
Iteration 1000: Loss = -10808.199003339338
Iteration 1100: Loss = -10808.090284641778
Iteration 1200: Loss = -10808.007791241971
Iteration 1300: Loss = -10807.94866024358
Iteration 1400: Loss = -10807.913693811466
Iteration 1500: Loss = -10807.881625624332
Iteration 1600: Loss = -10807.857930473123
Iteration 1700: Loss = -10807.807156575973
Iteration 1800: Loss = -10807.530475276018
Iteration 1900: Loss = -10807.41500356459
Iteration 2000: Loss = -10807.395217379275
Iteration 2100: Loss = -10807.387120976346
Iteration 2200: Loss = -10807.381553953377
Iteration 2300: Loss = -10807.375015063764
Iteration 2400: Loss = -10807.369971228693
Iteration 2500: Loss = -10807.367456032722
Iteration 2600: Loss = -10807.364048550697
Iteration 2700: Loss = -10807.35654499796
Iteration 2800: Loss = -10807.346436899012
Iteration 2900: Loss = -10807.33968310621
Iteration 3000: Loss = -10807.336478112713
Iteration 3100: Loss = -10807.334249875723
Iteration 3200: Loss = -10807.312035966792
Iteration 3300: Loss = -10807.192381941857
Iteration 3400: Loss = -10807.055640737857
Iteration 3500: Loss = -10806.026422334442
Iteration 3600: Loss = -10806.008138521069
Iteration 3700: Loss = -10806.001665714703
Iteration 3800: Loss = -10806.004907971319
1
Iteration 3900: Loss = -10805.983691794625
Iteration 4000: Loss = -10805.703857025634
Iteration 4100: Loss = -10805.687326566771
Iteration 4200: Loss = -10805.674357319065
Iteration 4300: Loss = -10805.670815572601
Iteration 4400: Loss = -10805.669766263558
Iteration 4500: Loss = -10805.668243561428
Iteration 4600: Loss = -10805.66232541745
Iteration 4700: Loss = -10805.631798064336
Iteration 4800: Loss = -10805.631543495456
Iteration 4900: Loss = -10805.630319755686
Iteration 5000: Loss = -10805.628997054913
Iteration 5100: Loss = -10805.627012847839
Iteration 5200: Loss = -10805.624017622607
Iteration 5300: Loss = -10805.623460649173
Iteration 5400: Loss = -10805.620939049311
Iteration 5500: Loss = -10805.617758431461
Iteration 5600: Loss = -10805.616620532204
Iteration 5700: Loss = -10805.595314333703
Iteration 5800: Loss = -10805.590203542275
Iteration 5900: Loss = -10805.580126876235
Iteration 6000: Loss = -10805.606773550748
1
Iteration 6100: Loss = -10805.574359903105
Iteration 6200: Loss = -10805.373800734027
Iteration 6300: Loss = -10805.373676991645
Iteration 6400: Loss = -10805.348472647825
Iteration 6500: Loss = -10805.348306369113
Iteration 6600: Loss = -10805.349022320719
1
Iteration 6700: Loss = -10805.347998394045
Iteration 6800: Loss = -10805.349257432057
1
Iteration 6900: Loss = -10805.347517245506
Iteration 7000: Loss = -10805.346861950075
Iteration 7100: Loss = -10805.284276363454
Iteration 7200: Loss = -10805.276562483426
Iteration 7300: Loss = -10805.264192084598
Iteration 7400: Loss = -10805.355256382756
1
Iteration 7500: Loss = -10805.261210454899
Iteration 7600: Loss = -10805.248304163793
Iteration 7700: Loss = -10805.243111441934
Iteration 7800: Loss = -10805.227062949152
Iteration 7900: Loss = -10805.226273966628
Iteration 8000: Loss = -10805.230775022572
1
Iteration 8100: Loss = -10805.227492209206
2
Iteration 8200: Loss = -10805.240604007447
3
Iteration 8300: Loss = -10805.2262628848
Iteration 8400: Loss = -10805.226070933666
Iteration 8500: Loss = -10805.226106783362
1
Iteration 8600: Loss = -10805.225551641677
Iteration 8700: Loss = -10805.191004766788
Iteration 8800: Loss = -10805.191519588714
1
Iteration 8900: Loss = -10805.19991042485
2
Iteration 9000: Loss = -10775.165149503351
Iteration 9100: Loss = -10770.324001109258
Iteration 9200: Loss = -10769.626644458538
Iteration 9300: Loss = -10769.367401121974
Iteration 9400: Loss = -10764.886529109317
Iteration 9500: Loss = -10764.808859794884
Iteration 9600: Loss = -10764.80810740342
Iteration 9700: Loss = -10764.808278449167
1
Iteration 9800: Loss = -10764.814149949842
2
Iteration 9900: Loss = -10764.807620489413
Iteration 10000: Loss = -10764.81029641819
1
Iteration 10100: Loss = -10764.806702100437
Iteration 10200: Loss = -10764.799263585348
Iteration 10300: Loss = -10765.01156782856
1
Iteration 10400: Loss = -10764.799208396758
Iteration 10500: Loss = -10764.80582816003
1
Iteration 10600: Loss = -10764.799138040862
Iteration 10700: Loss = -10764.802251839568
1
Iteration 10800: Loss = -10764.7991213103
Iteration 10900: Loss = -10764.980677507332
1
Iteration 11000: Loss = -10764.588098983932
Iteration 11100: Loss = -10764.587395394827
Iteration 11200: Loss = -10764.589649222396
1
Iteration 11300: Loss = -10764.587309644687
Iteration 11400: Loss = -10764.586646258873
Iteration 11500: Loss = -10764.58661625037
Iteration 11600: Loss = -10764.586547125498
Iteration 11700: Loss = -10764.597619135373
1
Iteration 11800: Loss = -10764.585971988341
Iteration 11900: Loss = -10764.47015495396
Iteration 12000: Loss = -10764.48021363551
1
Iteration 12100: Loss = -10764.529152178937
2
Iteration 12200: Loss = -10764.457622010199
Iteration 12300: Loss = -10764.457812795052
1
Iteration 12400: Loss = -10764.459937482054
2
Iteration 12500: Loss = -10764.456527684315
Iteration 12600: Loss = -10764.476975862177
1
Iteration 12700: Loss = -10764.456523077568
Iteration 12800: Loss = -10764.497033912314
1
Iteration 12900: Loss = -10764.456497223926
Iteration 13000: Loss = -10764.456457451039
Iteration 13100: Loss = -10764.456490082246
1
Iteration 13200: Loss = -10764.456365712022
Iteration 13300: Loss = -10764.613727707952
1
Iteration 13400: Loss = -10764.456035698766
Iteration 13500: Loss = -10764.455989406535
Iteration 13600: Loss = -10764.48493292145
1
Iteration 13700: Loss = -10764.455952654982
Iteration 13800: Loss = -10764.480933766949
1
Iteration 13900: Loss = -10764.455829421962
Iteration 14000: Loss = -10764.501594529582
1
Iteration 14100: Loss = -10764.455794677473
Iteration 14200: Loss = -10764.463743912169
1
Iteration 14300: Loss = -10764.45549064895
Iteration 14400: Loss = -10764.45748212723
1
Iteration 14500: Loss = -10764.455296619773
Iteration 14600: Loss = -10764.455938183086
1
Iteration 14700: Loss = -10764.473470495712
2
Iteration 14800: Loss = -10764.454853218336
Iteration 14900: Loss = -10764.454834695265
Iteration 15000: Loss = -10764.454746099022
Iteration 15100: Loss = -10764.453703303669
Iteration 15200: Loss = -10764.45179676254
Iteration 15300: Loss = -10764.468217247546
1
Iteration 15400: Loss = -10764.45146469855
Iteration 15500: Loss = -10764.452565168525
1
Iteration 15600: Loss = -10764.451273858165
Iteration 15700: Loss = -10764.457467745593
1
Iteration 15800: Loss = -10764.451080885583
Iteration 15900: Loss = -10764.454137141229
1
Iteration 16000: Loss = -10764.452471718327
2
Iteration 16100: Loss = -10764.521216155399
3
Iteration 16200: Loss = -10764.523762448434
4
Iteration 16300: Loss = -10764.451463784279
5
Stopping early at iteration 16300 due to no improvement.
pi: tensor([[0.8174, 0.1826],
        [0.2254, 0.7746]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.4372, 0.5628], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2613, 0.0941],
         [0.5736, 0.1890]],

        [[0.5724, 0.0973],
         [0.5426, 0.7168]],

        [[0.5712, 0.1025],
         [0.6242, 0.6733]],

        [[0.5390, 0.1035],
         [0.7153, 0.7267]],

        [[0.5314, 0.0933],
         [0.5507, 0.5720]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 6
Adjusted Rand Index: 0.7721248949511927
time is 1
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 0])
tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 0, 0])
Difference count: 4
Adjusted Rand Index: 0.8448543354594036
time is 2
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 3
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 3
Adjusted Rand Index: 0.8824242424242424
time is 4
tensor([1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
        1, 1, 0, 0])
Difference count: 7
Adjusted Rand Index: 0.7369696969696969
Global Adjusted Rand Index: 0.8314064937856682
Average Adjusted Rand Index: 0.831435257191895
10816.159260534829
[0.8314064937856682, 0.5349076424153784, 0.8314064937856682, 0.8314064937856682] [0.831435257191895, 0.6417141008041947, 0.831435257191895, 0.831435257191895] [10764.45115390448, 10805.392902290036, 10764.497316371951, 10764.451463784279]
-----------------------------------------------------------------------------------------
This iteration is 18
True Objective function: Loss = -10919.47047611485
Iteration 0: Loss = -11231.069921780743
Iteration 10: Loss = -11010.78268303511
Iteration 20: Loss = -11006.880334117235
Iteration 30: Loss = -10950.844984550553
Iteration 40: Loss = -10950.1187092012
Iteration 50: Loss = -10949.44414501302
Iteration 60: Loss = -10934.732768790236
Iteration 70: Loss = -10921.147045251724
Iteration 80: Loss = -10919.866296460359
Iteration 90: Loss = -10899.984434266644
Iteration 100: Loss = -10890.469308311776
Iteration 110: Loss = -10890.694681947733
1
Iteration 120: Loss = -10890.740691320123
2
Iteration 130: Loss = -10890.746776295931
3
Stopping early at iteration 130 due to no improvement.
pi: tensor([[0.7124, 0.2876],
        [0.3327, 0.6673]], dtype=torch.float64)
alpha: tensor([0.4941, 0.5059])
beta: tensor([[[0.2538, 0.1076],
         [0.9056, 0.1879]],

        [[0.2980, 0.0976],
         [0.2363, 0.7519]],

        [[0.8478, 0.1041],
         [0.4408, 0.4753]],

        [[0.4404, 0.0999],
         [0.2521, 0.0676]],

        [[0.9596, 0.0981],
         [0.8412, 0.9515]]], dtype=torch.float64)
time is 0
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 7
Adjusted Rand Index: 0.7369913366172994
time is 1
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 15
Adjusted Rand Index: 0.4849271883524868
time is 2
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 10
Adjusted Rand Index: 0.6363695132199884
time is 3
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 4
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
Global Adjusted Rand Index: 0.7322003732689008
Average Adjusted Rand Index: 0.7399783575879292
Iteration 0: Loss = -11120.18365961048
Iteration 10: Loss = -11014.201101534281
Iteration 20: Loss = -11013.557788596212
Iteration 30: Loss = -11013.355109623419
Iteration 40: Loss = -11013.34228660627
Iteration 50: Loss = -11013.356393585393
1
Iteration 60: Loss = -11012.437394374161
Iteration 70: Loss = -11011.415106048531
Iteration 80: Loss = -11011.0491767042
Iteration 90: Loss = -11010.738875153556
Iteration 100: Loss = -11010.407876192096
Iteration 110: Loss = -11009.858331497231
Iteration 120: Loss = -11000.003689141873
Iteration 130: Loss = -10950.89069232331
Iteration 140: Loss = -10950.46132671498
Iteration 150: Loss = -10950.079480855373
Iteration 160: Loss = -10949.344709638302
Iteration 170: Loss = -10929.442390048962
Iteration 180: Loss = -10921.107238753968
Iteration 190: Loss = -10919.590551282268
Iteration 200: Loss = -10897.134114491551
Iteration 210: Loss = -10890.493984128072
Iteration 220: Loss = -10890.702066936688
1
Iteration 230: Loss = -10890.741695640787
2
Iteration 240: Loss = -10890.746890803934
3
Stopping early at iteration 240 due to no improvement.
pi: tensor([[0.7124, 0.2876],
        [0.3327, 0.6673]], dtype=torch.float64)
alpha: tensor([0.4941, 0.5059])
beta: tensor([[[0.2538, 0.1076],
         [0.3354, 0.1879]],

        [[0.3105, 0.0976],
         [0.1668, 0.8868]],

        [[0.0732, 0.1041],
         [0.1713, 0.9103]],

        [[0.4379, 0.0999],
         [0.9583, 0.4071]],

        [[0.1919, 0.0981],
         [0.5787, 0.1836]]], dtype=torch.float64)
time is 0
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 7
Adjusted Rand Index: 0.7369913366172994
time is 1
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 15
Adjusted Rand Index: 0.4849271883524868
time is 2
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 10
Adjusted Rand Index: 0.6363695132199884
time is 3
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208031161549398
time is 4
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
Global Adjusted Rand Index: 0.7322003732689008
Average Adjusted Rand Index: 0.7399783575879292
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24843.25803741915
Iteration 100: Loss = -11018.028311694461
Iteration 200: Loss = -11016.270060601795
Iteration 300: Loss = -11015.705520709264
Iteration 400: Loss = -11015.303542370553
Iteration 500: Loss = -11014.718481925192
Iteration 600: Loss = -11014.10926157709
Iteration 700: Loss = -11013.67903087372
Iteration 800: Loss = -11013.258842771273
Iteration 900: Loss = -11012.623991688406
Iteration 1000: Loss = -11010.107012653358
Iteration 1100: Loss = -10932.874623345673
Iteration 1200: Loss = -10902.350434413898
Iteration 1300: Loss = -10899.838228434619
Iteration 1400: Loss = -10899.571104621305
Iteration 1500: Loss = -10899.480029991644
Iteration 1600: Loss = -10899.403198566864
Iteration 1700: Loss = -10899.296589814272
Iteration 1800: Loss = -10899.257911334904
Iteration 1900: Loss = -10899.20953433975
Iteration 2000: Loss = -10899.141108321392
Iteration 2100: Loss = -10899.128777686987
Iteration 2200: Loss = -10899.118009596688
Iteration 2300: Loss = -10899.108118524204
Iteration 2400: Loss = -10899.101920727
Iteration 2500: Loss = -10899.096790920757
Iteration 2600: Loss = -10899.092538983117
Iteration 2700: Loss = -10899.086422776343
Iteration 2800: Loss = -10899.02931341473
Iteration 2900: Loss = -10899.020125695137
Iteration 3000: Loss = -10899.015564740055
Iteration 3100: Loss = -10899.013178054995
Iteration 3200: Loss = -10899.011190771982
Iteration 3300: Loss = -10899.015531052459
1
Iteration 3400: Loss = -10899.00820876408
Iteration 3500: Loss = -10899.00643374002
Iteration 3600: Loss = -10899.005086535126
Iteration 3700: Loss = -10899.004011127405
Iteration 3800: Loss = -10899.006085262992
1
Iteration 3900: Loss = -10898.999215461305
Iteration 4000: Loss = -10898.998399567789
Iteration 4100: Loss = -10898.997327104
Iteration 4200: Loss = -10898.99665230227
Iteration 4300: Loss = -10898.995864174647
Iteration 4400: Loss = -10898.995302020045
Iteration 4500: Loss = -10898.994496389752
Iteration 4600: Loss = -10898.995430387246
1
Iteration 4700: Loss = -10898.99153601405
Iteration 4800: Loss = -10898.989290365433
Iteration 4900: Loss = -10898.988669131648
Iteration 5000: Loss = -10898.988229461997
Iteration 5100: Loss = -10898.987892278023
Iteration 5200: Loss = -10898.987477650504
Iteration 5300: Loss = -10898.98707368256
Iteration 5400: Loss = -10898.996047963195
1
Iteration 5500: Loss = -10898.986369249827
Iteration 5600: Loss = -10898.985862321491
Iteration 5700: Loss = -10898.990753828899
1
Iteration 5800: Loss = -10898.973253887903
Iteration 5900: Loss = -10898.972742764854
Iteration 6000: Loss = -10898.972733264303
Iteration 6100: Loss = -10898.97217151837
Iteration 6200: Loss = -10898.979337344426
1
Iteration 6300: Loss = -10898.97168339994
Iteration 6400: Loss = -10898.971465731394
Iteration 6500: Loss = -10898.971405107535
Iteration 6600: Loss = -10898.970994787436
Iteration 6700: Loss = -10898.972430937105
1
Iteration 6800: Loss = -10898.970286758105
Iteration 6900: Loss = -10898.969454472008
Iteration 7000: Loss = -10898.96862797832
Iteration 7100: Loss = -10898.967104669111
Iteration 7200: Loss = -10898.961226124571
Iteration 7300: Loss = -10896.909753546659
Iteration 7400: Loss = -10896.788881699595
Iteration 7500: Loss = -10896.806337554202
1
Iteration 7600: Loss = -10896.783708107017
Iteration 7700: Loss = -10896.783158740353
Iteration 7800: Loss = -10896.783026831068
Iteration 7900: Loss = -10896.782172084233
Iteration 8000: Loss = -10896.773105724755
Iteration 8100: Loss = -10896.769933778833
Iteration 8200: Loss = -10895.505735476656
Iteration 8300: Loss = -10893.767575563617
Iteration 8400: Loss = -10893.741019806888
Iteration 8500: Loss = -10893.811390499202
1
Iteration 8600: Loss = -10893.729203276715
Iteration 8700: Loss = -10893.72618014693
Iteration 8800: Loss = -10893.758240647614
1
Iteration 8900: Loss = -10893.72141415565
Iteration 9000: Loss = -10893.71980049887
Iteration 9100: Loss = -10893.728334008474
1
Iteration 9200: Loss = -10893.717331827816
Iteration 9300: Loss = -10893.716984691011
Iteration 9400: Loss = -10893.717226458035
1
Iteration 9500: Loss = -10893.716538341587
Iteration 9600: Loss = -10893.715784557366
Iteration 9700: Loss = -10893.725189934747
1
Iteration 9800: Loss = -10893.715423025546
Iteration 9900: Loss = -10893.71213836966
Iteration 10000: Loss = -10893.711754925787
Iteration 10100: Loss = -10893.709683589628
Iteration 10200: Loss = -10893.737910335267
1
Iteration 10300: Loss = -10893.70965611608
Iteration 10400: Loss = -10893.7096364964
Iteration 10500: Loss = -10893.936331365838
1
Iteration 10600: Loss = -10893.709619167286
Iteration 10700: Loss = -10893.709567450434
Iteration 10800: Loss = -10893.721327790057
1
Iteration 10900: Loss = -10893.70960829426
2
Iteration 11000: Loss = -10893.709567981476
3
Iteration 11100: Loss = -10893.711530084962
4
Iteration 11200: Loss = -10893.709552317432
Iteration 11300: Loss = -10893.709457611341
Iteration 11400: Loss = -10893.72944811632
1
Iteration 11500: Loss = -10893.705717857896
Iteration 11600: Loss = -10893.705672431002
Iteration 11700: Loss = -10893.734731706463
1
Iteration 11800: Loss = -10893.705685469573
2
Iteration 11900: Loss = -10893.705672067112
Iteration 12000: Loss = -10893.706805603837
1
Iteration 12100: Loss = -10893.705651336932
Iteration 12200: Loss = -10893.705672112585
1
Iteration 12300: Loss = -10893.707103444996
2
Iteration 12400: Loss = -10893.70567021771
3
Iteration 12500: Loss = -10893.705652993925
4
Iteration 12600: Loss = -10893.705962081767
5
Stopping early at iteration 12600 due to no improvement.
pi: tensor([[0.7803, 0.2197],
        [0.3490, 0.6510]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0482, 0.9518], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2595, 0.0865],
         [0.7125, 0.1803]],

        [[0.6542, 0.0976],
         [0.5127, 0.6939]],

        [[0.6104, 0.1041],
         [0.6615, 0.6364]],

        [[0.5487, 0.0990],
         [0.6030, 0.5383]],

        [[0.5601, 0.0979],
         [0.5757, 0.6050]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.0032454170932134756
time is 1
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 11
Adjusted Rand Index: 0.6045048763898703
time is 2
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 8
Adjusted Rand Index: 0.7026203884610964
time is 3
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
Global Adjusted Rand Index: 0.5232231679026662
Average Adjusted Rand Index: 0.6369356574341
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22862.826837260418
Iteration 100: Loss = -11016.780582848409
Iteration 200: Loss = -11015.689261541771
Iteration 300: Loss = -11015.046866110877
Iteration 400: Loss = -11014.134201782328
Iteration 500: Loss = -11013.366370247566
Iteration 600: Loss = -11012.932691960466
Iteration 700: Loss = -11012.41384997181
Iteration 800: Loss = -10991.928262607933
Iteration 900: Loss = -10902.487264002773
Iteration 1000: Loss = -10899.647607630108
Iteration 1100: Loss = -10899.44524421012
Iteration 1200: Loss = -10899.353436780242
Iteration 1300: Loss = -10899.300030466615
Iteration 1400: Loss = -10899.256993611014
Iteration 1500: Loss = -10899.174417498587
Iteration 1600: Loss = -10899.150320089124
Iteration 1700: Loss = -10899.130464547789
Iteration 1800: Loss = -10899.053862066654
Iteration 1900: Loss = -10899.02208743244
Iteration 2000: Loss = -10899.013843852224
Iteration 2100: Loss = -10899.00786802345
Iteration 2200: Loss = -10899.00452685766
Iteration 2300: Loss = -10898.998815176881
Iteration 2400: Loss = -10898.9949722108
Iteration 2500: Loss = -10899.026776194158
1
Iteration 2600: Loss = -10898.987528626396
Iteration 2700: Loss = -10898.984714812888
Iteration 2800: Loss = -10898.98241874361
Iteration 2900: Loss = -10898.980400058335
Iteration 3000: Loss = -10898.978402560457
Iteration 3100: Loss = -10898.976635598618
Iteration 3200: Loss = -10898.974942987874
Iteration 3300: Loss = -10898.973280119933
Iteration 3400: Loss = -10898.971478955222
Iteration 3500: Loss = -10898.970876846492
Iteration 3600: Loss = -10898.9674176327
Iteration 3700: Loss = -10898.96311643193
Iteration 3800: Loss = -10898.929668284258
Iteration 3900: Loss = -10896.845373445829
Iteration 4000: Loss = -10896.799205839574
Iteration 4100: Loss = -10896.788670965965
Iteration 4200: Loss = -10896.804394569173
1
Iteration 4300: Loss = -10896.780013976666
Iteration 4400: Loss = -10896.69715171752
Iteration 4500: Loss = -10894.236074229966
Iteration 4600: Loss = -10893.786860073302
Iteration 4700: Loss = -10893.763015790624
Iteration 4800: Loss = -10893.740558327645
Iteration 4900: Loss = -10893.734849385552
Iteration 5000: Loss = -10893.730628436912
Iteration 5100: Loss = -10893.726731224986
Iteration 5200: Loss = -10893.754861944806
1
Iteration 5300: Loss = -10893.722327659016
Iteration 5400: Loss = -10893.720541635385
Iteration 5500: Loss = -10893.718290405825
Iteration 5600: Loss = -10893.717578681653
Iteration 5700: Loss = -10893.717118529486
Iteration 5800: Loss = -10893.71657918385
Iteration 5900: Loss = -10893.716031948796
Iteration 6000: Loss = -10893.715176946807
Iteration 6100: Loss = -10893.713051984587
Iteration 6200: Loss = -10893.712174833892
Iteration 6300: Loss = -10893.711757310653
Iteration 6400: Loss = -10893.711500701313
Iteration 6500: Loss = -10893.725491813355
1
Iteration 6600: Loss = -10893.71105952048
Iteration 6700: Loss = -10893.71052401732
Iteration 6800: Loss = -10893.707900372889
Iteration 6900: Loss = -10893.70764442005
Iteration 7000: Loss = -10893.723643743959
1
Iteration 7100: Loss = -10893.707107570443
Iteration 7200: Loss = -10893.707139319888
1
Iteration 7300: Loss = -10893.706556522182
Iteration 7400: Loss = -10893.711105461647
1
Iteration 7500: Loss = -10893.70646926976
Iteration 7600: Loss = -10893.70645671284
Iteration 7700: Loss = -10893.73074831496
1
Iteration 7800: Loss = -10893.706445578711
Iteration 7900: Loss = -10893.708649687049
1
Iteration 8000: Loss = -10893.7063433052
Iteration 8100: Loss = -10893.706470368057
1
Iteration 8200: Loss = -10893.706324615121
Iteration 8300: Loss = -10893.706410321029
1
Iteration 8400: Loss = -10893.710235653503
2
Iteration 8500: Loss = -10893.706326773447
3
Iteration 8600: Loss = -10893.706260180084
Iteration 8700: Loss = -10893.706902971408
1
Iteration 8800: Loss = -10893.706230371807
Iteration 8900: Loss = -10893.70828884964
1
Iteration 9000: Loss = -10893.706177535418
Iteration 9100: Loss = -10893.705921573008
Iteration 9200: Loss = -10893.722264033628
1
Iteration 9300: Loss = -10893.705639237318
Iteration 9400: Loss = -10893.705769488686
1
Iteration 9500: Loss = -10893.705666722371
2
Iteration 9600: Loss = -10893.705621374565
Iteration 9700: Loss = -10893.705849171738
1
Iteration 9800: Loss = -10893.705632805091
2
Iteration 9900: Loss = -10893.730594398796
3
Iteration 10000: Loss = -10893.70562233538
4
Iteration 10100: Loss = -10893.705606756866
Iteration 10200: Loss = -10893.705659413352
1
Iteration 10300: Loss = -10893.705579177917
Iteration 10400: Loss = -10893.705603808561
1
Iteration 10500: Loss = -10893.705673145678
2
Iteration 10600: Loss = -10893.705588969624
3
Iteration 10700: Loss = -10893.705577452349
Iteration 10800: Loss = -10893.705755927289
1
Iteration 10900: Loss = -10893.705569645907
Iteration 11000: Loss = -10893.705590084592
1
Iteration 11100: Loss = -10893.705869779371
2
Iteration 11200: Loss = -10893.70557327837
3
Iteration 11300: Loss = -10893.71093562006
4
Iteration 11400: Loss = -10893.70558042555
5
Stopping early at iteration 11400 due to no improvement.
pi: tensor([[0.7803, 0.2197],
        [0.3487, 0.6513]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0482, 0.9518], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2596, 0.0865],
         [0.7261, 0.1802]],

        [[0.6218, 0.0976],
         [0.5596, 0.7034]],

        [[0.5941, 0.1041],
         [0.6260, 0.5919]],

        [[0.5067, 0.0990],
         [0.6239, 0.6225]],

        [[0.5517, 0.0979],
         [0.5209, 0.7004]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 47
Adjusted Rand Index: -0.0032454170932134756
time is 1
tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 11
Adjusted Rand Index: 0.6045048763898703
time is 2
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 8
Adjusted Rand Index: 0.7026203884610964
time is 3
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 1
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 2
Adjusted Rand Index: 0.9208006335949313
Global Adjusted Rand Index: 0.5232231679026662
Average Adjusted Rand Index: 0.6369356574341
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22650.73704585812
Iteration 100: Loss = -11016.818234772356
Iteration 200: Loss = -11015.759659194893
Iteration 300: Loss = -11014.868639160391
Iteration 400: Loss = -11014.2309878474
Iteration 500: Loss = -11013.467172884375
Iteration 600: Loss = -11011.330868429774
Iteration 700: Loss = -11008.546035462341
Iteration 800: Loss = -11007.07233582255
Iteration 900: Loss = -11006.791825702474
Iteration 1000: Loss = -11006.60769455718
Iteration 1100: Loss = -11006.538248665764
Iteration 1200: Loss = -11006.489674377559
Iteration 1300: Loss = -11006.453918976644
Iteration 1400: Loss = -11006.428778455125
Iteration 1500: Loss = -11006.410372152373
Iteration 1600: Loss = -11006.396456584407
Iteration 1700: Loss = -11006.385333024207
Iteration 1800: Loss = -11006.376203384383
Iteration 1900: Loss = -11006.368694308794
Iteration 2000: Loss = -11006.362442402753
Iteration 2100: Loss = -11006.357037203648
Iteration 2200: Loss = -11006.352276178339
Iteration 2300: Loss = -11006.348071192177
Iteration 2400: Loss = -11006.344329259005
Iteration 2500: Loss = -11006.341130149565
Iteration 2600: Loss = -11006.338377887281
Iteration 2700: Loss = -11006.335906834147
Iteration 2800: Loss = -11006.333687139771
Iteration 2900: Loss = -11006.33167410962
Iteration 3000: Loss = -11006.329903297523
Iteration 3100: Loss = -11006.328131918202
Iteration 3200: Loss = -11006.326179802149
Iteration 3300: Loss = -11006.323707114452
Iteration 3400: Loss = -11006.317968670728
Iteration 3500: Loss = -11006.313138694599
Iteration 3600: Loss = -11006.310974326014
Iteration 3700: Loss = -11006.309301898917
Iteration 3800: Loss = -11006.307493047747
Iteration 3900: Loss = -11006.303935267184
Iteration 4000: Loss = -11006.299774305468
Iteration 4100: Loss = -11006.298549779332
Iteration 4200: Loss = -11006.297455384147
Iteration 4300: Loss = -11006.295824807343
Iteration 4400: Loss = -11006.286671260868
Iteration 4500: Loss = -11006.27059657068
Iteration 4600: Loss = -11006.266378790731
Iteration 4700: Loss = -11006.26377960845
Iteration 4800: Loss = -11006.25303666611
Iteration 4900: Loss = -11006.243733190211
Iteration 5000: Loss = -11006.238425014422
Iteration 5100: Loss = -11006.222566996357
Iteration 5200: Loss = -11006.220280962629
Iteration 5300: Loss = -11006.219539398271
Iteration 5400: Loss = -11006.218930900584
Iteration 5500: Loss = -11006.21831312017
Iteration 5600: Loss = -11006.21780427427
Iteration 5700: Loss = -11006.217249331485
Iteration 5800: Loss = -11006.216630767007
Iteration 5900: Loss = -11006.214913216834
Iteration 6000: Loss = -11006.214251232263
Iteration 6100: Loss = -11006.213858531346
Iteration 6200: Loss = -11006.213512673336
Iteration 6300: Loss = -11006.21324700833
Iteration 6400: Loss = -11006.21291783206
Iteration 6500: Loss = -11006.212641658714
Iteration 6600: Loss = -11006.212305151466
Iteration 6700: Loss = -11006.21190432812
Iteration 6800: Loss = -11006.211189934436
Iteration 6900: Loss = -11006.211493773591
1
Iteration 7000: Loss = -11006.209508167576
Iteration 7100: Loss = -11006.209072256623
Iteration 7200: Loss = -11006.208534875956
Iteration 7300: Loss = -11006.20559881163
Iteration 7400: Loss = -11006.204613368314
Iteration 7500: Loss = -11006.204035894621
Iteration 7600: Loss = -11006.202956635185
Iteration 7700: Loss = -11006.201891817305
Iteration 7800: Loss = -11006.19857330646
Iteration 7900: Loss = -11006.198253903116
Iteration 8000: Loss = -11006.198257510647
1
Iteration 8100: Loss = -11006.187927230589
Iteration 8200: Loss = -11006.187586570239
Iteration 8300: Loss = -11006.187711962299
1
Iteration 8400: Loss = -11006.2030105241
2
Iteration 8500: Loss = -11006.186839917167
Iteration 8600: Loss = -11006.186569656487
Iteration 8700: Loss = -11006.186423295585
Iteration 8800: Loss = -11006.186706067372
1
Iteration 8900: Loss = -11006.185962998596
Iteration 9000: Loss = -11006.1946711855
1
Iteration 9100: Loss = -11006.185238758464
Iteration 9200: Loss = -11006.175669809661
Iteration 9300: Loss = -11006.207938595751
1
Iteration 9400: Loss = -11006.174525082266
Iteration 9500: Loss = -11006.17443197354
Iteration 9600: Loss = -11006.175316465558
1
Iteration 9700: Loss = -11006.174140390414
Iteration 9800: Loss = -11006.173771318188
Iteration 9900: Loss = -11006.172963636476
Iteration 10000: Loss = -11006.173004271628
1
Iteration 10100: Loss = -11006.172429045166
Iteration 10200: Loss = -11006.172255182848
Iteration 10300: Loss = -11006.17617408266
1
Iteration 10400: Loss = -11006.171428370999
Iteration 10500: Loss = -11006.171346035855
Iteration 10600: Loss = -11006.174355207846
1
Iteration 10700: Loss = -11006.170844030565
Iteration 10800: Loss = -11006.170815356352
Iteration 10900: Loss = -11006.170782525616
Iteration 11000: Loss = -11006.17132834757
1
Iteration 11100: Loss = -11006.170502120984
Iteration 11200: Loss = -11006.170371691
Iteration 11300: Loss = -11006.172720845003
1
Iteration 11400: Loss = -11006.16994523389
Iteration 11500: Loss = -11006.169542573574
Iteration 11600: Loss = -11006.697293840158
1
Iteration 11700: Loss = -11006.169026953361
Iteration 11800: Loss = -11006.168952866683
Iteration 11900: Loss = -11006.23162240113
1
Iteration 12000: Loss = -11006.207999817269
2
Iteration 12100: Loss = -11006.168804876968
Iteration 12200: Loss = -11006.168751868865
Iteration 12300: Loss = -11006.248184533177
1
Iteration 12400: Loss = -11006.16861433134
Iteration 12500: Loss = -11006.16870188786
1
Iteration 12600: Loss = -11006.173465771803
2
Iteration 12700: Loss = -11006.16856475082
Iteration 12800: Loss = -11006.272919971274
1
Iteration 12900: Loss = -11006.167504625064
Iteration 13000: Loss = -11006.170795423053
1
Iteration 13100: Loss = -11006.167166003523
Iteration 13200: Loss = -11006.170048711097
1
Iteration 13300: Loss = -11006.167046169418
Iteration 13400: Loss = -11006.172302803325
1
Iteration 13500: Loss = -11006.167112439378
2
Iteration 13600: Loss = -11006.18586928787
3
Iteration 13700: Loss = -11006.17999486051
4
Iteration 13800: Loss = -11006.16867388828
5
Stopping early at iteration 13800 due to no improvement.
pi: tensor([[9.9559e-01, 4.4053e-03],
        [6.3448e-06, 9.9999e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9571, 0.0429], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1610, 0.1787],
         [0.6598, 0.2876]],

        [[0.6199, 0.0834],
         [0.5458, 0.6827]],

        [[0.5097, 0.1867],
         [0.6888, 0.5387]],

        [[0.6259, 0.1965],
         [0.5044, 0.6763]],

        [[0.5812, 0.2424],
         [0.6870, 0.5025]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 52
Adjusted Rand Index: -0.005396094422863973
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 53
Adjusted Rand Index: 0.0017137835707030447
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 46
Adjusted Rand Index: -0.007419103651150718
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 51
Adjusted Rand Index: -0.004212316740111065
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 47
Adjusted Rand Index: -0.014778186472389411
Global Adjusted Rand Index: -0.0019336126205117723
Average Adjusted Rand Index: -0.006018383543162424
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22591.97397556349
Iteration 100: Loss = -11017.64215544594
Iteration 200: Loss = -11016.263448752139
Iteration 300: Loss = -11015.644899465125
Iteration 400: Loss = -11015.312579796871
Iteration 500: Loss = -11014.983058061698
Iteration 600: Loss = -11014.393197142314
Iteration 700: Loss = -11013.850070574885
Iteration 800: Loss = -11013.217560074163
Iteration 900: Loss = -11012.236017358515
Iteration 1000: Loss = -11008.55362310886
Iteration 1100: Loss = -10910.951825893893
Iteration 1200: Loss = -10904.095805226369
Iteration 1300: Loss = -10900.314157096556
Iteration 1400: Loss = -10900.169294910875
Iteration 1500: Loss = -10900.112776602326
Iteration 1600: Loss = -10900.068857817538
Iteration 1700: Loss = -10900.001401128595
Iteration 1800: Loss = -10899.933048431845
Iteration 1900: Loss = -10899.893257520052
Iteration 2000: Loss = -10899.09302268376
Iteration 2100: Loss = -10899.071218881318
Iteration 2200: Loss = -10899.03118911037
Iteration 2300: Loss = -10898.72868677214
Iteration 2400: Loss = -10894.893189741673
Iteration 2500: Loss = -10894.00627365662
Iteration 2600: Loss = -10893.881926515207
Iteration 2700: Loss = -10893.846347753477
Iteration 2800: Loss = -10893.837386840965
Iteration 2900: Loss = -10893.832259525494
Iteration 3000: Loss = -10893.828652566728
Iteration 3100: Loss = -10893.826476582306
Iteration 3200: Loss = -10893.82344065959
Iteration 3300: Loss = -10893.820420890737
Iteration 3400: Loss = -10893.823820630703
1
Iteration 3500: Loss = -10893.817171843119
Iteration 3600: Loss = -10893.815950025093
Iteration 3700: Loss = -10893.811533394572
Iteration 3800: Loss = -10893.794516951177
Iteration 3900: Loss = -10893.797844959969
1
Iteration 4000: Loss = -10893.79007777265
Iteration 4100: Loss = -10893.782900228198
Iteration 4200: Loss = -10893.781963197522
Iteration 4300: Loss = -10893.774733419972
Iteration 4400: Loss = -10893.729519128323
Iteration 4500: Loss = -10893.728999206729
Iteration 4600: Loss = -10893.727746252835
Iteration 4700: Loss = -10893.726842491744
Iteration 4800: Loss = -10893.741113455162
1
Iteration 4900: Loss = -10893.72508722841
Iteration 5000: Loss = -10893.724853118922
Iteration 5100: Loss = -10893.725401948046
1
Iteration 5200: Loss = -10893.746386502306
2
Iteration 5300: Loss = -10893.723996789742
Iteration 5400: Loss = -10893.724846395313
1
Iteration 5500: Loss = -10893.723547253261
Iteration 5600: Loss = -10893.72366376605
1
Iteration 5700: Loss = -10893.722459629678
Iteration 5800: Loss = -10893.71834607379
Iteration 5900: Loss = -10893.71758461232
Iteration 6000: Loss = -10893.717312033656
Iteration 6100: Loss = -10893.744953514108
1
Iteration 6200: Loss = -10893.717107308696
Iteration 6300: Loss = -10893.722578063001
1
Iteration 6400: Loss = -10893.716994651097
Iteration 6500: Loss = -10893.716913756802
Iteration 6600: Loss = -10893.716887409835
Iteration 6700: Loss = -10893.716832897382
Iteration 6800: Loss = -10893.717118844886
1
Iteration 6900: Loss = -10893.716737614359
Iteration 7000: Loss = -10893.716706164649
Iteration 7100: Loss = -10893.716994637149
1
Iteration 7200: Loss = -10893.716622287402
Iteration 7300: Loss = -10893.71689489068
1
Iteration 7400: Loss = -10893.716630180123
2
Iteration 7500: Loss = -10893.716780343562
3
Iteration 7600: Loss = -10893.716637061734
4
Iteration 7700: Loss = -10893.718822246894
5
Stopping early at iteration 7700 due to no improvement.
pi: tensor([[0.6505, 0.3495],
        [0.2192, 0.7808]], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9518, 0.0482], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1803, 0.0865],
         [0.5487, 0.2594]],

        [[0.5518, 0.0976],
         [0.5583, 0.6845]],

        [[0.5810, 0.1041],
         [0.6690, 0.7123]],

        [[0.5577, 0.0990],
         [0.5570, 0.6615]],

        [[0.5359, 0.0979],
         [0.5115, 0.5281]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0])
tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,
        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 0, 1, 1])
Difference count: 53
Adjusted Rand Index: -0.0032454170932134756
time is 1
tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0,
        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 0, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 0, 1, 0])
Difference count: 89
Adjusted Rand Index: 0.6045048763898703
time is 2
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,
        1, 0, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,
        0, 1, 0, 0])
Difference count: 92
Adjusted Rand Index: 0.7026203884610964
time is 3
tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1, 0])
tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1])
Difference count: 99
Adjusted Rand Index: 0.9599978058178152
time is 4
tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1])
Difference count: 98
Adjusted Rand Index: 0.9208006335949313
Global Adjusted Rand Index: 0.5232231679026662
Average Adjusted Rand Index: 0.6369356574341
10919.47047611485
[0.5232231679026662, 0.5232231679026662, -0.0019336126205117723, 0.5232231679026662] [0.6369356574341, 0.6369356574341, -0.006018383543162424, 0.6369356574341] [10893.705962081767, 10893.70558042555, 11006.16867388828, 10893.718822246894]
-----------------------------------------------------------------------------------------
This iteration is 19
True Objective function: Loss = -11016.260330462863
Iteration 0: Loss = -11229.795285221377
Iteration 10: Loss = -11054.272170857523
Iteration 20: Loss = -11054.171956101518
Iteration 30: Loss = -11054.098356912391
Iteration 40: Loss = -11054.056149823871
Iteration 50: Loss = -11054.02869686811
Iteration 60: Loss = -11054.007281046046
Iteration 70: Loss = -11053.989546271261
Iteration 80: Loss = -11053.975105342759
Iteration 90: Loss = -11053.963319115597
Iteration 100: Loss = -11053.953653718674
Iteration 110: Loss = -11053.945556211165
Iteration 120: Loss = -11053.93861179556
Iteration 130: Loss = -11053.932595006312
Iteration 140: Loss = -11053.927346678269
Iteration 150: Loss = -11053.92273389986
Iteration 160: Loss = -11053.918668592234
Iteration 170: Loss = -11053.915057291846
Iteration 180: Loss = -11053.91183480314
Iteration 190: Loss = -11053.908971071804
Iteration 200: Loss = -11053.906376405277
Iteration 210: Loss = -11053.90406398101
Iteration 220: Loss = -11053.902004451475
Iteration 230: Loss = -11053.90011831411
Iteration 240: Loss = -11053.898442399199
Iteration 250: Loss = -11053.896876254836
Iteration 260: Loss = -11053.89550119433
Iteration 270: Loss = -11053.894258180613
Iteration 280: Loss = -11053.893085781856
Iteration 290: Loss = -11053.89205910303
Iteration 300: Loss = -11053.891118181107
Iteration 310: Loss = -11053.890265928952
Iteration 320: Loss = -11053.889475613732
Iteration 330: Loss = -11053.888739568112
Iteration 340: Loss = -11053.88808034939
Iteration 350: Loss = -11053.887462090428
Iteration 360: Loss = -11053.886909815048
Iteration 370: Loss = -11053.88641609704
Iteration 380: Loss = -11053.885941892797
Iteration 390: Loss = -11053.885485766312
Iteration 400: Loss = -11053.885129232616
Iteration 410: Loss = -11053.884714104315
Iteration 420: Loss = -11053.884351294653
Iteration 430: Loss = -11053.884118911645
Iteration 440: Loss = -11053.883816318024
Iteration 450: Loss = -11053.88353704102
Iteration 460: Loss = -11053.883330243798
Iteration 470: Loss = -11053.883078646817
Iteration 480: Loss = -11053.882847861632
Iteration 490: Loss = -11053.882670398869
Iteration 500: Loss = -11053.882503166691
Iteration 510: Loss = -11053.882312202353
Iteration 520: Loss = -11053.882127776984
Iteration 530: Loss = -11053.881997156808
Iteration 540: Loss = -11053.881860109252
Iteration 550: Loss = -11053.881757435882
Iteration 560: Loss = -11053.881655091978
Iteration 570: Loss = -11053.881563199404
Iteration 580: Loss = -11053.881432771957
Iteration 590: Loss = -11053.881296242575
Iteration 600: Loss = -11053.881267645962
Iteration 610: Loss = -11053.881144453166
Iteration 620: Loss = -11053.88107672123
Iteration 630: Loss = -11053.881023994967
Iteration 640: Loss = -11053.880963880161
Iteration 650: Loss = -11053.88093259592
Iteration 660: Loss = -11053.880860223053
Iteration 670: Loss = -11053.880786498758
Iteration 680: Loss = -11053.880723498087
Iteration 690: Loss = -11053.880639098175
Iteration 700: Loss = -11053.880668480937
1
Iteration 710: Loss = -11053.8806337509
Iteration 720: Loss = -11053.880541380697
Iteration 730: Loss = -11053.88056028684
1
Iteration 740: Loss = -11053.880508263896
Iteration 750: Loss = -11053.880474894575
Iteration 760: Loss = -11053.880469286682
Iteration 770: Loss = -11053.880381759915
Iteration 780: Loss = -11053.880406335586
1
Iteration 790: Loss = -11053.880374772063
Iteration 800: Loss = -11053.880382610423
1
Iteration 810: Loss = -11053.880340710453
Iteration 820: Loss = -11053.880315215223
Iteration 830: Loss = -11053.880280834599
Iteration 840: Loss = -11053.8802606413
Iteration 850: Loss = -11053.88027565278
1
Iteration 860: Loss = -11053.880267233515
2
Iteration 870: Loss = -11053.880252657258
Iteration 880: Loss = -11053.880194027388
Iteration 890: Loss = -11053.880225412679
1
Iteration 900: Loss = -11053.88018198495
Iteration 910: Loss = -11053.880168582444
Iteration 920: Loss = -11053.880158400341
Iteration 930: Loss = -11053.880164277612
1
Iteration 940: Loss = -11053.88015644305
Iteration 950: Loss = -11053.880166999546
1
Iteration 960: Loss = -11053.880143743538
Iteration 970: Loss = -11053.880129467296
Iteration 980: Loss = -11053.88013488646
1
Iteration 990: Loss = -11053.880144649429
2
Iteration 1000: Loss = -11053.880134516334
3
Stopping early at iteration 1000 due to no improvement.
pi: tensor([[0.9685, 0.0315],
        [0.8825, 0.1175]], dtype=torch.float64)
alpha: tensor([0.9658, 0.0342])
beta: tensor([[[0.1637, 0.1014],
         [0.9118, 0.1431]],

        [[0.8407, 0.1522],
         [0.6842, 0.4661]],

        [[0.8056, 0.1453],
         [0.2478, 0.3782]],

        [[0.0295, 0.1788],
         [0.0325, 0.9446]],

        [[0.9572, 0.1592],
         [0.6516, 0.7366]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 48
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: -0.0001568647537703564
Average Adjusted Rand Index: 0.0
pi: tensor([[3.5593e-10, 1.0000e+00],
        [7.6118e-08, 1.0000e+00]], dtype=torch.float64)
alpha: tensor([0.0083, 0.9917])
beta: tensor([[[0.2182, 0.2007],
         [0.0139, 0.1620]],

        [[0.0790,    nan],
         [0.2858, 0.4847]],

        [[0.7022, 0.2766],
         [0.5732, 0.1693]],

        [[0.7507, 0.2394],
         [0.8046, 0.1627]],

        [[0.7445, 0.2438],
         [0.5022, 0.4722]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 52
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -24224.75150845947
Iteration 100: Loss = -11056.198060240526
Iteration 200: Loss = -11054.913082480613
Iteration 300: Loss = -11054.56536794532
Iteration 400: Loss = -11054.404512799178
Iteration 500: Loss = -11054.300944690878
Iteration 600: Loss = -11054.217446422981
Iteration 700: Loss = -11054.142213410665
Iteration 800: Loss = -11054.074330296955
Iteration 900: Loss = -11054.01193468611
Iteration 1000: Loss = -11053.952431384172
Iteration 1100: Loss = -11053.89122567886
Iteration 1200: Loss = -11053.820229205181
Iteration 1300: Loss = -11053.728375868133
Iteration 1400: Loss = -11053.625593339557
Iteration 1500: Loss = -11053.551483404259
Iteration 1600: Loss = -11053.502312070983
Iteration 1700: Loss = -11053.466772898813
Iteration 1800: Loss = -11053.440396226126
Iteration 1900: Loss = -11053.420594259742
Iteration 2000: Loss = -11053.405566809868
Iteration 2100: Loss = -11053.393798685503
Iteration 2200: Loss = -11053.384273521258
Iteration 2300: Loss = -11053.375958020979
Iteration 2400: Loss = -11053.36812817323
Iteration 2500: Loss = -11053.359696230944
Iteration 2600: Loss = -11053.34891231991
Iteration 2700: Loss = -11053.331848058631
Iteration 2800: Loss = -11053.29973083983
Iteration 2900: Loss = -11053.25084622228
Iteration 3000: Loss = -11053.213203276651
Iteration 3100: Loss = -11053.186951502348
Iteration 3200: Loss = -11053.167597721496
Iteration 3300: Loss = -11053.15298307323
Iteration 3400: Loss = -11053.141696544157
Iteration 3500: Loss = -11053.132843629483
Iteration 3600: Loss = -11053.12576718504
Iteration 3700: Loss = -11053.120118004928
Iteration 3800: Loss = -11053.115516108284
Iteration 3900: Loss = -11053.111783663715
Iteration 4000: Loss = -11053.108660707612
Iteration 4100: Loss = -11053.106101241468
Iteration 4200: Loss = -11053.103928681023
Iteration 4300: Loss = -11053.102126242853
Iteration 4400: Loss = -11053.100554753933
Iteration 4500: Loss = -11053.099203055113
Iteration 4600: Loss = -11053.098059115318
Iteration 4700: Loss = -11053.097028957074
Iteration 4800: Loss = -11053.096108317864
Iteration 4900: Loss = -11053.0953622865
Iteration 5000: Loss = -11053.10039362043
1
Iteration 5100: Loss = -11053.093969660466
Iteration 5200: Loss = -11053.093386436854
Iteration 5300: Loss = -11053.092848905959
Iteration 5400: Loss = -11053.092379951271
Iteration 5500: Loss = -11053.092166147628
Iteration 5600: Loss = -11053.091480022289
Iteration 5700: Loss = -11053.091131467054
Iteration 5800: Loss = -11053.092926988105
1
Iteration 5900: Loss = -11053.090434271007
Iteration 6000: Loss = -11053.090109447585
Iteration 6100: Loss = -11053.089835479499
Iteration 6200: Loss = -11053.089557202024
Iteration 6300: Loss = -11053.089292860197
Iteration 6400: Loss = -11053.089110661693
Iteration 6500: Loss = -11053.088859666894
Iteration 6600: Loss = -11053.088732568325
Iteration 6700: Loss = -11053.088413679205
Iteration 6800: Loss = -11053.088255398363
Iteration 6900: Loss = -11053.08809927133
Iteration 7000: Loss = -11053.08792257348
Iteration 7100: Loss = -11053.0916396172
1
Iteration 7200: Loss = -11053.08758554152
Iteration 7300: Loss = -11053.08743347708
Iteration 7400: Loss = -11053.087401248198
Iteration 7500: Loss = -11053.087173785949
Iteration 7600: Loss = -11053.088533649467
1
Iteration 7700: Loss = -11053.086926349577
Iteration 7800: Loss = -11053.087045547938
1
Iteration 7900: Loss = -11053.086712984203
Iteration 8000: Loss = -11053.086584043489
Iteration 8100: Loss = -11053.086743690808
1
Iteration 8200: Loss = -11053.086434152374
Iteration 8300: Loss = -11053.097389662611
1
Iteration 8400: Loss = -11053.08626762949
Iteration 8500: Loss = -11053.090521057977
1
Iteration 8600: Loss = -11053.086139588124
Iteration 8700: Loss = -11053.086015630151
Iteration 8800: Loss = -11053.08802859449
1
Iteration 8900: Loss = -11053.085926142176
Iteration 9000: Loss = -11053.08585349741
Iteration 9100: Loss = -11053.086015158808
1
Iteration 9200: Loss = -11053.085699185973
Iteration 9300: Loss = -11053.085645985513
Iteration 9400: Loss = -11053.085607992922
Iteration 9500: Loss = -11053.0855828689
Iteration 9600: Loss = -11053.085526489122
Iteration 9700: Loss = -11053.08555030557
1
Iteration 9800: Loss = -11053.08539130498
Iteration 9900: Loss = -11053.085372372801
Iteration 10000: Loss = -11053.086573724073
1
Iteration 10100: Loss = -11053.085304300916
Iteration 10200: Loss = -11053.08528302733
Iteration 10300: Loss = -11053.086532439233
1
Iteration 10400: Loss = -11053.085208174782
Iteration 10500: Loss = -11053.085182672176
Iteration 10600: Loss = -11053.08590254961
1
Iteration 10700: Loss = -11053.085141438412
Iteration 10800: Loss = -11053.08510468654
Iteration 10900: Loss = -11053.086559525495
1
Iteration 11000: Loss = -11053.085044783482
Iteration 11100: Loss = -11053.08503778536
Iteration 11200: Loss = -11053.087145618203
1
Iteration 11300: Loss = -11053.084970696882
Iteration 11400: Loss = -11053.085014397257
1
Iteration 11500: Loss = -11053.088952759395
2
Iteration 11600: Loss = -11053.084913442313
Iteration 11700: Loss = -11053.084882435189
Iteration 11800: Loss = -11053.085065649097
1
Iteration 11900: Loss = -11053.084862961907
Iteration 12000: Loss = -11053.084874414972
1
Iteration 12100: Loss = -11053.084900852324
2
Iteration 12200: Loss = -11053.084858766151
Iteration 12300: Loss = -11053.086412111421
1
Iteration 12400: Loss = -11053.084807593998
Iteration 12500: Loss = -11053.08864858238
1
Iteration 12600: Loss = -11053.085250625525
2
Iteration 12700: Loss = -11053.107214233361
3
Iteration 12800: Loss = -11053.084747258932
Iteration 12900: Loss = -11053.090465395397
1
Iteration 13000: Loss = -11053.085030153894
2
Iteration 13100: Loss = -11053.084799882778
3
Iteration 13200: Loss = -11053.205420198103
4
Iteration 13300: Loss = -11053.084701770144
Iteration 13400: Loss = -11053.085607984382
1
Iteration 13500: Loss = -11053.112015496976
2
Iteration 13600: Loss = -11053.08475015015
3
Iteration 13700: Loss = -11053.08532844823
4
Iteration 13800: Loss = -11053.084998755554
5
Stopping early at iteration 13800 due to no improvement.
pi: tensor([[9.6111e-01, 3.8889e-02],
        [9.9983e-01, 1.7342e-04]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9880, 0.0120], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1623, 0.0801],
         [0.5549, 0.2260]],

        [[0.6737, 0.1959],
         [0.5468, 0.5175]],

        [[0.5068, 0.2369],
         [0.6092, 0.6285]],

        [[0.5699, 0.1952],
         [0.5381, 0.6602]],

        [[0.6654, 0.1752],
         [0.5985, 0.7294]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00016326739678139136
Average Adjusted Rand Index: -0.0003077958928485548
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21182.644798809833
Iteration 100: Loss = -11055.552372035925
Iteration 200: Loss = -11054.754175619892
Iteration 300: Loss = -11054.486662786647
Iteration 400: Loss = -11054.38004425471
Iteration 500: Loss = -11054.329257774076
Iteration 600: Loss = -11054.300736861664
Iteration 700: Loss = -11054.279071068526
Iteration 800: Loss = -11054.253513997928
Iteration 900: Loss = -11054.206721099368
Iteration 1000: Loss = -11054.087914123555
Iteration 1100: Loss = -11053.876936545574
Iteration 1200: Loss = -11053.741904943005
Iteration 1300: Loss = -11053.656417609924
Iteration 1400: Loss = -11053.592915684005
Iteration 1500: Loss = -11053.545321345617
Iteration 1600: Loss = -11053.508162337464
Iteration 1700: Loss = -11053.476922444228
Iteration 1800: Loss = -11053.451629373476
Iteration 1900: Loss = -11053.431480893023
Iteration 2000: Loss = -11053.414985096711
Iteration 2100: Loss = -11053.400851948283
Iteration 2200: Loss = -11053.388638905599
Iteration 2300: Loss = -11053.378259572473
Iteration 2400: Loss = -11053.369642398507
Iteration 2500: Loss = -11053.362495110334
Iteration 2600: Loss = -11053.356713305755
Iteration 2700: Loss = -11053.3520822965
Iteration 2800: Loss = -11053.348095308964
Iteration 2900: Loss = -11053.34448870013
Iteration 3000: Loss = -11053.340724216867
Iteration 3100: Loss = -11053.335275498808
Iteration 3200: Loss = -11053.323096697408
Iteration 3300: Loss = -11053.264599131784
Iteration 3400: Loss = -11053.145020857297
Iteration 3500: Loss = -11053.117900863072
Iteration 3600: Loss = -11053.105683396408
Iteration 3700: Loss = -11053.099164794112
Iteration 3800: Loss = -11053.095402584135
Iteration 3900: Loss = -11053.093096573974
Iteration 4000: Loss = -11053.091557356274
Iteration 4100: Loss = -11053.09047592957
Iteration 4200: Loss = -11053.089734359619
Iteration 4300: Loss = -11053.089163069833
Iteration 4400: Loss = -11053.088712422526
Iteration 4500: Loss = -11053.088327388145
Iteration 4600: Loss = -11053.08799848957
Iteration 4700: Loss = -11053.087773924939
Iteration 4800: Loss = -11053.087538659212
Iteration 4900: Loss = -11053.094551516528
1
Iteration 5000: Loss = -11053.087215918576
Iteration 5100: Loss = -11053.087021079376
Iteration 5200: Loss = -11053.086865079164
Iteration 5300: Loss = -11053.086761358722
Iteration 5400: Loss = -11053.087097386939
1
Iteration 5500: Loss = -11053.086507986334
Iteration 5600: Loss = -11053.08644564282
Iteration 5700: Loss = -11053.086323169797
Iteration 5800: Loss = -11053.08623139977
Iteration 5900: Loss = -11053.086163273983
Iteration 6000: Loss = -11053.086060020945
Iteration 6100: Loss = -11053.086014923605
Iteration 6200: Loss = -11053.085953710495
Iteration 6300: Loss = -11053.086025953793
1
Iteration 6400: Loss = -11053.085809358503
Iteration 6500: Loss = -11053.08610167147
1
Iteration 6600: Loss = -11053.08566952777
Iteration 6700: Loss = -11053.085631190474
Iteration 6800: Loss = -11053.085580334608
Iteration 6900: Loss = -11053.085529867216
Iteration 7000: Loss = -11053.086975694332
1
Iteration 7100: Loss = -11053.085471196026
Iteration 7200: Loss = -11053.10034176686
1
Iteration 7300: Loss = -11053.085365341161
Iteration 7400: Loss = -11053.085320969727
Iteration 7500: Loss = -11053.085309949043
Iteration 7600: Loss = -11053.085241370158
Iteration 7700: Loss = -11053.085234901097
Iteration 7800: Loss = -11053.085221764612
Iteration 7900: Loss = -11053.086269303229
1
Iteration 8000: Loss = -11053.08519643493
Iteration 8100: Loss = -11053.085257483996
1
Iteration 8200: Loss = -11053.085109896612
Iteration 8300: Loss = -11053.096092832455
1
Iteration 8400: Loss = -11053.085032504658
Iteration 8500: Loss = -11053.085138440463
1
Iteration 8600: Loss = -11053.08495852336
Iteration 8700: Loss = -11053.098330300596
1
Iteration 8800: Loss = -11053.084941302137
Iteration 8900: Loss = -11053.085278701981
1
Iteration 9000: Loss = -11053.084977882416
2
Iteration 9100: Loss = -11053.084911495866
Iteration 9200: Loss = -11053.085183691355
1
Iteration 9300: Loss = -11053.084867817714
Iteration 9400: Loss = -11053.084826726012
Iteration 9500: Loss = -11053.296894702895
1
Iteration 9600: Loss = -11053.084858280161
2
Iteration 9700: Loss = -11053.084825182761
Iteration 9800: Loss = -11053.421653374902
1
Iteration 9900: Loss = -11053.08480484839
Iteration 10000: Loss = -11053.084818290316
1
Iteration 10100: Loss = -11053.084786904416
Iteration 10200: Loss = -11053.08570632731
1
Iteration 10300: Loss = -11053.084763356777
Iteration 10400: Loss = -11053.08474707762
Iteration 10500: Loss = -11053.095010177112
1
Iteration 10600: Loss = -11053.084714637858
Iteration 10700: Loss = -11053.084708072083
Iteration 10800: Loss = -11053.085587197973
1
Iteration 10900: Loss = -11053.0847070868
Iteration 11000: Loss = -11053.084717707443
1
Iteration 11100: Loss = -11053.106679982135
2
Iteration 11200: Loss = -11053.084651988185
Iteration 11300: Loss = -11053.084668988266
1
Iteration 11400: Loss = -11053.08561848908
2
Iteration 11500: Loss = -11053.084654624025
3
Iteration 11600: Loss = -11053.084668950993
4
Iteration 11700: Loss = -11053.084957202713
5
Stopping early at iteration 11700 due to no improvement.
pi: tensor([[1.3400e-04, 9.9987e-01],
        [3.8984e-02, 9.6102e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0119, 0.9881], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2260, 0.0801],
         [0.5124, 0.1622]],

        [[0.6223, 0.1959],
         [0.6704, 0.6442]],

        [[0.7251, 0.2373],
         [0.6593, 0.5500]],

        [[0.6449, 0.1953],
         [0.6288, 0.7028]],

        [[0.6395, 0.1754],
         [0.6829, 0.5395]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00016326739678139136
Average Adjusted Rand Index: -0.0003077958928485548
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -22837.407122691326
Iteration 100: Loss = -11056.642160391591
Iteration 200: Loss = -11055.336476605897
Iteration 300: Loss = -11054.870664703394
Iteration 400: Loss = -11054.651220589376
Iteration 500: Loss = -11054.529786366182
Iteration 600: Loss = -11054.448464328278
Iteration 700: Loss = -11054.385457288317
Iteration 800: Loss = -11054.328987996809
Iteration 900: Loss = -11054.271136137202
Iteration 1000: Loss = -11054.207405421186
Iteration 1100: Loss = -11054.139227384381
Iteration 1200: Loss = -11054.070973682019
Iteration 1300: Loss = -11054.004222514688
Iteration 1400: Loss = -11053.937260472203
Iteration 1500: Loss = -11053.865791343824
Iteration 1600: Loss = -11053.782690400378
Iteration 1700: Loss = -11053.679174895158
Iteration 1800: Loss = -11053.57374568652
Iteration 1900: Loss = -11053.506004607945
Iteration 2000: Loss = -11053.46579515912
Iteration 2100: Loss = -11053.43868283264
Iteration 2200: Loss = -11053.418816458974
Iteration 2300: Loss = -11053.402893340304
Iteration 2400: Loss = -11053.388764758593
Iteration 2500: Loss = -11053.374463962295
Iteration 2600: Loss = -11053.35711852343
Iteration 2700: Loss = -11053.331434721733
Iteration 2800: Loss = -11053.292607547999
Iteration 2900: Loss = -11053.25359781516
Iteration 3000: Loss = -11053.224605499492
Iteration 3100: Loss = -11053.202287383774
Iteration 3200: Loss = -11053.184489284804
Iteration 3300: Loss = -11053.170158250088
Iteration 3400: Loss = -11053.158435651885
Iteration 3500: Loss = -11053.148703048606
Iteration 3600: Loss = -11053.140600134531
Iteration 3700: Loss = -11053.133787800998
Iteration 3800: Loss = -11053.128103213738
Iteration 3900: Loss = -11053.123281643499
Iteration 4000: Loss = -11053.11920584491
Iteration 4100: Loss = -11053.115654322155
Iteration 4200: Loss = -11053.11266798111
Iteration 4300: Loss = -11053.110089941318
Iteration 4400: Loss = -11053.107855524198
Iteration 4500: Loss = -11053.10587495914
Iteration 4600: Loss = -11053.104222832539
Iteration 4700: Loss = -11053.102685197025
Iteration 4800: Loss = -11053.101325815749
Iteration 4900: Loss = -11053.100144054393
Iteration 5000: Loss = -11053.099059780428
Iteration 5100: Loss = -11053.098124664833
Iteration 5200: Loss = -11053.097226961814
Iteration 5300: Loss = -11053.097297081795
1
Iteration 5400: Loss = -11053.095740743445
Iteration 5500: Loss = -11053.095039685875
Iteration 5600: Loss = -11053.094414150542
Iteration 5700: Loss = -11053.0938536337
Iteration 5800: Loss = -11053.093325736485
Iteration 5900: Loss = -11053.092830345846
Iteration 6000: Loss = -11053.09242491841
Iteration 6100: Loss = -11053.098648679883
1
Iteration 6200: Loss = -11053.091589634654
Iteration 6300: Loss = -11053.091220370035
Iteration 6400: Loss = -11053.090905060713
Iteration 6500: Loss = -11053.090560228855
Iteration 6600: Loss = -11053.094271348673
1
Iteration 6700: Loss = -11053.089939713995
Iteration 6800: Loss = -11053.089688868762
Iteration 6900: Loss = -11053.089707059322
1
Iteration 7000: Loss = -11053.089212693716
Iteration 7100: Loss = -11053.088942505417
Iteration 7200: Loss = -11053.101218997006
1
Iteration 7300: Loss = -11053.088562216031
Iteration 7400: Loss = -11053.088341808849
Iteration 7500: Loss = -11053.089020477886
1
Iteration 7600: Loss = -11053.089808027335
2
Iteration 7700: Loss = -11053.108868888887
3
Iteration 7800: Loss = -11053.0876998033
Iteration 7900: Loss = -11053.08752950146
Iteration 8000: Loss = -11053.093361321266
1
Iteration 8100: Loss = -11053.08727398899
Iteration 8200: Loss = -11053.08712755401
Iteration 8300: Loss = -11053.08710080001
Iteration 8400: Loss = -11053.086934540572
Iteration 8500: Loss = -11053.086775271931
Iteration 8600: Loss = -11053.087879068114
1
Iteration 8700: Loss = -11053.086582529875
Iteration 8800: Loss = -11053.086502048496
Iteration 8900: Loss = -11053.704098506329
1
Iteration 9000: Loss = -11053.086302738235
Iteration 9100: Loss = -11053.08623942224
Iteration 9200: Loss = -11053.086162786754
Iteration 9300: Loss = -11053.095339752743
1
Iteration 9400: Loss = -11053.086020688757
Iteration 9500: Loss = -11053.085928918135
Iteration 9600: Loss = -11053.325000732444
1
Iteration 9700: Loss = -11053.085831269764
Iteration 9800: Loss = -11053.08572725689
Iteration 9900: Loss = -11053.25191208727
1
Iteration 10000: Loss = -11053.085655679177
Iteration 10100: Loss = -11053.085569556326
Iteration 10200: Loss = -11053.085540764378
Iteration 10300: Loss = -11053.085913135734
1
Iteration 10400: Loss = -11053.085433829217
Iteration 10500: Loss = -11053.08539192465
Iteration 10600: Loss = -11053.085419336285
1
Iteration 10700: Loss = -11053.085325230108
Iteration 10800: Loss = -11053.085670747645
1
Iteration 10900: Loss = -11053.08530066035
Iteration 11000: Loss = -11053.085245224232
Iteration 11100: Loss = -11053.092669253765
1
Iteration 11200: Loss = -11053.085191492502
Iteration 11300: Loss = -11053.085135818737
Iteration 11400: Loss = -11053.087826433513
1
Iteration 11500: Loss = -11053.085151914398
2
Iteration 11600: Loss = -11053.085054337665
Iteration 11700: Loss = -11053.085067344146
1
Iteration 11800: Loss = -11053.092067603271
2
Iteration 11900: Loss = -11053.08496044905
Iteration 12000: Loss = -11053.084977103086
1
Iteration 12100: Loss = -11053.085028477277
2
Iteration 12200: Loss = -11053.084977822196
3
Iteration 12300: Loss = -11053.311629765109
4
Iteration 12400: Loss = -11053.084912263002
Iteration 12500: Loss = -11053.084918901575
1
Iteration 12600: Loss = -11053.090163918738
2
Iteration 12700: Loss = -11053.153213924174
3
Iteration 12800: Loss = -11053.084874726257
Iteration 12900: Loss = -11053.092256328138
1
Iteration 13000: Loss = -11053.084836054395
Iteration 13100: Loss = -11053.08493830122
1
Iteration 13200: Loss = -11053.085032104385
2
Iteration 13300: Loss = -11053.08485520086
3
Iteration 13400: Loss = -11053.084824604455
Iteration 13500: Loss = -11053.084797235619
Iteration 13600: Loss = -11053.085139202189
1
Iteration 13700: Loss = -11053.10537369392
2
Iteration 13800: Loss = -11053.084977611596
3
Iteration 13900: Loss = -11053.087168118545
4
Iteration 14000: Loss = -11053.13865356046
5
Stopping early at iteration 14000 due to no improvement.
pi: tensor([[9.6147e-01, 3.8527e-02],
        [9.9978e-01, 2.2032e-04]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9882, 0.0118], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1616, 0.0801],
         [0.6276, 0.2261]],

        [[0.5470, 0.1976],
         [0.5174, 0.5594]],

        [[0.5768, 0.2393],
         [0.7093, 0.5479]],

        [[0.7214, 0.1969],
         [0.7037, 0.5814]],

        [[0.6098, 0.1766],
         [0.5267, 0.5467]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 52
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: -0.001538979464242774
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00016326739678139136
Average Adjusted Rand Index: -0.0003077958928485548
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20696.828909583804
Iteration 100: Loss = -11055.487032510191
Iteration 200: Loss = -11054.561275980575
Iteration 300: Loss = -11054.404950442744
Iteration 400: Loss = -11054.350300769653
Iteration 500: Loss = -11054.321686702555
Iteration 600: Loss = -11054.299867679334
Iteration 700: Loss = -11054.275186497489
Iteration 800: Loss = -11054.233575530512
Iteration 900: Loss = -11054.13565343576
Iteration 1000: Loss = -11053.92320930522
Iteration 1100: Loss = -11053.712825153636
Iteration 1200: Loss = -11053.596521577527
Iteration 1300: Loss = -11053.529046890626
Iteration 1400: Loss = -11053.486188969257
Iteration 1500: Loss = -11053.456084693518
Iteration 1600: Loss = -11053.43384502217
Iteration 1700: Loss = -11053.41656518333
Iteration 1800: Loss = -11053.402792958683
Iteration 1900: Loss = -11053.391803804443
Iteration 2000: Loss = -11053.383222268
Iteration 2100: Loss = -11053.376688083623
Iteration 2200: Loss = -11053.371838488725
Iteration 2300: Loss = -11053.368248978799
Iteration 2400: Loss = -11053.365668059654
Iteration 2500: Loss = -11053.36363358794
Iteration 2600: Loss = -11053.361974745794
Iteration 2700: Loss = -11053.360563345394
Iteration 2800: Loss = -11053.359251779053
Iteration 2900: Loss = -11053.35803280227
Iteration 3000: Loss = -11053.356883680568
Iteration 3100: Loss = -11053.355834361593
Iteration 3200: Loss = -11053.354780559494
Iteration 3300: Loss = -11053.353835943193
Iteration 3400: Loss = -11053.35288926153
Iteration 3500: Loss = -11053.351991572248
Iteration 3600: Loss = -11053.351127632488
Iteration 3700: Loss = -11053.350242429551
Iteration 3800: Loss = -11053.349458536577
Iteration 3900: Loss = -11053.348631458275
Iteration 4000: Loss = -11053.347857553264
Iteration 4100: Loss = -11053.347026561838
Iteration 4200: Loss = -11053.346202053492
Iteration 4300: Loss = -11053.345402686915
Iteration 4400: Loss = -11053.344487174956
Iteration 4500: Loss = -11053.34342710208
Iteration 4600: Loss = -11053.342329366578
Iteration 4700: Loss = -11053.340898975775
Iteration 4800: Loss = -11053.338864592364
Iteration 4900: Loss = -11053.335467425824
Iteration 5000: Loss = -11053.327131543083
Iteration 5100: Loss = -11053.261902618553
Iteration 5200: Loss = -11053.122175545446
Iteration 5300: Loss = -11053.104185573613
Iteration 5400: Loss = -11053.09639948909
Iteration 5500: Loss = -11053.092510202821
Iteration 5600: Loss = -11053.090359550395
Iteration 5700: Loss = -11053.091561529647
1
Iteration 5800: Loss = -11053.088178092417
Iteration 5900: Loss = -11053.087641690785
Iteration 6000: Loss = -11053.08745269861
Iteration 6100: Loss = -11053.086920342455
Iteration 6200: Loss = -11053.086665504841
Iteration 6300: Loss = -11053.086554650014
Iteration 6400: Loss = -11053.08633886413
Iteration 6500: Loss = -11053.086189985675
Iteration 6600: Loss = -11053.086069664909
Iteration 6700: Loss = -11053.085967718762
Iteration 6800: Loss = -11053.085877359346
Iteration 6900: Loss = -11053.085817316778
Iteration 7000: Loss = -11053.087750258983
1
Iteration 7100: Loss = -11053.085662265392
Iteration 7200: Loss = -11053.085612221437
Iteration 7300: Loss = -11053.085555452279
Iteration 7400: Loss = -11053.085488469309
Iteration 7500: Loss = -11053.085463676676
Iteration 7600: Loss = -11053.085427970893
Iteration 7700: Loss = -11053.085637500271
1
Iteration 7800: Loss = -11053.194131930366
2
Iteration 7900: Loss = -11053.085307729554
Iteration 8000: Loss = -11053.085400244085
1
Iteration 8100: Loss = -11053.088622086321
2
Iteration 8200: Loss = -11053.08519070552
Iteration 8300: Loss = -11053.08518369129
Iteration 8400: Loss = -11053.097154136534
1
Iteration 8500: Loss = -11053.085092026928
Iteration 8600: Loss = -11053.085060851407
Iteration 8700: Loss = -11053.268360282342
1
Iteration 8800: Loss = -11053.085028768708
Iteration 8900: Loss = -11053.084965985578
Iteration 9000: Loss = -11053.113863322596
1
Iteration 9100: Loss = -11053.08499843462
2
Iteration 9200: Loss = -11053.084933101445
Iteration 9300: Loss = -11053.568995858299
1
Iteration 9400: Loss = -11053.084923892808
Iteration 9500: Loss = -11053.084871216422
Iteration 9600: Loss = -11053.317152406697
1
Iteration 9700: Loss = -11053.084845597432
Iteration 9800: Loss = -11053.084841664737
Iteration 9900: Loss = -11053.084937281263
1
Iteration 10000: Loss = -11053.08487351307
2
Iteration 10100: Loss = -11053.08481600962
Iteration 10200: Loss = -11053.084792578868
Iteration 10300: Loss = -11053.085086858324
1
Iteration 10400: Loss = -11053.08475526291
Iteration 10500: Loss = -11053.084744856638
Iteration 10600: Loss = -11053.085050695858
1
Iteration 10700: Loss = -11053.084756677741
2
Iteration 10800: Loss = -11053.084747342555
3
Iteration 10900: Loss = -11053.084814235224
4
Iteration 11000: Loss = -11053.084667840612
Iteration 11100: Loss = -11053.084758442155
1
Iteration 11200: Loss = -11053.08482234019
2
Iteration 11300: Loss = -11053.084690273508
3
Iteration 11400: Loss = -11053.084684119865
4
Iteration 11500: Loss = -11053.086170553548
5
Stopping early at iteration 11500 due to no improvement.
pi: tensor([[1.8916e-04, 9.9981e-01],
        [3.8992e-02, 9.6101e-01]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0119, 0.9881], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.2260, 0.0801],
         [0.7231, 0.1622]],

        [[0.6550, 0.1959],
         [0.5181, 0.5251]],

        [[0.7038, 0.2376],
         [0.5558, 0.6352]],

        [[0.7230, 0.1953],
         [0.6572, 0.5750]],

        [[0.6870, 0.1753],
         [0.7046, 0.6291]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
        0, 1, 1, 0])
Difference count: 50
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,
        0, 0, 1, 0])
Difference count: 48
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,
        0, 1, 1, 1])
Difference count: 51
Adjusted Rand Index: -0.001538979464242774
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        0, 1, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.00016326739678139136
Average Adjusted Rand Index: -0.0003077958928485548
11016.260330462863
[0.00016326739678139136, 0.00016326739678139136, 0.00016326739678139136, 0.00016326739678139136] [-0.0003077958928485548, -0.0003077958928485548, -0.0003077958928485548, -0.0003077958928485548] [11053.084998755554, 11053.084957202713, 11053.13865356046, 11053.086170553548]
-----------------------------------------------------------------------------------------
This iteration is 20
True Objective function: Loss = -10941.133397861695
Iteration 0: Loss = -11118.98977093831
Iteration 10: Loss = -10985.149833717025
Iteration 20: Loss = -10985.148048825404
Iteration 30: Loss = -10985.147876593437
Iteration 40: Loss = -10985.14780109267
Iteration 50: Loss = -10985.147762296938
Iteration 60: Loss = -10985.147740346489
Iteration 70: Loss = -10985.14767879027
Iteration 80: Loss = -10985.14762430984
Iteration 90: Loss = -10985.147555360962
Iteration 100: Loss = -10985.147449513684
Iteration 110: Loss = -10985.147428106577
Iteration 120: Loss = -10985.147432358375
1
Iteration 130: Loss = -10985.147345863443
Iteration 140: Loss = -10985.14731199167
Iteration 150: Loss = -10985.147255656964
Iteration 160: Loss = -10985.147212911204
Iteration 170: Loss = -10985.147174881211
Iteration 180: Loss = -10985.147129428691
Iteration 190: Loss = -10985.147110754917
Iteration 200: Loss = -10985.147050554757
Iteration 210: Loss = -10985.147049325273
Iteration 220: Loss = -10985.147010805224
Iteration 230: Loss = -10985.146952044866
Iteration 240: Loss = -10985.14693471827
Iteration 250: Loss = -10985.146890281818
Iteration 260: Loss = -10985.14688865458
Iteration 270: Loss = -10985.146851558184
Iteration 280: Loss = -10985.1468443963
Iteration 290: Loss = -10985.146737009882
Iteration 300: Loss = -10985.146763108522
1
Iteration 310: Loss = -10985.146732027026
Iteration 320: Loss = -10985.146682289487
Iteration 330: Loss = -10985.14666923366
Iteration 340: Loss = -10985.146682482937
1
Iteration 350: Loss = -10985.14662447588
Iteration 360: Loss = -10985.146575241262
Iteration 370: Loss = -10985.14660892607
1
Iteration 380: Loss = -10985.146562358896
Iteration 390: Loss = -10985.146566409094
1
Iteration 400: Loss = -10985.146542494258
Iteration 410: Loss = -10985.146514361826
Iteration 420: Loss = -10985.146507742424
Iteration 430: Loss = -10985.146475358488
Iteration 440: Loss = -10985.146439747137
Iteration 450: Loss = -10985.146473182138
1
Iteration 460: Loss = -10985.14644511708
2
Iteration 470: Loss = -10985.146391862714
Iteration 480: Loss = -10985.146410086849
1
Iteration 490: Loss = -10985.1463890636
Iteration 500: Loss = -10985.14633309578
Iteration 510: Loss = -10985.14632331052
Iteration 520: Loss = -10985.146350156814
1
Iteration 530: Loss = -10985.146350655195
2
Iteration 540: Loss = -10985.146321536913
Iteration 550: Loss = -10985.146301424606
Iteration 560: Loss = -10985.146267877877
Iteration 570: Loss = -10985.146270546755
1
Iteration 580: Loss = -10985.146268589673
2
Iteration 590: Loss = -10985.146204897233
Iteration 600: Loss = -10985.146190512403
Iteration 610: Loss = -10985.146231636012
1
Iteration 620: Loss = -10985.146232092751
2
Iteration 630: Loss = -10985.146169506148
Iteration 640: Loss = -10985.146215564344
1
Iteration 650: Loss = -10985.146203302722
2
Iteration 660: Loss = -10985.146192211676
3
Stopping early at iteration 660 due to no improvement.
pi: tensor([[0.7372, 0.2628],
        [0.2449, 0.7551]], dtype=torch.float64)
alpha: tensor([0.4823, 0.5177])
beta: tensor([[[0.1609, 0.1608],
         [0.0462, 0.1609]],

        [[0.7624, 0.1652],
         [0.6804, 0.6512]],

        [[0.6906, 0.1534],
         [0.3099, 0.9395]],

        [[0.4024, 0.1642],
         [0.3122, 0.2551]],

        [[0.8723, 0.1610],
         [0.3237, 0.0991]]], dtype=torch.float64)
time is 0
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 46
Adjusted Rand Index: 0.0
time is 1
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 49
Adjusted Rand Index: 0.0
time is 2
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 45
Adjusted Rand Index: 0.0
time is 3
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 44
Adjusted Rand Index: 0.0
time is 4
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 49
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
Iteration 0: Loss = -11216.243137440353
Iteration 10: Loss = -10985.449747994631
Iteration 20: Loss = -10985.40017654907
Iteration 30: Loss = -10985.360471197899
Iteration 40: Loss = -10985.32540546817
Iteration 50: Loss = -10985.294149145888
Iteration 60: Loss = -10985.266438720582
Iteration 70: Loss = -10985.242002665917
Iteration 80: Loss = -10985.220591738927
Iteration 90: Loss = -10985.201967395931
Iteration 100: Loss = -10985.185830660777
Iteration 110: Loss = -10985.17189108334
Iteration 120: Loss = -10985.16005340603
Iteration 130: Loss = -10985.149961399604
Iteration 140: Loss = -10985.141363315013
Iteration 150: Loss = -10985.134304238103
Iteration 160: Loss = -10985.128372655836
Iteration 170: Loss = -10985.123563860789
Iteration 180: Loss = -10985.119641768953
Iteration 190: Loss = -10985.116506812199
Iteration 200: Loss = -10985.11403167248
Iteration 210: Loss = -10985.111986988306
Iteration 220: Loss = -10985.110351158764
Iteration 230: Loss = -10985.10901290833
Iteration 240: Loss = -10985.107860339167
Iteration 250: Loss = -10985.106891981579
Iteration 260: Loss = -10985.106047279229
Iteration 270: Loss = -10985.105270013783
Iteration 280: Loss = -10985.104551673163
Iteration 290: Loss = -10985.103887135445
Iteration 300: Loss = -10985.10323761812
Iteration 310: Loss = -10985.102541067765
Iteration 320: Loss = -10985.101948760073
Iteration 330: Loss = -10985.101286794641
Iteration 340: Loss = -10985.100653870046
Iteration 350: Loss = -10985.10000741845
Iteration 360: Loss = -10985.099345739725
Iteration 370: Loss = -10985.098704501834
Iteration 380: Loss = -10985.098035629408
Iteration 390: Loss = -10985.09729975883
Iteration 400: Loss = -10985.096625446467
Iteration 410: Loss = -10985.095879647626
Iteration 420: Loss = -10985.09514124864
Iteration 430: Loss = -10985.094392659454
Iteration 440: Loss = -10985.093617323913
Iteration 450: Loss = -10985.092803138856
Iteration 460: Loss = -10985.091978542217
Iteration 470: Loss = -10985.091138636022
Iteration 480: Loss = -10985.090281149993
Iteration 490: Loss = -10985.089384906683
Iteration 500: Loss = -10985.088457172742
Iteration 510: Loss = -10985.087549530417
Iteration 520: Loss = -10985.08657563194
Iteration 530: Loss = -10985.085588214679
Iteration 540: Loss = -10985.084634559518
Iteration 550: Loss = -10985.083593346775
Iteration 560: Loss = -10985.082538191784
Iteration 570: Loss = -10985.081529778787
Iteration 580: Loss = -10985.080466388146
Iteration 590: Loss = -10985.079381395912
Iteration 600: Loss = -10985.078291927193
Iteration 610: Loss = -10985.077218129727
Iteration 620: Loss = -10985.07614406596
Iteration 630: Loss = -10985.075084820688
Iteration 640: Loss = -10985.074066351908
Iteration 650: Loss = -10985.073043040686
Iteration 660: Loss = -10985.072045271305
Iteration 670: Loss = -10985.071113424709
Iteration 680: Loss = -10985.070205329836
Iteration 690: Loss = -10985.069323262933
Iteration 700: Loss = -10985.06852368512
Iteration 710: Loss = -10985.067744127027
Iteration 720: Loss = -10985.06703565059
Iteration 730: Loss = -10985.06635282888
Iteration 740: Loss = -10985.06576667845
Iteration 750: Loss = -10985.065200264316
Iteration 760: Loss = -10985.064727010853
Iteration 770: Loss = -10985.064248198401
Iteration 780: Loss = -10985.06384282374
Iteration 790: Loss = -10985.063487212867
Iteration 800: Loss = -10985.063192562946
Iteration 810: Loss = -10985.062874656918
Iteration 820: Loss = -10985.062653937897
Iteration 830: Loss = -10985.06241565976
Iteration 840: Loss = -10985.062254935154
Iteration 850: Loss = -10985.062061238958
Iteration 860: Loss = -10985.061925699409
Iteration 870: Loss = -10985.061803497727
Iteration 880: Loss = -10985.061680164354
Iteration 890: Loss = -10985.061607188834
Iteration 900: Loss = -10985.061525503792
Iteration 910: Loss = -10985.061424717996
Iteration 920: Loss = -10985.06137161989
Iteration 930: Loss = -10985.061299192706
Iteration 940: Loss = -10985.061234029952
Iteration 950: Loss = -10985.061209261812
Iteration 960: Loss = -10985.061157318367
Iteration 970: Loss = -10985.061146635631
Iteration 980: Loss = -10985.06112615032
Iteration 990: Loss = -10985.061098930992
Iteration 1000: Loss = -10985.061046108889
Iteration 1010: Loss = -10985.061027139203
Iteration 1020: Loss = -10985.061011117934
Iteration 1030: Loss = -10985.06098917345
Iteration 1040: Loss = -10985.06097210217
Iteration 1050: Loss = -10985.061013750226
1
Iteration 1060: Loss = -10985.060980935097
2
Iteration 1070: Loss = -10985.060954783332
Iteration 1080: Loss = -10985.060917649336
Iteration 1090: Loss = -10985.060925073003
1
Iteration 1100: Loss = -10985.060941203348
2
Iteration 1110: Loss = -10985.060899539734
Iteration 1120: Loss = -10985.060882966947
Iteration 1130: Loss = -10985.060921372335
1
Iteration 1140: Loss = -10985.060904374957
2
Iteration 1150: Loss = -10985.060917622686
3
Stopping early at iteration 1150 due to no improvement.
pi: tensor([[0.9509, 0.0491],
        [0.9514, 0.0486]], dtype=torch.float64)
alpha: tensor([0.9509, 0.0491])
beta: tensor([[[0.1606, 0.1634],
         [0.9084, 0.1695]],

        [[0.9370, 0.1923],
         [0.3918, 0.6752]],

        [[0.0936, 0.1226],
         [0.3603, 0.6453]],

        [[0.0297, 0.1754],
         [0.9506, 0.2341]],

        [[0.3041, 0.1635],
         [0.8659, 0.1039]]], dtype=torch.float64)
time is 0
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 54
Adjusted Rand Index: 0.0
time is 1
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 51
Adjusted Rand Index: 0.0
time is 2
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 55
Adjusted Rand Index: 0.0
time is 3
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 56
Adjusted Rand Index: 0.0
time is 4
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 51
Adjusted Rand Index: 0.0
Global Adjusted Rand Index: 0.0
Average Adjusted Rand Index: 0.0
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -20826.337951240548
Iteration 100: Loss = -10986.272185636599
Iteration 200: Loss = -10985.908770787353
Iteration 300: Loss = -10985.799957827601
Iteration 400: Loss = -10985.746501936384
Iteration 500: Loss = -10985.713086936974
Iteration 600: Loss = -10985.686654872163
Iteration 700: Loss = -10985.661152139257
Iteration 800: Loss = -10985.63301842805
Iteration 900: Loss = -10985.598953637369
Iteration 1000: Loss = -10985.554687762278
Iteration 1100: Loss = -10985.492236095255
Iteration 1200: Loss = -10985.39537352087
Iteration 1300: Loss = -10985.225169660025
Iteration 1400: Loss = -10984.96784692919
Iteration 1500: Loss = -10984.694434050718
Iteration 1600: Loss = -10984.4309422226
Iteration 1700: Loss = -10984.178088277287
Iteration 1800: Loss = -10983.879889553022
Iteration 1900: Loss = -10983.468431503745
Iteration 2000: Loss = -10983.010650631844
Iteration 2100: Loss = -10982.68410473397
Iteration 2200: Loss = -10982.516202671835
Iteration 2300: Loss = -10982.419926944258
Iteration 2400: Loss = -10982.359127210095
Iteration 2500: Loss = -10982.318541913839
Iteration 2600: Loss = -10982.290181650307
Iteration 2700: Loss = -10982.269347538366
Iteration 2800: Loss = -10982.25298072003
Iteration 2900: Loss = -10982.238282860388
Iteration 3000: Loss = -10982.22233385844
Iteration 3100: Loss = -10982.212129491954
Iteration 3200: Loss = -10982.204990989076
Iteration 3300: Loss = -10982.199057205868
Iteration 3400: Loss = -10982.19390713951
Iteration 3500: Loss = -10982.189360226743
Iteration 3600: Loss = -10982.185354124687
Iteration 3700: Loss = -10982.181871438872
Iteration 3800: Loss = -10982.178893117392
Iteration 3900: Loss = -10982.176285323201
Iteration 4000: Loss = -10982.173993589866
Iteration 4100: Loss = -10982.171934485894
Iteration 4200: Loss = -10982.170027477605
Iteration 4300: Loss = -10982.168357051994
Iteration 4400: Loss = -10982.16684389144
Iteration 4500: Loss = -10982.165422612856
Iteration 4600: Loss = -10982.164156184035
Iteration 4700: Loss = -10982.163006527662
Iteration 4800: Loss = -10982.161885273046
Iteration 4900: Loss = -10982.16093960617
Iteration 5000: Loss = -10982.160024198893
Iteration 5100: Loss = -10982.159210319098
Iteration 5200: Loss = -10982.158421891821
Iteration 5300: Loss = -10982.157678294874
Iteration 5400: Loss = -10982.157034251159
Iteration 5500: Loss = -10982.15640397983
Iteration 5600: Loss = -10982.155789576434
Iteration 5700: Loss = -10982.155263957726
Iteration 5800: Loss = -10982.154757574574
Iteration 5900: Loss = -10982.15427347486
Iteration 6000: Loss = -10982.153811156159
Iteration 6100: Loss = -10982.153393996348
Iteration 6200: Loss = -10982.152985162737
Iteration 6300: Loss = -10982.152544830344
Iteration 6400: Loss = -10982.152139080941
Iteration 6500: Loss = -10982.15180300993
Iteration 6600: Loss = -10982.151478258827
Iteration 6700: Loss = -10982.15118646551
Iteration 6800: Loss = -10982.150865836853
Iteration 6900: Loss = -10982.150593754117
Iteration 7000: Loss = -10982.15029398405
Iteration 7100: Loss = -10982.1499453403
Iteration 7200: Loss = -10982.14962132729
Iteration 7300: Loss = -10982.149189896474
Iteration 7400: Loss = -10982.148214322613
Iteration 7500: Loss = -10982.202826895493
1
Iteration 7600: Loss = -10982.146961218554
Iteration 7700: Loss = -10982.146795388267
Iteration 7800: Loss = -10982.146601820676
Iteration 7900: Loss = -10982.146568219921
Iteration 8000: Loss = -10982.146284184786
Iteration 8100: Loss = -10982.146178737079
Iteration 8200: Loss = -10982.1483614983
1
Iteration 8300: Loss = -10982.145938985774
Iteration 8400: Loss = -10982.145801757719
Iteration 8500: Loss = -10982.145682990029
Iteration 8600: Loss = -10982.145731856901
1
Iteration 8700: Loss = -10982.145442078661
Iteration 8800: Loss = -10982.145309375685
Iteration 8900: Loss = -10982.145185471789
Iteration 9000: Loss = -10982.14459559316
Iteration 9100: Loss = -10982.142560365652
Iteration 9200: Loss = -10982.312236156697
1
Iteration 9300: Loss = -10982.142111094685
Iteration 9400: Loss = -10982.138640358959
Iteration 9500: Loss = -10982.138517502177
Iteration 9600: Loss = -10982.14802093724
1
Iteration 9700: Loss = -10982.138348153958
Iteration 9800: Loss = -10982.138325406733
Iteration 9900: Loss = -10982.138276368078
Iteration 10000: Loss = -10982.138181059616
Iteration 10100: Loss = -10982.138140297033
Iteration 10200: Loss = -10982.138117824994
Iteration 10300: Loss = -10982.223946131257
1
Iteration 10400: Loss = -10982.138018453523
Iteration 10500: Loss = -10982.13793755731
Iteration 10600: Loss = -10982.138395423328
1
Iteration 10700: Loss = -10982.137498254151
Iteration 10800: Loss = -10982.137287623284
Iteration 10900: Loss = -10982.1371557288
Iteration 11000: Loss = -10982.137108665613
Iteration 11100: Loss = -10982.13704955426
Iteration 11200: Loss = -10982.136935479708
Iteration 11300: Loss = -10982.177079354238
1
Iteration 11400: Loss = -10982.136794290187
Iteration 11500: Loss = -10982.135785153887
Iteration 11600: Loss = -10982.135755096588
Iteration 11700: Loss = -10982.135825122223
1
Iteration 11800: Loss = -10982.135613838569
Iteration 11900: Loss = -10982.135511911429
Iteration 12000: Loss = -10982.135503518306
Iteration 12100: Loss = -10982.135436106653
Iteration 12200: Loss = -10982.135461517726
1
Iteration 12300: Loss = -10982.556694954223
2
Iteration 12400: Loss = -10982.135416791807
Iteration 12500: Loss = -10982.135388406734
Iteration 12600: Loss = -10982.135349180478
Iteration 12700: Loss = -10982.135695339468
1
Iteration 12800: Loss = -10982.135357892663
2
Iteration 12900: Loss = -10982.135337557376
Iteration 13000: Loss = -10982.135369244626
1
Iteration 13100: Loss = -10982.13530138104
Iteration 13200: Loss = -10982.135194883444
Iteration 13300: Loss = -10982.1355635624
1
Iteration 13400: Loss = -10982.135158157256
Iteration 13500: Loss = -10982.13514481466
Iteration 13600: Loss = -10982.138787979326
1
Iteration 13700: Loss = -10982.135123727701
Iteration 13800: Loss = -10982.135249117071
1
Iteration 13900: Loss = -10982.135102178876
Iteration 14000: Loss = -10982.135676204065
1
Iteration 14100: Loss = -10982.135068228128
Iteration 14200: Loss = -10982.154962932947
1
Iteration 14300: Loss = -10982.1350365728
Iteration 14400: Loss = -10982.13919038866
1
Iteration 14500: Loss = -10982.135177011096
2
Iteration 14600: Loss = -10982.135006831857
Iteration 14700: Loss = -10982.135132169344
1
Iteration 14800: Loss = -10982.135021534043
2
Iteration 14900: Loss = -10982.135432735107
3
Iteration 15000: Loss = -10982.135024131134
4
Iteration 15100: Loss = -10982.146789450759
5
Stopping early at iteration 15100 due to no improvement.
pi: tensor([[1.0000e+00, 1.7033e-07],
        [3.2750e-06, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9739, 0.0261], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1614, 0.2397],
         [0.7297, 0.3358]],

        [[0.6594, 0.2354],
         [0.5831, 0.6351]],

        [[0.6099, 0.1421],
         [0.7122, 0.6500]],

        [[0.6257, 0.1949],
         [0.6348, 0.5210]],

        [[0.6569, 0.1271],
         [0.5353, 0.6519]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0031985518644306096
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.004878730976372607
time is 2
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 3
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.010427427162149738
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
Global Adjusted Rand Index: -0.001187080066103339
Average Adjusted Rand Index: -0.0011830252214111113
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21989.278849021463
Iteration 100: Loss = -10986.672308434281
Iteration 200: Loss = -10986.143899101111
Iteration 300: Loss = -10985.94847925383
Iteration 400: Loss = -10985.84853836132
Iteration 500: Loss = -10985.792571232665
Iteration 600: Loss = -10985.75862745579
Iteration 700: Loss = -10985.736270991723
Iteration 800: Loss = -10985.720317500118
Iteration 900: Loss = -10985.70777406774
Iteration 1000: Loss = -10985.696659391146
Iteration 1100: Loss = -10985.684161544677
Iteration 1200: Loss = -10985.659506121465
Iteration 1300: Loss = -10985.379039058189
Iteration 1400: Loss = -10983.739716769607
Iteration 1500: Loss = -10983.14432706967
Iteration 1600: Loss = -10982.668947459308
Iteration 1700: Loss = -10982.495376681067
Iteration 1800: Loss = -10982.428485325085
Iteration 1900: Loss = -10982.387824722322
Iteration 2000: Loss = -10982.360814971582
Iteration 2100: Loss = -10982.341613633516
Iteration 2200: Loss = -10982.326693409732
Iteration 2300: Loss = -10982.314385228541
Iteration 2400: Loss = -10982.304643056217
Iteration 2500: Loss = -10982.296441752542
Iteration 2600: Loss = -10982.289443508585
Iteration 2700: Loss = -10982.284039607941
Iteration 2800: Loss = -10982.279583021562
Iteration 2900: Loss = -10982.275583825642
Iteration 3000: Loss = -10982.256916580016
Iteration 3100: Loss = -10982.24151695443
Iteration 3200: Loss = -10982.238146583537
Iteration 3300: Loss = -10982.235456637414
Iteration 3400: Loss = -10982.23288910789
Iteration 3500: Loss = -10982.230629274662
Iteration 3600: Loss = -10982.228653985592
Iteration 3700: Loss = -10982.226756579996
Iteration 3800: Loss = -10982.224400665713
Iteration 3900: Loss = -10982.222605594863
Iteration 4000: Loss = -10982.221264560572
Iteration 4100: Loss = -10982.219128032923
Iteration 4200: Loss = -10982.216098780309
Iteration 4300: Loss = -10982.215046311117
Iteration 4400: Loss = -10982.214282171719
Iteration 4500: Loss = -10982.213594184137
Iteration 4600: Loss = -10982.212973765469
Iteration 4700: Loss = -10982.212415223701
Iteration 4800: Loss = -10982.211908587215
Iteration 4900: Loss = -10982.21139470721
Iteration 5000: Loss = -10982.210901548406
Iteration 5100: Loss = -10982.208513382624
Iteration 5200: Loss = -10982.206535795187
Iteration 5300: Loss = -10982.205994730668
Iteration 5400: Loss = -10982.205549730712
Iteration 5500: Loss = -10982.205039661003
Iteration 5600: Loss = -10982.204598574415
Iteration 5700: Loss = -10982.204237311173
Iteration 5800: Loss = -10982.203942056305
Iteration 5900: Loss = -10982.203630860797
Iteration 6000: Loss = -10982.20263782652
Iteration 6100: Loss = -10982.20192412348
Iteration 6200: Loss = -10982.201565883006
Iteration 6300: Loss = -10982.200317854782
Iteration 6400: Loss = -10982.197940999875
Iteration 6500: Loss = -10982.197730865182
Iteration 6600: Loss = -10982.197553818394
Iteration 6700: Loss = -10982.197372045106
Iteration 6800: Loss = -10982.197231422859
Iteration 6900: Loss = -10982.1970556665
Iteration 7000: Loss = -10982.196932452709
Iteration 7100: Loss = -10982.196816501722
Iteration 7200: Loss = -10982.196648229725
Iteration 7300: Loss = -10982.191124777562
Iteration 7400: Loss = -10982.189319651332
Iteration 7500: Loss = -10982.188732072067
Iteration 7600: Loss = -10982.188555719506
Iteration 7700: Loss = -10982.188505659773
Iteration 7800: Loss = -10982.18840525501
Iteration 7900: Loss = -10982.188280658815
Iteration 8000: Loss = -10982.188193428456
Iteration 8100: Loss = -10982.188117821976
Iteration 8200: Loss = -10982.18806912828
Iteration 8300: Loss = -10982.188013875206
Iteration 8400: Loss = -10982.214252303213
1
Iteration 8500: Loss = -10982.211394709519
2
Iteration 8600: Loss = -10982.1878638758
Iteration 8700: Loss = -10982.187782969946
Iteration 8800: Loss = -10982.187904633716
1
Iteration 8900: Loss = -10982.187675709854
Iteration 9000: Loss = -10982.211345261876
1
Iteration 9100: Loss = -10982.186203104911
Iteration 9200: Loss = -10982.186056644448
Iteration 9300: Loss = -10982.18660584583
1
Iteration 9400: Loss = -10982.185969415215
Iteration 9500: Loss = -10982.185945712969
Iteration 9600: Loss = -10982.186398796584
1
Iteration 9700: Loss = -10982.185859691155
Iteration 9800: Loss = -10982.185840009379
Iteration 9900: Loss = -10982.18342757895
Iteration 10000: Loss = -10982.183079058344
Iteration 10100: Loss = -10982.183046532013
Iteration 10200: Loss = -10982.183463990765
1
Iteration 10300: Loss = -10982.1830218721
Iteration 10400: Loss = -10982.18271168824
Iteration 10500: Loss = -10982.182683119761
Iteration 10600: Loss = -10982.182656228457
Iteration 10700: Loss = -10982.182606816876
Iteration 10800: Loss = -10982.18256338361
Iteration 10900: Loss = -10982.209323669187
1
Iteration 11000: Loss = -10982.182509444056
Iteration 11100: Loss = -10982.182482175438
Iteration 11200: Loss = -10982.325646720265
1
Iteration 11300: Loss = -10982.182490394704
2
Iteration 11400: Loss = -10982.178084259493
Iteration 11500: Loss = -10982.177912482346
Iteration 11600: Loss = -10982.177933996003
1
Iteration 11700: Loss = -10982.176688474325
Iteration 11800: Loss = -10982.176664345528
Iteration 11900: Loss = -10982.193356466392
1
Iteration 12000: Loss = -10982.176714740379
2
Iteration 12100: Loss = -10982.17665907637
Iteration 12200: Loss = -10982.24301014808
1
Iteration 12300: Loss = -10982.1766621588
2
Iteration 12400: Loss = -10982.176654390085
Iteration 12500: Loss = -10982.302892872269
1
Iteration 12600: Loss = -10982.176667983413
2
Iteration 12700: Loss = -10982.176657797087
3
Iteration 12800: Loss = -10982.176707988961
4
Iteration 12900: Loss = -10982.176894247192
5
Stopping early at iteration 12900 due to no improvement.
pi: tensor([[1.0000e+00, 2.8031e-06],
        [3.4627e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0251, 0.9749], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3476, 0.2410],
         [0.7060, 0.1611]],

        [[0.5018, 0.2368],
         [0.7297, 0.7244]],

        [[0.5361, 0.1427],
         [0.7289, 0.6423]],

        [[0.7049, 0.1963],
         [0.6787, 0.6018]],

        [[0.6799, 0.1276],
         [0.6163, 0.5473]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0031985518644306096
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 46
Adjusted Rand Index: 0.004878730976372607
time is 2
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 46
Adjusted Rand Index: -0.0036363636363636364
time is 3
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 47
Adjusted Rand Index: -0.010427427162149738
time is 4
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 52
Adjusted Rand Index: 7.13818506546007e-05
Global Adjusted Rand Index: -0.001187080066103339
Average Adjusted Rand Index: -0.0011830252214111113
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21639.995347901233
Iteration 100: Loss = -10986.745576784802
Iteration 200: Loss = -10986.168241363015
Iteration 300: Loss = -10985.954621267547
Iteration 400: Loss = -10985.858074077614
Iteration 500: Loss = -10985.806975186435
Iteration 600: Loss = -10985.776459344857
Iteration 700: Loss = -10985.756035647193
Iteration 800: Loss = -10985.74079975667
Iteration 900: Loss = -10985.728206169983
Iteration 1000: Loss = -10985.716908717597
Iteration 1100: Loss = -10985.705804102989
Iteration 1200: Loss = -10985.69422157579
Iteration 1300: Loss = -10985.681532001972
Iteration 1400: Loss = -10985.666919623678
Iteration 1500: Loss = -10985.64942570147
Iteration 1600: Loss = -10985.626400716823
Iteration 1700: Loss = -10985.587872674892
Iteration 1800: Loss = -10985.446214254329
Iteration 1900: Loss = -10984.791772908597
Iteration 2000: Loss = -10984.049061224157
Iteration 2100: Loss = -10983.37352397907
Iteration 2200: Loss = -10982.881147193744
Iteration 2300: Loss = -10982.643106517886
Iteration 2400: Loss = -10982.517969090806
Iteration 2500: Loss = -10982.441951391462
Iteration 2600: Loss = -10982.391665016943
Iteration 2700: Loss = -10982.35627961269
Iteration 2800: Loss = -10982.330081899405
Iteration 2900: Loss = -10982.310012162403
Iteration 3000: Loss = -10982.293975786062
Iteration 3100: Loss = -10982.280768406834
Iteration 3200: Loss = -10982.269622261003
Iteration 3300: Loss = -10982.26002358884
Iteration 3400: Loss = -10982.25177674288
Iteration 3500: Loss = -10982.244772992964
Iteration 3600: Loss = -10982.238768320574
Iteration 3700: Loss = -10982.233610981291
Iteration 3800: Loss = -10982.229005887357
Iteration 3900: Loss = -10982.224924919814
Iteration 4000: Loss = -10982.22126169452
Iteration 4100: Loss = -10982.217960039363
Iteration 4200: Loss = -10982.214883825089
Iteration 4300: Loss = -10982.212053304163
Iteration 4400: Loss = -10982.209466619526
Iteration 4500: Loss = -10982.207130691895
Iteration 4600: Loss = -10982.204957066557
Iteration 4700: Loss = -10982.202819132277
Iteration 4800: Loss = -10982.200517493922
Iteration 4900: Loss = -10982.198263459726
Iteration 5000: Loss = -10982.196585765018
Iteration 5100: Loss = -10982.19519355183
Iteration 5200: Loss = -10982.19385497986
Iteration 5300: Loss = -10982.192425853935
Iteration 5400: Loss = -10982.190915983243
Iteration 5500: Loss = -10982.18972105913
Iteration 5600: Loss = -10982.188699705715
Iteration 5700: Loss = -10982.18772925993
Iteration 5800: Loss = -10982.186868818566
Iteration 5900: Loss = -10982.185937464812
Iteration 6000: Loss = -10982.185065280526
Iteration 6100: Loss = -10982.184288581935
Iteration 6200: Loss = -10982.183587774236
Iteration 6300: Loss = -10982.182886722583
Iteration 6400: Loss = -10982.182146277804
Iteration 6500: Loss = -10982.1806148944
Iteration 6600: Loss = -10982.168663316888
Iteration 6700: Loss = -10982.167411788298
Iteration 6800: Loss = -10982.166941252577
Iteration 6900: Loss = -10982.166532649675
Iteration 7000: Loss = -10982.166134640947
Iteration 7100: Loss = -10982.1657726337
Iteration 7200: Loss = -10982.165430372186
Iteration 7300: Loss = -10982.164993393766
Iteration 7400: Loss = -10982.164459536038
Iteration 7500: Loss = -10982.218409571358
1
Iteration 7600: Loss = -10982.162918990984
Iteration 7700: Loss = -10982.162533551536
Iteration 7800: Loss = -10982.22798041372
1
Iteration 7900: Loss = -10982.16197500498
Iteration 8000: Loss = -10982.161686160358
Iteration 8100: Loss = -10982.404630360765
1
Iteration 8200: Loss = -10982.161035796693
Iteration 8300: Loss = -10982.160337915884
Iteration 8400: Loss = -10982.160072278675
Iteration 8500: Loss = -10982.159663930026
Iteration 8600: Loss = -10982.15945576004
Iteration 8700: Loss = -10982.159283610481
Iteration 8800: Loss = -10982.17369218499
1
Iteration 8900: Loss = -10982.159008252203
Iteration 9000: Loss = -10982.158822978976
Iteration 9100: Loss = -10982.715529271269
1
Iteration 9200: Loss = -10982.158587681915
Iteration 9300: Loss = -10982.15842876145
Iteration 9400: Loss = -10982.158314190698
Iteration 9500: Loss = -10982.164671437964
1
Iteration 9600: Loss = -10982.158021007135
Iteration 9700: Loss = -10982.157811729552
Iteration 9800: Loss = -10982.157289157767
Iteration 9900: Loss = -10982.388447071628
1
Iteration 10000: Loss = -10982.156707940587
Iteration 10100: Loss = -10982.15655752649
Iteration 10200: Loss = -10982.156489575149
Iteration 10300: Loss = -10982.154600387435
Iteration 10400: Loss = -10982.150477251158
Iteration 10500: Loss = -10982.150416707173
Iteration 10600: Loss = -10982.150653205064
1
Iteration 10700: Loss = -10982.150306519587
Iteration 10800: Loss = -10982.150254637889
Iteration 10900: Loss = -10982.246438916645
1
Iteration 11000: Loss = -10982.15016083937
Iteration 11100: Loss = -10982.150121988203
Iteration 11200: Loss = -10982.150129033509
1
Iteration 11300: Loss = -10982.155830221047
2
Iteration 11400: Loss = -10982.150051310073
Iteration 11500: Loss = -10982.150041987981
Iteration 11600: Loss = -10982.293177492986
1
Iteration 11700: Loss = -10982.14995172803
Iteration 11800: Loss = -10982.149933675162
Iteration 11900: Loss = -10982.149982284109
1
Iteration 12000: Loss = -10982.149926329665
Iteration 12100: Loss = -10982.149831132589
Iteration 12200: Loss = -10982.14976044851
Iteration 12300: Loss = -10982.150896010637
1
Iteration 12400: Loss = -10982.149026998257
Iteration 12500: Loss = -10982.148807813028
Iteration 12600: Loss = -10982.148956052191
1
Iteration 12700: Loss = -10982.14877900303
Iteration 12800: Loss = -10982.148751781184
Iteration 12900: Loss = -10982.232864146203
1
Iteration 13000: Loss = -10982.14870925213
Iteration 13100: Loss = -10982.148691905746
Iteration 13200: Loss = -10982.148669224422
Iteration 13300: Loss = -10982.148474816098
Iteration 13400: Loss = -10982.148356510108
Iteration 13500: Loss = -10982.148284077108
Iteration 13600: Loss = -10982.149910663473
1
Iteration 13700: Loss = -10982.148303149448
2
Iteration 13800: Loss = -10982.14830740048
3
Iteration 13900: Loss = -10982.15338868666
4
Iteration 14000: Loss = -10982.14824890356
Iteration 14100: Loss = -10982.15266006278
1
Iteration 14200: Loss = -10982.14831426773
2
Iteration 14300: Loss = -10982.170313765899
3
Iteration 14400: Loss = -10982.148242625753
Iteration 14500: Loss = -10982.519558896582
1
Iteration 14600: Loss = -10982.148254793869
2
Iteration 14700: Loss = -10982.148546355647
3
Iteration 14800: Loss = -10982.190398485252
4
Iteration 14900: Loss = -10982.14940898631
5
Stopping early at iteration 14900 due to no improvement.
pi: tensor([[1.0000e+00, 2.0063e-07],
        [4.6767e-06, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.9742, 0.0258], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.1612, 0.2402],
         [0.6863, 0.3390]],

        [[0.5478, 0.2361],
         [0.7040, 0.5775]],

        [[0.6052, 0.1423],
         [0.5998, 0.5285]],

        [[0.6243, 0.1951],
         [0.5660, 0.5170]],

        [[0.5538, 0.1270],
         [0.6647, 0.6654]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 55
Adjusted Rand Index: 0.0031985518644306096
time is 1
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
Difference count: 54
Adjusted Rand Index: 0.004878730976372607
time is 2
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0])
Difference count: 54
Adjusted Rand Index: -0.0036363636363636364
time is 3
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 1, 0, 0])
Difference count: 53
Adjusted Rand Index: -0.010427427162149738
time is 4
tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0])
tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,
        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0])
Difference count: 48
Adjusted Rand Index: 7.13818506546007e-05
Global Adjusted Rand Index: -0.001187080066103339
Average Adjusted Rand Index: -0.0011830252214111113
time_stamp: 5
num_nodes: 100
num_latent: 2
Iteration 0: Loss = -21582.310370300736
Iteration 100: Loss = -10986.184730657327
Iteration 200: Loss = -10985.906999570869
Iteration 300: Loss = -10985.82352124344
Iteration 400: Loss = -10985.781529032169
Iteration 500: Loss = -10985.75648381374
Iteration 600: Loss = -10985.739533324704
Iteration 700: Loss = -10985.726951690669
Iteration 800: Loss = -10985.716454647778
Iteration 900: Loss = -10985.706644505699
Iteration 1000: Loss = -10985.69591211698
Iteration 1100: Loss = -10985.68095487542
Iteration 1200: Loss = -10985.64133623593
Iteration 1300: Loss = -10984.998446285246
Iteration 1400: Loss = -10982.762244592848
Iteration 1500: Loss = -10982.456288779178
Iteration 1600: Loss = -10982.350558942326
Iteration 1700: Loss = -10982.290102127552
Iteration 1800: Loss = -10982.245834179943
Iteration 1900: Loss = -10982.226020355658
Iteration 2000: Loss = -10982.212854456353
Iteration 2100: Loss = -10982.203597428128
Iteration 2200: Loss = -10982.19683361383
Iteration 2300: Loss = -10982.191605702068
Iteration 2400: Loss = -10982.187410141192
Iteration 2500: Loss = -10982.183981998816
Iteration 2600: Loss = -10982.181039527568
Iteration 2700: Loss = -10982.178576348042
Iteration 2800: Loss = -10982.176405172462
Iteration 2900: Loss = -10982.174420156565
Iteration 3000: Loss = -10982.172712507565
Iteration 3100: Loss = -10982.171276741878
Iteration 3200: Loss = -10982.169979658673
Iteration 3300: Loss = -10982.168640772998
Iteration 3400: Loss = -10982.16686564577
Iteration 3500: Loss = -10982.165251311033
Iteration 3600: Loss = -10982.164395725198
Iteration 3700: Loss = -10982.163631273508
Iteration 3800: Loss = -10982.162866886385
Iteration 3900: Loss = -10982.162140716015
Iteration 4000: Loss = -10982.16139233839
Iteration 4100: Loss = -10982.160763493208
Iteration 4200: Loss = -10982.160239322187
Iteration 4300: Loss = -10982.15984621755
Iteration 4400: Loss = -10982.159356077334
Iteration 4500: Loss = -10982.15889632333
Iteration 4600: Loss = -10982.158362734745
Iteration 4700: Loss = -10982.157820903927
Iteration 4800: Loss = -10982.157259067311
Iteration 4900: Loss = -10982.15640580145
Iteration 5000: Loss = -10982.155433520249
Iteration 5100: Loss = -10982.154882305458
Iteration 5200: Loss = -10982.154550337142
Iteration 5300: Loss = -10982.154297799025
Iteration 5400: Loss = -10982.15406128733
Iteration 5500: Loss = -10982.153750970596
Iteration 5600: Loss = -10982.14976238073
Iteration 5700: Loss = -10982.147650776142
Iteration 5800: Loss = -10982.147487656226
Iteration 5900: Loss = -10982.147367583157
Iteration 6000: Loss = -10982.147255900549
Iteration 6100: Loss = -10982.147122794617
Iteration 6200: Loss = -10982.147008470061
Iteration 6300: Loss = -10982.146953456466
Iteration 6400: Loss = -10982.14682876148
Iteration 6500: Loss = -10982.146684064559
Iteration 6600: Loss = -10982.146616081242
Iteration 6700: Loss = -10982.146526000339
Iteration 6800: Loss = -10982.146399603524
Iteration 6900: Loss = -10982.145973188837
Iteration 7000: Loss = -10982.144993034422
Iteration 7100: Loss = -10982.144884971043
Iteration 7200: Loss = -10982.144781013125
Iteration 7300: Loss = -10982.144695833651
Iteration 7400: Loss = -10982.144676464532
Iteration 7500: Loss = -10982.144620994528
Iteration 7600: Loss = -10982.144573883685
Iteration 7700: Loss = -10982.14450154866
Iteration 7800: Loss = -10982.14450163888
1
Iteration 7900: Loss = -10982.144437608134
Iteration 8000: Loss = -10982.144425391964
Iteration 8100: Loss = -10982.14437197389
Iteration 8200: Loss = -10982.147738854927
1
Iteration 8300: Loss = -10982.145412634705
2
Iteration 8400: Loss = -10982.144338242766
Iteration 8500: Loss = -10982.144990301964
1
Iteration 8600: Loss = -10982.146624264777
2
Iteration 8700: Loss = -10982.144096774702
Iteration 8800: Loss = -10982.1462624584
1
Iteration 8900: Loss = -10982.143687105907
Iteration 9000: Loss = -10982.147248710839
1
Iteration 9100: Loss = -10982.14299666416
Iteration 9200: Loss = -10982.195981961473
1
Iteration 9300: Loss = -10982.14295748566
Iteration 9400: Loss = -10982.142918437221
Iteration 9500: Loss = -10982.16128547916
1
Iteration 9600: Loss = -10982.142624428505
Iteration 9700: Loss = -10982.142555807333
Iteration 9800: Loss = -10982.14265592892
1
Iteration 9900: Loss = -10982.14245767626
Iteration 10000: Loss = -10982.142423484709
Iteration 10100: Loss = -10982.144359112219
1
Iteration 10200: Loss = -10982.142416445366
Iteration 10300: Loss = -10982.142406338335
Iteration 10400: Loss = -10982.14241494981
1
Iteration 10500: Loss = -10982.14240005967
Iteration 10600: Loss = -10982.14239568025
Iteration 10700: Loss = -10982.142399223287
1
Iteration 10800: Loss = -10982.143175109171
2
Iteration 10900: Loss = -10982.142353214398
Iteration 11000: Loss = -10982.142323621445
Iteration 11100: Loss = -10982.14976794464
1
Iteration 11200: Loss = -10982.14233631996
2
Iteration 11300: Loss = -10982.142320461786
Iteration 11400: Loss = -10982.14269207956
1
Iteration 11500: Loss = -10982.142058968257
Iteration 11600: Loss = -10982.138418996065
Iteration 11700: Loss = -10982.138383568179
Iteration 11800: Loss = -10982.147558802382
1
Iteration 11900: Loss = -10982.138396094115
2
Iteration 12000: Loss = -10982.138387580037
3
Iteration 12100: Loss = -10982.138379550302
Iteration 12200: Loss = -10982.1383892696
1
Iteration 12300: Loss = -10982.138351803631
Iteration 12400: Loss = -10982.138366979972
1
Iteration 12500: Loss = -10982.228181814904
2
Iteration 12600: Loss = -10982.138389814292
3
Iteration 12700: Loss = -10982.138365579893
4
Iteration 12800: Loss = -10982.138366778998
5
Stopping early at iteration 12800 due to no improvement.
pi: tensor([[1.0000e+00, 2.0736e-06],
        [1.7822e-07, 1.0000e+00]], dtype=torch.float64,
       grad_fn=<SoftmaxBackward0>)
alpha: tensor([0.0262, 0.9738], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)
beta: tensor([[[0.3367, 0.2396],
         [0.6747, 0.1611]],

        [[0.6718, 0.2355],
         [0.7235, 0.6468]],

        [[0.6736, 0.1425],
         [0.6764, 0.7010]],

        [[0.7259, 0.1946],
         [0.6870, 0.5321]],

        [[0.5216, 0.1268],
         [0.6889, 0.5457]]], dtype=torch.float64, grad_fn=<SigmoidBackward0>)
time is 0
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
        0, 1, 0, 1])
Difference count: 45
Adjusted Rand Index: 0.0031985518644306096
time is 1
tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1])
tensor([1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,
        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 1])
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 22%|██▏       | 22/100 [9:25:59<35:59:36, 1661.23s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 23%|██▎       | 23/100 [10:00:10<38:02:08, 1778.29s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 24%|██▍       | 24/100 [10:17:13<32:45:19, 1551.58s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 25%|██▌       | 25/100 [10:36:55<30:00:54, 1440.72s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 26%|██▌       | 26/100 [11:05:11<31:11:10, 1517.17s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 27%|██▋       | 27/100 [11:36:33<32:59:08, 1626.70s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 28%|██▊       | 28/100 [11:58:22<30:37:51, 1531.54s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
 29%|██▉       | 29/100 [12:24:14<30:19:34, 1537.67s/it]/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/true_model.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  pi, alpha, beta, tau_init, tau_transition = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/{mode}_estimation/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/estimate_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}_{trial}.pt')
/home/users/aiwwdw/git_repository/Statistical-clustering-of-temporal-networks/evaluation.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  init_dist,transition_matrix, Bernoulli_parameter, Z = torch.load(f'parameter/{num_nodes}_{time_stamp}_{str_stability}/true/{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}/true_para_{bernoulli_case}_{num_nodes}_{time_stamp}_{str_stability}_{total_iteration}.pt')
